[INFO][2018-05-24 19:42:13,186][org.apache.spark.SparkContext]Running Spark version 2.2.0
[INFO][2018-05-24 19:42:14,046][org.apache.spark.SparkContext]Submitted application: ReceiveKafkaData$
[INFO][2018-05-24 19:42:14,070][org.apache.spark.SecurityManager]Changing view acls to: seven
[INFO][2018-05-24 19:42:14,071][org.apache.spark.SecurityManager]Changing modify acls to: seven
[INFO][2018-05-24 19:42:14,071][org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO][2018-05-24 19:42:14,072][org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO][2018-05-24 19:42:14,073][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(seven); groups with view permissions: Set(); users  with modify permissions: Set(seven); groups with modify permissions: Set()
[INFO][2018-05-24 19:42:14,387][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 65257.
[INFO][2018-05-24 19:42:14,406][org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO][2018-05-24 19:42:14,424][org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO][2018-05-24 19:42:14,427][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2018-05-24 19:42:14,428][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
[INFO][2018-05-24 19:42:14,438][org.apache.spark.storage.DiskBlockManager]Created local directory at /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/blockmgr-e742b4a0-c5fe-429d-a750-f035b62696ca
[INFO][2018-05-24 19:42:14,454][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 912.3 MB
[INFO][2018-05-24 19:42:14,549][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO][2018-05-24 19:42:14,637][org.spark_project.jetty.util.log]Logging initialized @2455ms
[INFO][2018-05-24 19:42:14,709][org.spark_project.jetty.server.Server]jetty-9.3.z-SNAPSHOT
[INFO][2018-05-24 19:42:14,721][org.spark_project.jetty.server.Server]Started @2541ms
[INFO][2018-05-24 19:42:14,750][org.spark_project.jetty.server.AbstractConnector]Started ServerConnector@68d6972f{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-05-24 19:42:14,750][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4040.
[INFO][2018-05-24 19:42:14,781][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7fcbe147{/jobs,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:42:14,782][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@89c10b7{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:42:14,783][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4fe89c24{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:42:14,785][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@119f1f2a{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:42:14,786][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5b970f7{/stages,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:42:14,787][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@165b8a71{/stages/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:42:14,789][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2f058b8a{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:42:14,792][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@76c7beb3{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:42:14,796][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2cf92cc7{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:42:14,797][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7b139eab{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:42:14,797][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@611df6e3{/storage,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:42:14,798][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6273c5a4{/storage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:42:14,799][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@53e211ee{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:42:14,804][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3d8bbcdc{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:42:14,806][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@117e0fe5{/environment,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:42:14,807][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@78aea4b9{/environment/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:42:14,807][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4b85880b{/executors,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:42:14,808][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4215838f{/executors/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:42:14,809][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2289aca5{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:42:14,811][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@184497d1{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:42:14,822][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6ffab045{/static,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:42:14,823][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5e8f9e2d{/,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:42:14,824][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@fd46303{/api,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:42:14,827][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3e2822{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:42:14,828][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@29a60c27{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:42:14,831][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://192.168.0.102:4040
[INFO][2018-05-24 19:42:14,978][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
[INFO][2018-05-24 19:42:15,003][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 65258.
[INFO][2018-05-24 19:42:15,004][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 192.168.0.102:65258
[INFO][2018-05-24 19:42:15,006][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2018-05-24 19:42:15,008][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 192.168.0.102, 65258, None)
[INFO][2018-05-24 19:42:15,012][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 192.168.0.102:65258 with 912.3 MB RAM, BlockManagerId(driver, 192.168.0.102, 65258, None)
[INFO][2018-05-24 19:42:15,014][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 192.168.0.102, 65258, None)
[INFO][2018-05-24 19:42:15,015][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 192.168.0.102, 65258, None)
[INFO][2018-05-24 19:42:15,219][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6b530eb9{/metrics/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:42:15,432][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 19:42:15,436][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 19:42:15,436][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 19:42:27,554][org.apache.spark.SparkContext]Running Spark version 2.2.0
[INFO][2018-05-24 19:42:28,300][org.apache.spark.SparkContext]Submitted application: SimulationKafkaSendOutData$
[INFO][2018-05-24 19:42:28,323][org.apache.spark.SecurityManager]Changing view acls to: seven
[INFO][2018-05-24 19:42:28,324][org.apache.spark.SecurityManager]Changing modify acls to: seven
[INFO][2018-05-24 19:42:28,324][org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO][2018-05-24 19:42:28,325][org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO][2018-05-24 19:42:28,326][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(seven); groups with view permissions: Set(); users  with modify permissions: Set(seven); groups with modify permissions: Set()
[INFO][2018-05-24 19:42:28,615][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 65265.
[INFO][2018-05-24 19:42:28,651][org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO][2018-05-24 19:42:28,666][org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO][2018-05-24 19:42:28,670][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2018-05-24 19:42:28,670][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
[INFO][2018-05-24 19:42:28,679][org.apache.spark.storage.DiskBlockManager]Created local directory at /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/blockmgr-0093f14b-e6e6-465f-91ec-35db0a493392
[INFO][2018-05-24 19:42:28,701][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 912.3 MB
[INFO][2018-05-24 19:42:28,800][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO][2018-05-24 19:42:28,888][org.spark_project.jetty.util.log]Logging initialized @2329ms
[INFO][2018-05-24 19:42:28,935][org.spark_project.jetty.server.Server]jetty-9.3.z-SNAPSHOT
[INFO][2018-05-24 19:42:28,946][org.spark_project.jetty.server.Server]Started @2389ms
[WARN][2018-05-24 19:42:28,959][org.apache.spark.util.Utils]Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[INFO][2018-05-24 19:42:28,963][org.spark_project.jetty.server.AbstractConnector]Started ServerConnector@1643d68f{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}
[INFO][2018-05-24 19:42:28,963][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4041.
[INFO][2018-05-24 19:42:28,982][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2a39fe6a{/jobs,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:42:28,983][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@649725e3{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:42:28,983][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4c168660{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:42:28,984][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4eed46ee{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:42:28,985][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4fad94a7{/stages,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:42:28,985][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6326d182{/stages/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:42:28,985][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@716a7124{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:42:28,986][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5cc126dc{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:42:28,987][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@72bd06ca{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:42:28,987][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5dbe30be{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:42:28,988][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@55795845{/storage,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:42:28,988][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@119f1f2a{/storage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:42:28,989][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5b970f7{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:42:28,990][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@165b8a71{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:42:28,991][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2f058b8a{/environment,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:42:28,991][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3f2ef586{/environment/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:42:28,992][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@76c7beb3{/executors,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:42:28,993][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2cf92cc7{/executors/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:42:28,994][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7b139eab{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:42:28,994][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@611df6e3{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:42:29,000][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6273c5a4{/static,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:42:29,001][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3e2943ab{/,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:42:29,002][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4a9f80d3{/api,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:42:29,003][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@191ae03f{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:42:29,003][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@53ab0286{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:42:29,006][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://192.168.0.102:4041
[INFO][2018-05-24 19:42:29,090][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
[INFO][2018-05-24 19:42:29,107][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 65266.
[INFO][2018-05-24 19:42:29,108][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 192.168.0.102:65266
[INFO][2018-05-24 19:42:29,109][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2018-05-24 19:42:29,112][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 192.168.0.102, 65266, None)
[INFO][2018-05-24 19:42:29,115][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 192.168.0.102:65266 with 912.3 MB RAM, BlockManagerId(driver, 192.168.0.102, 65266, None)
[INFO][2018-05-24 19:42:29,117][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 192.168.0.102, 65266, None)
[INFO][2018-05-24 19:42:29,118][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 192.168.0.102, 65266, None)
[INFO][2018-05-24 19:42:29,386][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6c2f1700{/metrics/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:42:30,024][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0 stored as values in memory (estimated size 228.1 KB, free 912.1 MB)
[INFO][2018-05-24 19:42:30,102][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0_piece0 stored as bytes in memory (estimated size 22.1 KB, free 912.1 MB)
[INFO][2018-05-24 19:42:30,104][org.apache.spark.storage.BlockManagerInfo]Added broadcast_0_piece0 in memory on 192.168.0.102:65266 (size: 22.1 KB, free: 912.3 MB)
[INFO][2018-05-24 19:42:30,109][org.apache.spark.SparkContext]Created broadcast 0 from textFile at SimulationKafkaSendOutData.scala:25
[INFO][2018-05-24 19:42:30,948][org.apache.spark.streaming.kafka.DirectKafkaInputDStream]Slide time = 20000 ms
[INFO][2018-05-24 19:42:30,948][org.apache.spark.streaming.kafka.DirectKafkaInputDStream]Storage level = Serialized 1x Replicated
[INFO][2018-05-24 19:42:30,949][org.apache.spark.streaming.kafka.DirectKafkaInputDStream]Checkpoint interval = null
[INFO][2018-05-24 19:42:30,950][org.apache.spark.streaming.kafka.DirectKafkaInputDStream]Remember interval = 20000 ms
[INFO][2018-05-24 19:42:30,951][org.apache.spark.streaming.kafka.DirectKafkaInputDStream]Initialized and validated org.apache.spark.streaming.kafka.DirectKafkaInputDStream@2c69745f
[INFO][2018-05-24 19:42:30,951][org.apache.spark.streaming.dstream.MappedDStream]Slide time = 20000 ms
[INFO][2018-05-24 19:42:30,951][org.apache.spark.streaming.dstream.MappedDStream]Storage level = Serialized 1x Replicated
[INFO][2018-05-24 19:42:30,951][org.apache.spark.streaming.dstream.MappedDStream]Checkpoint interval = null
[INFO][2018-05-24 19:42:30,951][org.apache.spark.streaming.dstream.MappedDStream]Remember interval = 20000 ms
[INFO][2018-05-24 19:42:30,952][org.apache.spark.streaming.dstream.MappedDStream]Initialized and validated org.apache.spark.streaming.dstream.MappedDStream@4546bf84
[INFO][2018-05-24 19:42:30,952][org.apache.spark.streaming.dstream.ForEachDStream]Slide time = 20000 ms
[INFO][2018-05-24 19:42:30,952][org.apache.spark.streaming.dstream.ForEachDStream]Storage level = Serialized 1x Replicated
[INFO][2018-05-24 19:42:30,952][org.apache.spark.streaming.dstream.ForEachDStream]Checkpoint interval = null
[INFO][2018-05-24 19:42:30,952][org.apache.spark.streaming.dstream.ForEachDStream]Remember interval = 20000 ms
[INFO][2018-05-24 19:42:30,952][org.apache.spark.streaming.dstream.ForEachDStream]Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@46f18417
[INFO][2018-05-24 19:42:30,998][org.apache.spark.streaming.util.RecurringTimer]Started timer for JobGenerator at time 1527162160000
[INFO][2018-05-24 19:42:30,998][org.apache.spark.streaming.scheduler.JobGenerator]Started JobGenerator at 1527162160000 ms
[INFO][2018-05-24 19:42:30,999][org.apache.spark.streaming.scheduler.JobScheduler]Started JobScheduler
[INFO][2018-05-24 19:42:31,004][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4eb30d44{/streaming,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:42:31,005][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6972c30a{/streaming/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:42:31,007][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@351e414e{/streaming/batch,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:42:31,008][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5109e8cf{/streaming/batch/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:42:31,009][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@327c7bea{/static/streaming,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:42:31,010][org.apache.spark.streaming.StreamingContext]StreamingContext started
[WARN][2018-05-24 19:42:35,608][org.apache.hadoop.util.NativeCodeLoader]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO][2018-05-24 19:42:35,775][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 1
[INFO][2018-05-24 19:42:35,873][org.apache.spark.SparkContext]Starting job: collect at SimulationKafkaSendOutData.scala:25
[INFO][2018-05-24 19:42:35,885][org.apache.spark.scheduler.DAGScheduler]Got job 0 (collect at SimulationKafkaSendOutData.scala:25) with 2 output partitions
[INFO][2018-05-24 19:42:35,885][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 0 (collect at SimulationKafkaSendOutData.scala:25)
[INFO][2018-05-24 19:42:35,886][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 19:42:35,888][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 19:42:35,898][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 0 (hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/* MapPartitionsRDD[1] at textFile at SimulationKafkaSendOutData.scala:25), which has no missing parents
[INFO][2018-05-24 19:42:35,919][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1 stored as values in memory (estimated size 3.3 KB, free 912.1 MB)
[INFO][2018-05-24 19:42:35,921][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1_piece0 stored as bytes in memory (estimated size 1969.0 B, free 912.1 MB)
[INFO][2018-05-24 19:42:35,921][org.apache.spark.storage.BlockManagerInfo]Added broadcast_1_piece0 in memory on 192.168.0.102:65266 (size: 1969.0 B, free: 912.3 MB)
[INFO][2018-05-24 19:42:35,922][org.apache.spark.SparkContext]Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 19:42:35,937][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ResultStage 0 (hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/* MapPartitionsRDD[1] at textFile at SimulationKafkaSendOutData.scala:25) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2018-05-24 19:42:35,938][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 0.0 with 2 tasks
[INFO][2018-05-24 19:42:35,978][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 4883 bytes)
[INFO][2018-05-24 19:42:35,981][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 4883 bytes)
[INFO][2018-05-24 19:42:35,989][org.apache.spark.executor.Executor]Running task 0.0 in stage 0.0 (TID 0)
[INFO][2018-05-24 19:42:35,989][org.apache.spark.executor.Executor]Running task 1.0 in stage 0.0 (TID 1)
[INFO][2018-05-24 19:42:36,047][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/part-00000:10717435+10717436
[INFO][2018-05-24 19:42:36,047][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/part-00000:0+10717435
[INFO][2018-05-24 19:42:40,051][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 19:42:40,052][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 19:42:40,052][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 19:42:45,177][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527162160000 ms
[INFO][2018-05-24 19:42:45,181][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527162160000 ms.0 from job set of time 1527162160000 ms
[INFO][2018-05-24 19:42:45,227][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:64
[INFO][2018-05-24 19:42:45,246][org.apache.spark.scheduler.DAGScheduler]Got job 0 (foreachPartition at ReceiveKafkaData.scala:64) with 1 output partitions
[INFO][2018-05-24 19:42:45,247][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 0 (foreachPartition at ReceiveKafkaData.scala:64)
[INFO][2018-05-24 19:42:45,248][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 19:42:45,249][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 19:42:45,257][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 0 (MapPartitionsRDD[1] at map at ReceiveKafkaData.scala:62), which has no missing parents
[INFO][2018-05-24 19:42:45,650][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-24 19:42:45,696][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0_piece0 stored as bytes in memory (estimated size 2007.0 B, free 912.3 MB)
[INFO][2018-05-24 19:42:45,698][org.apache.spark.storage.BlockManagerInfo]Added broadcast_0_piece0 in memory on 192.168.0.102:65258 (size: 2007.0 B, free: 912.3 MB)
[INFO][2018-05-24 19:42:45,703][org.apache.spark.SparkContext]Created broadcast 0 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 19:42:45,727][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at map at ReceiveKafkaData.scala:62) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 19:42:45,729][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 0.0 with 1 tasks
[INFO][2018-05-24 19:42:45,781][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 19:42:45,799][org.apache.spark.executor.Executor]Running task 0.0 in stage 0.0 (TID 0)
[INFO][2018-05-24 19:42:45,846][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 11715 is the same as ending offset skipping seven 0
[INFO][2018-05-24 19:42:46,037][org.apache.spark.storage.memory.MemoryStore]Block taskresult_1 stored as bytes in memory (estimated size 10.4 MB, free 901.7 MB)
[INFO][2018-05-24 19:42:46,038][org.apache.spark.storage.BlockManagerInfo]Added taskresult_1 in memory on 192.168.0.102:65266 (size: 10.4 MB, free: 901.9 MB)
[INFO][2018-05-24 19:42:46,039][org.apache.spark.executor.Executor]Finished task 1.0 in stage 0.0 (TID 1). 10855899 bytes result sent via BlockManager)
[INFO][2018-05-24 19:42:46,087][org.apache.spark.network.client.TransportClientFactory]Successfully created connection to /192.168.0.102:65266 after 24 ms (0 ms spent in bootstraps)
[INFO][2018-05-24 19:42:46,389][org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper]Process identifier=hconnection-0x2429615c connecting to ZooKeeper ensemble=vm-xaj-bigdata-da-d01:2181,vm-xaj-bigdata-da-d02:2181,vm-xaj-bigdata-da-d03:2181
[INFO][2018-05-24 19:42:46,395][org.apache.zookeeper.ZooKeeper]Client environment:zookeeper.version=3.4.5-cdh5.7.0--1, built on 04/06/2016 14:24 GMT
[INFO][2018-05-24 19:42:46,395][org.apache.zookeeper.ZooKeeper]Client environment:host.name=192.168.0.102
[INFO][2018-05-24 19:42:46,395][org.apache.zookeeper.ZooKeeper]Client environment:java.version=1.8.0_161
[INFO][2018-05-24 19:42:46,396][org.apache.zookeeper.ZooKeeper]Client environment:java.vendor=Oracle Corporation
[INFO][2018-05-24 19:42:46,396][org.apache.zookeeper.ZooKeeper]Client environment:java.home=/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre
[INFO][2018-05-24 19:42:46,396][org.apache.zookeeper.ZooKeeper]Client environment:java.class.path=/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/charsets.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/deploy.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/cldrdata.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/dnsns.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/jaccess.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/jfxrt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/localedata.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/nashorn.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/sunec.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/sunjce_provider.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/sunpkcs11.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/zipfs.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/javaws.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/jce.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/jfr.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/jfxswt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/jsse.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/management-agent.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/plugin.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/resources.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/rt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/ant-javafx.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/dt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/javafx-mx.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/jconsole.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/packager.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/sa-jdi.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/tools.jar:/Users/seven/project/scala/dataMining/target/classes:/Users/seven/software/maven/repository/org/scala-lang/scala-library/2.11.12/scala-library-2.11.12.jar:/Users/seven/software/maven/repository/org/scala-lang/scala-compiler/2.11.12/scala-compiler-2.11.12.jar:/Users/seven/software/maven/repository/org/scala-lang/modules/scala-xml_2.11/1.0.5/scala-xml_2.11-1.0.5.jar:/Users/seven/software/maven/repository/org/scala-lang/modules/scala-parser-combinators_2.11/1.0.4/scala-parser-combinators_2.11-1.0.4.jar:/Users/seven/software/maven/repository/org/scala-lang/scala-reflect/2.11.12/scala-reflect-2.11.12.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-client/2.6.0-cdh5.7.0/hadoop-client-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-common/2.6.0-cdh5.7.0/hadoop-common-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/xmlenc/xmlenc/0.52/xmlenc-0.52.jar:/Users/seven/software/maven/repository/commons-net/commons-net/3.1/commons-net-3.1.jar:/Users/seven/software/maven/repository/commons-configuration/commons-configuration/1.6/commons-configuration-1.6.jar:/Users/seven/software/maven/repository/commons-digester/commons-digester/1.8/commons-digester-1.8.jar:/Users/seven/software/maven/repository/commons-beanutils/commons-beanutils/1.7.0/commons-beanutils-1.7.0.jar:/Users/seven/software/maven/repository/org/slf4j/slf4j-api/1.7.5/slf4j-api-1.7.5.jar:/Users/seven/software/maven/repository/org/slf4j/slf4j-log4j12/1.7.5/slf4j-log4j12-1.7.5.jar:/Users/seven/software/maven/repository/org/apache/avro/avro/1.7.6-cdh5.7.0/avro-1.7.6-cdh5.7.0.jar:/Users/seven/software/maven/repository/com/google/code/gson/gson/2.2.4/gson-2.2.4.jar:/Users/seven/software/maven/repository/org/apache/curator/curator-client/2.7.1/curator-client-2.7.1.jar:/Users/seven/software/maven/repository/org/apache/curator/curator-recipes/2.7.1/curator-recipes-2.7.1.jar:/Users/seven/software/maven/repository/com/google/code/findbugs/jsr305/3.0.0/jsr305-3.0.0.jar:/Users/seven/software/maven/repository/org/apache/htrace/htrace-core4/4.0.1-incubating/htrace-core4-4.0.1-incubating.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-compress/1.4.1/commons-compress-1.4.1.jar:/Users/seven/software/maven/repository/org/tukaani/xz/1.0/xz-1.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-hdfs/2.6.0-cdh5.7.0/hadoop-hdfs-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/io/netty/netty/3.6.2.Final/netty-3.6.2.Final.jar:/Users/seven/software/maven/repository/xerces/xercesImpl/2.9.1/xercesImpl-2.9.1.jar:/Users/seven/software/maven/repository/xml-apis/xml-apis/1.3.04/xml-apis-1.3.04.jar:/Users/seven/software/maven/repository/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-app/2.6.0-cdh5.7.0/hadoop-mapreduce-client-app-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-common/2.6.0-cdh5.7.0/hadoop-mapreduce-client-common-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-yarn-client/2.6.0-cdh5.7.0/hadoop-yarn-client-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-yarn-server-common/2.6.0-cdh5.7.0/hadoop-yarn-server-common-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-shuffle/2.6.0-cdh5.7.0/hadoop-mapreduce-client-shuffle-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-yarn-api/2.6.0-cdh5.7.0/hadoop-yarn-api-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-core/2.6.0-cdh5.7.0/hadoop-mapreduce-client-core-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-yarn-common/2.6.0-cdh5.7.0/hadoop-yarn-common-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/javax/xml/bind/jaxb-api/2.2.2/jaxb-api-2.2.2.jar:/Users/seven/software/maven/repository/javax/xml/stream/stax-api/1.0-2/stax-api-1.0-2.jar:/Users/seven/software/maven/repository/javax/activation/activation/1.1/activation-1.1.jar:/Users/seven/software/maven/repository/com/sun/jersey/jersey-client/1.9/jersey-client-1.9.jar:/Users/seven/software/maven/repository/org/codehaus/jackson/jackson-xc/1.8.8/jackson-xc-1.8.8.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.6.0-cdh5.7.0/hadoop-mapreduce-client-jobclient-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-aws/2.6.0-cdh5.7.0/hadoop-aws-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/com/amazonaws/aws-java-sdk-s3/1.10.6/aws-java-sdk-s3-1.10.6.jar:/Users/seven/software/maven/repository/com/amazonaws/aws-java-sdk-kms/1.10.6/aws-java-sdk-kms-1.10.6.jar:/Users/seven/software/maven/repository/com/amazonaws/aws-java-sdk-core/1.10.6/aws-java-sdk-core-1.10.6.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-annotations/2.6.0-cdh5.7.0/hadoop-annotations-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-client/1.2.0-cdh5.7.0/hbase-client-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-annotations/1.2.0-cdh5.7.0/hbase-annotations-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-common/1.2.0-cdh5.7.0/hbase-common-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-protocol/1.2.0-cdh5.7.0/hbase-protocol-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/commons-codec/commons-codec/1.9/commons-codec-1.9.jar:/Users/seven/software/maven/repository/commons-io/commons-io/2.4/commons-io-2.4.jar:/Users/seven/software/maven/repository/commons-lang/commons-lang/2.6/commons-lang-2.6.jar:/Users/seven/software/maven/repository/commons-logging/commons-logging/1.2/commons-logging-1.2.jar:/Users/seven/software/maven/repository/com/google/guava/guava/12.0.1/guava-12.0.1.jar:/Users/seven/software/maven/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/Users/seven/software/maven/repository/io/netty/netty-all/4.0.23.Final/netty-all-4.0.23.Final.jar:/Users/seven/software/maven/repository/org/apache/zookeeper/zookeeper/3.4.5-cdh5.7.0/zookeeper-3.4.5-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/htrace/htrace-core/3.2.0-incubating/htrace-core-3.2.0-incubating.jar:/Users/seven/software/maven/repository/org/codehaus/jackson/jackson-mapper-asl/1.8.8/jackson-mapper-asl-1.8.8.jar:/Users/seven/software/maven/repository/org/jruby/jcodings/jcodings/1.0.8/jcodings-1.0.8.jar:/Users/seven/software/maven/repository/org/jruby/joni/joni/2.1.2/joni-2.1.2.jar:/Users/seven/software/maven/repository/com/yammer/metrics/metrics-core/2.2.0/metrics-core-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-auth/2.6.0-cdh5.7.0/hadoop-auth-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/httpcomponents/httpclient/4.2.5/httpclient-4.2.5.jar:/Users/seven/software/maven/repository/org/apache/httpcomponents/httpcore/4.2.4/httpcore-4.2.4.jar:/Users/seven/software/maven/repository/org/apache/directory/server/apacheds-kerberos-codec/2.0.0-M15/apacheds-kerberos-codec-2.0.0-M15.jar:/Users/seven/software/maven/repository/org/apache/directory/server/apacheds-i18n/2.0.0-M15/apacheds-i18n-2.0.0-M15.jar:/Users/seven/software/maven/repository/org/apache/directory/api/api-asn1-api/1.0.0-M20/api-asn1-api-1.0.0-M20.jar:/Users/seven/software/maven/repository/org/apache/directory/api/api-util/1.0.0-M20/api-util-1.0.0-M20.jar:/Users/seven/software/maven/repository/org/apache/curator/curator-framework/2.7.1/curator-framework-2.7.1.jar:/Users/seven/software/maven/repository/com/github/stephenc/findbugs/findbugs-annotations/1.3.9-1/findbugs-annotations-1.3.9-1.jar:/Users/seven/software/maven/repository/junit/junit/4.12/junit-4.12.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-server/1.2.0-cdh5.7.0/hbase-server-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-procedure/1.2.0-cdh5.7.0/hbase-procedure-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-common/1.2.0-cdh5.7.0/hbase-common-1.2.0-cdh5.7.0-tests.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-prefix-tree/1.2.0-cdh5.7.0/hbase-prefix-tree-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar:/Users/seven/software/maven/repository/commons-collections/commons-collections/3.2.2/commons-collections-3.2.2.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-hadoop-compat/1.2.0-cdh5.7.0/hbase-hadoop-compat-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-hadoop2-compat/1.2.0-cdh5.7.0/hbase-hadoop2-compat-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/com/sun/jersey/jersey-core/1.9/jersey-core-1.9.jar:/Users/seven/software/maven/repository/com/sun/jersey/jersey-server/1.9/jersey-server-1.9.jar:/Users/seven/software/maven/repository/asm/asm/3.1/asm-3.1.jar:/Users/seven/software/maven/repository/commons-cli/commons-cli/1.2/commons-cli-1.2.jar:/Users/seven/software/maven/repository/com/github/stephenc/high-scale-lib/high-scale-lib/1.1.1/high-scale-lib-1.1.1.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-math/2.1/commons-math-2.1.jar:/Users/seven/software/maven/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jetty/6.1.26.cloudera.4/jetty-6.1.26.cloudera.4.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jetty-util/6.1.26.cloudera.4/jetty-util-6.1.26.cloudera.4.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jetty-sslengine/6.1.26.cloudera.4/jetty-sslengine-6.1.26.cloudera.4.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jsp-2.1/6.1.14/jsp-2.1-6.1.14.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jsp-api-2.1/6.1.14/jsp-api-2.1-6.1.14.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/servlet-api-2.5/6.1.14/servlet-api-2.5-6.1.14.jar:/Users/seven/software/maven/repository/org/codehaus/jackson/jackson-core-asl/1.8.8/jackson-core-asl-1.8.8.jar:/Users/seven/software/maven/repository/org/codehaus/jackson/jackson-jaxrs/1.8.8/jackson-jaxrs-1.8.8.jar:/Users/seven/software/maven/repository/tomcat/jasper-compiler/5.5.23/jasper-compiler-5.5.23.jar:/Users/seven/software/maven/repository/tomcat/jasper-runtime/5.5.23/jasper-runtime-5.5.23.jar:/Users/seven/software/maven/repository/commons-el/commons-el/1.0/commons-el-1.0.jar:/Users/seven/software/maven/repository/org/jamon/jamon-runtime/2.4.1/jamon-runtime-2.4.1.jar:/Users/seven/software/maven/repository/com/lmax/disruptor/3.3.0/disruptor-3.3.0.jar:/Users/seven/software/maven/repository/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar:/Users/seven/software/maven/repository/org/owasp/esapi/esapi/2.1.0/esapi-2.1.0.jar:/Users/seven/software/maven/repository/commons-beanutils/commons-beanutils-core/1.7.0/commons-beanutils-core-1.7.0.jar:/Users/seven/software/maven/repository/commons-fileupload/commons-fileupload/1.2/commons-fileupload-1.2.jar:/Users/seven/software/maven/repository/xom/xom/1.2.5/xom-1.2.5.jar:/Users/seven/software/maven/repository/xalan/xalan/2.7.0/xalan-2.7.0.jar:/Users/seven/software/maven/repository/org/beanshell/bsh-core/2.0b4/bsh-core-2.0b4.jar:/Users/seven/software/maven/repository/org/owasp/antisamy/antisamy/1.4.3/antisamy-1.4.3.jar:/Users/seven/software/maven/repository/org/apache/xmlgraphics/batik-css/1.7/batik-css-1.7.jar:/Users/seven/software/maven/repository/org/apache/xmlgraphics/batik-ext/1.7/batik-ext-1.7.jar:/Users/seven/software/maven/repository/org/apache/xmlgraphics/batik-util/1.7/batik-util-1.7.jar:/Users/seven/software/maven/repository/xml-apis/xml-apis-ext/1.3.04/xml-apis-ext-1.3.04.jar:/Users/seven/software/maven/repository/net/sourceforge/nekohtml/nekohtml/1.9.12/nekohtml-1.9.12.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-core/2.6.0-mr1-cdh5.7.0/hadoop-core-2.6.0-mr1-cdh5.7.0.jar:/Users/seven/software/maven/repository/javax/servlet/jsp/jsp-api/2.1/jsp-api-2.1.jar:/Users/seven/software/maven/repository/net/java/dev/jets3t/jets3t/0.6.1/jets3t-0.6.1.jar:/Users/seven/software/maven/repository/javax/servlet/servlet-api/2.5/servlet-api-2.5.jar:/Users/seven/software/maven/repository/hsqldb/hsqldb/1.8.0.10/hsqldb-1.8.0.10.jar:/Users/seven/software/maven/repository/org/eclipse/jdt/core/3.1.1/core-3.1.1.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-hdfs/2.6.0-cdh5.7.0/hadoop-hdfs-2.6.0-cdh5.7.0-tests.jar:/Users/seven/software/maven/repository/commons-daemon/commons-daemon/1.0.13/commons-daemon-1.0.13.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-lang3/3.5/commons-lang3-3.5.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-pool2/2.4.2/commons-pool2-2.4.2.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-mllib_2.11/2.2.0/spark-mllib_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-core_2.11/2.2.0/spark-core_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/avro/avro-mapred/1.7.7/avro-mapred-1.7.7-hadoop2.jar:/Users/seven/software/maven/repository/org/apache/avro/avro-ipc/1.7.7/avro-ipc-1.7.7-tests.jar:/Users/seven/software/maven/repository/com/twitter/chill_2.11/0.8.0/chill_2.11-0.8.0.jar:/Users/seven/software/maven/repository/com/esotericsoftware/kryo-shaded/3.0.3/kryo-shaded-3.0.3.jar:/Users/seven/software/maven/repository/com/esotericsoftware/minlog/1.3.0/minlog-1.3.0.jar:/Users/seven/software/maven/repository/org/objenesis/objenesis/2.1/objenesis-2.1.jar:/Users/seven/software/maven/repository/com/twitter/chill-java/0.8.0/chill-java-0.8.0.jar:/Users/seven/software/maven/repository/org/apache/xbean/xbean-asm5-shaded/4.4/xbean-asm5-shaded-4.4.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-launcher_2.11/2.2.0/spark-launcher_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-network-common_2.11/2.2.0/spark-network-common_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-network-shuffle_2.11/2.2.0/spark-network-shuffle_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-unsafe_2.11/2.2.0/spark-unsafe_2.11-2.2.0.jar:/Users/seven/software/maven/repository/javax/servlet/javax.servlet-api/3.1.0/javax.servlet-api-3.1.0.jar:/Users/seven/software/maven/repository/org/slf4j/jul-to-slf4j/1.7.16/jul-to-slf4j-1.7.16.jar:/Users/seven/software/maven/repository/org/slf4j/jcl-over-slf4j/1.7.16/jcl-over-slf4j-1.7.16.jar:/Users/seven/software/maven/repository/com/ning/compress-lzf/1.0.3/compress-lzf-1.0.3.jar:/Users/seven/software/maven/repository/org/xerial/snappy/snappy-java/1.1.2.6/snappy-java-1.1.2.6.jar:/Users/seven/software/maven/repository/org/roaringbitmap/RoaringBitmap/0.5.11/RoaringBitmap-0.5.11.jar:/Users/seven/software/maven/repository/org/json4s/json4s-jackson_2.11/3.2.11/json4s-jackson_2.11-3.2.11.jar:/Users/seven/software/maven/repository/org/json4s/json4s-core_2.11/3.2.11/json4s-core_2.11-3.2.11.jar:/Users/seven/software/maven/repository/org/json4s/json4s-ast_2.11/3.2.11/json4s-ast_2.11-3.2.11.jar:/Users/seven/software/maven/repository/org/scala-lang/scalap/2.11.0/scalap-2.11.0.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/core/jersey-client/2.22.2/jersey-client-2.22.2.jar:/Users/seven/software/maven/repository/javax/ws/rs/javax.ws.rs-api/2.0.1/javax.ws.rs-api-2.0.1.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/hk2-api/2.4.0-b34/hk2-api-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/hk2-utils/2.4.0-b34/hk2-utils-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/external/aopalliance-repackaged/2.4.0-b34/aopalliance-repackaged-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/external/javax.inject/2.4.0-b34/javax.inject-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/hk2-locator/2.4.0-b34/hk2-locator-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/javassist/javassist/3.18.1-GA/javassist-3.18.1-GA.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/core/jersey-common/2.22.2/jersey-common-2.22.2.jar:/Users/seven/software/maven/repository/javax/annotation/javax.annotation-api/1.2/javax.annotation-api-1.2.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/bundles/repackaged/jersey-guava/2.22.2/jersey-guava-2.22.2.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/osgi-resource-locator/1.0.1/osgi-resource-locator-1.0.1.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/core/jersey-server/2.22.2/jersey-server-2.22.2.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/media/jersey-media-jaxb/2.22.2/jersey-media-jaxb-2.22.2.jar:/Users/seven/software/maven/repository/javax/validation/validation-api/1.1.0.Final/validation-api-1.1.0.Final.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/containers/jersey-container-servlet/2.22.2/jersey-container-servlet-2.22.2.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/containers/jersey-container-servlet-core/2.22.2/jersey-container-servlet-core-2.22.2.jar:/Users/seven/software/maven/repository/com/clearspring/analytics/stream/2.7.0/stream-2.7.0.jar:/Users/seven/software/maven/repository/io/dropwizard/metrics/metrics-core/3.1.2/metrics-core-3.1.2.jar:/Users/seven/software/maven/repository/io/dropwizard/metrics/metrics-jvm/3.1.2/metrics-jvm-3.1.2.jar:/Users/seven/software/maven/repository/io/dropwizard/metrics/metrics-json/3.1.2/metrics-json-3.1.2.jar:/Users/seven/software/maven/repository/io/dropwizard/metrics/metrics-graphite/3.1.2/metrics-graphite-3.1.2.jar:/Users/seven/software/maven/repository/org/apache/ivy/ivy/2.4.0/ivy-2.4.0.jar:/Users/seven/software/maven/repository/oro/oro/2.0.8/oro-2.0.8.jar:/Users/seven/software/maven/repository/net/razorvine/pyrolite/4.13/pyrolite-4.13.jar:/Users/seven/software/maven/repository/net/sf/py4j/py4j/0.10.4/py4j-0.10.4.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-crypto/1.0.0/commons-crypto-1.0.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-streaming_2.11/2.2.0/spark-streaming_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-sql_2.11/2.2.0/spark-sql_2.11-2.2.0.jar:/Users/seven/software/maven/repository/com/univocity/univocity-parsers/2.2.1/univocity-parsers-2.2.1.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-sketch_2.11/2.2.0/spark-sketch_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-catalyst_2.11/2.2.0/spark-catalyst_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/codehaus/janino/janino/3.0.0/janino-3.0.0.jar:/Users/seven/software/maven/repository/org/codehaus/janino/commons-compiler/3.0.0/commons-compiler-3.0.0.jar:/Users/seven/software/maven/repository/org/antlr/antlr4-runtime/4.5.3/antlr4-runtime-4.5.3.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-column/1.8.2/parquet-column-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-common/1.8.2/parquet-common-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-encoding/1.8.2/parquet-encoding-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-hadoop/1.8.2/parquet-hadoop-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-format/2.3.1/parquet-format-2.3.1.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-jackson/1.8.2/parquet-jackson-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-graphx_2.11/2.2.0/spark-graphx_2.11-2.2.0.jar:/Users/seven/software/maven/repository/com/github/fommil/netlib/core/1.1.2/core-1.1.2.jar:/Users/seven/software/maven/repository/net/sourceforge/f2j/arpack_combined_all/0.1/arpack_combined_all-0.1.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-mllib-local_2.11/2.2.0/spark-mllib-local_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/scalanlp/breeze_2.11/0.13.1/breeze_2.11-0.13.1.jar:/Users/seven/software/maven/repository/org/scalanlp/breeze-macros_2.11/0.13.1/breeze-macros_2.11-0.13.1.jar:/Users/seven/software/maven/repository/net/sf/opencsv/opencsv/2.3/opencsv-2.3.jar:/Users/seven/software/maven/repository/com/github/rwl/jtransforms/2.4.0/jtransforms-2.4.0.jar:/Users/seven/software/maven/repository/org/spire-math/spire_2.11/0.13.0/spire_2.11-0.13.0.jar:/Users/seven/software/maven/repository/org/spire-math/spire-macros_2.11/0.13.0/spire-macros_2.11-0.13.0.jar:/Users/seven/software/maven/repository/org/typelevel/machinist_2.11/0.6.1/machinist_2.11-0.6.1.jar:/Users/seven/software/maven/repository/com/chuusai/shapeless_2.11/2.3.2/shapeless_2.11-2.3.2.jar:/Users/seven/software/maven/repository/org/typelevel/macro-compat_2.11/1.1.1/macro-compat_2.11-1.1.1.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-math3/3.4.1/commons-math3-3.4.1.jar:/Users/seven/software/maven/repository/org/jpmml/pmml-model/1.2.15/pmml-model-1.2.15.jar:/Users/seven/software/maven/repository/org/jpmml/pmml-schema/1.2.15/pmml-schema-1.2.15.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-tags_2.11/2.2.0/spark-tags_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-streaming-kafka-0-8_2.11/2.2.0/spark-streaming-kafka-0-8_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/kafka/kafka_2.11/0.8.2.1/kafka_2.11-0.8.2.1.jar:/Users/seven/software/maven/repository/com/101tec/zkclient/0.3/zkclient-0.3.jar:/Users/seven/software/maven/repository/org/apache/kafka/kafka-clients/0.8.2.1/kafka-clients-0.8.2.1.jar:/Users/seven/software/maven/repository/mysql/mysql-connector-java/5.1.31/mysql-connector-java-5.1.31.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/module/jackson-module-scala_2.11/2.6.5/jackson-module-scala_2.11-2.6.5.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/core/jackson-core/2.6.5/jackson-core-2.6.5.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/core/jackson-annotations/2.6.5/jackson-annotations-2.6.5.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/core/jackson-databind/2.6.5/jackson-databind-2.6.5.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/module/jackson-module-paranamer/2.6.5/jackson-module-paranamer-2.6.5.jar:/Users/seven/software/maven/repository/com/thoughtworks/paranamer/paranamer/2.6/paranamer-2.6.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-streaming-flume_2.11/2.2.0/spark-streaming-flume_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/flume/flume-ng-core/1.6.0/flume-ng-core-1.6.0.jar:/Users/seven/software/maven/repository/org/apache/flume/flume-ng-configuration/1.6.0/flume-ng-configuration-1.6.0.jar:/Users/seven/software/maven/repository/org/apache/avro/avro-ipc/1.7.4/avro-ipc-1.7.4.jar:/Users/seven/software/maven/repository/org/apache/velocity/velocity/1.7/velocity-1.7.jar:/Users/seven/software/maven/repository/joda-time/joda-time/2.1/joda-time-2.1.jar:/Users/seven/software/maven/repository/org/apache/mina/mina-core/2.0.4/mina-core-2.0.4.jar:/Users/seven/software/maven/repository/org/apache/flume/flume-ng-sdk/1.6.0/flume-ng-sdk-1.6.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-streaming-flume-sink_2.11/2.2.0/spark-streaming-flume-sink_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/flume/flume-ng-clients/flume-ng-log4jappender/1.6.0/flume-ng-log4jappender-1.6.0.jar:/Users/seven/software/maven/repository/net/jpountz/lz4/lz4/1.3.0/lz4-1.3.0.jar:/Applications/IntelliJ IDEA.app/Contents/lib/idea_rt.jar
[INFO][2018-05-24 19:42:46,397][org.apache.zookeeper.ZooKeeper]Client environment:java.library.path=/Users/seven/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:.
[INFO][2018-05-24 19:42:46,397][org.apache.zookeeper.ZooKeeper]Client environment:java.io.tmpdir=/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/
[INFO][2018-05-24 19:42:46,397][org.apache.zookeeper.ZooKeeper]Client environment:java.compiler=<NA>
[INFO][2018-05-24 19:42:46,397][org.apache.zookeeper.ZooKeeper]Client environment:os.name=Mac OS X
[INFO][2018-05-24 19:42:46,397][org.apache.zookeeper.ZooKeeper]Client environment:os.arch=x86_64
[INFO][2018-05-24 19:42:46,397][org.apache.zookeeper.ZooKeeper]Client environment:os.version=10.13.4
[INFO][2018-05-24 19:42:46,397][org.apache.zookeeper.ZooKeeper]Client environment:user.name=seven
[INFO][2018-05-24 19:42:46,397][org.apache.zookeeper.ZooKeeper]Client environment:user.home=/Users/seven
[INFO][2018-05-24 19:42:46,397][org.apache.zookeeper.ZooKeeper]Client environment:user.dir=/Users/seven/project/scala/dataMining
[INFO][2018-05-24 19:42:46,398][org.apache.zookeeper.ZooKeeper]Initiating client connection, connectString=vm-xaj-bigdata-da-d01:2181,vm-xaj-bigdata-da-d02:2181,vm-xaj-bigdata-da-d03:2181 sessionTimeout=60000 watcher=hconnection-0x2429615c0x0, quorum=vm-xaj-bigdata-da-d01:2181,vm-xaj-bigdata-da-d02:2181,vm-xaj-bigdata-da-d03:2181, baseZNode=/hbase
[INFO][2018-05-24 19:42:46,435][org.apache.spark.storage.memory.MemoryStore]Block taskresult_0 stored as bytes in memory (estimated size 10.4 MB, free 891.3 MB)
[INFO][2018-05-24 19:42:46,435][org.apache.spark.storage.BlockManagerInfo]Added taskresult_0 in memory on 192.168.0.102:65266 (size: 10.4 MB, free: 891.6 MB)
[INFO][2018-05-24 19:42:46,436][org.apache.spark.executor.Executor]Finished task 0.0 in stage 0.0 (TID 0). 10856349 bytes result sent via BlockManager)
[INFO][2018-05-24 19:42:46,479][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 0.0 (TID 1) in 10496 ms on localhost (executor driver) (1/2)
[INFO][2018-05-24 19:42:46,480][org.apache.spark.storage.BlockManagerInfo]Removed taskresult_1 on 192.168.0.102:65266 in memory (size: 10.4 MB, free: 901.9 MB)
[INFO][2018-05-24 19:42:46,513][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 0.0 (TID 0) in 10547 ms on localhost (executor driver) (2/2)
[INFO][2018-05-24 19:42:46,514][org.apache.spark.storage.BlockManagerInfo]Removed taskresult_0 on 192.168.0.102:65266 in memory (size: 10.4 MB, free: 912.3 MB)
[INFO][2018-05-24 19:42:46,516][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 19:42:46,516][org.apache.spark.scheduler.DAGScheduler]ResultStage 0 (collect at SimulationKafkaSendOutData.scala:25) finished in 10.561 s
[INFO][2018-05-24 19:42:46,522][org.apache.spark.scheduler.DAGScheduler]Job 0 finished: collect at SimulationKafkaSendOutData.scala:25, took 10.648839 s
[INFO][2018-05-24 19:42:46,583][org.spark_project.jetty.server.AbstractConnector]Stopped Spark@1643d68f{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}
[INFO][2018-05-24 19:42:46,585][org.apache.spark.ui.SparkUI]Stopped Spark web UI at http://192.168.0.102:4041
[INFO][2018-05-24 19:42:46,595][org.apache.spark.MapOutputTrackerMasterEndpoint]MapOutputTrackerMasterEndpoint stopped!
[INFO][2018-05-24 19:42:46,613][org.apache.spark.storage.memory.MemoryStore]MemoryStore cleared
[INFO][2018-05-24 19:42:46,613][org.apache.spark.storage.BlockManager]BlockManager stopped
[INFO][2018-05-24 19:42:46,614][org.apache.spark.storage.BlockManagerMaster]BlockManagerMaster stopped
[INFO][2018-05-24 19:42:46,616][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint]OutputCommitCoordinator stopped!
[INFO][2018-05-24 19:42:46,618][org.apache.spark.SparkContext]Successfully stopped SparkContext
[INFO][2018-05-24 19:42:46,645][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	compression.type = none
	metric.reporters = []
	metadata.max.age.ms = 300000
	metadata.fetch.timeout.ms = 60000
	acks = all
	batch.size = 16384
	reconnect.backoff.ms = 10
	bootstrap.servers = [vm-xaj-bigdata-da-d01:9092, vm-xaj-bigdata-da-d02:9092, vm-xaj-bigdata-da-d03:9092, vm-xaj-bigdata-da-d04:9092, vm-xaj-bigdata-da-d05:9092, vm-xaj-bigdata-da-d06:9092, vm-xaj-bigdata-da-d07:9092]
	receive.buffer.bytes = 32768
	retry.backoff.ms = 100
	buffer.memory = 33554432
	timeout.ms = 30000
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	retries = 0
	max.request.size = 1048576
	block.on.buffer.full = true
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
	metrics.sample.window.ms = 30000
	send.buffer.bytes = 131072
	max.in.flight.requests.per.connection = 5
	metrics.num.samples = 2
	linger.ms = 1
	client.id = 

[INFO][2018-05-24 19:42:56,428][org.apache.zookeeper.ClientCnxn]Opening socket connection to server vm-xaj-bigdata-da-d03/10.213.4.27:2181. Will not attempt to authenticate using SASL (unknown error)
[INFO][2018-05-24 19:42:56,438][org.apache.zookeeper.ClientCnxn]Socket connection established, initiating session, client: /10.0.8.39:65283, server: vm-xaj-bigdata-da-d03/10.213.4.27:2181
[INFO][2018-05-24 19:42:56,459][org.apache.zookeeper.ClientCnxn]Session establishment complete on server vm-xaj-bigdata-da-d03/10.213.4.27:2181, sessionid = 0x362b50d75ec5efc, negotiated timeout = 60000
[WARN][2018-05-24 19:42:56,938][org.apache.hadoop.util.NativeCodeLoader]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO][2018-05-24 19:42:57,021][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 19:42:57,033][org.apache.spark.executor.Executor]Finished task 0.0 in stage 0.0 (TID 0). 708 bytes result sent to driver
[INFO][2018-05-24 19:42:57,040][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 0.0 (TID 0) in 11274 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 19:42:57,042][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 19:42:57,044][org.apache.spark.scheduler.DAGScheduler]ResultStage 0 (foreachPartition at ReceiveKafkaData.scala:64) finished in 11.290 s
[INFO][2018-05-24 19:42:57,051][org.apache.spark.scheduler.DAGScheduler]Job 0 finished: foreachPartition at ReceiveKafkaData.scala:64, took 11.823370 s
[INFO][2018-05-24 19:42:57,056][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527162160000 ms.0 from job set of time 1527162160000 ms
[INFO][2018-05-24 19:42:57,057][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 17.055 s for time 1527162160000 ms (execution: 11.876 s)
[INFO][2018-05-24 19:42:57,065][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 19:42:57,071][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 
[INFO][2018-05-24 19:43:00,059][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527162180000 ms
[INFO][2018-05-24 19:43:00,060][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527162180000 ms.0 from job set of time 1527162180000 ms
[INFO][2018-05-24 19:43:00,073][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:64
[INFO][2018-05-24 19:43:00,074][org.apache.spark.scheduler.DAGScheduler]Got job 1 (foreachPartition at ReceiveKafkaData.scala:64) with 1 output partitions
[INFO][2018-05-24 19:43:00,074][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 1 (foreachPartition at ReceiveKafkaData.scala:64)
[INFO][2018-05-24 19:43:00,074][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 19:43:00,074][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 19:43:00,074][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 1 (MapPartitionsRDD[3] at map at ReceiveKafkaData.scala:62), which has no missing parents
[INFO][2018-05-24 19:43:00,076][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-24 19:43:00,080][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1_piece0 stored as bytes in memory (estimated size 2008.0 B, free 912.3 MB)
[INFO][2018-05-24 19:43:00,083][org.apache.spark.storage.BlockManagerInfo]Added broadcast_1_piece0 in memory on 192.168.0.102:65258 (size: 2008.0 B, free: 912.3 MB)
[INFO][2018-05-24 19:43:00,085][org.apache.spark.SparkContext]Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 19:43:00,086][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at map at ReceiveKafkaData.scala:62) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 19:43:00,086][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 1.0 with 1 tasks
[INFO][2018-05-24 19:43:00,088][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 19:43:00,088][org.apache.spark.executor.Executor]Running task 0.0 in stage 1.0 (TID 1)
[INFO][2018-05-24 19:43:00,117][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 11715 is the same as ending offset skipping seven 0
[INFO][2018-05-24 19:43:00,117][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 19:43:00,119][org.apache.spark.executor.Executor]Finished task 0.0 in stage 1.0 (TID 1). 708 bytes result sent to driver
[INFO][2018-05-24 19:43:00,120][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 1.0 (TID 1) in 33 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 19:43:00,120][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 19:43:00,121][org.apache.spark.scheduler.DAGScheduler]ResultStage 1 (foreachPartition at ReceiveKafkaData.scala:64) finished in 0.034 s
[INFO][2018-05-24 19:43:00,123][org.apache.spark.scheduler.DAGScheduler]Job 1 finished: foreachPartition at ReceiveKafkaData.scala:64, took 0.049593 s
[INFO][2018-05-24 19:43:00,124][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527162180000 ms.0 from job set of time 1527162180000 ms
[INFO][2018-05-24 19:43:00,125][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.124 s for time 1527162180000 ms (execution: 0.065 s)
[INFO][2018-05-24 19:43:00,126][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 1 from persistence list
[INFO][2018-05-24 19:43:00,133][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 0 from persistence list
[INFO][2018-05-24 19:43:00,134][org.apache.spark.storage.BlockManager]Removing RDD 1
[INFO][2018-05-24 19:43:00,136][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 19:43:00,137][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 
[INFO][2018-05-24 19:43:00,137][org.apache.spark.storage.BlockManager]Removing RDD 0
[INFO][2018-05-24 19:43:30,065][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527162200000 ms
[INFO][2018-05-24 19:43:30,066][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527162200000 ms.0 from job set of time 1527162200000 ms
[INFO][2018-05-24 19:43:30,077][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:64
[INFO][2018-05-24 19:43:30,078][org.apache.spark.scheduler.DAGScheduler]Got job 2 (foreachPartition at ReceiveKafkaData.scala:64) with 1 output partitions
[INFO][2018-05-24 19:43:30,078][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 2 (foreachPartition at ReceiveKafkaData.scala:64)
[INFO][2018-05-24 19:43:30,078][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 19:43:30,078][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 19:43:30,079][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 2 (MapPartitionsRDD[5] at map at ReceiveKafkaData.scala:62), which has no missing parents
[INFO][2018-05-24 19:43:30,082][org.apache.spark.storage.memory.MemoryStore]Block broadcast_2 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-24 19:43:30,085][org.apache.spark.storage.memory.MemoryStore]Block broadcast_2_piece0 stored as bytes in memory (estimated size 2008.0 B, free 912.3 MB)
[INFO][2018-05-24 19:43:30,086][org.apache.spark.storage.BlockManagerInfo]Added broadcast_2_piece0 in memory on 192.168.0.102:65258 (size: 2008.0 B, free: 912.3 MB)
[INFO][2018-05-24 19:43:30,087][org.apache.spark.SparkContext]Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 19:43:30,087][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[5] at map at ReceiveKafkaData.scala:62) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 19:43:30,088][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 2.0 with 1 tasks
[INFO][2018-05-24 19:43:30,088][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 19:43:30,089][org.apache.spark.executor.Executor]Running task 0.0 in stage 2.0 (TID 2)
[INFO][2018-05-24 19:43:30,097][org.apache.spark.streaming.kafka.KafkaRDD]Computing topic seven, partition 0 offsets 11715 -> 11724
[INFO][2018-05-24 19:43:30,098][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 19:43:30,098][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 19:43:30,098][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 19:43:30,394][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_1_piece0 on 192.168.0.102:65258 in memory (size: 2008.0 B, free: 912.3 MB)
[INFO][2018-05-24 19:43:30,398][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_0_piece0 on 192.168.0.102:65258 in memory (size: 2007.0 B, free: 912.3 MB)
[INFO][2018-05-24 19:43:35,626][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 19:43:35,627][org.apache.spark.executor.Executor]Finished task 0.0 in stage 2.0 (TID 2). 794 bytes result sent to driver
[INFO][2018-05-24 19:43:35,628][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 2.0 (TID 2) in 5540 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 19:43:35,628][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 19:43:35,629][org.apache.spark.scheduler.DAGScheduler]ResultStage 2 (foreachPartition at ReceiveKafkaData.scala:64) finished in 5.541 s
[INFO][2018-05-24 19:43:35,629][org.apache.spark.scheduler.DAGScheduler]Job 2 finished: foreachPartition at ReceiveKafkaData.scala:64, took 5.552288 s
[INFO][2018-05-24 19:43:35,630][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527162200000 ms.0 from job set of time 1527162200000 ms
[INFO][2018-05-24 19:43:35,630][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 3 from persistence list
[INFO][2018-05-24 19:43:35,630][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 15.630 s for time 1527162200000 ms (execution: 5.564 s)
[INFO][2018-05-24 19:43:35,631][org.apache.spark.storage.BlockManager]Removing RDD 3
[INFO][2018-05-24 19:43:35,631][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 2 from persistence list
[INFO][2018-05-24 19:43:35,631][org.apache.spark.storage.BlockManager]Removing RDD 2
[INFO][2018-05-24 19:43:35,632][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 19:43:35,632][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527162160000 ms
[INFO][2018-05-24 19:43:45,082][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527162220000 ms
[INFO][2018-05-24 19:43:45,082][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527162220000 ms.0 from job set of time 1527162220000 ms
[INFO][2018-05-24 19:43:45,090][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:64
[INFO][2018-05-24 19:43:45,090][org.apache.spark.scheduler.DAGScheduler]Got job 3 (foreachPartition at ReceiveKafkaData.scala:64) with 1 output partitions
[INFO][2018-05-24 19:43:45,091][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 3 (foreachPartition at ReceiveKafkaData.scala:64)
[INFO][2018-05-24 19:43:45,091][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 19:43:45,092][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 19:43:45,093][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 3 (MapPartitionsRDD[7] at map at ReceiveKafkaData.scala:62), which has no missing parents
[INFO][2018-05-24 19:43:45,096][org.apache.spark.storage.memory.MemoryStore]Block broadcast_3 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-24 19:43:45,098][org.apache.spark.storage.memory.MemoryStore]Block broadcast_3_piece0 stored as bytes in memory (estimated size 2008.0 B, free 912.3 MB)
[INFO][2018-05-24 19:43:45,098][org.apache.spark.storage.BlockManagerInfo]Added broadcast_3_piece0 in memory on 192.168.0.102:65258 (size: 2008.0 B, free: 912.3 MB)
[INFO][2018-05-24 19:43:45,099][org.apache.spark.SparkContext]Created broadcast 3 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 19:43:45,100][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[7] at map at ReceiveKafkaData.scala:62) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 19:43:45,100][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 3.0 with 1 tasks
[INFO][2018-05-24 19:43:45,101][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 3.0 (TID 3, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 19:43:45,101][org.apache.spark.executor.Executor]Running task 0.0 in stage 3.0 (TID 3)
[INFO][2018-05-24 19:43:45,104][org.apache.spark.streaming.kafka.KafkaRDD]Computing topic seven, partition 0 offsets 11724 -> 11738
[INFO][2018-05-24 19:43:45,104][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 19:43:45,104][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 19:43:45,104][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 19:43:45,205][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 19:43:45,206][org.apache.spark.executor.Executor]Finished task 0.0 in stage 3.0 (TID 3). 708 bytes result sent to driver
[INFO][2018-05-24 19:43:45,208][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 3.0 (TID 3) in 106 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 19:43:45,208][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 3.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 19:43:45,208][org.apache.spark.scheduler.DAGScheduler]ResultStage 3 (foreachPartition at ReceiveKafkaData.scala:64) finished in 0.108 s
[INFO][2018-05-24 19:43:45,209][org.apache.spark.scheduler.DAGScheduler]Job 3 finished: foreachPartition at ReceiveKafkaData.scala:64, took 0.119045 s
[INFO][2018-05-24 19:43:45,209][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527162220000 ms.0 from job set of time 1527162220000 ms
[INFO][2018-05-24 19:43:45,210][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 5 from persistence list
[INFO][2018-05-24 19:43:45,210][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 5.209 s for time 1527162220000 ms (execution: 0.127 s)
[INFO][2018-05-24 19:43:45,210][org.apache.spark.storage.BlockManager]Removing RDD 5
[INFO][2018-05-24 19:43:45,210][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 4 from persistence list
[INFO][2018-05-24 19:43:45,210][org.apache.spark.storage.BlockManager]Removing RDD 4
[INFO][2018-05-24 19:43:45,211][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 19:43:45,211][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527162180000 ms
[INFO][2018-05-24 19:44:05,097][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527162240000 ms
[INFO][2018-05-24 19:44:05,097][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527162240000 ms.0 from job set of time 1527162240000 ms
[INFO][2018-05-24 19:44:05,107][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:64
[INFO][2018-05-24 19:44:05,108][org.apache.spark.scheduler.DAGScheduler]Got job 4 (foreachPartition at ReceiveKafkaData.scala:64) with 1 output partitions
[INFO][2018-05-24 19:44:05,109][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 4 (foreachPartition at ReceiveKafkaData.scala:64)
[INFO][2018-05-24 19:44:05,109][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 19:44:05,109][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 19:44:05,109][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 4 (MapPartitionsRDD[9] at map at ReceiveKafkaData.scala:62), which has no missing parents
[INFO][2018-05-24 19:44:05,112][org.apache.spark.storage.memory.MemoryStore]Block broadcast_4 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-24 19:44:05,114][org.apache.spark.storage.memory.MemoryStore]Block broadcast_4_piece0 stored as bytes in memory (estimated size 2008.0 B, free 912.3 MB)
[INFO][2018-05-24 19:44:05,115][org.apache.spark.storage.BlockManagerInfo]Added broadcast_4_piece0 in memory on 192.168.0.102:65258 (size: 2008.0 B, free: 912.3 MB)
[INFO][2018-05-24 19:44:05,116][org.apache.spark.SparkContext]Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 19:44:05,116][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[9] at map at ReceiveKafkaData.scala:62) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 19:44:05,117][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 4.0 with 1 tasks
[INFO][2018-05-24 19:44:05,118][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 4.0 (TID 4, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 19:44:05,118][org.apache.spark.executor.Executor]Running task 0.0 in stage 4.0 (TID 4)
[INFO][2018-05-24 19:44:05,121][org.apache.spark.streaming.kafka.KafkaRDD]Computing topic seven, partition 0 offsets 11738 -> 11758
[INFO][2018-05-24 19:44:05,122][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 19:44:05,122][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 19:44:05,122][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 19:44:05,215][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 19:44:05,216][org.apache.spark.executor.Executor]Finished task 0.0 in stage 4.0 (TID 4). 708 bytes result sent to driver
[INFO][2018-05-24 19:44:05,218][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 4.0 (TID 4) in 101 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 19:44:05,218][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 4.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 19:44:05,219][org.apache.spark.scheduler.DAGScheduler]ResultStage 4 (foreachPartition at ReceiveKafkaData.scala:64) finished in 0.102 s
[INFO][2018-05-24 19:44:05,219][org.apache.spark.scheduler.DAGScheduler]Job 4 finished: foreachPartition at ReceiveKafkaData.scala:64, took 0.112135 s
[INFO][2018-05-24 19:44:05,220][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527162240000 ms.0 from job set of time 1527162240000 ms
[INFO][2018-05-24 19:44:05,220][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 7 from persistence list
[INFO][2018-05-24 19:44:05,220][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 5.220 s for time 1527162240000 ms (execution: 0.123 s)
[INFO][2018-05-24 19:44:05,221][org.apache.spark.storage.BlockManager]Removing RDD 7
[INFO][2018-05-24 19:44:05,221][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 6 from persistence list
[INFO][2018-05-24 19:44:05,221][org.apache.spark.storage.BlockManager]Removing RDD 6
[INFO][2018-05-24 19:44:05,221][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 19:44:05,221][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527162200000 ms
[INFO][2018-05-24 19:44:25,065][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527162260000 ms
[INFO][2018-05-24 19:44:25,065][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527162260000 ms.0 from job set of time 1527162260000 ms
[INFO][2018-05-24 19:44:25,073][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:64
[INFO][2018-05-24 19:44:25,074][org.apache.spark.scheduler.DAGScheduler]Got job 5 (foreachPartition at ReceiveKafkaData.scala:64) with 1 output partitions
[INFO][2018-05-24 19:44:25,074][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 5 (foreachPartition at ReceiveKafkaData.scala:64)
[INFO][2018-05-24 19:44:25,074][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 19:44:25,074][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 19:44:25,074][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 5 (MapPartitionsRDD[11] at map at ReceiveKafkaData.scala:62), which has no missing parents
[INFO][2018-05-24 19:44:25,076][org.apache.spark.storage.memory.MemoryStore]Block broadcast_5 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-24 19:44:25,077][org.apache.spark.storage.memory.MemoryStore]Block broadcast_5_piece0 stored as bytes in memory (estimated size 2008.0 B, free 912.3 MB)
[INFO][2018-05-24 19:44:25,078][org.apache.spark.storage.BlockManagerInfo]Added broadcast_5_piece0 in memory on 192.168.0.102:65258 (size: 2008.0 B, free: 912.3 MB)
[INFO][2018-05-24 19:44:25,079][org.apache.spark.SparkContext]Created broadcast 5 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 19:44:25,080][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[11] at map at ReceiveKafkaData.scala:62) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 19:44:25,080][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 5.0 with 1 tasks
[INFO][2018-05-24 19:44:25,081][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 5.0 (TID 5, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 19:44:25,081][org.apache.spark.executor.Executor]Running task 0.0 in stage 5.0 (TID 5)
[INFO][2018-05-24 19:44:25,083][org.apache.spark.streaming.kafka.KafkaRDD]Computing topic seven, partition 0 offsets 11758 -> 11778
[INFO][2018-05-24 19:44:25,083][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 19:44:25,083][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 19:44:25,083][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 19:44:25,163][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 19:44:25,164][org.apache.spark.executor.Executor]Finished task 0.0 in stage 5.0 (TID 5). 708 bytes result sent to driver
[INFO][2018-05-24 19:44:25,165][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 5.0 (TID 5) in 85 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 19:44:25,165][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 5.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 19:44:25,166][org.apache.spark.scheduler.DAGScheduler]ResultStage 5 (foreachPartition at ReceiveKafkaData.scala:64) finished in 0.085 s
[INFO][2018-05-24 19:44:25,166][org.apache.spark.scheduler.DAGScheduler]Job 5 finished: foreachPartition at ReceiveKafkaData.scala:64, took 0.092669 s
[INFO][2018-05-24 19:44:25,166][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527162260000 ms.0 from job set of time 1527162260000 ms
[INFO][2018-05-24 19:44:25,167][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 5.166 s for time 1527162260000 ms (execution: 0.101 s)
[INFO][2018-05-24 19:44:25,167][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 9 from persistence list
[INFO][2018-05-24 19:44:25,167][org.apache.spark.storage.BlockManager]Removing RDD 9
[INFO][2018-05-24 19:44:25,167][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 8 from persistence list
[INFO][2018-05-24 19:44:25,168][org.apache.spark.storage.BlockManager]Removing RDD 8
[INFO][2018-05-24 19:44:25,168][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 19:44:25,168][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527162220000 ms
[INFO][2018-05-24 19:44:50,080][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527162280000 ms
[INFO][2018-05-24 19:44:50,080][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527162280000 ms.0 from job set of time 1527162280000 ms
[INFO][2018-05-24 19:44:50,088][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:64
[INFO][2018-05-24 19:44:50,089][org.apache.spark.scheduler.DAGScheduler]Got job 6 (foreachPartition at ReceiveKafkaData.scala:64) with 1 output partitions
[INFO][2018-05-24 19:44:50,090][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 6 (foreachPartition at ReceiveKafkaData.scala:64)
[INFO][2018-05-24 19:44:50,090][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 19:44:50,090][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 19:44:50,090][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 6 (MapPartitionsRDD[13] at map at ReceiveKafkaData.scala:62), which has no missing parents
[INFO][2018-05-24 19:44:50,093][org.apache.spark.storage.memory.MemoryStore]Block broadcast_6 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-24 19:44:50,095][org.apache.spark.storage.memory.MemoryStore]Block broadcast_6_piece0 stored as bytes in memory (estimated size 2008.0 B, free 912.3 MB)
[INFO][2018-05-24 19:44:50,096][org.apache.spark.storage.BlockManagerInfo]Added broadcast_6_piece0 in memory on 192.168.0.102:65258 (size: 2008.0 B, free: 912.3 MB)
[INFO][2018-05-24 19:44:50,096][org.apache.spark.SparkContext]Created broadcast 6 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 19:44:50,097][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[13] at map at ReceiveKafkaData.scala:62) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 19:44:50,097][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 6.0 with 1 tasks
[INFO][2018-05-24 19:44:50,098][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 6.0 (TID 6, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 19:44:50,098][org.apache.spark.executor.Executor]Running task 0.0 in stage 6.0 (TID 6)
[INFO][2018-05-24 19:44:50,101][org.apache.spark.streaming.kafka.KafkaRDD]Computing topic seven, partition 0 offsets 11778 -> 11803
[INFO][2018-05-24 19:44:50,101][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 19:44:50,101][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 19:44:50,101][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 19:44:50,196][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 19:44:50,197][org.apache.spark.executor.Executor]Finished task 0.0 in stage 6.0 (TID 6). 708 bytes result sent to driver
[INFO][2018-05-24 19:44:50,198][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 6.0 (TID 6) in 101 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 19:44:50,199][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 6.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 19:44:50,199][org.apache.spark.scheduler.DAGScheduler]ResultStage 6 (foreachPartition at ReceiveKafkaData.scala:64) finished in 0.102 s
[INFO][2018-05-24 19:44:50,200][org.apache.spark.scheduler.DAGScheduler]Job 6 finished: foreachPartition at ReceiveKafkaData.scala:64, took 0.110921 s
[INFO][2018-05-24 19:44:50,200][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527162280000 ms.0 from job set of time 1527162280000 ms
[INFO][2018-05-24 19:44:50,201][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 11 from persistence list
[INFO][2018-05-24 19:44:50,201][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 10.200 s for time 1527162280000 ms (execution: 0.120 s)
[INFO][2018-05-24 19:44:50,201][org.apache.spark.storage.BlockManager]Removing RDD 11
[INFO][2018-05-24 19:44:50,201][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 10 from persistence list
[INFO][2018-05-24 19:44:50,201][org.apache.spark.storage.BlockManager]Removing RDD 10
[INFO][2018-05-24 19:44:50,202][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 19:44:50,202][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527162240000 ms
[INFO][2018-05-24 19:44:50,809][org.apache.spark.util.ShutdownHookManager]Shutdown hook called
[INFO][2018-05-24 19:44:50,824][org.apache.spark.util.ShutdownHookManager]Deleting directory /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/spark-99af3a70-1ecb-4ade-808f-51565fd0a471
[INFO][2018-05-24 19:44:50,837][org.apache.spark.streaming.StreamingContext]Invoking stop(stopGracefully=false) from shutdown hook
[INFO][2018-05-24 19:44:50,841][org.apache.spark.streaming.scheduler.ReceiverTracker]ReceiverTracker stopped
[INFO][2018-05-24 19:44:50,842][org.apache.spark.streaming.scheduler.JobGenerator]Stopping JobGenerator immediately
[INFO][2018-05-24 19:44:50,844][org.apache.spark.streaming.util.RecurringTimer]Stopped timer for JobGenerator after time 1527162280000
[INFO][2018-05-24 19:44:50,847][org.apache.spark.streaming.scheduler.JobGenerator]Stopped JobGenerator
[INFO][2018-05-24 19:44:50,851][org.apache.spark.streaming.scheduler.JobScheduler]Stopped JobScheduler
[INFO][2018-05-24 19:44:50,857][org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@4eb30d44{/streaming,null,UNAVAILABLE,@Spark}
[INFO][2018-05-24 19:44:50,858][org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@351e414e{/streaming/batch,null,UNAVAILABLE,@Spark}
[INFO][2018-05-24 19:44:50,861][org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@327c7bea{/static/streaming,null,UNAVAILABLE,@Spark}
[INFO][2018-05-24 19:44:50,863][org.apache.spark.streaming.StreamingContext]StreamingContext stopped successfully
[INFO][2018-05-24 19:44:50,864][org.apache.spark.SparkContext]Invoking stop() from shutdown hook
[INFO][2018-05-24 19:44:50,874][org.spark_project.jetty.server.AbstractConnector]Stopped Spark@68d6972f{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-05-24 19:44:50,875][org.apache.spark.ui.SparkUI]Stopped Spark web UI at http://192.168.0.102:4040
[INFO][2018-05-24 19:44:50,883][org.apache.spark.MapOutputTrackerMasterEndpoint]MapOutputTrackerMasterEndpoint stopped!
[INFO][2018-05-24 19:44:50,901][org.apache.spark.storage.memory.MemoryStore]MemoryStore cleared
[INFO][2018-05-24 19:44:50,901][org.apache.spark.storage.BlockManager]BlockManager stopped
[INFO][2018-05-24 19:44:50,902][org.apache.spark.storage.BlockManagerMaster]BlockManagerMaster stopped
[INFO][2018-05-24 19:44:50,904][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint]OutputCommitCoordinator stopped!
[INFO][2018-05-24 19:44:50,905][org.apache.spark.SparkContext]Successfully stopped SparkContext
[INFO][2018-05-24 19:44:50,906][org.apache.spark.util.ShutdownHookManager]Shutdown hook called
[INFO][2018-05-24 19:44:50,907][org.apache.spark.util.ShutdownHookManager]Deleting directory /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/spark-d62e3a13-4a24-4f08-a643-44b98a9d5ee1
[INFO][2018-05-24 19:46:27,765][org.apache.spark.SparkContext]Running Spark version 2.2.0
[INFO][2018-05-24 19:46:28,443][org.apache.spark.SparkContext]Submitted application: ReceiveKafkaData$
[INFO][2018-05-24 19:46:28,461][org.apache.spark.SecurityManager]Changing view acls to: seven
[INFO][2018-05-24 19:46:28,462][org.apache.spark.SecurityManager]Changing modify acls to: seven
[INFO][2018-05-24 19:46:28,463][org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO][2018-05-24 19:46:28,463][org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO][2018-05-24 19:46:28,464][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(seven); groups with view permissions: Set(); users  with modify permissions: Set(seven); groups with modify permissions: Set()
[INFO][2018-05-24 19:46:28,734][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 65342.
[INFO][2018-05-24 19:46:28,756][org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO][2018-05-24 19:46:28,772][org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO][2018-05-24 19:46:28,775][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2018-05-24 19:46:28,775][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
[INFO][2018-05-24 19:46:28,785][org.apache.spark.storage.DiskBlockManager]Created local directory at /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/blockmgr-11348ce1-31b7-48fd-a838-232d0892714b
[INFO][2018-05-24 19:46:28,799][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 912.3 MB
[INFO][2018-05-24 19:46:28,877][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO][2018-05-24 19:46:28,944][org.spark_project.jetty.util.log]Logging initialized @2099ms
[INFO][2018-05-24 19:46:29,009][org.spark_project.jetty.server.Server]jetty-9.3.z-SNAPSHOT
[INFO][2018-05-24 19:46:29,022][org.spark_project.jetty.server.Server]Started @2178ms
[INFO][2018-05-24 19:46:29,041][org.spark_project.jetty.server.AbstractConnector]Started ServerConnector@2090b9b9{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-05-24 19:46:29,042][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4040.
[INFO][2018-05-24 19:46:29,067][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@757d6814{/jobs,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:46:29,068][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@119f1f2a{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:46:29,068][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5b970f7{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:46:29,069][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6175619b{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:46:29,071][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@756cf158{/stages,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:46:29,072][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@751d3241{/stages/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:46:29,073][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@64337702{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:46:29,074][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4e76dac{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:46:29,075][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5f2f577{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:46:29,076][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5d465e4b{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:46:29,076][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@41a90fa8{/storage,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:46:29,077][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@52500920{/storage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:46:29,078][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@18a3962d{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:46:29,078][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2a65bb85{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:46:29,079][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4f936da8{/environment,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:46:29,081][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@452ba1db{/environment/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:46:29,083][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@76a36b71{/executors,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:46:29,084][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@f9d87b{/executors/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:46:29,084][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@26fb628{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:46:29,085][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@70dd7e15{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:46:29,100][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@35beb15e{/static,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:46:29,100][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6a62689d{/,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:46:29,104][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@60fa3495{/api,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:46:29,105][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1ca25c47{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:46:29,105][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@533b266e{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:46:29,107][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://192.168.0.102:4040
[INFO][2018-05-24 19:46:29,217][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
[INFO][2018-05-24 19:46:29,239][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 65343.
[INFO][2018-05-24 19:46:29,244][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 192.168.0.102:65343
[INFO][2018-05-24 19:46:29,246][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2018-05-24 19:46:29,248][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 192.168.0.102, 65343, None)
[INFO][2018-05-24 19:46:29,257][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 192.168.0.102:65343 with 912.3 MB RAM, BlockManagerId(driver, 192.168.0.102, 65343, None)
[INFO][2018-05-24 19:46:29,263][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 192.168.0.102, 65343, None)
[INFO][2018-05-24 19:46:29,265][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 192.168.0.102, 65343, None)
[INFO][2018-05-24 19:46:29,464][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7d2a6eac{/metrics/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:46:29,611][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 19:46:29,614][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 19:46:29,614][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 19:46:40,019][org.apache.spark.streaming.kafka.DirectKafkaInputDStream]Slide time = 20000 ms
[INFO][2018-05-24 19:46:40,020][org.apache.spark.streaming.kafka.DirectKafkaInputDStream]Storage level = Serialized 1x Replicated
[INFO][2018-05-24 19:46:40,020][org.apache.spark.streaming.kafka.DirectKafkaInputDStream]Checkpoint interval = null
[INFO][2018-05-24 19:46:40,021][org.apache.spark.streaming.kafka.DirectKafkaInputDStream]Remember interval = 20000 ms
[INFO][2018-05-24 19:46:40,022][org.apache.spark.streaming.kafka.DirectKafkaInputDStream]Initialized and validated org.apache.spark.streaming.kafka.DirectKafkaInputDStream@72c23843
[INFO][2018-05-24 19:46:40,022][org.apache.spark.streaming.dstream.MappedDStream]Slide time = 20000 ms
[INFO][2018-05-24 19:46:40,023][org.apache.spark.streaming.dstream.MappedDStream]Storage level = Serialized 1x Replicated
[INFO][2018-05-24 19:46:40,023][org.apache.spark.streaming.dstream.MappedDStream]Checkpoint interval = null
[INFO][2018-05-24 19:46:40,023][org.apache.spark.streaming.dstream.MappedDStream]Remember interval = 20000 ms
[INFO][2018-05-24 19:46:40,023][org.apache.spark.streaming.dstream.MappedDStream]Initialized and validated org.apache.spark.streaming.dstream.MappedDStream@2794e4a7
[INFO][2018-05-24 19:46:40,023][org.apache.spark.streaming.dstream.ForEachDStream]Slide time = 20000 ms
[INFO][2018-05-24 19:46:40,023][org.apache.spark.streaming.dstream.ForEachDStream]Storage level = Serialized 1x Replicated
[INFO][2018-05-24 19:46:40,023][org.apache.spark.streaming.dstream.ForEachDStream]Checkpoint interval = null
[INFO][2018-05-24 19:46:40,024][org.apache.spark.streaming.dstream.ForEachDStream]Remember interval = 20000 ms
[INFO][2018-05-24 19:46:40,024][org.apache.spark.streaming.dstream.ForEachDStream]Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@179dc7ca
[INFO][2018-05-24 19:46:40,066][org.apache.spark.streaming.util.RecurringTimer]Started timer for JobGenerator at time 1527162420000
[INFO][2018-05-24 19:46:40,067][org.apache.spark.streaming.scheduler.JobGenerator]Started JobGenerator at 1527162420000 ms
[INFO][2018-05-24 19:46:40,069][org.apache.spark.streaming.scheduler.JobScheduler]Started JobScheduler
[INFO][2018-05-24 19:46:40,073][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3910fe11{/streaming,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:46:40,073][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@351e414e{/streaming/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:46:40,074][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2c2db130{/streaming/batch,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:46:40,074][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@348d18a3{/streaming/batch/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:46:40,077][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@20e6c4dc{/static/streaming,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:46:40,077][org.apache.spark.streaming.StreamingContext]StreamingContext started
[INFO][2018-05-24 19:47:00,045][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 19:47:00,046][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 19:47:00,046][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 19:47:00,126][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527162420000 ms
[INFO][2018-05-24 19:47:00,129][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527162420000 ms.0 from job set of time 1527162420000 ms
[INFO][2018-05-24 19:47:00,169][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:64
[INFO][2018-05-24 19:47:00,180][org.apache.spark.scheduler.DAGScheduler]Got job 0 (foreachPartition at ReceiveKafkaData.scala:64) with 1 output partitions
[INFO][2018-05-24 19:47:00,181][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 0 (foreachPartition at ReceiveKafkaData.scala:64)
[INFO][2018-05-24 19:47:00,181][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 19:47:00,182][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 19:47:00,191][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 0 (MapPartitionsRDD[1] at map at ReceiveKafkaData.scala:62), which has no missing parents
[INFO][2018-05-24 19:47:00,298][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-24 19:47:00,329][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0_piece0 stored as bytes in memory (estimated size 2007.0 B, free 912.3 MB)
[INFO][2018-05-24 19:47:00,330][org.apache.spark.storage.BlockManagerInfo]Added broadcast_0_piece0 in memory on 192.168.0.102:65343 (size: 2007.0 B, free: 912.3 MB)
[INFO][2018-05-24 19:47:00,334][org.apache.spark.SparkContext]Created broadcast 0 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 19:47:00,353][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at map at ReceiveKafkaData.scala:62) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 19:47:00,354][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 0.0 with 1 tasks
[INFO][2018-05-24 19:47:00,397][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 19:47:00,413][org.apache.spark.executor.Executor]Running task 0.0 in stage 0.0 (TID 0)
[INFO][2018-05-24 19:47:00,442][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 11804 is the same as ending offset skipping seven 0
[INFO][2018-05-24 19:47:00,617][org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper]Process identifier=hconnection-0x3ca35b55 connecting to ZooKeeper ensemble=vm-xaj-bigdata-da-d01:2181,vm-xaj-bigdata-da-d02:2181,vm-xaj-bigdata-da-d03:2181
[INFO][2018-05-24 19:47:00,631][org.apache.zookeeper.ZooKeeper]Client environment:zookeeper.version=3.4.5-cdh5.7.0--1, built on 04/06/2016 14:24 GMT
[INFO][2018-05-24 19:47:00,631][org.apache.zookeeper.ZooKeeper]Client environment:host.name=192.168.0.102
[INFO][2018-05-24 19:47:00,631][org.apache.zookeeper.ZooKeeper]Client environment:java.version=1.8.0_161
[INFO][2018-05-24 19:47:00,631][org.apache.zookeeper.ZooKeeper]Client environment:java.vendor=Oracle Corporation
[INFO][2018-05-24 19:47:00,631][org.apache.zookeeper.ZooKeeper]Client environment:java.home=/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre
[INFO][2018-05-24 19:47:00,631][org.apache.zookeeper.ZooKeeper]Client environment:java.class.path=/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/charsets.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/deploy.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/cldrdata.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/dnsns.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/jaccess.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/jfxrt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/localedata.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/nashorn.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/sunec.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/sunjce_provider.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/sunpkcs11.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/zipfs.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/javaws.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/jce.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/jfr.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/jfxswt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/jsse.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/management-agent.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/plugin.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/resources.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/rt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/ant-javafx.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/dt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/javafx-mx.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/jconsole.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/packager.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/sa-jdi.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/tools.jar:/Users/seven/project/scala/dataMining/target/classes:/Users/seven/software/maven/repository/org/scala-lang/scala-library/2.11.12/scala-library-2.11.12.jar:/Users/seven/software/maven/repository/org/scala-lang/scala-compiler/2.11.12/scala-compiler-2.11.12.jar:/Users/seven/software/maven/repository/org/scala-lang/modules/scala-xml_2.11/1.0.5/scala-xml_2.11-1.0.5.jar:/Users/seven/software/maven/repository/org/scala-lang/modules/scala-parser-combinators_2.11/1.0.4/scala-parser-combinators_2.11-1.0.4.jar:/Users/seven/software/maven/repository/org/scala-lang/scala-reflect/2.11.12/scala-reflect-2.11.12.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-client/2.6.0-cdh5.7.0/hadoop-client-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-common/2.6.0-cdh5.7.0/hadoop-common-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/xmlenc/xmlenc/0.52/xmlenc-0.52.jar:/Users/seven/software/maven/repository/commons-net/commons-net/3.1/commons-net-3.1.jar:/Users/seven/software/maven/repository/commons-configuration/commons-configuration/1.6/commons-configuration-1.6.jar:/Users/seven/software/maven/repository/commons-digester/commons-digester/1.8/commons-digester-1.8.jar:/Users/seven/software/maven/repository/commons-beanutils/commons-beanutils/1.7.0/commons-beanutils-1.7.0.jar:/Users/seven/software/maven/repository/org/slf4j/slf4j-api/1.7.5/slf4j-api-1.7.5.jar:/Users/seven/software/maven/repository/org/slf4j/slf4j-log4j12/1.7.5/slf4j-log4j12-1.7.5.jar:/Users/seven/software/maven/repository/org/apache/avro/avro/1.7.6-cdh5.7.0/avro-1.7.6-cdh5.7.0.jar:/Users/seven/software/maven/repository/com/google/code/gson/gson/2.2.4/gson-2.2.4.jar:/Users/seven/software/maven/repository/org/apache/curator/curator-client/2.7.1/curator-client-2.7.1.jar:/Users/seven/software/maven/repository/org/apache/curator/curator-recipes/2.7.1/curator-recipes-2.7.1.jar:/Users/seven/software/maven/repository/com/google/code/findbugs/jsr305/3.0.0/jsr305-3.0.0.jar:/Users/seven/software/maven/repository/org/apache/htrace/htrace-core4/4.0.1-incubating/htrace-core4-4.0.1-incubating.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-compress/1.4.1/commons-compress-1.4.1.jar:/Users/seven/software/maven/repository/org/tukaani/xz/1.0/xz-1.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-hdfs/2.6.0-cdh5.7.0/hadoop-hdfs-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/io/netty/netty/3.6.2.Final/netty-3.6.2.Final.jar:/Users/seven/software/maven/repository/xerces/xercesImpl/2.9.1/xercesImpl-2.9.1.jar:/Users/seven/software/maven/repository/xml-apis/xml-apis/1.3.04/xml-apis-1.3.04.jar:/Users/seven/software/maven/repository/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-app/2.6.0-cdh5.7.0/hadoop-mapreduce-client-app-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-common/2.6.0-cdh5.7.0/hadoop-mapreduce-client-common-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-yarn-client/2.6.0-cdh5.7.0/hadoop-yarn-client-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-yarn-server-common/2.6.0-cdh5.7.0/hadoop-yarn-server-common-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-shuffle/2.6.0-cdh5.7.0/hadoop-mapreduce-client-shuffle-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-yarn-api/2.6.0-cdh5.7.0/hadoop-yarn-api-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-core/2.6.0-cdh5.7.0/hadoop-mapreduce-client-core-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-yarn-common/2.6.0-cdh5.7.0/hadoop-yarn-common-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/javax/xml/bind/jaxb-api/2.2.2/jaxb-api-2.2.2.jar:/Users/seven/software/maven/repository/javax/xml/stream/stax-api/1.0-2/stax-api-1.0-2.jar:/Users/seven/software/maven/repository/javax/activation/activation/1.1/activation-1.1.jar:/Users/seven/software/maven/repository/com/sun/jersey/jersey-client/1.9/jersey-client-1.9.jar:/Users/seven/software/maven/repository/org/codehaus/jackson/jackson-xc/1.8.8/jackson-xc-1.8.8.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.6.0-cdh5.7.0/hadoop-mapreduce-client-jobclient-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-aws/2.6.0-cdh5.7.0/hadoop-aws-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/com/amazonaws/aws-java-sdk-s3/1.10.6/aws-java-sdk-s3-1.10.6.jar:/Users/seven/software/maven/repository/com/amazonaws/aws-java-sdk-kms/1.10.6/aws-java-sdk-kms-1.10.6.jar:/Users/seven/software/maven/repository/com/amazonaws/aws-java-sdk-core/1.10.6/aws-java-sdk-core-1.10.6.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-annotations/2.6.0-cdh5.7.0/hadoop-annotations-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-client/1.2.0-cdh5.7.0/hbase-client-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-annotations/1.2.0-cdh5.7.0/hbase-annotations-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-common/1.2.0-cdh5.7.0/hbase-common-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-protocol/1.2.0-cdh5.7.0/hbase-protocol-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/commons-codec/commons-codec/1.9/commons-codec-1.9.jar:/Users/seven/software/maven/repository/commons-io/commons-io/2.4/commons-io-2.4.jar:/Users/seven/software/maven/repository/commons-lang/commons-lang/2.6/commons-lang-2.6.jar:/Users/seven/software/maven/repository/commons-logging/commons-logging/1.2/commons-logging-1.2.jar:/Users/seven/software/maven/repository/com/google/guava/guava/12.0.1/guava-12.0.1.jar:/Users/seven/software/maven/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/Users/seven/software/maven/repository/io/netty/netty-all/4.0.23.Final/netty-all-4.0.23.Final.jar:/Users/seven/software/maven/repository/org/apache/zookeeper/zookeeper/3.4.5-cdh5.7.0/zookeeper-3.4.5-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/htrace/htrace-core/3.2.0-incubating/htrace-core-3.2.0-incubating.jar:/Users/seven/software/maven/repository/org/codehaus/jackson/jackson-mapper-asl/1.8.8/jackson-mapper-asl-1.8.8.jar:/Users/seven/software/maven/repository/org/jruby/jcodings/jcodings/1.0.8/jcodings-1.0.8.jar:/Users/seven/software/maven/repository/org/jruby/joni/joni/2.1.2/joni-2.1.2.jar:/Users/seven/software/maven/repository/com/yammer/metrics/metrics-core/2.2.0/metrics-core-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-auth/2.6.0-cdh5.7.0/hadoop-auth-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/httpcomponents/httpclient/4.2.5/httpclient-4.2.5.jar:/Users/seven/software/maven/repository/org/apache/httpcomponents/httpcore/4.2.4/httpcore-4.2.4.jar:/Users/seven/software/maven/repository/org/apache/directory/server/apacheds-kerberos-codec/2.0.0-M15/apacheds-kerberos-codec-2.0.0-M15.jar:/Users/seven/software/maven/repository/org/apache/directory/server/apacheds-i18n/2.0.0-M15/apacheds-i18n-2.0.0-M15.jar:/Users/seven/software/maven/repository/org/apache/directory/api/api-asn1-api/1.0.0-M20/api-asn1-api-1.0.0-M20.jar:/Users/seven/software/maven/repository/org/apache/directory/api/api-util/1.0.0-M20/api-util-1.0.0-M20.jar:/Users/seven/software/maven/repository/org/apache/curator/curator-framework/2.7.1/curator-framework-2.7.1.jar:/Users/seven/software/maven/repository/com/github/stephenc/findbugs/findbugs-annotations/1.3.9-1/findbugs-annotations-1.3.9-1.jar:/Users/seven/software/maven/repository/junit/junit/4.12/junit-4.12.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-server/1.2.0-cdh5.7.0/hbase-server-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-procedure/1.2.0-cdh5.7.0/hbase-procedure-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-common/1.2.0-cdh5.7.0/hbase-common-1.2.0-cdh5.7.0-tests.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-prefix-tree/1.2.0-cdh5.7.0/hbase-prefix-tree-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar:/Users/seven/software/maven/repository/commons-collections/commons-collections/3.2.2/commons-collections-3.2.2.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-hadoop-compat/1.2.0-cdh5.7.0/hbase-hadoop-compat-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-hadoop2-compat/1.2.0-cdh5.7.0/hbase-hadoop2-compat-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/com/sun/jersey/jersey-core/1.9/jersey-core-1.9.jar:/Users/seven/software/maven/repository/com/sun/jersey/jersey-server/1.9/jersey-server-1.9.jar:/Users/seven/software/maven/repository/asm/asm/3.1/asm-3.1.jar:/Users/seven/software/maven/repository/commons-cli/commons-cli/1.2/commons-cli-1.2.jar:/Users/seven/software/maven/repository/com/github/stephenc/high-scale-lib/high-scale-lib/1.1.1/high-scale-lib-1.1.1.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-math/2.1/commons-math-2.1.jar:/Users/seven/software/maven/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jetty/6.1.26.cloudera.4/jetty-6.1.26.cloudera.4.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jetty-util/6.1.26.cloudera.4/jetty-util-6.1.26.cloudera.4.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jetty-sslengine/6.1.26.cloudera.4/jetty-sslengine-6.1.26.cloudera.4.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jsp-2.1/6.1.14/jsp-2.1-6.1.14.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jsp-api-2.1/6.1.14/jsp-api-2.1-6.1.14.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/servlet-api-2.5/6.1.14/servlet-api-2.5-6.1.14.jar:/Users/seven/software/maven/repository/org/codehaus/jackson/jackson-core-asl/1.8.8/jackson-core-asl-1.8.8.jar:/Users/seven/software/maven/repository/org/codehaus/jackson/jackson-jaxrs/1.8.8/jackson-jaxrs-1.8.8.jar:/Users/seven/software/maven/repository/tomcat/jasper-compiler/5.5.23/jasper-compiler-5.5.23.jar:/Users/seven/software/maven/repository/tomcat/jasper-runtime/5.5.23/jasper-runtime-5.5.23.jar:/Users/seven/software/maven/repository/commons-el/commons-el/1.0/commons-el-1.0.jar:/Users/seven/software/maven/repository/org/jamon/jamon-runtime/2.4.1/jamon-runtime-2.4.1.jar:/Users/seven/software/maven/repository/com/lmax/disruptor/3.3.0/disruptor-3.3.0.jar:/Users/seven/software/maven/repository/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar:/Users/seven/software/maven/repository/org/owasp/esapi/esapi/2.1.0/esapi-2.1.0.jar:/Users/seven/software/maven/repository/commons-beanutils/commons-beanutils-core/1.7.0/commons-beanutils-core-1.7.0.jar:/Users/seven/software/maven/repository/commons-fileupload/commons-fileupload/1.2/commons-fileupload-1.2.jar:/Users/seven/software/maven/repository/xom/xom/1.2.5/xom-1.2.5.jar:/Users/seven/software/maven/repository/xalan/xalan/2.7.0/xalan-2.7.0.jar:/Users/seven/software/maven/repository/org/beanshell/bsh-core/2.0b4/bsh-core-2.0b4.jar:/Users/seven/software/maven/repository/org/owasp/antisamy/antisamy/1.4.3/antisamy-1.4.3.jar:/Users/seven/software/maven/repository/org/apache/xmlgraphics/batik-css/1.7/batik-css-1.7.jar:/Users/seven/software/maven/repository/org/apache/xmlgraphics/batik-ext/1.7/batik-ext-1.7.jar:/Users/seven/software/maven/repository/org/apache/xmlgraphics/batik-util/1.7/batik-util-1.7.jar:/Users/seven/software/maven/repository/xml-apis/xml-apis-ext/1.3.04/xml-apis-ext-1.3.04.jar:/Users/seven/software/maven/repository/net/sourceforge/nekohtml/nekohtml/1.9.12/nekohtml-1.9.12.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-core/2.6.0-mr1-cdh5.7.0/hadoop-core-2.6.0-mr1-cdh5.7.0.jar:/Users/seven/software/maven/repository/javax/servlet/jsp/jsp-api/2.1/jsp-api-2.1.jar:/Users/seven/software/maven/repository/net/java/dev/jets3t/jets3t/0.6.1/jets3t-0.6.1.jar:/Users/seven/software/maven/repository/javax/servlet/servlet-api/2.5/servlet-api-2.5.jar:/Users/seven/software/maven/repository/hsqldb/hsqldb/1.8.0.10/hsqldb-1.8.0.10.jar:/Users/seven/software/maven/repository/org/eclipse/jdt/core/3.1.1/core-3.1.1.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-hdfs/2.6.0-cdh5.7.0/hadoop-hdfs-2.6.0-cdh5.7.0-tests.jar:/Users/seven/software/maven/repository/commons-daemon/commons-daemon/1.0.13/commons-daemon-1.0.13.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-lang3/3.5/commons-lang3-3.5.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-pool2/2.4.2/commons-pool2-2.4.2.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-mllib_2.11/2.2.0/spark-mllib_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-core_2.11/2.2.0/spark-core_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/avro/avro-mapred/1.7.7/avro-mapred-1.7.7-hadoop2.jar:/Users/seven/software/maven/repository/org/apache/avro/avro-ipc/1.7.7/avro-ipc-1.7.7-tests.jar:/Users/seven/software/maven/repository/com/twitter/chill_2.11/0.8.0/chill_2.11-0.8.0.jar:/Users/seven/software/maven/repository/com/esotericsoftware/kryo-shaded/3.0.3/kryo-shaded-3.0.3.jar:/Users/seven/software/maven/repository/com/esotericsoftware/minlog/1.3.0/minlog-1.3.0.jar:/Users/seven/software/maven/repository/org/objenesis/objenesis/2.1/objenesis-2.1.jar:/Users/seven/software/maven/repository/com/twitter/chill-java/0.8.0/chill-java-0.8.0.jar:/Users/seven/software/maven/repository/org/apache/xbean/xbean-asm5-shaded/4.4/xbean-asm5-shaded-4.4.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-launcher_2.11/2.2.0/spark-launcher_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-network-common_2.11/2.2.0/spark-network-common_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-network-shuffle_2.11/2.2.0/spark-network-shuffle_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-unsafe_2.11/2.2.0/spark-unsafe_2.11-2.2.0.jar:/Users/seven/software/maven/repository/javax/servlet/javax.servlet-api/3.1.0/javax.servlet-api-3.1.0.jar:/Users/seven/software/maven/repository/org/slf4j/jul-to-slf4j/1.7.16/jul-to-slf4j-1.7.16.jar:/Users/seven/software/maven/repository/org/slf4j/jcl-over-slf4j/1.7.16/jcl-over-slf4j-1.7.16.jar:/Users/seven/software/maven/repository/com/ning/compress-lzf/1.0.3/compress-lzf-1.0.3.jar:/Users/seven/software/maven/repository/org/xerial/snappy/snappy-java/1.1.2.6/snappy-java-1.1.2.6.jar:/Users/seven/software/maven/repository/org/roaringbitmap/RoaringBitmap/0.5.11/RoaringBitmap-0.5.11.jar:/Users/seven/software/maven/repository/org/json4s/json4s-jackson_2.11/3.2.11/json4s-jackson_2.11-3.2.11.jar:/Users/seven/software/maven/repository/org/json4s/json4s-core_2.11/3.2.11/json4s-core_2.11-3.2.11.jar:/Users/seven/software/maven/repository/org/json4s/json4s-ast_2.11/3.2.11/json4s-ast_2.11-3.2.11.jar:/Users/seven/software/maven/repository/org/scala-lang/scalap/2.11.0/scalap-2.11.0.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/core/jersey-client/2.22.2/jersey-client-2.22.2.jar:/Users/seven/software/maven/repository/javax/ws/rs/javax.ws.rs-api/2.0.1/javax.ws.rs-api-2.0.1.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/hk2-api/2.4.0-b34/hk2-api-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/hk2-utils/2.4.0-b34/hk2-utils-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/external/aopalliance-repackaged/2.4.0-b34/aopalliance-repackaged-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/external/javax.inject/2.4.0-b34/javax.inject-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/hk2-locator/2.4.0-b34/hk2-locator-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/javassist/javassist/3.18.1-GA/javassist-3.18.1-GA.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/core/jersey-common/2.22.2/jersey-common-2.22.2.jar:/Users/seven/software/maven/repository/javax/annotation/javax.annotation-api/1.2/javax.annotation-api-1.2.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/bundles/repackaged/jersey-guava/2.22.2/jersey-guava-2.22.2.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/osgi-resource-locator/1.0.1/osgi-resource-locator-1.0.1.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/core/jersey-server/2.22.2/jersey-server-2.22.2.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/media/jersey-media-jaxb/2.22.2/jersey-media-jaxb-2.22.2.jar:/Users/seven/software/maven/repository/javax/validation/validation-api/1.1.0.Final/validation-api-1.1.0.Final.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/containers/jersey-container-servlet/2.22.2/jersey-container-servlet-2.22.2.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/containers/jersey-container-servlet-core/2.22.2/jersey-container-servlet-core-2.22.2.jar:/Users/seven/software/maven/repository/com/clearspring/analytics/stream/2.7.0/stream-2.7.0.jar:/Users/seven/software/maven/repository/io/dropwizard/metrics/metrics-core/3.1.2/metrics-core-3.1.2.jar:/Users/seven/software/maven/repository/io/dropwizard/metrics/metrics-jvm/3.1.2/metrics-jvm-3.1.2.jar:/Users/seven/software/maven/repository/io/dropwizard/metrics/metrics-json/3.1.2/metrics-json-3.1.2.jar:/Users/seven/software/maven/repository/io/dropwizard/metrics/metrics-graphite/3.1.2/metrics-graphite-3.1.2.jar:/Users/seven/software/maven/repository/org/apache/ivy/ivy/2.4.0/ivy-2.4.0.jar:/Users/seven/software/maven/repository/oro/oro/2.0.8/oro-2.0.8.jar:/Users/seven/software/maven/repository/net/razorvine/pyrolite/4.13/pyrolite-4.13.jar:/Users/seven/software/maven/repository/net/sf/py4j/py4j/0.10.4/py4j-0.10.4.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-crypto/1.0.0/commons-crypto-1.0.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-streaming_2.11/2.2.0/spark-streaming_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-sql_2.11/2.2.0/spark-sql_2.11-2.2.0.jar:/Users/seven/software/maven/repository/com/univocity/univocity-parsers/2.2.1/univocity-parsers-2.2.1.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-sketch_2.11/2.2.0/spark-sketch_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-catalyst_2.11/2.2.0/spark-catalyst_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/codehaus/janino/janino/3.0.0/janino-3.0.0.jar:/Users/seven/software/maven/repository/org/codehaus/janino/commons-compiler/3.0.0/commons-compiler-3.0.0.jar:/Users/seven/software/maven/repository/org/antlr/antlr4-runtime/4.5.3/antlr4-runtime-4.5.3.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-column/1.8.2/parquet-column-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-common/1.8.2/parquet-common-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-encoding/1.8.2/parquet-encoding-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-hadoop/1.8.2/parquet-hadoop-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-format/2.3.1/parquet-format-2.3.1.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-jackson/1.8.2/parquet-jackson-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-graphx_2.11/2.2.0/spark-graphx_2.11-2.2.0.jar:/Users/seven/software/maven/repository/com/github/fommil/netlib/core/1.1.2/core-1.1.2.jar:/Users/seven/software/maven/repository/net/sourceforge/f2j/arpack_combined_all/0.1/arpack_combined_all-0.1.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-mllib-local_2.11/2.2.0/spark-mllib-local_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/scalanlp/breeze_2.11/0.13.1/breeze_2.11-0.13.1.jar:/Users/seven/software/maven/repository/org/scalanlp/breeze-macros_2.11/0.13.1/breeze-macros_2.11-0.13.1.jar:/Users/seven/software/maven/repository/net/sf/opencsv/opencsv/2.3/opencsv-2.3.jar:/Users/seven/software/maven/repository/com/github/rwl/jtransforms/2.4.0/jtransforms-2.4.0.jar:/Users/seven/software/maven/repository/org/spire-math/spire_2.11/0.13.0/spire_2.11-0.13.0.jar:/Users/seven/software/maven/repository/org/spire-math/spire-macros_2.11/0.13.0/spire-macros_2.11-0.13.0.jar:/Users/seven/software/maven/repository/org/typelevel/machinist_2.11/0.6.1/machinist_2.11-0.6.1.jar:/Users/seven/software/maven/repository/com/chuusai/shapeless_2.11/2.3.2/shapeless_2.11-2.3.2.jar:/Users/seven/software/maven/repository/org/typelevel/macro-compat_2.11/1.1.1/macro-compat_2.11-1.1.1.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-math3/3.4.1/commons-math3-3.4.1.jar:/Users/seven/software/maven/repository/org/jpmml/pmml-model/1.2.15/pmml-model-1.2.15.jar:/Users/seven/software/maven/repository/org/jpmml/pmml-schema/1.2.15/pmml-schema-1.2.15.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-tags_2.11/2.2.0/spark-tags_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-streaming-kafka-0-8_2.11/2.2.0/spark-streaming-kafka-0-8_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/kafka/kafka_2.11/0.8.2.1/kafka_2.11-0.8.2.1.jar:/Users/seven/software/maven/repository/com/101tec/zkclient/0.3/zkclient-0.3.jar:/Users/seven/software/maven/repository/org/apache/kafka/kafka-clients/0.8.2.1/kafka-clients-0.8.2.1.jar:/Users/seven/software/maven/repository/mysql/mysql-connector-java/5.1.31/mysql-connector-java-5.1.31.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/module/jackson-module-scala_2.11/2.6.5/jackson-module-scala_2.11-2.6.5.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/core/jackson-core/2.6.5/jackson-core-2.6.5.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/core/jackson-annotations/2.6.5/jackson-annotations-2.6.5.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/core/jackson-databind/2.6.5/jackson-databind-2.6.5.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/module/jackson-module-paranamer/2.6.5/jackson-module-paranamer-2.6.5.jar:/Users/seven/software/maven/repository/com/thoughtworks/paranamer/paranamer/2.6/paranamer-2.6.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-streaming-flume_2.11/2.2.0/spark-streaming-flume_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/flume/flume-ng-core/1.6.0/flume-ng-core-1.6.0.jar:/Users/seven/software/maven/repository/org/apache/flume/flume-ng-configuration/1.6.0/flume-ng-configuration-1.6.0.jar:/Users/seven/software/maven/repository/org/apache/avro/avro-ipc/1.7.4/avro-ipc-1.7.4.jar:/Users/seven/software/maven/repository/org/apache/velocity/velocity/1.7/velocity-1.7.jar:/Users/seven/software/maven/repository/joda-time/joda-time/2.1/joda-time-2.1.jar:/Users/seven/software/maven/repository/org/apache/mina/mina-core/2.0.4/mina-core-2.0.4.jar:/Users/seven/software/maven/repository/org/apache/flume/flume-ng-sdk/1.6.0/flume-ng-sdk-1.6.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-streaming-flume-sink_2.11/2.2.0/spark-streaming-flume-sink_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/flume/flume-ng-clients/flume-ng-log4jappender/1.6.0/flume-ng-log4jappender-1.6.0.jar:/Users/seven/software/maven/repository/net/jpountz/lz4/lz4/1.3.0/lz4-1.3.0.jar:/Applications/IntelliJ IDEA.app/Contents/lib/idea_rt.jar
[INFO][2018-05-24 19:47:00,632][org.apache.zookeeper.ZooKeeper]Client environment:java.library.path=/Users/seven/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:.
[INFO][2018-05-24 19:47:00,632][org.apache.zookeeper.ZooKeeper]Client environment:java.io.tmpdir=/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/
[INFO][2018-05-24 19:47:00,632][org.apache.zookeeper.ZooKeeper]Client environment:java.compiler=<NA>
[INFO][2018-05-24 19:47:00,632][org.apache.zookeeper.ZooKeeper]Client environment:os.name=Mac OS X
[INFO][2018-05-24 19:47:00,632][org.apache.zookeeper.ZooKeeper]Client environment:os.arch=x86_64
[INFO][2018-05-24 19:47:00,632][org.apache.zookeeper.ZooKeeper]Client environment:os.version=10.13.4
[INFO][2018-05-24 19:47:00,633][org.apache.zookeeper.ZooKeeper]Client environment:user.name=seven
[INFO][2018-05-24 19:47:00,633][org.apache.zookeeper.ZooKeeper]Client environment:user.home=/Users/seven
[INFO][2018-05-24 19:47:00,633][org.apache.zookeeper.ZooKeeper]Client environment:user.dir=/Users/seven/project/scala/dataMining
[INFO][2018-05-24 19:47:00,634][org.apache.zookeeper.ZooKeeper]Initiating client connection, connectString=vm-xaj-bigdata-da-d01:2181,vm-xaj-bigdata-da-d02:2181,vm-xaj-bigdata-da-d03:2181 sessionTimeout=60000 watcher=hconnection-0x3ca35b550x0, quorum=vm-xaj-bigdata-da-d01:2181,vm-xaj-bigdata-da-d02:2181,vm-xaj-bigdata-da-d03:2181, baseZNode=/hbase
[INFO][2018-05-24 19:47:15,836][org.apache.zookeeper.ClientCnxn]Opening socket connection to server vm-xaj-bigdata-da-d02/10.213.4.26:2181. Will not attempt to authenticate using SASL (unknown error)
[INFO][2018-05-24 19:47:15,848][org.apache.zookeeper.ClientCnxn]Socket connection established, initiating session, client: /10.0.8.39:65359, server: vm-xaj-bigdata-da-d02/10.213.4.26:2181
[INFO][2018-05-24 19:47:15,870][org.apache.zookeeper.ClientCnxn]Session establishment complete on server vm-xaj-bigdata-da-d02/10.213.4.26:2181, sessionid = 0x262b4dc569b5f07, negotiated timeout = 60000
[WARN][2018-05-24 19:47:16,318][org.apache.hadoop.util.NativeCodeLoader]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO][2018-05-24 19:47:16,393][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 19:47:16,402][org.apache.spark.executor.Executor]Finished task 0.0 in stage 0.0 (TID 0). 794 bytes result sent to driver
[INFO][2018-05-24 19:47:16,409][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 0.0 (TID 0) in 16022 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 19:47:16,410][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 19:47:16,413][org.apache.spark.scheduler.DAGScheduler]ResultStage 0 (foreachPartition at ReceiveKafkaData.scala:64) finished in 16.034 s
[INFO][2018-05-24 19:47:16,418][org.apache.spark.scheduler.DAGScheduler]Job 0 finished: foreachPartition at ReceiveKafkaData.scala:64, took 16.247880 s
[INFO][2018-05-24 19:47:16,422][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527162420000 ms.0 from job set of time 1527162420000 ms
[INFO][2018-05-24 19:47:16,423][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 16.421 s for time 1527162420000 ms (execution: 16.292 s)
[INFO][2018-05-24 19:47:16,427][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 19:47:16,432][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 
[INFO][2018-05-24 19:47:21,486][org.apache.spark.SparkContext]Running Spark version 2.2.0
[INFO][2018-05-24 19:47:22,255][org.apache.spark.SparkContext]Submitted application: SimulationKafkaSendOutData$
[INFO][2018-05-24 19:47:22,279][org.apache.spark.SecurityManager]Changing view acls to: seven
[INFO][2018-05-24 19:47:22,280][org.apache.spark.SecurityManager]Changing modify acls to: seven
[INFO][2018-05-24 19:47:22,281][org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO][2018-05-24 19:47:22,281][org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO][2018-05-24 19:47:22,282][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(seven); groups with view permissions: Set(); users  with modify permissions: Set(seven); groups with modify permissions: Set()
[INFO][2018-05-24 19:47:22,618][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 65364.
[INFO][2018-05-24 19:47:22,641][org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO][2018-05-24 19:47:22,659][org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO][2018-05-24 19:47:22,662][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2018-05-24 19:47:22,663][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
[INFO][2018-05-24 19:47:22,674][org.apache.spark.storage.DiskBlockManager]Created local directory at /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/blockmgr-166a1f15-baab-4d90-a78d-6430a73e3f40
[INFO][2018-05-24 19:47:22,697][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 912.3 MB
[INFO][2018-05-24 19:47:22,794][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO][2018-05-24 19:47:22,906][org.spark_project.jetty.util.log]Logging initialized @2557ms
[INFO][2018-05-24 19:47:22,973][org.spark_project.jetty.server.Server]jetty-9.3.z-SNAPSHOT
[INFO][2018-05-24 19:47:22,987][org.spark_project.jetty.server.Server]Started @2641ms
[WARN][2018-05-24 19:47:23,004][org.apache.spark.util.Utils]Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[INFO][2018-05-24 19:47:23,009][org.spark_project.jetty.server.AbstractConnector]Started ServerConnector@346a361{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}
[INFO][2018-05-24 19:47:23,010][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4041.
[INFO][2018-05-24 19:47:23,033][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@42b64ab8{/jobs,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:47:23,034][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2c1b9e4b{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:47:23,034][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@649725e3{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:47:23,035][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@52b56a3e{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:47:23,036][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4eed46ee{/stages,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:47:23,036][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4fad94a7{/stages/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:47:23,037][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6326d182{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:47:23,038][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@226642a5{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:47:23,039][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5cc126dc{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:47:23,039][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@72bd06ca{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:47:23,040][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5dbe30be{/storage,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:47:23,040][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@55795845{/storage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:47:23,041][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@119f1f2a{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:47:23,042][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5b970f7{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:47:23,042][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@165b8a71{/environment,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:47:23,043][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2f058b8a{/environment/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:47:23,044][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3f2ef586{/executors,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:47:23,044][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@76c7beb3{/executors/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:47:23,045][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2cf92cc7{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:47:23,046][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7b139eab{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:47:23,057][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@611df6e3{/static,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:47:23,058][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6ffab045{/,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:47:23,063][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3e2943ab{/api,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:47:23,064][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6c67e137{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:47:23,066][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@191ae03f{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:47:23,074][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://192.168.0.102:4041
[INFO][2018-05-24 19:47:23,208][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
[INFO][2018-05-24 19:47:23,245][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 65365.
[INFO][2018-05-24 19:47:23,246][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 192.168.0.102:65365
[INFO][2018-05-24 19:47:23,247][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2018-05-24 19:47:23,249][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 192.168.0.102, 65365, None)
[INFO][2018-05-24 19:47:23,256][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 192.168.0.102:65365 with 912.3 MB RAM, BlockManagerId(driver, 192.168.0.102, 65365, None)
[INFO][2018-05-24 19:47:23,259][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 192.168.0.102, 65365, None)
[INFO][2018-05-24 19:47:23,260][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 192.168.0.102, 65365, None)
[INFO][2018-05-24 19:47:23,451][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3e1162e7{/metrics/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:47:23,954][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0 stored as values in memory (estimated size 228.1 KB, free 912.1 MB)
[INFO][2018-05-24 19:47:24,022][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0_piece0 stored as bytes in memory (estimated size 22.1 KB, free 912.1 MB)
[INFO][2018-05-24 19:47:24,024][org.apache.spark.storage.BlockManagerInfo]Added broadcast_0_piece0 in memory on 192.168.0.102:65365 (size: 22.1 KB, free: 912.3 MB)
[INFO][2018-05-24 19:47:24,034][org.apache.spark.SparkContext]Created broadcast 0 from textFile at SimulationKafkaSendOutData.scala:25
[INFO][2018-05-24 19:47:25,068][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527162440000 ms
[INFO][2018-05-24 19:47:25,068][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527162440000 ms.0 from job set of time 1527162440000 ms
[INFO][2018-05-24 19:47:25,077][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:64
[INFO][2018-05-24 19:47:25,078][org.apache.spark.scheduler.DAGScheduler]Got job 1 (foreachPartition at ReceiveKafkaData.scala:64) with 1 output partitions
[INFO][2018-05-24 19:47:25,078][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 1 (foreachPartition at ReceiveKafkaData.scala:64)
[INFO][2018-05-24 19:47:25,078][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 19:47:25,078][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 19:47:25,079][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 1 (MapPartitionsRDD[3] at map at ReceiveKafkaData.scala:62), which has no missing parents
[INFO][2018-05-24 19:47:25,081][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-24 19:47:25,082][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1_piece0 stored as bytes in memory (estimated size 2008.0 B, free 912.3 MB)
[INFO][2018-05-24 19:47:25,083][org.apache.spark.storage.BlockManagerInfo]Added broadcast_1_piece0 in memory on 192.168.0.102:65343 (size: 2008.0 B, free: 912.3 MB)
[INFO][2018-05-24 19:47:25,083][org.apache.spark.SparkContext]Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 19:47:25,084][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at map at ReceiveKafkaData.scala:62) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 19:47:25,084][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 1.0 with 1 tasks
[INFO][2018-05-24 19:47:25,085][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 19:47:25,086][org.apache.spark.executor.Executor]Running task 0.0 in stage 1.0 (TID 1)
[INFO][2018-05-24 19:47:25,112][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 11804 is the same as ending offset skipping seven 0
[INFO][2018-05-24 19:47:25,112][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 19:47:25,115][org.apache.spark.executor.Executor]Finished task 0.0 in stage 1.0 (TID 1). 751 bytes result sent to driver
[INFO][2018-05-24 19:47:25,118][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 1.0 (TID 1) in 33 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 19:47:25,120][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 19:47:25,123][org.apache.spark.scheduler.DAGScheduler]ResultStage 1 (foreachPartition at ReceiveKafkaData.scala:64) finished in 0.038 s
[INFO][2018-05-24 19:47:25,124][org.apache.spark.scheduler.DAGScheduler]Job 1 finished: foreachPartition at ReceiveKafkaData.scala:64, took 0.046921 s
[INFO][2018-05-24 19:47:25,126][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527162440000 ms.0 from job set of time 1527162440000 ms
[INFO][2018-05-24 19:47:25,126][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 5.125 s for time 1527162440000 ms (execution: 0.057 s)
[INFO][2018-05-24 19:47:25,128][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 1 from persistence list
[INFO][2018-05-24 19:47:25,142][org.apache.spark.storage.BlockManager]Removing RDD 1
[INFO][2018-05-24 19:47:25,142][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 0 from persistence list
[INFO][2018-05-24 19:47:25,148][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 19:47:25,148][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 
[INFO][2018-05-24 19:47:25,148][org.apache.spark.storage.BlockManager]Removing RDD 0
[WARN][2018-05-24 19:47:29,444][org.apache.hadoop.util.NativeCodeLoader]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO][2018-05-24 19:47:29,577][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 1
[INFO][2018-05-24 19:47:29,659][org.apache.spark.SparkContext]Starting job: collect at SimulationKafkaSendOutData.scala:25
[INFO][2018-05-24 19:47:29,671][org.apache.spark.scheduler.DAGScheduler]Got job 0 (collect at SimulationKafkaSendOutData.scala:25) with 2 output partitions
[INFO][2018-05-24 19:47:29,671][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 0 (collect at SimulationKafkaSendOutData.scala:25)
[INFO][2018-05-24 19:47:29,672][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 19:47:29,673][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 19:47:29,680][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 0 (hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/* MapPartitionsRDD[1] at textFile at SimulationKafkaSendOutData.scala:25), which has no missing parents
[INFO][2018-05-24 19:47:29,708][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1 stored as values in memory (estimated size 3.3 KB, free 912.1 MB)
[INFO][2018-05-24 19:47:29,710][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1_piece0 stored as bytes in memory (estimated size 1969.0 B, free 912.1 MB)
[INFO][2018-05-24 19:47:29,710][org.apache.spark.storage.BlockManagerInfo]Added broadcast_1_piece0 in memory on 192.168.0.102:65365 (size: 1969.0 B, free: 912.3 MB)
[INFO][2018-05-24 19:47:29,711][org.apache.spark.SparkContext]Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 19:47:29,725][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ResultStage 0 (hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/* MapPartitionsRDD[1] at textFile at SimulationKafkaSendOutData.scala:25) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2018-05-24 19:47:29,726][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 0.0 with 2 tasks
[INFO][2018-05-24 19:47:29,761][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 4883 bytes)
[INFO][2018-05-24 19:47:29,763][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 4883 bytes)
[INFO][2018-05-24 19:47:29,771][org.apache.spark.executor.Executor]Running task 0.0 in stage 0.0 (TID 0)
[INFO][2018-05-24 19:47:29,771][org.apache.spark.executor.Executor]Running task 1.0 in stage 0.0 (TID 1)
[INFO][2018-05-24 19:47:29,836][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/part-00000:0+10717435
[INFO][2018-05-24 19:47:29,836][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/part-00000:10717435+10717436
[INFO][2018-05-24 19:47:32,479][org.apache.spark.storage.memory.MemoryStore]Block taskresult_0 stored as bytes in memory (estimated size 10.4 MB, free 901.7 MB)
[INFO][2018-05-24 19:47:32,482][org.apache.spark.storage.BlockManagerInfo]Added taskresult_0 in memory on 192.168.0.102:65365 (size: 10.4 MB, free: 901.9 MB)
[INFO][2018-05-24 19:47:32,483][org.apache.spark.executor.Executor]Finished task 0.0 in stage 0.0 (TID 0). 10856349 bytes result sent via BlockManager)
[INFO][2018-05-24 19:47:32,521][org.apache.spark.network.client.TransportClientFactory]Successfully created connection to /192.168.0.102:65365 after 21 ms (0 ms spent in bootstraps)
[INFO][2018-05-24 19:47:32,915][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 0.0 (TID 0) in 3165 ms on localhost (executor driver) (1/2)
[INFO][2018-05-24 19:47:32,918][org.apache.spark.storage.BlockManagerInfo]Removed taskresult_0 on 192.168.0.102:65365 in memory (size: 10.4 MB, free: 912.3 MB)
[INFO][2018-05-24 19:47:40,786][org.apache.spark.storage.memory.MemoryStore]Block taskresult_1 stored as bytes in memory (estimated size 10.4 MB, free 901.7 MB)
[INFO][2018-05-24 19:47:40,787][org.apache.spark.storage.BlockManagerInfo]Added taskresult_1 in memory on 192.168.0.102:65365 (size: 10.4 MB, free: 901.9 MB)
[INFO][2018-05-24 19:47:40,788][org.apache.spark.executor.Executor]Finished task 1.0 in stage 0.0 (TID 1). 10855899 bytes result sent via BlockManager)
[INFO][2018-05-24 19:47:40,852][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 0.0 (TID 1) in 11089 ms on localhost (executor driver) (2/2)
[INFO][2018-05-24 19:47:40,853][org.apache.spark.storage.BlockManagerInfo]Removed taskresult_1 on 192.168.0.102:65365 in memory (size: 10.4 MB, free: 912.3 MB)
[INFO][2018-05-24 19:47:40,854][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 19:47:40,854][org.apache.spark.scheduler.DAGScheduler]ResultStage 0 (collect at SimulationKafkaSendOutData.scala:25) finished in 11.115 s
[INFO][2018-05-24 19:47:40,859][org.apache.spark.scheduler.DAGScheduler]Job 0 finished: collect at SimulationKafkaSendOutData.scala:25, took 11.199230 s
[INFO][2018-05-24 19:47:41,025][org.spark_project.jetty.server.AbstractConnector]Stopped Spark@346a361{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}
[INFO][2018-05-24 19:47:41,029][org.apache.spark.ui.SparkUI]Stopped Spark web UI at http://192.168.0.102:4041
[INFO][2018-05-24 19:47:41,037][org.apache.spark.MapOutputTrackerMasterEndpoint]MapOutputTrackerMasterEndpoint stopped!
[INFO][2018-05-24 19:47:41,059][org.apache.spark.storage.memory.MemoryStore]MemoryStore cleared
[INFO][2018-05-24 19:47:41,059][org.apache.spark.storage.BlockManager]BlockManager stopped
[INFO][2018-05-24 19:47:41,060][org.apache.spark.storage.BlockManagerMaster]BlockManagerMaster stopped
[INFO][2018-05-24 19:47:41,062][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint]OutputCommitCoordinator stopped!
[INFO][2018-05-24 19:47:41,064][org.apache.spark.SparkContext]Successfully stopped SparkContext
[INFO][2018-05-24 19:47:41,079][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	compression.type = none
	metric.reporters = []
	metadata.max.age.ms = 300000
	metadata.fetch.timeout.ms = 60000
	acks = all
	batch.size = 16384
	reconnect.backoff.ms = 10
	bootstrap.servers = [vm-xaj-bigdata-da-d01:9092, vm-xaj-bigdata-da-d02:9092, vm-xaj-bigdata-da-d03:9092, vm-xaj-bigdata-da-d04:9092, vm-xaj-bigdata-da-d05:9092, vm-xaj-bigdata-da-d06:9092, vm-xaj-bigdata-da-d07:9092]
	receive.buffer.bytes = 32768
	retry.backoff.ms = 100
	buffer.memory = 33554432
	timeout.ms = 30000
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	retries = 0
	max.request.size = 1048576
	block.on.buffer.full = true
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
	metrics.sample.window.ms = 30000
	send.buffer.bytes = 131072
	max.in.flight.requests.per.connection = 5
	metrics.num.samples = 2
	linger.ms = 1
	client.id = 

[INFO][2018-05-24 19:47:45,084][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527162460000 ms
[INFO][2018-05-24 19:47:45,085][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527162460000 ms.0 from job set of time 1527162460000 ms
[INFO][2018-05-24 19:47:45,095][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:64
[INFO][2018-05-24 19:47:45,096][org.apache.spark.scheduler.DAGScheduler]Got job 2 (foreachPartition at ReceiveKafkaData.scala:64) with 1 output partitions
[INFO][2018-05-24 19:47:45,096][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 2 (foreachPartition at ReceiveKafkaData.scala:64)
[INFO][2018-05-24 19:47:45,096][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 19:47:45,096][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 19:47:45,097][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 2 (MapPartitionsRDD[5] at map at ReceiveKafkaData.scala:62), which has no missing parents
[INFO][2018-05-24 19:47:45,101][org.apache.spark.storage.memory.MemoryStore]Block broadcast_2 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-24 19:47:45,103][org.apache.spark.storage.memory.MemoryStore]Block broadcast_2_piece0 stored as bytes in memory (estimated size 2008.0 B, free 912.3 MB)
[INFO][2018-05-24 19:47:45,104][org.apache.spark.storage.BlockManagerInfo]Added broadcast_2_piece0 in memory on 192.168.0.102:65343 (size: 2008.0 B, free: 912.3 MB)
[INFO][2018-05-24 19:47:45,105][org.apache.spark.SparkContext]Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 19:47:45,106][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[5] at map at ReceiveKafkaData.scala:62) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 19:47:45,106][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 2.0 with 1 tasks
[INFO][2018-05-24 19:47:45,107][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 19:47:45,108][org.apache.spark.executor.Executor]Running task 0.0 in stage 2.0 (TID 2)
[INFO][2018-05-24 19:47:45,111][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 11804 is the same as ending offset skipping seven 0
[INFO][2018-05-24 19:47:45,112][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 19:47:45,113][org.apache.spark.executor.Executor]Finished task 0.0 in stage 2.0 (TID 2). 708 bytes result sent to driver
[INFO][2018-05-24 19:47:45,114][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 2.0 (TID 2) in 7 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 19:47:45,115][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 19:47:45,115][org.apache.spark.scheduler.DAGScheduler]ResultStage 2 (foreachPartition at ReceiveKafkaData.scala:64) finished in 0.009 s
[INFO][2018-05-24 19:47:45,116][org.apache.spark.scheduler.DAGScheduler]Job 2 finished: foreachPartition at ReceiveKafkaData.scala:64, took 0.020975 s
[INFO][2018-05-24 19:47:45,116][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527162460000 ms.0 from job set of time 1527162460000 ms
[INFO][2018-05-24 19:47:45,117][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 3 from persistence list
[INFO][2018-05-24 19:47:45,117][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 5.116 s for time 1527162460000 ms (execution: 0.031 s)
[INFO][2018-05-24 19:47:45,117][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 2 from persistence list
[INFO][2018-05-24 19:47:45,117][org.apache.spark.storage.BlockManager]Removing RDD 3
[INFO][2018-05-24 19:47:45,118][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 19:47:45,118][org.apache.spark.storage.BlockManager]Removing RDD 2
[INFO][2018-05-24 19:47:45,118][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527162420000 ms
[INFO][2018-05-24 19:48:05,079][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527162480000 ms
[INFO][2018-05-24 19:48:05,080][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527162480000 ms.0 from job set of time 1527162480000 ms
[INFO][2018-05-24 19:48:05,089][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:64
[INFO][2018-05-24 19:48:05,090][org.apache.spark.scheduler.DAGScheduler]Got job 3 (foreachPartition at ReceiveKafkaData.scala:64) with 1 output partitions
[INFO][2018-05-24 19:48:05,090][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 3 (foreachPartition at ReceiveKafkaData.scala:64)
[INFO][2018-05-24 19:48:05,090][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 19:48:05,090][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 19:48:05,090][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 3 (MapPartitionsRDD[7] at map at ReceiveKafkaData.scala:62), which has no missing parents
[INFO][2018-05-24 19:48:05,093][org.apache.spark.storage.memory.MemoryStore]Block broadcast_3 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-24 19:48:05,099][org.apache.spark.storage.memory.MemoryStore]Block broadcast_3_piece0 stored as bytes in memory (estimated size 2008.0 B, free 912.3 MB)
[INFO][2018-05-24 19:48:05,100][org.apache.spark.storage.BlockManagerInfo]Added broadcast_3_piece0 in memory on 192.168.0.102:65343 (size: 2008.0 B, free: 912.3 MB)
[INFO][2018-05-24 19:48:05,100][org.apache.spark.SparkContext]Created broadcast 3 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 19:48:05,101][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[7] at map at ReceiveKafkaData.scala:62) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 19:48:05,101][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 3.0 with 1 tasks
[INFO][2018-05-24 19:48:05,102][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 3.0 (TID 3, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 19:48:05,102][org.apache.spark.executor.Executor]Running task 0.0 in stage 3.0 (TID 3)
[INFO][2018-05-24 19:48:05,106][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 11804 is the same as ending offset skipping seven 0
[INFO][2018-05-24 19:48:05,106][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 19:48:05,107][org.apache.spark.executor.Executor]Finished task 0.0 in stage 3.0 (TID 3). 751 bytes result sent to driver
[INFO][2018-05-24 19:48:05,108][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 3.0 (TID 3) in 6 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 19:48:05,108][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 3.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 19:48:05,109][org.apache.spark.scheduler.DAGScheduler]ResultStage 3 (foreachPartition at ReceiveKafkaData.scala:64) finished in 0.007 s
[INFO][2018-05-24 19:48:05,109][org.apache.spark.scheduler.DAGScheduler]Job 3 finished: foreachPartition at ReceiveKafkaData.scala:64, took 0.020318 s
[INFO][2018-05-24 19:48:05,110][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527162480000 ms.0 from job set of time 1527162480000 ms
[INFO][2018-05-24 19:48:05,110][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 5 from persistence list
[INFO][2018-05-24 19:48:05,110][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 5.110 s for time 1527162480000 ms (execution: 0.030 s)
[INFO][2018-05-24 19:48:05,110][org.apache.spark.storage.BlockManager]Removing RDD 5
[INFO][2018-05-24 19:48:05,111][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 4 from persistence list
[INFO][2018-05-24 19:48:05,111][org.apache.spark.storage.BlockManager]Removing RDD 4
[INFO][2018-05-24 19:48:05,111][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 19:48:05,111][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527162440000 ms
[INFO][2018-05-24 19:48:25,070][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527162500000 ms
[INFO][2018-05-24 19:48:25,070][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527162500000 ms.0 from job set of time 1527162500000 ms
[INFO][2018-05-24 19:48:25,084][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:64
[INFO][2018-05-24 19:48:25,086][org.apache.spark.scheduler.DAGScheduler]Got job 4 (foreachPartition at ReceiveKafkaData.scala:64) with 1 output partitions
[INFO][2018-05-24 19:48:25,087][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 4 (foreachPartition at ReceiveKafkaData.scala:64)
[INFO][2018-05-24 19:48:25,087][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 19:48:25,087][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 19:48:25,087][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 4 (MapPartitionsRDD[9] at map at ReceiveKafkaData.scala:62), which has no missing parents
[INFO][2018-05-24 19:48:25,089][org.apache.spark.storage.memory.MemoryStore]Block broadcast_4 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-24 19:48:25,099][org.apache.spark.storage.memory.MemoryStore]Block broadcast_4_piece0 stored as bytes in memory (estimated size 2008.0 B, free 912.3 MB)
[INFO][2018-05-24 19:48:25,101][org.apache.spark.storage.BlockManagerInfo]Added broadcast_4_piece0 in memory on 192.168.0.102:65343 (size: 2008.0 B, free: 912.3 MB)
[INFO][2018-05-24 19:48:25,101][org.apache.spark.SparkContext]Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 19:48:25,102][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[9] at map at ReceiveKafkaData.scala:62) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 19:48:25,102][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 4.0 with 1 tasks
[INFO][2018-05-24 19:48:25,103][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 4.0 (TID 4, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 19:48:25,103][org.apache.spark.executor.Executor]Running task 0.0 in stage 4.0 (TID 4)
[INFO][2018-05-24 19:48:25,111][org.apache.spark.streaming.kafka.KafkaRDD]Computing topic seven, partition 0 offsets 11804 -> 11813
[INFO][2018-05-24 19:48:25,111][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 19:48:25,111][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 19:48:25,111][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 19:48:25,377][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_0_piece0 on 192.168.0.102:65343 in memory (size: 2007.0 B, free: 912.3 MB)
[INFO][2018-05-24 19:48:25,380][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_2_piece0 on 192.168.0.102:65343 in memory (size: 2008.0 B, free: 912.3 MB)
[INFO][2018-05-24 19:48:25,382][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_1_piece0 on 192.168.0.102:65343 in memory (size: 2008.0 B, free: 912.3 MB)
[INFO][2018-05-24 19:48:25,383][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_3_piece0 on 192.168.0.102:65343 in memory (size: 2008.0 B, free: 912.3 MB)
[INFO][2018-05-24 19:48:25,613][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 19:48:25,614][org.apache.spark.executor.Executor]Finished task 0.0 in stage 4.0 (TID 4). 751 bytes result sent to driver
[INFO][2018-05-24 19:48:25,615][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 4.0 (TID 4) in 512 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 19:48:25,615][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 4.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 19:48:25,616][org.apache.spark.scheduler.DAGScheduler]ResultStage 4 (foreachPartition at ReceiveKafkaData.scala:64) finished in 0.513 s
[INFO][2018-05-24 19:48:25,616][org.apache.spark.scheduler.DAGScheduler]Job 4 finished: foreachPartition at ReceiveKafkaData.scala:64, took 0.531305 s
[INFO][2018-05-24 19:48:25,617][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527162500000 ms.0 from job set of time 1527162500000 ms
[INFO][2018-05-24 19:48:25,617][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 7 from persistence list
[INFO][2018-05-24 19:48:25,617][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 5.617 s for time 1527162500000 ms (execution: 0.547 s)
[INFO][2018-05-24 19:48:25,617][org.apache.spark.storage.BlockManager]Removing RDD 7
[INFO][2018-05-24 19:48:25,618][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 6 from persistence list
[INFO][2018-05-24 19:48:25,618][org.apache.spark.storage.BlockManager]Removing RDD 6
[INFO][2018-05-24 19:48:25,618][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 19:48:25,618][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527162460000 ms
[INFO][2018-05-24 19:48:50,068][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527162520000 ms
[INFO][2018-05-24 19:48:50,068][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527162520000 ms.0 from job set of time 1527162520000 ms
[INFO][2018-05-24 19:48:50,075][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:64
[INFO][2018-05-24 19:48:50,076][org.apache.spark.scheduler.DAGScheduler]Got job 5 (foreachPartition at ReceiveKafkaData.scala:64) with 1 output partitions
[INFO][2018-05-24 19:48:50,076][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 5 (foreachPartition at ReceiveKafkaData.scala:64)
[INFO][2018-05-24 19:48:50,076][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 19:48:50,076][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 19:48:50,077][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 5 (MapPartitionsRDD[11] at map at ReceiveKafkaData.scala:62), which has no missing parents
[INFO][2018-05-24 19:48:50,079][org.apache.spark.storage.memory.MemoryStore]Block broadcast_5 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-24 19:48:50,080][org.apache.spark.storage.memory.MemoryStore]Block broadcast_5_piece0 stored as bytes in memory (estimated size 2008.0 B, free 912.3 MB)
[INFO][2018-05-24 19:48:50,081][org.apache.spark.storage.BlockManagerInfo]Added broadcast_5_piece0 in memory on 192.168.0.102:65343 (size: 2008.0 B, free: 912.3 MB)
[INFO][2018-05-24 19:48:50,081][org.apache.spark.SparkContext]Created broadcast 5 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 19:48:50,082][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[11] at map at ReceiveKafkaData.scala:62) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 19:48:50,082][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 5.0 with 1 tasks
[INFO][2018-05-24 19:48:50,083][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 5.0 (TID 5, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 19:48:50,083][org.apache.spark.executor.Executor]Running task 0.0 in stage 5.0 (TID 5)
[INFO][2018-05-24 19:48:50,086][org.apache.spark.streaming.kafka.KafkaRDD]Computing topic seven, partition 0 offsets 11813 -> 11838
[INFO][2018-05-24 19:48:50,086][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 19:48:50,086][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 19:48:50,086][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 19:48:50,179][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 19:48:50,180][org.apache.spark.executor.Executor]Finished task 0.0 in stage 5.0 (TID 5). 708 bytes result sent to driver
[INFO][2018-05-24 19:48:50,181][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 5.0 (TID 5) in 99 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 19:48:50,181][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 5.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 19:48:50,182][org.apache.spark.scheduler.DAGScheduler]ResultStage 5 (foreachPartition at ReceiveKafkaData.scala:64) finished in 0.100 s
[INFO][2018-05-24 19:48:50,182][org.apache.spark.scheduler.DAGScheduler]Job 5 finished: foreachPartition at ReceiveKafkaData.scala:64, took 0.106778 s
[INFO][2018-05-24 19:48:50,183][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527162520000 ms.0 from job set of time 1527162520000 ms
[INFO][2018-05-24 19:48:50,183][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 9 from persistence list
[INFO][2018-05-24 19:48:50,183][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 10.183 s for time 1527162520000 ms (execution: 0.115 s)
[INFO][2018-05-24 19:48:50,184][org.apache.spark.storage.BlockManager]Removing RDD 9
[INFO][2018-05-24 19:48:50,184][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 8 from persistence list
[INFO][2018-05-24 19:48:50,184][org.apache.spark.storage.BlockManager]Removing RDD 8
[INFO][2018-05-24 19:48:50,184][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 19:48:50,184][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527162480000 ms
[INFO][2018-05-24 19:49:05,082][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527162540000 ms
[INFO][2018-05-24 19:49:05,083][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527162540000 ms.0 from job set of time 1527162540000 ms
[INFO][2018-05-24 19:49:05,090][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:64
[INFO][2018-05-24 19:49:05,091][org.apache.spark.scheduler.DAGScheduler]Got job 6 (foreachPartition at ReceiveKafkaData.scala:64) with 1 output partitions
[INFO][2018-05-24 19:49:05,091][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 6 (foreachPartition at ReceiveKafkaData.scala:64)
[INFO][2018-05-24 19:49:05,091][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 19:49:05,092][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 19:49:05,092][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 6 (MapPartitionsRDD[13] at map at ReceiveKafkaData.scala:62), which has no missing parents
[INFO][2018-05-24 19:49:05,094][org.apache.spark.storage.memory.MemoryStore]Block broadcast_6 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-24 19:49:05,096][org.apache.spark.storage.memory.MemoryStore]Block broadcast_6_piece0 stored as bytes in memory (estimated size 2008.0 B, free 912.3 MB)
[INFO][2018-05-24 19:49:05,096][org.apache.spark.storage.BlockManagerInfo]Added broadcast_6_piece0 in memory on 192.168.0.102:65343 (size: 2008.0 B, free: 912.3 MB)
[INFO][2018-05-24 19:49:05,097][org.apache.spark.SparkContext]Created broadcast 6 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 19:49:05,098][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[13] at map at ReceiveKafkaData.scala:62) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 19:49:05,098][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 6.0 with 1 tasks
[INFO][2018-05-24 19:49:05,099][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 6.0 (TID 6, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 19:49:05,099][org.apache.spark.executor.Executor]Running task 0.0 in stage 6.0 (TID 6)
[INFO][2018-05-24 19:49:05,101][org.apache.spark.streaming.kafka.KafkaRDD]Computing topic seven, partition 0 offsets 11838 -> 11853
[INFO][2018-05-24 19:49:05,101][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 19:49:05,101][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 19:49:05,102][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 19:49:05,197][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 19:49:05,198][org.apache.spark.executor.Executor]Finished task 0.0 in stage 6.0 (TID 6). 708 bytes result sent to driver
[INFO][2018-05-24 19:49:05,199][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 6.0 (TID 6) in 101 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 19:49:05,200][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 6.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 19:49:05,200][org.apache.spark.scheduler.DAGScheduler]ResultStage 6 (foreachPartition at ReceiveKafkaData.scala:64) finished in 0.102 s
[INFO][2018-05-24 19:49:05,201][org.apache.spark.scheduler.DAGScheduler]Job 6 finished: foreachPartition at ReceiveKafkaData.scala:64, took 0.110559 s
[INFO][2018-05-24 19:49:05,202][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527162540000 ms.0 from job set of time 1527162540000 ms
[INFO][2018-05-24 19:49:05,202][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 5.202 s for time 1527162540000 ms (execution: 0.119 s)
[INFO][2018-05-24 19:49:05,202][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 11 from persistence list
[INFO][2018-05-24 19:49:05,203][org.apache.spark.storage.BlockManager]Removing RDD 11
[INFO][2018-05-24 19:49:05,203][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 10 from persistence list
[INFO][2018-05-24 19:49:05,203][org.apache.spark.storage.BlockManager]Removing RDD 10
[INFO][2018-05-24 19:49:05,203][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 19:49:05,203][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527162500000 ms
[INFO][2018-05-24 19:49:28,187][org.apache.spark.util.ShutdownHookManager]Shutdown hook called
[INFO][2018-05-24 19:49:28,188][org.apache.spark.util.ShutdownHookManager]Deleting directory /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/spark-cc706d72-3efd-4cbb-b4e1-e405b51fdbc5
[INFO][2018-05-24 19:49:28,220][org.apache.spark.streaming.StreamingContext]Invoking stop(stopGracefully=false) from shutdown hook
[INFO][2018-05-24 19:49:28,221][org.apache.spark.streaming.scheduler.ReceiverTracker]ReceiverTracker stopped
[INFO][2018-05-24 19:49:28,223][org.apache.spark.streaming.scheduler.JobGenerator]Stopping JobGenerator immediately
[INFO][2018-05-24 19:49:28,225][org.apache.spark.streaming.util.RecurringTimer]Stopped timer for JobGenerator after time 1527162560000
[INFO][2018-05-24 19:49:30,073][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527162560000 ms
[INFO][2018-05-24 19:49:30,074][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527162560000 ms.0 from job set of time 1527162560000 ms
[INFO][2018-05-24 19:49:30,077][org.apache.spark.streaming.scheduler.JobGenerator]Stopped JobGenerator
[INFO][2018-05-24 19:49:30,083][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:64
[INFO][2018-05-24 19:49:30,084][org.apache.spark.scheduler.DAGScheduler]Got job 7 (foreachPartition at ReceiveKafkaData.scala:64) with 1 output partitions
[INFO][2018-05-24 19:49:30,085][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 7 (foreachPartition at ReceiveKafkaData.scala:64)
[INFO][2018-05-24 19:49:30,085][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 19:49:30,085][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 19:49:30,085][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 7 (MapPartitionsRDD[15] at map at ReceiveKafkaData.scala:62), which has no missing parents
[INFO][2018-05-24 19:49:30,090][org.apache.spark.storage.memory.MemoryStore]Block broadcast_7 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-24 19:49:30,101][org.apache.spark.storage.memory.MemoryStore]Block broadcast_7_piece0 stored as bytes in memory (estimated size 2008.0 B, free 912.3 MB)
[INFO][2018-05-24 19:49:30,102][org.apache.spark.storage.BlockManagerInfo]Added broadcast_7_piece0 in memory on 192.168.0.102:65343 (size: 2008.0 B, free: 912.3 MB)
[INFO][2018-05-24 19:49:30,102][org.apache.spark.SparkContext]Created broadcast 7 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 19:49:30,104][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[15] at map at ReceiveKafkaData.scala:62) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 19:49:30,104][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 7.0 with 1 tasks
[INFO][2018-05-24 19:49:30,109][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 7.0 (TID 7, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 19:49:30,109][org.apache.spark.executor.Executor]Running task 0.0 in stage 7.0 (TID 7)
[INFO][2018-05-24 19:49:30,112][org.apache.spark.streaming.kafka.KafkaRDD]Computing topic seven, partition 0 offsets 11853 -> 11876
[INFO][2018-05-24 19:49:30,112][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 19:49:30,112][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 19:49:30,112][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 19:49:32,085][org.apache.spark.streaming.scheduler.JobScheduler]Stopped JobScheduler
[INFO][2018-05-24 19:49:32,098][org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@3910fe11{/streaming,null,UNAVAILABLE,@Spark}
[INFO][2018-05-24 19:49:32,099][org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@2c2db130{/streaming/batch,null,UNAVAILABLE,@Spark}
[INFO][2018-05-24 19:49:32,100][org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@20e6c4dc{/static/streaming,null,UNAVAILABLE,@Spark}
[INFO][2018-05-24 19:49:32,101][org.apache.spark.streaming.StreamingContext]StreamingContext stopped successfully
[INFO][2018-05-24 19:49:32,102][org.apache.spark.SparkContext]Invoking stop() from shutdown hook
[INFO][2018-05-24 19:49:32,113][org.spark_project.jetty.server.AbstractConnector]Stopped Spark@2090b9b9{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-05-24 19:49:32,115][org.apache.spark.ui.SparkUI]Stopped Spark web UI at http://192.168.0.102:4040
[INFO][2018-05-24 19:49:32,122][org.apache.spark.scheduler.DAGScheduler]ResultStage 7 (foreachPartition at ReceiveKafkaData.scala:64) failed in 2.016 s due to Stage cancelled because SparkContext was shut down
[ERROR][2018-05-24 19:49:32,123][org.apache.spark.scheduler.LiveListenerBus]SparkListenerBus has already stopped! Dropping event SparkListenerStageCompleted(org.apache.spark.scheduler.StageInfo@4db38d3e)
[ERROR][2018-05-24 19:49:32,124][org.apache.spark.scheduler.LiveListenerBus]SparkListenerBus has already stopped! Dropping event SparkListenerJobEnd(7,1527162572123,JobFailed(org.apache.spark.SparkException: Job 7 cancelled because SparkContext was shut down))
[INFO][2018-05-24 19:49:32,129][org.apache.spark.MapOutputTrackerMasterEndpoint]MapOutputTrackerMasterEndpoint stopped!
[INFO][2018-05-24 19:49:32,158][org.apache.spark.storage.memory.MemoryStore]MemoryStore cleared
[INFO][2018-05-24 19:49:32,158][org.apache.spark.storage.BlockManager]BlockManager stopped
[INFO][2018-05-24 19:49:32,160][org.apache.spark.storage.BlockManagerMaster]BlockManagerMaster stopped
[INFO][2018-05-24 19:49:32,162][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint]OutputCommitCoordinator stopped!
[INFO][2018-05-24 19:49:32,166][org.apache.spark.SparkContext]Successfully stopped SparkContext
[INFO][2018-05-24 19:49:32,166][org.apache.spark.util.ShutdownHookManager]Shutdown hook called
[INFO][2018-05-24 19:49:32,167][org.apache.spark.util.ShutdownHookManager]Deleting directory /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/spark-9f9af923-9145-46d8-86ab-6a18577734f3
[INFO][2018-05-24 19:50:04,633][org.apache.spark.SparkContext]Running Spark version 2.2.0
[INFO][2018-05-24 19:50:05,296][org.apache.spark.SparkContext]Submitted application: ReceiveKafkaData$
[INFO][2018-05-24 19:50:05,325][org.apache.spark.SecurityManager]Changing view acls to: seven
[INFO][2018-05-24 19:50:05,326][org.apache.spark.SecurityManager]Changing modify acls to: seven
[INFO][2018-05-24 19:50:05,327][org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO][2018-05-24 19:50:05,327][org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO][2018-05-24 19:50:05,328][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(seven); groups with view permissions: Set(); users  with modify permissions: Set(seven); groups with modify permissions: Set()
[INFO][2018-05-24 19:50:05,637][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 65422.
[INFO][2018-05-24 19:50:05,652][org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO][2018-05-24 19:50:05,668][org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO][2018-05-24 19:50:05,672][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2018-05-24 19:50:05,672][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
[INFO][2018-05-24 19:50:05,681][org.apache.spark.storage.DiskBlockManager]Created local directory at /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/blockmgr-f697e5c8-7c31-41c5-b73e-dc0a54dc5103
[INFO][2018-05-24 19:50:05,696][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 912.3 MB
[INFO][2018-05-24 19:50:05,767][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO][2018-05-24 19:50:05,842][org.spark_project.jetty.util.log]Logging initialized @2146ms
[INFO][2018-05-24 19:50:05,901][org.spark_project.jetty.server.Server]jetty-9.3.z-SNAPSHOT
[INFO][2018-05-24 19:50:05,913][org.spark_project.jetty.server.Server]Started @2218ms
[INFO][2018-05-24 19:50:05,931][org.spark_project.jetty.server.AbstractConnector]Started ServerConnector@6d366c9b{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-05-24 19:50:05,932][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4040.
[INFO][2018-05-24 19:50:05,954][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2c1b9e4b{/jobs,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:50:05,955][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3d08f3f5{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:50:05,956][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1a1da881{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:50:05,958][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@165b8a71{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:50:05,960][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2f058b8a{/stages,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:50:05,961][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3f2ef586{/stages/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:50:05,962][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@76c7beb3{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:50:05,964][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7b139eab{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:50:05,965][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@611df6e3{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:50:05,966][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6273c5a4{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:50:05,967][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@53e211ee{/storage,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:50:05,968][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3d8bbcdc{/storage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:50:05,968][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@117e0fe5{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:50:05,969][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@78aea4b9{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:50:05,970][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4b85880b{/environment,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:50:05,970][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4215838f{/environment/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:50:05,971][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2289aca5{/executors,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:50:05,972][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@184497d1{/executors/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:50:05,974][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6ffab045{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:50:05,975][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3e2943ab{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:50:05,985][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4a9f80d3{/static,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:50:05,986][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4204541c{/,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:50:05,987][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4602c2a9{/api,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:50:05,988][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@69c79f09{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:50:05,989][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5fcacc0{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:50:05,991][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://192.168.0.102:4040
[INFO][2018-05-24 19:50:06,116][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
[INFO][2018-05-24 19:50:06,150][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 65423.
[INFO][2018-05-24 19:50:06,152][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 192.168.0.102:65423
[INFO][2018-05-24 19:50:06,154][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2018-05-24 19:50:06,155][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 192.168.0.102, 65423, None)
[INFO][2018-05-24 19:50:06,163][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 192.168.0.102:65423 with 912.3 MB RAM, BlockManagerId(driver, 192.168.0.102, 65423, None)
[INFO][2018-05-24 19:50:06,168][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 192.168.0.102, 65423, None)
[INFO][2018-05-24 19:50:06,168][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 192.168.0.102, 65423, None)
[INFO][2018-05-24 19:50:06,385][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@64a1923a{/metrics/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:50:06,529][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 19:50:06,532][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 19:50:06,532][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 19:50:22,007][org.apache.spark.streaming.kafka.DirectKafkaInputDStream]Slide time = 20000 ms
[INFO][2018-05-24 19:50:22,007][org.apache.spark.streaming.kafka.DirectKafkaInputDStream]Storage level = Serialized 1x Replicated
[INFO][2018-05-24 19:50:22,008][org.apache.spark.streaming.kafka.DirectKafkaInputDStream]Checkpoint interval = null
[INFO][2018-05-24 19:50:22,008][org.apache.spark.streaming.kafka.DirectKafkaInputDStream]Remember interval = 20000 ms
[INFO][2018-05-24 19:50:22,009][org.apache.spark.streaming.kafka.DirectKafkaInputDStream]Initialized and validated org.apache.spark.streaming.kafka.DirectKafkaInputDStream@635f60ff
[INFO][2018-05-24 19:50:22,009][org.apache.spark.streaming.dstream.MappedDStream]Slide time = 20000 ms
[INFO][2018-05-24 19:50:22,009][org.apache.spark.streaming.dstream.MappedDStream]Storage level = Serialized 1x Replicated
[INFO][2018-05-24 19:50:22,009][org.apache.spark.streaming.dstream.MappedDStream]Checkpoint interval = null
[INFO][2018-05-24 19:50:22,010][org.apache.spark.streaming.dstream.MappedDStream]Remember interval = 20000 ms
[INFO][2018-05-24 19:50:22,010][org.apache.spark.streaming.dstream.MappedDStream]Initialized and validated org.apache.spark.streaming.dstream.MappedDStream@128240cb
[INFO][2018-05-24 19:50:22,010][org.apache.spark.streaming.dstream.ForEachDStream]Slide time = 20000 ms
[INFO][2018-05-24 19:50:22,010][org.apache.spark.streaming.dstream.ForEachDStream]Storage level = Serialized 1x Replicated
[INFO][2018-05-24 19:50:22,010][org.apache.spark.streaming.dstream.ForEachDStream]Checkpoint interval = null
[INFO][2018-05-24 19:50:22,010][org.apache.spark.streaming.dstream.ForEachDStream]Remember interval = 20000 ms
[INFO][2018-05-24 19:50:22,010][org.apache.spark.streaming.dstream.ForEachDStream]Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@7a7836e0
[INFO][2018-05-24 19:50:22,051][org.apache.spark.streaming.util.RecurringTimer]Started timer for JobGenerator at time 1527162640000
[INFO][2018-05-24 19:50:22,052][org.apache.spark.streaming.scheduler.JobGenerator]Started JobGenerator at 1527162640000 ms
[INFO][2018-05-24 19:50:22,053][org.apache.spark.streaming.scheduler.JobScheduler]Started JobScheduler
[INFO][2018-05-24 19:50:22,056][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6972c30a{/streaming,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:50:22,057][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1ab6718{/streaming/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:50:22,058][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5109e8cf{/streaming/batch,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:50:22,058][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@78b41097{/streaming/batch/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:50:22,059][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6c65860d{/static/streaming,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:50:22,060][org.apache.spark.streaming.StreamingContext]StreamingContext started
[INFO][2018-05-24 19:50:40,043][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 19:50:40,043][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 19:50:40,043][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 19:50:40,128][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527162640000 ms
[INFO][2018-05-24 19:50:40,130][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527162640000 ms.0 from job set of time 1527162640000 ms
[INFO][2018-05-24 19:50:40,176][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:64
[INFO][2018-05-24 19:50:40,190][org.apache.spark.scheduler.DAGScheduler]Got job 0 (foreachPartition at ReceiveKafkaData.scala:64) with 1 output partitions
[INFO][2018-05-24 19:50:40,190][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 0 (foreachPartition at ReceiveKafkaData.scala:64)
[INFO][2018-05-24 19:50:40,190][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 19:50:40,192][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 19:50:40,205][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 0 (MapPartitionsRDD[1] at map at ReceiveKafkaData.scala:62), which has no missing parents
[INFO][2018-05-24 19:50:40,338][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-24 19:50:40,361][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0_piece0 stored as bytes in memory (estimated size 2007.0 B, free 912.3 MB)
[INFO][2018-05-24 19:50:40,363][org.apache.spark.storage.BlockManagerInfo]Added broadcast_0_piece0 in memory on 192.168.0.102:65423 (size: 2007.0 B, free: 912.3 MB)
[INFO][2018-05-24 19:50:40,365][org.apache.spark.SparkContext]Created broadcast 0 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 19:50:40,377][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at map at ReceiveKafkaData.scala:62) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 19:50:40,377][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 0.0 with 1 tasks
[INFO][2018-05-24 19:50:40,414][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 19:50:40,424][org.apache.spark.executor.Executor]Running task 0.0 in stage 0.0 (TID 0)
[INFO][2018-05-24 19:50:40,460][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 11876 is the same as ending offset skipping seven 0
[INFO][2018-05-24 19:50:40,622][org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper]Process identifier=hconnection-0x2071f062 connecting to ZooKeeper ensemble=vm-xaj-bigdata-da-d01:2181,vm-xaj-bigdata-da-d02:2181,vm-xaj-bigdata-da-d03:2181
[INFO][2018-05-24 19:50:40,629][org.apache.zookeeper.ZooKeeper]Client environment:zookeeper.version=3.4.5-cdh5.7.0--1, built on 04/06/2016 14:24 GMT
[INFO][2018-05-24 19:50:40,629][org.apache.zookeeper.ZooKeeper]Client environment:host.name=192.168.0.102
[INFO][2018-05-24 19:50:40,629][org.apache.zookeeper.ZooKeeper]Client environment:java.version=1.8.0_161
[INFO][2018-05-24 19:50:40,629][org.apache.zookeeper.ZooKeeper]Client environment:java.vendor=Oracle Corporation
[INFO][2018-05-24 19:50:40,629][org.apache.zookeeper.ZooKeeper]Client environment:java.home=/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre
[INFO][2018-05-24 19:50:40,629][org.apache.zookeeper.ZooKeeper]Client environment:java.class.path=/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/charsets.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/deploy.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/cldrdata.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/dnsns.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/jaccess.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/jfxrt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/localedata.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/nashorn.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/sunec.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/sunjce_provider.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/sunpkcs11.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/zipfs.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/javaws.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/jce.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/jfr.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/jfxswt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/jsse.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/management-agent.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/plugin.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/resources.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/rt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/ant-javafx.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/dt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/javafx-mx.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/jconsole.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/packager.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/sa-jdi.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/tools.jar:/Users/seven/project/scala/dataMining/target/classes:/Users/seven/software/maven/repository/org/scala-lang/scala-library/2.11.12/scala-library-2.11.12.jar:/Users/seven/software/maven/repository/org/scala-lang/scala-compiler/2.11.12/scala-compiler-2.11.12.jar:/Users/seven/software/maven/repository/org/scala-lang/modules/scala-xml_2.11/1.0.5/scala-xml_2.11-1.0.5.jar:/Users/seven/software/maven/repository/org/scala-lang/modules/scala-parser-combinators_2.11/1.0.4/scala-parser-combinators_2.11-1.0.4.jar:/Users/seven/software/maven/repository/org/scala-lang/scala-reflect/2.11.12/scala-reflect-2.11.12.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-client/2.6.0-cdh5.7.0/hadoop-client-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-common/2.6.0-cdh5.7.0/hadoop-common-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/xmlenc/xmlenc/0.52/xmlenc-0.52.jar:/Users/seven/software/maven/repository/commons-net/commons-net/3.1/commons-net-3.1.jar:/Users/seven/software/maven/repository/commons-configuration/commons-configuration/1.6/commons-configuration-1.6.jar:/Users/seven/software/maven/repository/commons-digester/commons-digester/1.8/commons-digester-1.8.jar:/Users/seven/software/maven/repository/commons-beanutils/commons-beanutils/1.7.0/commons-beanutils-1.7.0.jar:/Users/seven/software/maven/repository/org/slf4j/slf4j-api/1.7.5/slf4j-api-1.7.5.jar:/Users/seven/software/maven/repository/org/slf4j/slf4j-log4j12/1.7.5/slf4j-log4j12-1.7.5.jar:/Users/seven/software/maven/repository/org/apache/avro/avro/1.7.6-cdh5.7.0/avro-1.7.6-cdh5.7.0.jar:/Users/seven/software/maven/repository/com/google/code/gson/gson/2.2.4/gson-2.2.4.jar:/Users/seven/software/maven/repository/org/apache/curator/curator-client/2.7.1/curator-client-2.7.1.jar:/Users/seven/software/maven/repository/org/apache/curator/curator-recipes/2.7.1/curator-recipes-2.7.1.jar:/Users/seven/software/maven/repository/com/google/code/findbugs/jsr305/3.0.0/jsr305-3.0.0.jar:/Users/seven/software/maven/repository/org/apache/htrace/htrace-core4/4.0.1-incubating/htrace-core4-4.0.1-incubating.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-compress/1.4.1/commons-compress-1.4.1.jar:/Users/seven/software/maven/repository/org/tukaani/xz/1.0/xz-1.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-hdfs/2.6.0-cdh5.7.0/hadoop-hdfs-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/io/netty/netty/3.6.2.Final/netty-3.6.2.Final.jar:/Users/seven/software/maven/repository/xerces/xercesImpl/2.9.1/xercesImpl-2.9.1.jar:/Users/seven/software/maven/repository/xml-apis/xml-apis/1.3.04/xml-apis-1.3.04.jar:/Users/seven/software/maven/repository/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-app/2.6.0-cdh5.7.0/hadoop-mapreduce-client-app-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-common/2.6.0-cdh5.7.0/hadoop-mapreduce-client-common-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-yarn-client/2.6.0-cdh5.7.0/hadoop-yarn-client-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-yarn-server-common/2.6.0-cdh5.7.0/hadoop-yarn-server-common-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-shuffle/2.6.0-cdh5.7.0/hadoop-mapreduce-client-shuffle-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-yarn-api/2.6.0-cdh5.7.0/hadoop-yarn-api-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-core/2.6.0-cdh5.7.0/hadoop-mapreduce-client-core-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-yarn-common/2.6.0-cdh5.7.0/hadoop-yarn-common-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/javax/xml/bind/jaxb-api/2.2.2/jaxb-api-2.2.2.jar:/Users/seven/software/maven/repository/javax/xml/stream/stax-api/1.0-2/stax-api-1.0-2.jar:/Users/seven/software/maven/repository/javax/activation/activation/1.1/activation-1.1.jar:/Users/seven/software/maven/repository/com/sun/jersey/jersey-client/1.9/jersey-client-1.9.jar:/Users/seven/software/maven/repository/org/codehaus/jackson/jackson-xc/1.8.8/jackson-xc-1.8.8.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.6.0-cdh5.7.0/hadoop-mapreduce-client-jobclient-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-aws/2.6.0-cdh5.7.0/hadoop-aws-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/com/amazonaws/aws-java-sdk-s3/1.10.6/aws-java-sdk-s3-1.10.6.jar:/Users/seven/software/maven/repository/com/amazonaws/aws-java-sdk-kms/1.10.6/aws-java-sdk-kms-1.10.6.jar:/Users/seven/software/maven/repository/com/amazonaws/aws-java-sdk-core/1.10.6/aws-java-sdk-core-1.10.6.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-annotations/2.6.0-cdh5.7.0/hadoop-annotations-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-client/1.2.0-cdh5.7.0/hbase-client-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-annotations/1.2.0-cdh5.7.0/hbase-annotations-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-common/1.2.0-cdh5.7.0/hbase-common-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-protocol/1.2.0-cdh5.7.0/hbase-protocol-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/commons-codec/commons-codec/1.9/commons-codec-1.9.jar:/Users/seven/software/maven/repository/commons-io/commons-io/2.4/commons-io-2.4.jar:/Users/seven/software/maven/repository/commons-lang/commons-lang/2.6/commons-lang-2.6.jar:/Users/seven/software/maven/repository/commons-logging/commons-logging/1.2/commons-logging-1.2.jar:/Users/seven/software/maven/repository/com/google/guava/guava/12.0.1/guava-12.0.1.jar:/Users/seven/software/maven/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/Users/seven/software/maven/repository/io/netty/netty-all/4.0.23.Final/netty-all-4.0.23.Final.jar:/Users/seven/software/maven/repository/org/apache/zookeeper/zookeeper/3.4.5-cdh5.7.0/zookeeper-3.4.5-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/htrace/htrace-core/3.2.0-incubating/htrace-core-3.2.0-incubating.jar:/Users/seven/software/maven/repository/org/codehaus/jackson/jackson-mapper-asl/1.8.8/jackson-mapper-asl-1.8.8.jar:/Users/seven/software/maven/repository/org/jruby/jcodings/jcodings/1.0.8/jcodings-1.0.8.jar:/Users/seven/software/maven/repository/org/jruby/joni/joni/2.1.2/joni-2.1.2.jar:/Users/seven/software/maven/repository/com/yammer/metrics/metrics-core/2.2.0/metrics-core-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-auth/2.6.0-cdh5.7.0/hadoop-auth-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/httpcomponents/httpclient/4.2.5/httpclient-4.2.5.jar:/Users/seven/software/maven/repository/org/apache/httpcomponents/httpcore/4.2.4/httpcore-4.2.4.jar:/Users/seven/software/maven/repository/org/apache/directory/server/apacheds-kerberos-codec/2.0.0-M15/apacheds-kerberos-codec-2.0.0-M15.jar:/Users/seven/software/maven/repository/org/apache/directory/server/apacheds-i18n/2.0.0-M15/apacheds-i18n-2.0.0-M15.jar:/Users/seven/software/maven/repository/org/apache/directory/api/api-asn1-api/1.0.0-M20/api-asn1-api-1.0.0-M20.jar:/Users/seven/software/maven/repository/org/apache/directory/api/api-util/1.0.0-M20/api-util-1.0.0-M20.jar:/Users/seven/software/maven/repository/org/apache/curator/curator-framework/2.7.1/curator-framework-2.7.1.jar:/Users/seven/software/maven/repository/com/github/stephenc/findbugs/findbugs-annotations/1.3.9-1/findbugs-annotations-1.3.9-1.jar:/Users/seven/software/maven/repository/junit/junit/4.12/junit-4.12.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-server/1.2.0-cdh5.7.0/hbase-server-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-procedure/1.2.0-cdh5.7.0/hbase-procedure-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-common/1.2.0-cdh5.7.0/hbase-common-1.2.0-cdh5.7.0-tests.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-prefix-tree/1.2.0-cdh5.7.0/hbase-prefix-tree-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar:/Users/seven/software/maven/repository/commons-collections/commons-collections/3.2.2/commons-collections-3.2.2.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-hadoop-compat/1.2.0-cdh5.7.0/hbase-hadoop-compat-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-hadoop2-compat/1.2.0-cdh5.7.0/hbase-hadoop2-compat-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/com/sun/jersey/jersey-core/1.9/jersey-core-1.9.jar:/Users/seven/software/maven/repository/com/sun/jersey/jersey-server/1.9/jersey-server-1.9.jar:/Users/seven/software/maven/repository/asm/asm/3.1/asm-3.1.jar:/Users/seven/software/maven/repository/commons-cli/commons-cli/1.2/commons-cli-1.2.jar:/Users/seven/software/maven/repository/com/github/stephenc/high-scale-lib/high-scale-lib/1.1.1/high-scale-lib-1.1.1.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-math/2.1/commons-math-2.1.jar:/Users/seven/software/maven/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jetty/6.1.26.cloudera.4/jetty-6.1.26.cloudera.4.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jetty-util/6.1.26.cloudera.4/jetty-util-6.1.26.cloudera.4.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jetty-sslengine/6.1.26.cloudera.4/jetty-sslengine-6.1.26.cloudera.4.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jsp-2.1/6.1.14/jsp-2.1-6.1.14.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jsp-api-2.1/6.1.14/jsp-api-2.1-6.1.14.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/servlet-api-2.5/6.1.14/servlet-api-2.5-6.1.14.jar:/Users/seven/software/maven/repository/org/codehaus/jackson/jackson-core-asl/1.8.8/jackson-core-asl-1.8.8.jar:/Users/seven/software/maven/repository/org/codehaus/jackson/jackson-jaxrs/1.8.8/jackson-jaxrs-1.8.8.jar:/Users/seven/software/maven/repository/tomcat/jasper-compiler/5.5.23/jasper-compiler-5.5.23.jar:/Users/seven/software/maven/repository/tomcat/jasper-runtime/5.5.23/jasper-runtime-5.5.23.jar:/Users/seven/software/maven/repository/commons-el/commons-el/1.0/commons-el-1.0.jar:/Users/seven/software/maven/repository/org/jamon/jamon-runtime/2.4.1/jamon-runtime-2.4.1.jar:/Users/seven/software/maven/repository/com/lmax/disruptor/3.3.0/disruptor-3.3.0.jar:/Users/seven/software/maven/repository/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar:/Users/seven/software/maven/repository/org/owasp/esapi/esapi/2.1.0/esapi-2.1.0.jar:/Users/seven/software/maven/repository/commons-beanutils/commons-beanutils-core/1.7.0/commons-beanutils-core-1.7.0.jar:/Users/seven/software/maven/repository/commons-fileupload/commons-fileupload/1.2/commons-fileupload-1.2.jar:/Users/seven/software/maven/repository/xom/xom/1.2.5/xom-1.2.5.jar:/Users/seven/software/maven/repository/xalan/xalan/2.7.0/xalan-2.7.0.jar:/Users/seven/software/maven/repository/org/beanshell/bsh-core/2.0b4/bsh-core-2.0b4.jar:/Users/seven/software/maven/repository/org/owasp/antisamy/antisamy/1.4.3/antisamy-1.4.3.jar:/Users/seven/software/maven/repository/org/apache/xmlgraphics/batik-css/1.7/batik-css-1.7.jar:/Users/seven/software/maven/repository/org/apache/xmlgraphics/batik-ext/1.7/batik-ext-1.7.jar:/Users/seven/software/maven/repository/org/apache/xmlgraphics/batik-util/1.7/batik-util-1.7.jar:/Users/seven/software/maven/repository/xml-apis/xml-apis-ext/1.3.04/xml-apis-ext-1.3.04.jar:/Users/seven/software/maven/repository/net/sourceforge/nekohtml/nekohtml/1.9.12/nekohtml-1.9.12.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-core/2.6.0-mr1-cdh5.7.0/hadoop-core-2.6.0-mr1-cdh5.7.0.jar:/Users/seven/software/maven/repository/javax/servlet/jsp/jsp-api/2.1/jsp-api-2.1.jar:/Users/seven/software/maven/repository/net/java/dev/jets3t/jets3t/0.6.1/jets3t-0.6.1.jar:/Users/seven/software/maven/repository/javax/servlet/servlet-api/2.5/servlet-api-2.5.jar:/Users/seven/software/maven/repository/hsqldb/hsqldb/1.8.0.10/hsqldb-1.8.0.10.jar:/Users/seven/software/maven/repository/org/eclipse/jdt/core/3.1.1/core-3.1.1.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-hdfs/2.6.0-cdh5.7.0/hadoop-hdfs-2.6.0-cdh5.7.0-tests.jar:/Users/seven/software/maven/repository/commons-daemon/commons-daemon/1.0.13/commons-daemon-1.0.13.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-lang3/3.5/commons-lang3-3.5.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-pool2/2.4.2/commons-pool2-2.4.2.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-mllib_2.11/2.2.0/spark-mllib_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-core_2.11/2.2.0/spark-core_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/avro/avro-mapred/1.7.7/avro-mapred-1.7.7-hadoop2.jar:/Users/seven/software/maven/repository/org/apache/avro/avro-ipc/1.7.7/avro-ipc-1.7.7-tests.jar:/Users/seven/software/maven/repository/com/twitter/chill_2.11/0.8.0/chill_2.11-0.8.0.jar:/Users/seven/software/maven/repository/com/esotericsoftware/kryo-shaded/3.0.3/kryo-shaded-3.0.3.jar:/Users/seven/software/maven/repository/com/esotericsoftware/minlog/1.3.0/minlog-1.3.0.jar:/Users/seven/software/maven/repository/org/objenesis/objenesis/2.1/objenesis-2.1.jar:/Users/seven/software/maven/repository/com/twitter/chill-java/0.8.0/chill-java-0.8.0.jar:/Users/seven/software/maven/repository/org/apache/xbean/xbean-asm5-shaded/4.4/xbean-asm5-shaded-4.4.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-launcher_2.11/2.2.0/spark-launcher_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-network-common_2.11/2.2.0/spark-network-common_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-network-shuffle_2.11/2.2.0/spark-network-shuffle_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-unsafe_2.11/2.2.0/spark-unsafe_2.11-2.2.0.jar:/Users/seven/software/maven/repository/javax/servlet/javax.servlet-api/3.1.0/javax.servlet-api-3.1.0.jar:/Users/seven/software/maven/repository/org/slf4j/jul-to-slf4j/1.7.16/jul-to-slf4j-1.7.16.jar:/Users/seven/software/maven/repository/org/slf4j/jcl-over-slf4j/1.7.16/jcl-over-slf4j-1.7.16.jar:/Users/seven/software/maven/repository/com/ning/compress-lzf/1.0.3/compress-lzf-1.0.3.jar:/Users/seven/software/maven/repository/org/xerial/snappy/snappy-java/1.1.2.6/snappy-java-1.1.2.6.jar:/Users/seven/software/maven/repository/org/roaringbitmap/RoaringBitmap/0.5.11/RoaringBitmap-0.5.11.jar:/Users/seven/software/maven/repository/org/json4s/json4s-jackson_2.11/3.2.11/json4s-jackson_2.11-3.2.11.jar:/Users/seven/software/maven/repository/org/json4s/json4s-core_2.11/3.2.11/json4s-core_2.11-3.2.11.jar:/Users/seven/software/maven/repository/org/json4s/json4s-ast_2.11/3.2.11/json4s-ast_2.11-3.2.11.jar:/Users/seven/software/maven/repository/org/scala-lang/scalap/2.11.0/scalap-2.11.0.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/core/jersey-client/2.22.2/jersey-client-2.22.2.jar:/Users/seven/software/maven/repository/javax/ws/rs/javax.ws.rs-api/2.0.1/javax.ws.rs-api-2.0.1.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/hk2-api/2.4.0-b34/hk2-api-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/hk2-utils/2.4.0-b34/hk2-utils-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/external/aopalliance-repackaged/2.4.0-b34/aopalliance-repackaged-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/external/javax.inject/2.4.0-b34/javax.inject-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/hk2-locator/2.4.0-b34/hk2-locator-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/javassist/javassist/3.18.1-GA/javassist-3.18.1-GA.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/core/jersey-common/2.22.2/jersey-common-2.22.2.jar:/Users/seven/software/maven/repository/javax/annotation/javax.annotation-api/1.2/javax.annotation-api-1.2.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/bundles/repackaged/jersey-guava/2.22.2/jersey-guava-2.22.2.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/osgi-resource-locator/1.0.1/osgi-resource-locator-1.0.1.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/core/jersey-server/2.22.2/jersey-server-2.22.2.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/media/jersey-media-jaxb/2.22.2/jersey-media-jaxb-2.22.2.jar:/Users/seven/software/maven/repository/javax/validation/validation-api/1.1.0.Final/validation-api-1.1.0.Final.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/containers/jersey-container-servlet/2.22.2/jersey-container-servlet-2.22.2.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/containers/jersey-container-servlet-core/2.22.2/jersey-container-servlet-core-2.22.2.jar:/Users/seven/software/maven/repository/com/clearspring/analytics/stream/2.7.0/stream-2.7.0.jar:/Users/seven/software/maven/repository/io/dropwizard/metrics/metrics-core/3.1.2/metrics-core-3.1.2.jar:/Users/seven/software/maven/repository/io/dropwizard/metrics/metrics-jvm/3.1.2/metrics-jvm-3.1.2.jar:/Users/seven/software/maven/repository/io/dropwizard/metrics/metrics-json/3.1.2/metrics-json-3.1.2.jar:/Users/seven/software/maven/repository/io/dropwizard/metrics/metrics-graphite/3.1.2/metrics-graphite-3.1.2.jar:/Users/seven/software/maven/repository/org/apache/ivy/ivy/2.4.0/ivy-2.4.0.jar:/Users/seven/software/maven/repository/oro/oro/2.0.8/oro-2.0.8.jar:/Users/seven/software/maven/repository/net/razorvine/pyrolite/4.13/pyrolite-4.13.jar:/Users/seven/software/maven/repository/net/sf/py4j/py4j/0.10.4/py4j-0.10.4.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-crypto/1.0.0/commons-crypto-1.0.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-streaming_2.11/2.2.0/spark-streaming_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-sql_2.11/2.2.0/spark-sql_2.11-2.2.0.jar:/Users/seven/software/maven/repository/com/univocity/univocity-parsers/2.2.1/univocity-parsers-2.2.1.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-sketch_2.11/2.2.0/spark-sketch_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-catalyst_2.11/2.2.0/spark-catalyst_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/codehaus/janino/janino/3.0.0/janino-3.0.0.jar:/Users/seven/software/maven/repository/org/codehaus/janino/commons-compiler/3.0.0/commons-compiler-3.0.0.jar:/Users/seven/software/maven/repository/org/antlr/antlr4-runtime/4.5.3/antlr4-runtime-4.5.3.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-column/1.8.2/parquet-column-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-common/1.8.2/parquet-common-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-encoding/1.8.2/parquet-encoding-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-hadoop/1.8.2/parquet-hadoop-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-format/2.3.1/parquet-format-2.3.1.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-jackson/1.8.2/parquet-jackson-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-graphx_2.11/2.2.0/spark-graphx_2.11-2.2.0.jar:/Users/seven/software/maven/repository/com/github/fommil/netlib/core/1.1.2/core-1.1.2.jar:/Users/seven/software/maven/repository/net/sourceforge/f2j/arpack_combined_all/0.1/arpack_combined_all-0.1.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-mllib-local_2.11/2.2.0/spark-mllib-local_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/scalanlp/breeze_2.11/0.13.1/breeze_2.11-0.13.1.jar:/Users/seven/software/maven/repository/org/scalanlp/breeze-macros_2.11/0.13.1/breeze-macros_2.11-0.13.1.jar:/Users/seven/software/maven/repository/net/sf/opencsv/opencsv/2.3/opencsv-2.3.jar:/Users/seven/software/maven/repository/com/github/rwl/jtransforms/2.4.0/jtransforms-2.4.0.jar:/Users/seven/software/maven/repository/org/spire-math/spire_2.11/0.13.0/spire_2.11-0.13.0.jar:/Users/seven/software/maven/repository/org/spire-math/spire-macros_2.11/0.13.0/spire-macros_2.11-0.13.0.jar:/Users/seven/software/maven/repository/org/typelevel/machinist_2.11/0.6.1/machinist_2.11-0.6.1.jar:/Users/seven/software/maven/repository/com/chuusai/shapeless_2.11/2.3.2/shapeless_2.11-2.3.2.jar:/Users/seven/software/maven/repository/org/typelevel/macro-compat_2.11/1.1.1/macro-compat_2.11-1.1.1.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-math3/3.4.1/commons-math3-3.4.1.jar:/Users/seven/software/maven/repository/org/jpmml/pmml-model/1.2.15/pmml-model-1.2.15.jar:/Users/seven/software/maven/repository/org/jpmml/pmml-schema/1.2.15/pmml-schema-1.2.15.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-tags_2.11/2.2.0/spark-tags_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-streaming-kafka-0-8_2.11/2.2.0/spark-streaming-kafka-0-8_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/kafka/kafka_2.11/0.8.2.1/kafka_2.11-0.8.2.1.jar:/Users/seven/software/maven/repository/com/101tec/zkclient/0.3/zkclient-0.3.jar:/Users/seven/software/maven/repository/org/apache/kafka/kafka-clients/0.8.2.1/kafka-clients-0.8.2.1.jar:/Users/seven/software/maven/repository/mysql/mysql-connector-java/5.1.31/mysql-connector-java-5.1.31.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/module/jackson-module-scala_2.11/2.6.5/jackson-module-scala_2.11-2.6.5.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/core/jackson-core/2.6.5/jackson-core-2.6.5.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/core/jackson-annotations/2.6.5/jackson-annotations-2.6.5.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/core/jackson-databind/2.6.5/jackson-databind-2.6.5.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/module/jackson-module-paranamer/2.6.5/jackson-module-paranamer-2.6.5.jar:/Users/seven/software/maven/repository/com/thoughtworks/paranamer/paranamer/2.6/paranamer-2.6.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-streaming-flume_2.11/2.2.0/spark-streaming-flume_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/flume/flume-ng-core/1.6.0/flume-ng-core-1.6.0.jar:/Users/seven/software/maven/repository/org/apache/flume/flume-ng-configuration/1.6.0/flume-ng-configuration-1.6.0.jar:/Users/seven/software/maven/repository/org/apache/avro/avro-ipc/1.7.4/avro-ipc-1.7.4.jar:/Users/seven/software/maven/repository/org/apache/velocity/velocity/1.7/velocity-1.7.jar:/Users/seven/software/maven/repository/joda-time/joda-time/2.1/joda-time-2.1.jar:/Users/seven/software/maven/repository/org/apache/mina/mina-core/2.0.4/mina-core-2.0.4.jar:/Users/seven/software/maven/repository/org/apache/flume/flume-ng-sdk/1.6.0/flume-ng-sdk-1.6.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-streaming-flume-sink_2.11/2.2.0/spark-streaming-flume-sink_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/flume/flume-ng-clients/flume-ng-log4jappender/1.6.0/flume-ng-log4jappender-1.6.0.jar:/Users/seven/software/maven/repository/net/jpountz/lz4/lz4/1.3.0/lz4-1.3.0.jar:/Applications/IntelliJ IDEA.app/Contents/lib/idea_rt.jar
[INFO][2018-05-24 19:50:40,630][org.apache.zookeeper.ZooKeeper]Client environment:java.library.path=/Users/seven/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:.
[INFO][2018-05-24 19:50:40,631][org.apache.zookeeper.ZooKeeper]Client environment:java.io.tmpdir=/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/
[INFO][2018-05-24 19:50:40,631][org.apache.zookeeper.ZooKeeper]Client environment:java.compiler=<NA>
[INFO][2018-05-24 19:50:40,631][org.apache.zookeeper.ZooKeeper]Client environment:os.name=Mac OS X
[INFO][2018-05-24 19:50:40,631][org.apache.zookeeper.ZooKeeper]Client environment:os.arch=x86_64
[INFO][2018-05-24 19:50:40,631][org.apache.zookeeper.ZooKeeper]Client environment:os.version=10.13.4
[INFO][2018-05-24 19:50:40,631][org.apache.zookeeper.ZooKeeper]Client environment:user.name=seven
[INFO][2018-05-24 19:50:40,631][org.apache.zookeeper.ZooKeeper]Client environment:user.home=/Users/seven
[INFO][2018-05-24 19:50:40,631][org.apache.zookeeper.ZooKeeper]Client environment:user.dir=/Users/seven/project/scala/dataMining
[INFO][2018-05-24 19:50:40,633][org.apache.zookeeper.ZooKeeper]Initiating client connection, connectString=vm-xaj-bigdata-da-d01:2181,vm-xaj-bigdata-da-d02:2181,vm-xaj-bigdata-da-d03:2181 sessionTimeout=60000 watcher=hconnection-0x2071f0620x0, quorum=vm-xaj-bigdata-da-d01:2181,vm-xaj-bigdata-da-d02:2181,vm-xaj-bigdata-da-d03:2181, baseZNode=/hbase
[INFO][2018-05-24 19:50:55,668][org.apache.zookeeper.ClientCnxn]Opening socket connection to server master/10.213.4.25:2181. Will not attempt to authenticate using SASL (unknown error)
[INFO][2018-05-24 19:50:55,684][org.apache.zookeeper.ClientCnxn]Socket connection established, initiating session, client: /10.0.8.39:65438, server: master/10.213.4.25:2181
[INFO][2018-05-24 19:50:55,709][org.apache.zookeeper.ClientCnxn]Session establishment complete on server master/10.213.4.25:2181, sessionid = 0x162b4dc56095e73, negotiated timeout = 60000
[WARN][2018-05-24 19:50:56,141][org.apache.hadoop.util.NativeCodeLoader]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO][2018-05-24 19:50:56,234][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 19:50:56,247][org.apache.spark.executor.Executor]Finished task 0.0 in stage 0.0 (TID 0). 708 bytes result sent to driver
[INFO][2018-05-24 19:50:56,254][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 0.0 (TID 0) in 15850 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 19:50:56,256][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 19:50:56,259][org.apache.spark.scheduler.DAGScheduler]ResultStage 0 (foreachPartition at ReceiveKafkaData.scala:64) finished in 15.867 s
[INFO][2018-05-24 19:50:56,265][org.apache.spark.scheduler.DAGScheduler]Job 0 finished: foreachPartition at ReceiveKafkaData.scala:64, took 16.088680 s
[INFO][2018-05-24 19:50:56,269][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527162640000 ms.0 from job set of time 1527162640000 ms
[INFO][2018-05-24 19:50:56,270][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 16.269 s for time 1527162640000 ms (execution: 16.139 s)
[INFO][2018-05-24 19:50:56,276][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 19:50:56,279][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 
[INFO][2018-05-24 19:51:01,585][org.apache.spark.SparkContext]Running Spark version 2.2.0
[INFO][2018-05-24 19:51:02,258][org.apache.spark.SparkContext]Submitted application: SimulationKafkaSendOutData$
[INFO][2018-05-24 19:51:02,283][org.apache.spark.SecurityManager]Changing view acls to: seven
[INFO][2018-05-24 19:51:02,284][org.apache.spark.SecurityManager]Changing modify acls to: seven
[INFO][2018-05-24 19:51:02,284][org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO][2018-05-24 19:51:02,285][org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO][2018-05-24 19:51:02,286][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(seven); groups with view permissions: Set(); users  with modify permissions: Set(seven); groups with modify permissions: Set()
[INFO][2018-05-24 19:51:02,559][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 65442.
[INFO][2018-05-24 19:51:02,588][org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO][2018-05-24 19:51:02,607][org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO][2018-05-24 19:51:02,610][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2018-05-24 19:51:02,611][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
[INFO][2018-05-24 19:51:02,620][org.apache.spark.storage.DiskBlockManager]Created local directory at /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/blockmgr-99d8a8aa-f2b9-4f07-b002-adbc02407d6a
[INFO][2018-05-24 19:51:02,639][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 912.3 MB
[INFO][2018-05-24 19:51:02,690][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO][2018-05-24 19:51:02,809][org.spark_project.jetty.util.log]Logging initialized @2106ms
[INFO][2018-05-24 19:51:02,864][org.spark_project.jetty.server.Server]jetty-9.3.z-SNAPSHOT
[INFO][2018-05-24 19:51:02,876][org.spark_project.jetty.server.Server]Started @2174ms
[WARN][2018-05-24 19:51:02,891][org.apache.spark.util.Utils]Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[INFO][2018-05-24 19:51:02,896][org.spark_project.jetty.server.AbstractConnector]Started ServerConnector@346a361{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}
[INFO][2018-05-24 19:51:02,896][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4041.
[INFO][2018-05-24 19:51:02,917][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@42b64ab8{/jobs,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:51:02,917][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2c1b9e4b{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:51:02,918][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@649725e3{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:51:02,919][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@52b56a3e{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:51:02,920][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4eed46ee{/stages,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:51:02,920][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4fad94a7{/stages/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:51:02,921][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6326d182{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:51:02,922][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@226642a5{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:51:02,923][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5cc126dc{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:51:02,923][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@72bd06ca{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:51:02,924][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5dbe30be{/storage,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:51:02,925][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@55795845{/storage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:51:02,926][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@119f1f2a{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:51:02,926][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5b970f7{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:51:02,927][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@165b8a71{/environment,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:51:02,928][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2f058b8a{/environment/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:51:02,929][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3f2ef586{/executors,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:51:02,929][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@76c7beb3{/executors/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:51:02,930][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2cf92cc7{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:51:02,931][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7b139eab{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:51:02,937][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@611df6e3{/static,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:51:02,938][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6ffab045{/,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:51:02,939][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3e2943ab{/api,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:51:02,941][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6c67e137{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:51:02,944][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@191ae03f{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:51:02,947][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://192.168.0.102:4041
[INFO][2018-05-24 19:51:03,068][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
[INFO][2018-05-24 19:51:03,113][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 65443.
[INFO][2018-05-24 19:51:03,114][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 192.168.0.102:65443
[INFO][2018-05-24 19:51:03,115][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2018-05-24 19:51:03,117][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 192.168.0.102, 65443, None)
[INFO][2018-05-24 19:51:03,121][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 192.168.0.102:65443 with 912.3 MB RAM, BlockManagerId(driver, 192.168.0.102, 65443, None)
[INFO][2018-05-24 19:51:03,127][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 192.168.0.102, 65443, None)
[INFO][2018-05-24 19:51:03,129][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 192.168.0.102, 65443, None)
[INFO][2018-05-24 19:51:03,364][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3e1162e7{/metrics/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:51:03,906][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0 stored as values in memory (estimated size 228.1 KB, free 912.1 MB)
[INFO][2018-05-24 19:51:03,979][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0_piece0 stored as bytes in memory (estimated size 22.1 KB, free 912.1 MB)
[INFO][2018-05-24 19:51:03,981][org.apache.spark.storage.BlockManagerInfo]Added broadcast_0_piece0 in memory on 192.168.0.102:65443 (size: 22.1 KB, free: 912.3 MB)
[INFO][2018-05-24 19:51:03,986][org.apache.spark.SparkContext]Created broadcast 0 from textFile at SimulationKafkaSendOutData.scala:25
[WARN][2018-05-24 19:51:09,414][org.apache.hadoop.util.NativeCodeLoader]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO][2018-05-24 19:51:09,547][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 1
[INFO][2018-05-24 19:51:09,646][org.apache.spark.SparkContext]Starting job: collect at SimulationKafkaSendOutData.scala:25
[INFO][2018-05-24 19:51:09,664][org.apache.spark.scheduler.DAGScheduler]Got job 0 (collect at SimulationKafkaSendOutData.scala:25) with 2 output partitions
[INFO][2018-05-24 19:51:09,665][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 0 (collect at SimulationKafkaSendOutData.scala:25)
[INFO][2018-05-24 19:51:09,665][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 19:51:09,667][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 19:51:09,676][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 0 (hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/* MapPartitionsRDD[1] at textFile at SimulationKafkaSendOutData.scala:25), which has no missing parents
[INFO][2018-05-24 19:51:09,691][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1 stored as values in memory (estimated size 3.3 KB, free 912.1 MB)
[INFO][2018-05-24 19:51:09,693][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1_piece0 stored as bytes in memory (estimated size 1969.0 B, free 912.1 MB)
[INFO][2018-05-24 19:51:09,693][org.apache.spark.storage.BlockManagerInfo]Added broadcast_1_piece0 in memory on 192.168.0.102:65443 (size: 1969.0 B, free: 912.3 MB)
[INFO][2018-05-24 19:51:09,694][org.apache.spark.SparkContext]Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 19:51:09,707][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ResultStage 0 (hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/* MapPartitionsRDD[1] at textFile at SimulationKafkaSendOutData.scala:25) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2018-05-24 19:51:09,708][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 0.0 with 2 tasks
[INFO][2018-05-24 19:51:09,751][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 4883 bytes)
[INFO][2018-05-24 19:51:09,763][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 4883 bytes)
[INFO][2018-05-24 19:51:09,775][org.apache.spark.executor.Executor]Running task 1.0 in stage 0.0 (TID 1)
[INFO][2018-05-24 19:51:09,775][org.apache.spark.executor.Executor]Running task 0.0 in stage 0.0 (TID 0)
[INFO][2018-05-24 19:51:09,868][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/part-00000:10717435+10717436
[INFO][2018-05-24 19:51:09,869][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/part-00000:0+10717435
[INFO][2018-05-24 19:51:10,320][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527162660000 ms
[INFO][2018-05-24 19:51:10,321][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527162660000 ms.0 from job set of time 1527162660000 ms
[INFO][2018-05-24 19:51:10,333][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:64
[INFO][2018-05-24 19:51:10,334][org.apache.spark.scheduler.DAGScheduler]Got job 1 (foreachPartition at ReceiveKafkaData.scala:64) with 1 output partitions
[INFO][2018-05-24 19:51:10,334][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 1 (foreachPartition at ReceiveKafkaData.scala:64)
[INFO][2018-05-24 19:51:10,334][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 19:51:10,335][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 19:51:10,335][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 1 (MapPartitionsRDD[3] at map at ReceiveKafkaData.scala:62), which has no missing parents
[INFO][2018-05-24 19:51:10,338][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-24 19:51:10,346][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1_piece0 stored as bytes in memory (estimated size 2008.0 B, free 912.3 MB)
[INFO][2018-05-24 19:51:10,347][org.apache.spark.storage.BlockManagerInfo]Added broadcast_1_piece0 in memory on 192.168.0.102:65423 (size: 2008.0 B, free: 912.3 MB)
[INFO][2018-05-24 19:51:10,348][org.apache.spark.SparkContext]Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 19:51:10,350][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at map at ReceiveKafkaData.scala:62) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 19:51:10,350][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 1.0 with 1 tasks
[INFO][2018-05-24 19:51:10,352][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 19:51:10,353][org.apache.spark.executor.Executor]Running task 0.0 in stage 1.0 (TID 1)
[INFO][2018-05-24 19:51:10,394][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 11876 is the same as ending offset skipping seven 0
[INFO][2018-05-24 19:51:10,394][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 19:51:10,396][org.apache.spark.executor.Executor]Finished task 0.0 in stage 1.0 (TID 1). 708 bytes result sent to driver
[INFO][2018-05-24 19:51:10,397][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 1.0 (TID 1) in 46 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 19:51:10,397][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 19:51:10,398][org.apache.spark.scheduler.DAGScheduler]ResultStage 1 (foreachPartition at ReceiveKafkaData.scala:64) finished in 0.047 s
[INFO][2018-05-24 19:51:10,398][org.apache.spark.scheduler.DAGScheduler]Job 1 finished: foreachPartition at ReceiveKafkaData.scala:64, took 0.065096 s
[INFO][2018-05-24 19:51:10,399][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527162660000 ms.0 from job set of time 1527162660000 ms
[INFO][2018-05-24 19:51:10,400][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 10.399 s for time 1527162660000 ms (execution: 0.078 s)
[INFO][2018-05-24 19:51:10,400][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 1 from persistence list
[INFO][2018-05-24 19:51:10,406][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 0 from persistence list
[INFO][2018-05-24 19:51:10,406][org.apache.spark.storage.BlockManager]Removing RDD 1
[INFO][2018-05-24 19:51:10,407][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 19:51:10,407][org.apache.spark.storage.BlockManager]Removing RDD 0
[INFO][2018-05-24 19:51:10,407][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 
[INFO][2018-05-24 19:51:12,232][org.apache.spark.storage.memory.MemoryStore]Block taskresult_0 stored as bytes in memory (estimated size 10.4 MB, free 901.7 MB)
[INFO][2018-05-24 19:51:12,234][org.apache.spark.storage.BlockManagerInfo]Added taskresult_0 in memory on 192.168.0.102:65443 (size: 10.4 MB, free: 901.9 MB)
[INFO][2018-05-24 19:51:12,235][org.apache.spark.executor.Executor]Finished task 0.0 in stage 0.0 (TID 0). 10856349 bytes result sent via BlockManager)
[INFO][2018-05-24 19:51:12,280][org.apache.spark.network.client.TransportClientFactory]Successfully created connection to /192.168.0.102:65443 after 30 ms (0 ms spent in bootstraps)
[INFO][2018-05-24 19:51:12,611][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 0.0 (TID 0) in 2874 ms on localhost (executor driver) (1/2)
[INFO][2018-05-24 19:51:12,613][org.apache.spark.storage.BlockManagerInfo]Removed taskresult_0 on 192.168.0.102:65443 in memory (size: 10.4 MB, free: 912.3 MB)
[INFO][2018-05-24 19:51:22,190][org.apache.spark.storage.memory.MemoryStore]Block taskresult_1 stored as bytes in memory (estimated size 10.4 MB, free 901.7 MB)
[INFO][2018-05-24 19:51:22,192][org.apache.spark.storage.BlockManagerInfo]Added taskresult_1 in memory on 192.168.0.102:65443 (size: 10.4 MB, free: 901.9 MB)
[INFO][2018-05-24 19:51:22,193][org.apache.spark.executor.Executor]Finished task 1.0 in stage 0.0 (TID 1). 10855899 bytes result sent via BlockManager)
[INFO][2018-05-24 19:51:22,252][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 0.0 (TID 1) in 12490 ms on localhost (executor driver) (2/2)
[INFO][2018-05-24 19:51:22,253][org.apache.spark.storage.BlockManagerInfo]Removed taskresult_1 on 192.168.0.102:65443 in memory (size: 10.4 MB, free: 912.3 MB)
[INFO][2018-05-24 19:51:22,254][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 19:51:22,255][org.apache.spark.scheduler.DAGScheduler]ResultStage 0 (collect at SimulationKafkaSendOutData.scala:25) finished in 12.532 s
[INFO][2018-05-24 19:51:22,259][org.apache.spark.scheduler.DAGScheduler]Job 0 finished: collect at SimulationKafkaSendOutData.scala:25, took 12.612373 s
[INFO][2018-05-24 19:51:22,394][org.spark_project.jetty.server.AbstractConnector]Stopped Spark@346a361{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}
[INFO][2018-05-24 19:51:22,396][org.apache.spark.ui.SparkUI]Stopped Spark web UI at http://192.168.0.102:4041
[INFO][2018-05-24 19:51:22,404][org.apache.spark.MapOutputTrackerMasterEndpoint]MapOutputTrackerMasterEndpoint stopped!
[INFO][2018-05-24 19:51:22,417][org.apache.spark.storage.memory.MemoryStore]MemoryStore cleared
[INFO][2018-05-24 19:51:22,418][org.apache.spark.storage.BlockManager]BlockManager stopped
[INFO][2018-05-24 19:51:22,419][org.apache.spark.storage.BlockManagerMaster]BlockManagerMaster stopped
[INFO][2018-05-24 19:51:22,420][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint]OutputCommitCoordinator stopped!
[INFO][2018-05-24 19:51:22,422][org.apache.spark.SparkContext]Successfully stopped SparkContext
[INFO][2018-05-24 19:51:22,433][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	compression.type = none
	metric.reporters = []
	metadata.max.age.ms = 300000
	metadata.fetch.timeout.ms = 60000
	acks = all
	batch.size = 16384
	reconnect.backoff.ms = 10
	bootstrap.servers = [vm-xaj-bigdata-da-d01:9092, vm-xaj-bigdata-da-d02:9092, vm-xaj-bigdata-da-d03:9092, vm-xaj-bigdata-da-d04:9092, vm-xaj-bigdata-da-d05:9092, vm-xaj-bigdata-da-d06:9092, vm-xaj-bigdata-da-d07:9092]
	receive.buffer.bytes = 32768
	retry.backoff.ms = 100
	buffer.memory = 33554432
	timeout.ms = 30000
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	retries = 0
	max.request.size = 1048576
	block.on.buffer.full = true
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
	metrics.sample.window.ms = 30000
	send.buffer.bytes = 131072
	max.in.flight.requests.per.connection = 5
	metrics.num.samples = 2
	linger.ms = 1
	client.id = 

[INFO][2018-05-24 19:51:25,067][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527162680000 ms
[INFO][2018-05-24 19:51:25,067][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527162680000 ms.0 from job set of time 1527162680000 ms
[INFO][2018-05-24 19:51:25,078][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:64
[INFO][2018-05-24 19:51:25,079][org.apache.spark.scheduler.DAGScheduler]Got job 2 (foreachPartition at ReceiveKafkaData.scala:64) with 1 output partitions
[INFO][2018-05-24 19:51:25,080][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 2 (foreachPartition at ReceiveKafkaData.scala:64)
[INFO][2018-05-24 19:51:25,080][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 19:51:25,080][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 19:51:25,082][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 2 (MapPartitionsRDD[5] at map at ReceiveKafkaData.scala:62), which has no missing parents
[INFO][2018-05-24 19:51:25,087][org.apache.spark.storage.memory.MemoryStore]Block broadcast_2 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-24 19:51:25,091][org.apache.spark.storage.memory.MemoryStore]Block broadcast_2_piece0 stored as bytes in memory (estimated size 2008.0 B, free 912.3 MB)
[INFO][2018-05-24 19:51:25,092][org.apache.spark.storage.BlockManagerInfo]Added broadcast_2_piece0 in memory on 192.168.0.102:65423 (size: 2008.0 B, free: 912.3 MB)
[INFO][2018-05-24 19:51:25,093][org.apache.spark.SparkContext]Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 19:51:25,094][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[5] at map at ReceiveKafkaData.scala:62) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 19:51:25,094][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 2.0 with 1 tasks
[INFO][2018-05-24 19:51:25,095][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 19:51:25,096][org.apache.spark.executor.Executor]Running task 0.0 in stage 2.0 (TID 2)
[INFO][2018-05-24 19:51:25,100][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 11876 is the same as ending offset skipping seven 0
[INFO][2018-05-24 19:51:25,101][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 19:51:25,104][org.apache.spark.executor.Executor]Finished task 0.0 in stage 2.0 (TID 2). 708 bytes result sent to driver
[INFO][2018-05-24 19:51:25,105][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 2.0 (TID 2) in 10 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 19:51:25,106][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 19:51:25,106][org.apache.spark.scheduler.DAGScheduler]ResultStage 2 (foreachPartition at ReceiveKafkaData.scala:64) finished in 0.011 s
[INFO][2018-05-24 19:51:25,107][org.apache.spark.scheduler.DAGScheduler]Job 2 finished: foreachPartition at ReceiveKafkaData.scala:64, took 0.028137 s
[INFO][2018-05-24 19:51:25,107][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527162680000 ms.0 from job set of time 1527162680000 ms
[INFO][2018-05-24 19:51:25,108][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 3 from persistence list
[INFO][2018-05-24 19:51:25,108][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 5.107 s for time 1527162680000 ms (execution: 0.040 s)
[INFO][2018-05-24 19:51:25,108][org.apache.spark.storage.BlockManager]Removing RDD 3
[INFO][2018-05-24 19:51:25,108][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 2 from persistence list
[INFO][2018-05-24 19:51:25,109][org.apache.spark.storage.BlockManager]Removing RDD 2
[INFO][2018-05-24 19:51:25,109][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 19:51:25,109][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527162640000 ms
[INFO][2018-05-24 19:51:50,077][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527162700000 ms
[INFO][2018-05-24 19:51:50,077][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527162700000 ms.0 from job set of time 1527162700000 ms
[INFO][2018-05-24 19:51:50,088][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:64
[INFO][2018-05-24 19:51:50,090][org.apache.spark.scheduler.DAGScheduler]Got job 3 (foreachPartition at ReceiveKafkaData.scala:64) with 1 output partitions
[INFO][2018-05-24 19:51:50,090][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 3 (foreachPartition at ReceiveKafkaData.scala:64)
[INFO][2018-05-24 19:51:50,090][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 19:51:50,090][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 19:51:50,092][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 3 (MapPartitionsRDD[7] at map at ReceiveKafkaData.scala:62), which has no missing parents
[INFO][2018-05-24 19:51:50,096][org.apache.spark.storage.memory.MemoryStore]Block broadcast_3 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-24 19:51:50,100][org.apache.spark.storage.memory.MemoryStore]Block broadcast_3_piece0 stored as bytes in memory (estimated size 2008.0 B, free 912.3 MB)
[INFO][2018-05-24 19:51:50,100][org.apache.spark.storage.BlockManagerInfo]Added broadcast_3_piece0 in memory on 192.168.0.102:65423 (size: 2008.0 B, free: 912.3 MB)
[INFO][2018-05-24 19:51:50,101][org.apache.spark.SparkContext]Created broadcast 3 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 19:51:50,102][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[7] at map at ReceiveKafkaData.scala:62) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 19:51:50,102][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 3.0 with 1 tasks
[INFO][2018-05-24 19:51:50,103][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 3.0 (TID 3, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 19:51:50,104][org.apache.spark.executor.Executor]Running task 0.0 in stage 3.0 (TID 3)
[INFO][2018-05-24 19:51:50,108][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 11876 is the same as ending offset skipping seven 0
[INFO][2018-05-24 19:51:50,109][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 19:51:50,110][org.apache.spark.executor.Executor]Finished task 0.0 in stage 3.0 (TID 3). 708 bytes result sent to driver
[INFO][2018-05-24 19:51:50,111][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 3.0 (TID 3) in 8 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 19:51:50,112][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 3.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 19:51:50,112][org.apache.spark.scheduler.DAGScheduler]ResultStage 3 (foreachPartition at ReceiveKafkaData.scala:64) finished in 0.010 s
[INFO][2018-05-24 19:51:50,113][org.apache.spark.scheduler.DAGScheduler]Job 3 finished: foreachPartition at ReceiveKafkaData.scala:64, took 0.024024 s
[INFO][2018-05-24 19:51:50,113][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527162700000 ms.0 from job set of time 1527162700000 ms
[INFO][2018-05-24 19:51:50,114][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 5 from persistence list
[INFO][2018-05-24 19:51:50,114][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 10.113 s for time 1527162700000 ms (execution: 0.036 s)
[INFO][2018-05-24 19:51:50,114][org.apache.spark.storage.BlockManager]Removing RDD 5
[INFO][2018-05-24 19:51:50,115][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 4 from persistence list
[INFO][2018-05-24 19:51:50,115][org.apache.spark.storage.BlockManager]Removing RDD 4
[INFO][2018-05-24 19:51:50,116][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 19:51:50,116][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527162660000 ms
[INFO][2018-05-24 19:52:05,078][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527162720000 ms
[INFO][2018-05-24 19:52:05,079][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527162720000 ms.0 from job set of time 1527162720000 ms
[INFO][2018-05-24 19:52:05,086][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:64
[INFO][2018-05-24 19:52:05,087][org.apache.spark.scheduler.DAGScheduler]Got job 4 (foreachPartition at ReceiveKafkaData.scala:64) with 1 output partitions
[INFO][2018-05-24 19:52:05,088][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 4 (foreachPartition at ReceiveKafkaData.scala:64)
[INFO][2018-05-24 19:52:05,088][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 19:52:05,088][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 19:52:05,088][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 4 (MapPartitionsRDD[9] at map at ReceiveKafkaData.scala:62), which has no missing parents
[INFO][2018-05-24 19:52:05,091][org.apache.spark.storage.memory.MemoryStore]Block broadcast_4 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-24 19:52:05,095][org.apache.spark.storage.memory.MemoryStore]Block broadcast_4_piece0 stored as bytes in memory (estimated size 2008.0 B, free 912.3 MB)
[INFO][2018-05-24 19:52:05,095][org.apache.spark.storage.BlockManagerInfo]Added broadcast_4_piece0 in memory on 192.168.0.102:65423 (size: 2008.0 B, free: 912.3 MB)
[INFO][2018-05-24 19:52:05,096][org.apache.spark.SparkContext]Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 19:52:05,097][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[9] at map at ReceiveKafkaData.scala:62) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 19:52:05,097][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 4.0 with 1 tasks
[INFO][2018-05-24 19:52:05,098][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 4.0 (TID 4, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 19:52:05,099][org.apache.spark.executor.Executor]Running task 0.0 in stage 4.0 (TID 4)
[INFO][2018-05-24 19:52:05,107][org.apache.spark.streaming.kafka.KafkaRDD]Computing topic seven, partition 0 offsets 11876 -> 11889
[INFO][2018-05-24 19:52:05,108][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 19:52:05,108][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 19:52:05,108][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 19:52:05,280][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_1_piece0 on 192.168.0.102:65423 in memory (size: 2008.0 B, free: 912.3 MB)
[INFO][2018-05-24 19:52:05,283][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_3_piece0 on 192.168.0.102:65423 in memory (size: 2008.0 B, free: 912.3 MB)
[INFO][2018-05-24 19:52:05,285][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_0_piece0 on 192.168.0.102:65423 in memory (size: 2007.0 B, free: 912.3 MB)
[INFO][2018-05-24 19:52:05,287][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_2_piece0 on 192.168.0.102:65423 in memory (size: 2008.0 B, free: 912.3 MB)
[INFO][2018-05-24 19:52:10,683][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 19:52:10,684][org.apache.spark.executor.Executor]Finished task 0.0 in stage 4.0 (TID 4). 751 bytes result sent to driver
[INFO][2018-05-24 19:52:10,685][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 4.0 (TID 4) in 5587 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 19:52:10,685][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 4.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 19:52:10,686][org.apache.spark.scheduler.DAGScheduler]ResultStage 4 (foreachPartition at ReceiveKafkaData.scala:64) finished in 5.588 s
[INFO][2018-05-24 19:52:10,686][org.apache.spark.scheduler.DAGScheduler]Job 4 finished: foreachPartition at ReceiveKafkaData.scala:64, took 5.600137 s
[INFO][2018-05-24 19:52:10,687][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527162720000 ms.0 from job set of time 1527162720000 ms
[INFO][2018-05-24 19:52:10,687][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 10.687 s for time 1527162720000 ms (execution: 5.608 s)
[INFO][2018-05-24 19:52:10,687][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 7 from persistence list
[INFO][2018-05-24 19:52:10,688][org.apache.spark.storage.BlockManager]Removing RDD 7
[INFO][2018-05-24 19:52:10,688][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 6 from persistence list
[INFO][2018-05-24 19:52:10,688][org.apache.spark.storage.BlockManager]Removing RDD 6
[INFO][2018-05-24 19:52:10,688][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 19:52:10,688][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527162680000 ms
[INFO][2018-05-24 19:52:30,082][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527162740000 ms
[INFO][2018-05-24 19:52:30,083][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527162740000 ms.0 from job set of time 1527162740000 ms
[INFO][2018-05-24 19:52:30,090][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:64
[INFO][2018-05-24 19:52:30,091][org.apache.spark.scheduler.DAGScheduler]Got job 5 (foreachPartition at ReceiveKafkaData.scala:64) with 1 output partitions
[INFO][2018-05-24 19:52:30,091][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 5 (foreachPartition at ReceiveKafkaData.scala:64)
[INFO][2018-05-24 19:52:30,091][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 19:52:30,091][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 19:52:30,092][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 5 (MapPartitionsRDD[11] at map at ReceiveKafkaData.scala:62), which has no missing parents
[INFO][2018-05-24 19:52:30,094][org.apache.spark.storage.memory.MemoryStore]Block broadcast_5 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-24 19:52:30,100][org.apache.spark.storage.memory.MemoryStore]Block broadcast_5_piece0 stored as bytes in memory (estimated size 2008.0 B, free 912.3 MB)
[INFO][2018-05-24 19:52:30,100][org.apache.spark.storage.BlockManagerInfo]Added broadcast_5_piece0 in memory on 192.168.0.102:65423 (size: 2008.0 B, free: 912.3 MB)
[INFO][2018-05-24 19:52:30,102][org.apache.spark.SparkContext]Created broadcast 5 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 19:52:30,104][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[11] at map at ReceiveKafkaData.scala:62) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 19:52:30,104][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 5.0 with 1 tasks
[INFO][2018-05-24 19:52:30,106][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 5.0 (TID 5, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 19:52:30,106][org.apache.spark.executor.Executor]Running task 0.0 in stage 5.0 (TID 5)
[INFO][2018-05-24 19:52:30,110][org.apache.spark.streaming.kafka.KafkaRDD]Computing topic seven, partition 0 offsets 11889 -> 11914
[INFO][2018-05-24 19:52:30,111][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 19:52:30,111][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 19:52:30,111][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 19:52:30,269][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 19:52:30,270][org.apache.spark.executor.Executor]Finished task 0.0 in stage 5.0 (TID 5). 708 bytes result sent to driver
[INFO][2018-05-24 19:52:30,271][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 5.0 (TID 5) in 166 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 19:52:30,271][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 5.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 19:52:30,272][org.apache.spark.scheduler.DAGScheduler]ResultStage 5 (foreachPartition at ReceiveKafkaData.scala:64) finished in 0.167 s
[INFO][2018-05-24 19:52:30,273][org.apache.spark.scheduler.DAGScheduler]Job 5 finished: foreachPartition at ReceiveKafkaData.scala:64, took 0.181987 s
[INFO][2018-05-24 19:52:30,273][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527162740000 ms.0 from job set of time 1527162740000 ms
[INFO][2018-05-24 19:52:30,273][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 10.273 s for time 1527162740000 ms (execution: 0.190 s)
[INFO][2018-05-24 19:52:30,273][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 9 from persistence list
[INFO][2018-05-24 19:52:30,274][org.apache.spark.storage.BlockManager]Removing RDD 9
[INFO][2018-05-24 19:52:30,274][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 8 from persistence list
[INFO][2018-05-24 19:52:30,275][org.apache.spark.storage.BlockManager]Removing RDD 8
[INFO][2018-05-24 19:52:30,275][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 19:52:30,275][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527162700000 ms
[INFO][2018-05-24 19:52:40,052][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527162760000 ms
[INFO][2018-05-24 19:52:40,053][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527162760000 ms.0 from job set of time 1527162760000 ms
[INFO][2018-05-24 19:52:40,065][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:64
[INFO][2018-05-24 19:52:40,066][org.apache.spark.scheduler.DAGScheduler]Got job 6 (foreachPartition at ReceiveKafkaData.scala:64) with 1 output partitions
[INFO][2018-05-24 19:52:40,066][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 6 (foreachPartition at ReceiveKafkaData.scala:64)
[INFO][2018-05-24 19:52:40,066][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 19:52:40,066][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 19:52:40,067][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 6 (MapPartitionsRDD[13] at map at ReceiveKafkaData.scala:62), which has no missing parents
[INFO][2018-05-24 19:52:40,069][org.apache.spark.storage.memory.MemoryStore]Block broadcast_6 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-24 19:52:40,075][org.apache.spark.storage.memory.MemoryStore]Block broadcast_6_piece0 stored as bytes in memory (estimated size 2008.0 B, free 912.3 MB)
[INFO][2018-05-24 19:52:40,076][org.apache.spark.storage.BlockManagerInfo]Added broadcast_6_piece0 in memory on 192.168.0.102:65423 (size: 2008.0 B, free: 912.3 MB)
[INFO][2018-05-24 19:52:40,077][org.apache.spark.SparkContext]Created broadcast 6 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 19:52:40,079][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[13] at map at ReceiveKafkaData.scala:62) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 19:52:40,079][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 6.0 with 1 tasks
[INFO][2018-05-24 19:52:40,080][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 6.0 (TID 6, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 19:52:40,080][org.apache.spark.executor.Executor]Running task 0.0 in stage 6.0 (TID 6)
[INFO][2018-05-24 19:52:40,083][org.apache.spark.streaming.kafka.KafkaRDD]Computing topic seven, partition 0 offsets 11914 -> 11924
[INFO][2018-05-24 19:52:40,084][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 19:52:40,084][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 19:52:40,084][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 19:52:40,156][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 19:52:40,157][org.apache.spark.executor.Executor]Finished task 0.0 in stage 6.0 (TID 6). 708 bytes result sent to driver
[INFO][2018-05-24 19:52:40,158][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 6.0 (TID 6) in 79 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 19:52:40,159][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 6.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 19:52:40,160][org.apache.spark.scheduler.DAGScheduler]ResultStage 6 (foreachPartition at ReceiveKafkaData.scala:64) finished in 0.080 s
[INFO][2018-05-24 19:52:40,161][org.apache.spark.scheduler.DAGScheduler]Job 6 finished: foreachPartition at ReceiveKafkaData.scala:64, took 0.094869 s
[INFO][2018-05-24 19:52:40,161][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527162760000 ms.0 from job set of time 1527162760000 ms
[INFO][2018-05-24 19:52:40,162][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.161 s for time 1527162760000 ms (execution: 0.108 s)
[INFO][2018-05-24 19:52:40,162][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 11 from persistence list
[INFO][2018-05-24 19:52:40,162][org.apache.spark.storage.BlockManager]Removing RDD 11
[INFO][2018-05-24 19:52:40,162][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 10 from persistence list
[INFO][2018-05-24 19:52:40,163][org.apache.spark.storage.BlockManager]Removing RDD 10
[INFO][2018-05-24 19:52:40,163][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 19:52:40,164][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527162720000 ms
[INFO][2018-05-24 19:53:10,080][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527162780000 ms
[INFO][2018-05-24 19:53:10,081][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527162780000 ms.0 from job set of time 1527162780000 ms
[INFO][2018-05-24 19:53:10,088][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:64
[INFO][2018-05-24 19:53:10,089][org.apache.spark.scheduler.DAGScheduler]Got job 7 (foreachPartition at ReceiveKafkaData.scala:64) with 1 output partitions
[INFO][2018-05-24 19:53:10,089][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 7 (foreachPartition at ReceiveKafkaData.scala:64)
[INFO][2018-05-24 19:53:10,089][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 19:53:10,089][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 19:53:10,090][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 7 (MapPartitionsRDD[15] at map at ReceiveKafkaData.scala:62), which has no missing parents
[INFO][2018-05-24 19:53:10,094][org.apache.spark.storage.memory.MemoryStore]Block broadcast_7 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-24 19:53:10,100][org.apache.spark.storage.memory.MemoryStore]Block broadcast_7_piece0 stored as bytes in memory (estimated size 2008.0 B, free 912.3 MB)
[INFO][2018-05-24 19:53:10,101][org.apache.spark.storage.BlockManagerInfo]Added broadcast_7_piece0 in memory on 192.168.0.102:65423 (size: 2008.0 B, free: 912.3 MB)
[INFO][2018-05-24 19:53:10,101][org.apache.spark.SparkContext]Created broadcast 7 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 19:53:10,102][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[15] at map at ReceiveKafkaData.scala:62) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 19:53:10,102][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 7.0 with 1 tasks
[INFO][2018-05-24 19:53:10,103][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 7.0 (TID 7, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 19:53:10,103][org.apache.spark.executor.Executor]Running task 0.0 in stage 7.0 (TID 7)
[INFO][2018-05-24 19:53:10,107][org.apache.spark.streaming.kafka.KafkaRDD]Computing topic seven, partition 0 offsets 11924 -> 11954
[INFO][2018-05-24 19:53:10,107][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 19:53:10,108][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 19:53:10,108][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 19:53:15,209][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 19:53:15,210][org.apache.spark.executor.Executor]Finished task 0.0 in stage 7.0 (TID 7). 708 bytes result sent to driver
[INFO][2018-05-24 19:53:15,211][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 7.0 (TID 7) in 5108 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 19:53:15,211][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 7.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 19:53:15,211][org.apache.spark.scheduler.DAGScheduler]ResultStage 7 (foreachPartition at ReceiveKafkaData.scala:64) finished in 5.109 s
[INFO][2018-05-24 19:53:15,212][org.apache.spark.scheduler.DAGScheduler]Job 7 finished: foreachPartition at ReceiveKafkaData.scala:64, took 5.123324 s
[INFO][2018-05-24 19:53:15,212][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527162780000 ms.0 from job set of time 1527162780000 ms
[INFO][2018-05-24 19:53:15,213][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 13 from persistence list
[INFO][2018-05-24 19:53:15,213][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 15.212 s for time 1527162780000 ms (execution: 5.131 s)
[INFO][2018-05-24 19:53:15,213][org.apache.spark.storage.BlockManager]Removing RDD 13
[INFO][2018-05-24 19:53:15,213][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 12 from persistence list
[INFO][2018-05-24 19:53:15,214][org.apache.spark.storage.BlockManager]Removing RDD 12
[INFO][2018-05-24 19:53:15,214][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 19:53:15,214][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527162740000 ms
[INFO][2018-05-24 19:53:25,071][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527162800000 ms
[INFO][2018-05-24 19:53:25,072][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527162800000 ms.0 from job set of time 1527162800000 ms
[INFO][2018-05-24 19:53:25,080][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:64
[INFO][2018-05-24 19:53:25,081][org.apache.spark.scheduler.DAGScheduler]Got job 8 (foreachPartition at ReceiveKafkaData.scala:64) with 1 output partitions
[INFO][2018-05-24 19:53:25,081][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 8 (foreachPartition at ReceiveKafkaData.scala:64)
[INFO][2018-05-24 19:53:25,081][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 19:53:25,081][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 19:53:25,081][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 8 (MapPartitionsRDD[17] at map at ReceiveKafkaData.scala:62), which has no missing parents
[INFO][2018-05-24 19:53:25,083][org.apache.spark.storage.memory.MemoryStore]Block broadcast_8 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-24 19:53:25,089][org.apache.spark.storage.memory.MemoryStore]Block broadcast_8_piece0 stored as bytes in memory (estimated size 2008.0 B, free 912.3 MB)
[INFO][2018-05-24 19:53:25,089][org.apache.spark.storage.BlockManagerInfo]Added broadcast_8_piece0 in memory on 192.168.0.102:65423 (size: 2008.0 B, free: 912.3 MB)
[INFO][2018-05-24 19:53:25,090][org.apache.spark.SparkContext]Created broadcast 8 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 19:53:25,091][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[17] at map at ReceiveKafkaData.scala:62) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 19:53:25,091][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 8.0 with 1 tasks
[INFO][2018-05-24 19:53:25,092][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 8.0 (TID 8, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 19:53:25,093][org.apache.spark.executor.Executor]Running task 0.0 in stage 8.0 (TID 8)
[INFO][2018-05-24 19:53:25,095][org.apache.spark.streaming.kafka.KafkaRDD]Computing topic seven, partition 0 offsets 11954 -> 11968
[INFO][2018-05-24 19:53:25,095][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 19:53:25,096][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 19:53:25,096][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 19:53:25,161][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 19:53:25,162][org.apache.spark.executor.Executor]Finished task 0.0 in stage 8.0 (TID 8). 708 bytes result sent to driver
[INFO][2018-05-24 19:53:25,163][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 8.0 (TID 8) in 71 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 19:53:25,163][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 8.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 19:53:25,166][org.apache.spark.scheduler.DAGScheduler]ResultStage 8 (foreachPartition at ReceiveKafkaData.scala:64) finished in 0.074 s
[INFO][2018-05-24 19:53:25,171][org.apache.spark.scheduler.DAGScheduler]Job 8 finished: foreachPartition at ReceiveKafkaData.scala:64, took 0.090033 s
[INFO][2018-05-24 19:53:25,172][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527162800000 ms.0 from job set of time 1527162800000 ms
[INFO][2018-05-24 19:53:25,173][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 5.172 s for time 1527162800000 ms (execution: 0.100 s)
[INFO][2018-05-24 19:53:25,173][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 15 from persistence list
[INFO][2018-05-24 19:53:25,175][org.apache.spark.storage.BlockManager]Removing RDD 15
[INFO][2018-05-24 19:53:25,177][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 14 from persistence list
[INFO][2018-05-24 19:53:25,178][org.apache.spark.storage.BlockManager]Removing RDD 14
[INFO][2018-05-24 19:53:25,179][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 19:53:25,179][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527162760000 ms
[INFO][2018-05-24 19:53:33,135][org.apache.spark.util.ShutdownHookManager]Shutdown hook called
[INFO][2018-05-24 19:53:33,141][org.apache.spark.util.ShutdownHookManager]Deleting directory /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/spark-2852bfa3-446b-4bf6-81ff-a449c8a04522
[INFO][2018-05-24 19:53:50,094][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527162820000 ms
[INFO][2018-05-24 19:53:50,095][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527162820000 ms.0 from job set of time 1527162820000 ms
[INFO][2018-05-24 19:53:50,110][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:64
[INFO][2018-05-24 19:53:50,111][org.apache.spark.scheduler.DAGScheduler]Got job 9 (foreachPartition at ReceiveKafkaData.scala:64) with 1 output partitions
[INFO][2018-05-24 19:53:50,111][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 9 (foreachPartition at ReceiveKafkaData.scala:64)
[INFO][2018-05-24 19:53:50,111][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 19:53:50,111][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 19:53:50,112][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 9 (MapPartitionsRDD[19] at map at ReceiveKafkaData.scala:62), which has no missing parents
[INFO][2018-05-24 19:53:50,114][org.apache.spark.storage.memory.MemoryStore]Block broadcast_9 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-24 19:53:50,120][org.apache.spark.storage.memory.MemoryStore]Block broadcast_9_piece0 stored as bytes in memory (estimated size 2008.0 B, free 912.3 MB)
[INFO][2018-05-24 19:53:50,120][org.apache.spark.storage.BlockManagerInfo]Added broadcast_9_piece0 in memory on 192.168.0.102:65423 (size: 2008.0 B, free: 912.3 MB)
[INFO][2018-05-24 19:53:50,121][org.apache.spark.SparkContext]Created broadcast 9 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 19:53:50,122][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[19] at map at ReceiveKafkaData.scala:62) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 19:53:50,122][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 9.0 with 1 tasks
[INFO][2018-05-24 19:53:50,122][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 9.0 (TID 9, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 19:53:50,123][org.apache.spark.executor.Executor]Running task 0.0 in stage 9.0 (TID 9)
[INFO][2018-05-24 19:53:50,125][org.apache.spark.streaming.kafka.KafkaRDD]Computing topic seven, partition 0 offsets 11968 -> 11976
[INFO][2018-05-24 19:53:50,125][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 19:53:50,125][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 19:53:50,125][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 19:53:55,194][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 19:53:55,229][org.apache.spark.executor.Executor]Finished task 0.0 in stage 9.0 (TID 9). 665 bytes result sent to driver
[INFO][2018-05-24 19:53:55,230][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 9.0 (TID 9) in 5108 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 19:53:55,231][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 9.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 19:53:55,231][org.apache.spark.scheduler.DAGScheduler]ResultStage 9 (foreachPartition at ReceiveKafkaData.scala:64) finished in 5.109 s
[INFO][2018-05-24 19:53:55,232][org.apache.spark.scheduler.DAGScheduler]Job 9 finished: foreachPartition at ReceiveKafkaData.scala:64, took 5.121371 s
[INFO][2018-05-24 19:53:55,233][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527162820000 ms.0 from job set of time 1527162820000 ms
[INFO][2018-05-24 19:53:55,233][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 17 from persistence list
[INFO][2018-05-24 19:53:55,233][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 15.233 s for time 1527162820000 ms (execution: 5.138 s)
[INFO][2018-05-24 19:53:55,234][org.apache.spark.storage.BlockManager]Removing RDD 17
[INFO][2018-05-24 19:53:55,234][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 16 from persistence list
[INFO][2018-05-24 19:53:55,234][org.apache.spark.storage.BlockManager]Removing RDD 16
[INFO][2018-05-24 19:53:55,234][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 19:53:55,234][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527162780000 ms
[INFO][2018-05-24 19:54:00,058][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527162840000 ms
[INFO][2018-05-24 19:54:00,060][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527162840000 ms.0 from job set of time 1527162840000 ms
[INFO][2018-05-24 19:54:00,072][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:64
[INFO][2018-05-24 19:54:00,073][org.apache.spark.scheduler.DAGScheduler]Got job 10 (foreachPartition at ReceiveKafkaData.scala:64) with 1 output partitions
[INFO][2018-05-24 19:54:00,073][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 10 (foreachPartition at ReceiveKafkaData.scala:64)
[INFO][2018-05-24 19:54:00,073][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 19:54:00,073][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 19:54:00,074][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 10 (MapPartitionsRDD[21] at map at ReceiveKafkaData.scala:62), which has no missing parents
[INFO][2018-05-24 19:54:00,075][org.apache.spark.storage.memory.MemoryStore]Block broadcast_10 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-24 19:54:00,083][org.apache.spark.storage.memory.MemoryStore]Block broadcast_10_piece0 stored as bytes in memory (estimated size 2008.0 B, free 912.3 MB)
[INFO][2018-05-24 19:54:00,083][org.apache.spark.storage.BlockManagerInfo]Added broadcast_10_piece0 in memory on 192.168.0.102:65423 (size: 2008.0 B, free: 912.3 MB)
[INFO][2018-05-24 19:54:00,084][org.apache.spark.SparkContext]Created broadcast 10 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 19:54:00,084][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[21] at map at ReceiveKafkaData.scala:62) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 19:54:00,084][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 10.0 with 1 tasks
[INFO][2018-05-24 19:54:00,085][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 10.0 (TID 10, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 19:54:00,085][org.apache.spark.executor.Executor]Running task 0.0 in stage 10.0 (TID 10)
[INFO][2018-05-24 19:54:00,087][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 11976 is the same as ending offset skipping seven 0
[INFO][2018-05-24 19:54:00,087][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 19:54:00,088][org.apache.spark.executor.Executor]Finished task 0.0 in stage 10.0 (TID 10). 708 bytes result sent to driver
[INFO][2018-05-24 19:54:00,089][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 10.0 (TID 10) in 4 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 19:54:00,089][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 10.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 19:54:00,090][org.apache.spark.scheduler.DAGScheduler]ResultStage 10 (foreachPartition at ReceiveKafkaData.scala:64) finished in 0.005 s
[INFO][2018-05-24 19:54:00,090][org.apache.spark.scheduler.DAGScheduler]Job 10 finished: foreachPartition at ReceiveKafkaData.scala:64, took 0.017870 s
[INFO][2018-05-24 19:54:00,091][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527162840000 ms.0 from job set of time 1527162840000 ms
[INFO][2018-05-24 19:54:00,091][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.091 s for time 1527162840000 ms (execution: 0.031 s)
[INFO][2018-05-24 19:54:00,091][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 19 from persistence list
[INFO][2018-05-24 19:54:00,091][org.apache.spark.storage.BlockManager]Removing RDD 19
[INFO][2018-05-24 19:54:00,092][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 18 from persistence list
[INFO][2018-05-24 19:54:00,092][org.apache.spark.storage.BlockManager]Removing RDD 18
[INFO][2018-05-24 19:54:00,093][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 19:54:00,093][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527162800000 ms
[INFO][2018-05-24 19:54:30,068][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527162860000 ms
[INFO][2018-05-24 19:54:30,068][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527162860000 ms.0 from job set of time 1527162860000 ms
[INFO][2018-05-24 19:54:30,076][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:64
[INFO][2018-05-24 19:54:30,076][org.apache.spark.scheduler.DAGScheduler]Got job 11 (foreachPartition at ReceiveKafkaData.scala:64) with 1 output partitions
[INFO][2018-05-24 19:54:30,077][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 11 (foreachPartition at ReceiveKafkaData.scala:64)
[INFO][2018-05-24 19:54:30,077][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 19:54:30,077][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 19:54:30,077][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 11 (MapPartitionsRDD[23] at map at ReceiveKafkaData.scala:62), which has no missing parents
[INFO][2018-05-24 19:54:30,079][org.apache.spark.storage.memory.MemoryStore]Block broadcast_11 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-24 19:54:30,085][org.apache.spark.storage.memory.MemoryStore]Block broadcast_11_piece0 stored as bytes in memory (estimated size 2008.0 B, free 912.3 MB)
[INFO][2018-05-24 19:54:30,086][org.apache.spark.storage.BlockManagerInfo]Added broadcast_11_piece0 in memory on 192.168.0.102:65423 (size: 2008.0 B, free: 912.3 MB)
[INFO][2018-05-24 19:54:30,086][org.apache.spark.SparkContext]Created broadcast 11 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 19:54:30,087][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[23] at map at ReceiveKafkaData.scala:62) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 19:54:30,087][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 11.0 with 1 tasks
[INFO][2018-05-24 19:54:30,088][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 11.0 (TID 11, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 19:54:30,088][org.apache.spark.executor.Executor]Running task 0.0 in stage 11.0 (TID 11)
[INFO][2018-05-24 19:54:30,090][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 11976 is the same as ending offset skipping seven 0
[INFO][2018-05-24 19:54:30,090][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 19:54:30,091][org.apache.spark.executor.Executor]Finished task 0.0 in stage 11.0 (TID 11). 708 bytes result sent to driver
[INFO][2018-05-24 19:54:30,092][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 11.0 (TID 11) in 5 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 19:54:30,092][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 11.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 19:54:30,093][org.apache.spark.scheduler.DAGScheduler]ResultStage 11 (foreachPartition at ReceiveKafkaData.scala:64) finished in 0.005 s
[INFO][2018-05-24 19:54:30,093][org.apache.spark.scheduler.DAGScheduler]Job 11 finished: foreachPartition at ReceiveKafkaData.scala:64, took 0.017130 s
[INFO][2018-05-24 19:54:30,094][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527162860000 ms.0 from job set of time 1527162860000 ms
[INFO][2018-05-24 19:54:30,094][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 21 from persistence list
[INFO][2018-05-24 19:54:30,094][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 10.093 s for time 1527162860000 ms (execution: 0.025 s)
[INFO][2018-05-24 19:54:30,094][org.apache.spark.storage.BlockManager]Removing RDD 21
[INFO][2018-05-24 19:54:30,094][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 20 from persistence list
[INFO][2018-05-24 19:54:30,095][org.apache.spark.storage.BlockManager]Removing RDD 20
[INFO][2018-05-24 19:54:30,095][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 19:54:30,095][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527162820000 ms
[INFO][2018-05-24 19:54:35,625][org.apache.spark.streaming.StreamingContext]Invoking stop(stopGracefully=false) from shutdown hook
[INFO][2018-05-24 19:54:35,629][org.apache.spark.streaming.scheduler.ReceiverTracker]ReceiverTracker stopped
[INFO][2018-05-24 19:54:35,630][org.apache.spark.streaming.scheduler.JobGenerator]Stopping JobGenerator immediately
[INFO][2018-05-24 19:54:35,631][org.apache.spark.streaming.util.RecurringTimer]Stopped timer for JobGenerator after time 1527162860000
[INFO][2018-05-24 19:54:35,634][org.apache.spark.streaming.scheduler.JobGenerator]Stopped JobGenerator
[INFO][2018-05-24 19:54:35,635][org.apache.spark.streaming.scheduler.JobScheduler]Stopped JobScheduler
[INFO][2018-05-24 19:54:35,644][org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@6972c30a{/streaming,null,UNAVAILABLE,@Spark}
[INFO][2018-05-24 19:54:35,645][org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@5109e8cf{/streaming/batch,null,UNAVAILABLE,@Spark}
[INFO][2018-05-24 19:54:35,647][org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@6c65860d{/static/streaming,null,UNAVAILABLE,@Spark}
[INFO][2018-05-24 19:54:35,648][org.apache.spark.streaming.StreamingContext]StreamingContext stopped successfully
[INFO][2018-05-24 19:54:35,649][org.apache.spark.SparkContext]Invoking stop() from shutdown hook
[INFO][2018-05-24 19:54:35,668][org.spark_project.jetty.server.AbstractConnector]Stopped Spark@6d366c9b{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-05-24 19:54:35,670][org.apache.spark.ui.SparkUI]Stopped Spark web UI at http://192.168.0.102:4040
[INFO][2018-05-24 19:54:35,691][org.apache.spark.MapOutputTrackerMasterEndpoint]MapOutputTrackerMasterEndpoint stopped!
[INFO][2018-05-24 19:54:35,711][org.apache.spark.storage.memory.MemoryStore]MemoryStore cleared
[INFO][2018-05-24 19:54:35,711][org.apache.spark.storage.BlockManager]BlockManager stopped
[INFO][2018-05-24 19:54:35,712][org.apache.spark.storage.BlockManagerMaster]BlockManagerMaster stopped
[INFO][2018-05-24 19:54:35,714][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint]OutputCommitCoordinator stopped!
[INFO][2018-05-24 19:54:35,716][org.apache.spark.SparkContext]Successfully stopped SparkContext
[INFO][2018-05-24 19:54:35,716][org.apache.spark.util.ShutdownHookManager]Shutdown hook called
[INFO][2018-05-24 19:54:35,717][org.apache.spark.util.ShutdownHookManager]Deleting directory /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/spark-b8eaa8ee-cee4-408c-8f33-4b3df1f9f3dc
[INFO][2018-05-24 19:57:07,753][org.apache.spark.SparkContext]Running Spark version 2.2.0
[INFO][2018-05-24 19:57:08,510][org.apache.spark.SparkContext]Submitted application: ReceiveKafkaData$
[INFO][2018-05-24 19:57:08,529][org.apache.spark.SecurityManager]Changing view acls to: seven
[INFO][2018-05-24 19:57:08,530][org.apache.spark.SecurityManager]Changing modify acls to: seven
[INFO][2018-05-24 19:57:08,530][org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO][2018-05-24 19:57:08,531][org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO][2018-05-24 19:57:08,531][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(seven); groups with view permissions: Set(); users  with modify permissions: Set(seven); groups with modify permissions: Set()
[INFO][2018-05-24 19:57:08,785][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 49170.
[INFO][2018-05-24 19:57:08,805][org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO][2018-05-24 19:57:08,822][org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO][2018-05-24 19:57:08,826][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2018-05-24 19:57:08,826][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
[INFO][2018-05-24 19:57:08,836][org.apache.spark.storage.DiskBlockManager]Created local directory at /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/blockmgr-e194edcd-7adf-400b-bbc2-116ed05536c2
[INFO][2018-05-24 19:57:08,849][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 912.3 MB
[INFO][2018-05-24 19:57:08,925][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO][2018-05-24 19:57:08,995][org.spark_project.jetty.util.log]Logging initialized @2181ms
[INFO][2018-05-24 19:57:09,070][org.spark_project.jetty.server.Server]jetty-9.3.z-SNAPSHOT
[INFO][2018-05-24 19:57:09,083][org.spark_project.jetty.server.Server]Started @2270ms
[INFO][2018-05-24 19:57:09,107][org.spark_project.jetty.server.AbstractConnector]Started ServerConnector@2090b9b9{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-05-24 19:57:09,107][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4040.
[INFO][2018-05-24 19:57:09,136][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@757d6814{/jobs,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:57:09,137][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@119f1f2a{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:57:09,138][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5b970f7{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:57:09,142][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6175619b{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:57:09,147][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@756cf158{/stages,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:57:09,152][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@751d3241{/stages/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:57:09,153][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@64337702{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:57:09,193][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4e76dac{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:57:09,195][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5f2f577{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:57:09,196][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5d465e4b{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:57:09,200][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@41a90fa8{/storage,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:57:09,207][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@52500920{/storage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:57:09,209][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@18a3962d{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:57:09,210][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2a65bb85{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:57:09,211][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4f936da8{/environment,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:57:09,211][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@452ba1db{/environment/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:57:09,212][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@76a36b71{/executors,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:57:09,216][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@f9d87b{/executors/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:57:09,225][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@26fb628{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:57:09,226][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@70dd7e15{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:57:09,261][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@35beb15e{/static,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:57:09,262][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6a62689d{/,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:57:09,276][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@60fa3495{/api,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:57:09,278][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1ca25c47{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:57:09,279][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@533b266e{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:57:09,283][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://192.168.0.102:4040
[INFO][2018-05-24 19:57:09,411][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
[INFO][2018-05-24 19:57:09,433][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 49171.
[INFO][2018-05-24 19:57:09,434][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 192.168.0.102:49171
[INFO][2018-05-24 19:57:09,436][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2018-05-24 19:57:09,440][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 192.168.0.102, 49171, None)
[INFO][2018-05-24 19:57:09,448][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 192.168.0.102:49171 with 912.3 MB RAM, BlockManagerId(driver, 192.168.0.102, 49171, None)
[INFO][2018-05-24 19:57:09,451][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 192.168.0.102, 49171, None)
[INFO][2018-05-24 19:57:09,452][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 192.168.0.102, 49171, None)
[INFO][2018-05-24 19:57:09,653][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7d2a6eac{/metrics/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:57:09,833][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 19:57:09,836][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 19:57:09,836][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 19:57:25,325][org.apache.spark.streaming.kafka.DirectKafkaInputDStream]Slide time = 60000 ms
[INFO][2018-05-24 19:57:25,326][org.apache.spark.streaming.kafka.DirectKafkaInputDStream]Storage level = Serialized 1x Replicated
[INFO][2018-05-24 19:57:25,326][org.apache.spark.streaming.kafka.DirectKafkaInputDStream]Checkpoint interval = null
[INFO][2018-05-24 19:57:25,327][org.apache.spark.streaming.kafka.DirectKafkaInputDStream]Remember interval = 60000 ms
[INFO][2018-05-24 19:57:25,327][org.apache.spark.streaming.kafka.DirectKafkaInputDStream]Initialized and validated org.apache.spark.streaming.kafka.DirectKafkaInputDStream@1c380177
[INFO][2018-05-24 19:57:25,328][org.apache.spark.streaming.dstream.MappedDStream]Slide time = 60000 ms
[INFO][2018-05-24 19:57:25,328][org.apache.spark.streaming.dstream.MappedDStream]Storage level = Serialized 1x Replicated
[INFO][2018-05-24 19:57:25,328][org.apache.spark.streaming.dstream.MappedDStream]Checkpoint interval = null
[INFO][2018-05-24 19:57:25,328][org.apache.spark.streaming.dstream.MappedDStream]Remember interval = 60000 ms
[INFO][2018-05-24 19:57:25,328][org.apache.spark.streaming.dstream.MappedDStream]Initialized and validated org.apache.spark.streaming.dstream.MappedDStream@2ed804fc
[INFO][2018-05-24 19:57:25,328][org.apache.spark.streaming.dstream.ForEachDStream]Slide time = 60000 ms
[INFO][2018-05-24 19:57:25,328][org.apache.spark.streaming.dstream.ForEachDStream]Storage level = Serialized 1x Replicated
[INFO][2018-05-24 19:57:25,328][org.apache.spark.streaming.dstream.ForEachDStream]Checkpoint interval = null
[INFO][2018-05-24 19:57:25,328][org.apache.spark.streaming.dstream.ForEachDStream]Remember interval = 60000 ms
[INFO][2018-05-24 19:57:25,328][org.apache.spark.streaming.dstream.ForEachDStream]Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@49b81857
[INFO][2018-05-24 19:57:25,362][org.apache.spark.streaming.util.RecurringTimer]Started timer for JobGenerator at time 1527163080000
[INFO][2018-05-24 19:57:25,363][org.apache.spark.streaming.scheduler.JobGenerator]Started JobGenerator at 1527163080000 ms
[INFO][2018-05-24 19:57:25,364][org.apache.spark.streaming.scheduler.JobScheduler]Started JobScheduler
[INFO][2018-05-24 19:57:25,366][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@57a48985{/streaming,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:57:25,367][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3910fe11{/streaming/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:57:25,368][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3f672204{/streaming/batch,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:57:25,369][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2c2db130{/streaming/batch/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:57:25,372][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2d000e80{/static/streaming,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:57:25,372][org.apache.spark.streaming.StreamingContext]StreamingContext started
[INFO][2018-05-24 19:58:00,063][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 19:58:00,064][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 19:58:00,064][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 19:58:10,163][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527163080000 ms
[INFO][2018-05-24 19:58:10,166][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527163080000 ms.0 from job set of time 1527163080000 ms
[INFO][2018-05-24 19:58:10,212][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:64
[INFO][2018-05-24 19:58:10,243][org.apache.spark.scheduler.DAGScheduler]Got job 0 (foreachPartition at ReceiveKafkaData.scala:64) with 1 output partitions
[INFO][2018-05-24 19:58:10,243][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 0 (foreachPartition at ReceiveKafkaData.scala:64)
[INFO][2018-05-24 19:58:10,243][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 19:58:10,245][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 19:58:10,253][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 0 (MapPartitionsRDD[1] at map at ReceiveKafkaData.scala:62), which has no missing parents
[INFO][2018-05-24 19:58:10,371][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-24 19:58:10,413][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0_piece0 stored as bytes in memory (estimated size 2007.0 B, free 912.3 MB)
[INFO][2018-05-24 19:58:10,414][org.apache.spark.storage.BlockManagerInfo]Added broadcast_0_piece0 in memory on 192.168.0.102:49171 (size: 2007.0 B, free: 912.3 MB)
[INFO][2018-05-24 19:58:10,417][org.apache.spark.SparkContext]Created broadcast 0 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 19:58:10,433][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at map at ReceiveKafkaData.scala:62) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 19:58:10,434][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 0.0 with 1 tasks
[INFO][2018-05-24 19:58:10,467][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 19:58:10,476][org.apache.spark.executor.Executor]Running task 0.0 in stage 0.0 (TID 0)
[INFO][2018-05-24 19:58:10,505][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 11976 is the same as ending offset skipping seven 0
[INFO][2018-05-24 19:58:10,669][org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper]Process identifier=hconnection-0x68404ee2 connecting to ZooKeeper ensemble=vm-xaj-bigdata-da-d01:2181,vm-xaj-bigdata-da-d02:2181,vm-xaj-bigdata-da-d03:2181
[INFO][2018-05-24 19:58:10,674][org.apache.zookeeper.ZooKeeper]Client environment:zookeeper.version=3.4.5-cdh5.7.0--1, built on 04/06/2016 14:24 GMT
[INFO][2018-05-24 19:58:10,675][org.apache.zookeeper.ZooKeeper]Client environment:host.name=192.168.0.102
[INFO][2018-05-24 19:58:10,675][org.apache.zookeeper.ZooKeeper]Client environment:java.version=1.8.0_161
[INFO][2018-05-24 19:58:10,675][org.apache.zookeeper.ZooKeeper]Client environment:java.vendor=Oracle Corporation
[INFO][2018-05-24 19:58:10,675][org.apache.zookeeper.ZooKeeper]Client environment:java.home=/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre
[INFO][2018-05-24 19:58:10,675][org.apache.zookeeper.ZooKeeper]Client environment:java.class.path=/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/charsets.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/deploy.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/cldrdata.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/dnsns.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/jaccess.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/jfxrt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/localedata.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/nashorn.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/sunec.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/sunjce_provider.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/sunpkcs11.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/zipfs.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/javaws.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/jce.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/jfr.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/jfxswt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/jsse.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/management-agent.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/plugin.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/resources.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/rt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/ant-javafx.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/dt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/javafx-mx.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/jconsole.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/packager.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/sa-jdi.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/tools.jar:/Users/seven/project/scala/dataMining/target/classes:/Users/seven/software/maven/repository/org/scala-lang/scala-library/2.11.12/scala-library-2.11.12.jar:/Users/seven/software/maven/repository/org/scala-lang/scala-compiler/2.11.12/scala-compiler-2.11.12.jar:/Users/seven/software/maven/repository/org/scala-lang/modules/scala-xml_2.11/1.0.5/scala-xml_2.11-1.0.5.jar:/Users/seven/software/maven/repository/org/scala-lang/modules/scala-parser-combinators_2.11/1.0.4/scala-parser-combinators_2.11-1.0.4.jar:/Users/seven/software/maven/repository/org/scala-lang/scala-reflect/2.11.12/scala-reflect-2.11.12.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-client/2.6.0-cdh5.7.0/hadoop-client-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-common/2.6.0-cdh5.7.0/hadoop-common-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/xmlenc/xmlenc/0.52/xmlenc-0.52.jar:/Users/seven/software/maven/repository/commons-net/commons-net/3.1/commons-net-3.1.jar:/Users/seven/software/maven/repository/commons-configuration/commons-configuration/1.6/commons-configuration-1.6.jar:/Users/seven/software/maven/repository/commons-digester/commons-digester/1.8/commons-digester-1.8.jar:/Users/seven/software/maven/repository/commons-beanutils/commons-beanutils/1.7.0/commons-beanutils-1.7.0.jar:/Users/seven/software/maven/repository/org/slf4j/slf4j-api/1.7.5/slf4j-api-1.7.5.jar:/Users/seven/software/maven/repository/org/slf4j/slf4j-log4j12/1.7.5/slf4j-log4j12-1.7.5.jar:/Users/seven/software/maven/repository/org/apache/avro/avro/1.7.6-cdh5.7.0/avro-1.7.6-cdh5.7.0.jar:/Users/seven/software/maven/repository/com/google/code/gson/gson/2.2.4/gson-2.2.4.jar:/Users/seven/software/maven/repository/org/apache/curator/curator-client/2.7.1/curator-client-2.7.1.jar:/Users/seven/software/maven/repository/org/apache/curator/curator-recipes/2.7.1/curator-recipes-2.7.1.jar:/Users/seven/software/maven/repository/com/google/code/findbugs/jsr305/3.0.0/jsr305-3.0.0.jar:/Users/seven/software/maven/repository/org/apache/htrace/htrace-core4/4.0.1-incubating/htrace-core4-4.0.1-incubating.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-compress/1.4.1/commons-compress-1.4.1.jar:/Users/seven/software/maven/repository/org/tukaani/xz/1.0/xz-1.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-hdfs/2.6.0-cdh5.7.0/hadoop-hdfs-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/io/netty/netty/3.6.2.Final/netty-3.6.2.Final.jar:/Users/seven/software/maven/repository/xerces/xercesImpl/2.9.1/xercesImpl-2.9.1.jar:/Users/seven/software/maven/repository/xml-apis/xml-apis/1.3.04/xml-apis-1.3.04.jar:/Users/seven/software/maven/repository/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-app/2.6.0-cdh5.7.0/hadoop-mapreduce-client-app-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-common/2.6.0-cdh5.7.0/hadoop-mapreduce-client-common-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-yarn-client/2.6.0-cdh5.7.0/hadoop-yarn-client-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-yarn-server-common/2.6.0-cdh5.7.0/hadoop-yarn-server-common-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-shuffle/2.6.0-cdh5.7.0/hadoop-mapreduce-client-shuffle-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-yarn-api/2.6.0-cdh5.7.0/hadoop-yarn-api-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-core/2.6.0-cdh5.7.0/hadoop-mapreduce-client-core-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-yarn-common/2.6.0-cdh5.7.0/hadoop-yarn-common-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/javax/xml/bind/jaxb-api/2.2.2/jaxb-api-2.2.2.jar:/Users/seven/software/maven/repository/javax/xml/stream/stax-api/1.0-2/stax-api-1.0-2.jar:/Users/seven/software/maven/repository/javax/activation/activation/1.1/activation-1.1.jar:/Users/seven/software/maven/repository/com/sun/jersey/jersey-client/1.9/jersey-client-1.9.jar:/Users/seven/software/maven/repository/org/codehaus/jackson/jackson-xc/1.8.8/jackson-xc-1.8.8.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.6.0-cdh5.7.0/hadoop-mapreduce-client-jobclient-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-aws/2.6.0-cdh5.7.0/hadoop-aws-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/com/amazonaws/aws-java-sdk-s3/1.10.6/aws-java-sdk-s3-1.10.6.jar:/Users/seven/software/maven/repository/com/amazonaws/aws-java-sdk-kms/1.10.6/aws-java-sdk-kms-1.10.6.jar:/Users/seven/software/maven/repository/com/amazonaws/aws-java-sdk-core/1.10.6/aws-java-sdk-core-1.10.6.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-annotations/2.6.0-cdh5.7.0/hadoop-annotations-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-client/1.2.0-cdh5.7.0/hbase-client-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-annotations/1.2.0-cdh5.7.0/hbase-annotations-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-common/1.2.0-cdh5.7.0/hbase-common-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-protocol/1.2.0-cdh5.7.0/hbase-protocol-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/commons-codec/commons-codec/1.9/commons-codec-1.9.jar:/Users/seven/software/maven/repository/commons-io/commons-io/2.4/commons-io-2.4.jar:/Users/seven/software/maven/repository/commons-lang/commons-lang/2.6/commons-lang-2.6.jar:/Users/seven/software/maven/repository/commons-logging/commons-logging/1.2/commons-logging-1.2.jar:/Users/seven/software/maven/repository/com/google/guava/guava/12.0.1/guava-12.0.1.jar:/Users/seven/software/maven/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/Users/seven/software/maven/repository/io/netty/netty-all/4.0.23.Final/netty-all-4.0.23.Final.jar:/Users/seven/software/maven/repository/org/apache/zookeeper/zookeeper/3.4.5-cdh5.7.0/zookeeper-3.4.5-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/htrace/htrace-core/3.2.0-incubating/htrace-core-3.2.0-incubating.jar:/Users/seven/software/maven/repository/org/codehaus/jackson/jackson-mapper-asl/1.8.8/jackson-mapper-asl-1.8.8.jar:/Users/seven/software/maven/repository/org/jruby/jcodings/jcodings/1.0.8/jcodings-1.0.8.jar:/Users/seven/software/maven/repository/org/jruby/joni/joni/2.1.2/joni-2.1.2.jar:/Users/seven/software/maven/repository/com/yammer/metrics/metrics-core/2.2.0/metrics-core-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-auth/2.6.0-cdh5.7.0/hadoop-auth-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/httpcomponents/httpclient/4.2.5/httpclient-4.2.5.jar:/Users/seven/software/maven/repository/org/apache/httpcomponents/httpcore/4.2.4/httpcore-4.2.4.jar:/Users/seven/software/maven/repository/org/apache/directory/server/apacheds-kerberos-codec/2.0.0-M15/apacheds-kerberos-codec-2.0.0-M15.jar:/Users/seven/software/maven/repository/org/apache/directory/server/apacheds-i18n/2.0.0-M15/apacheds-i18n-2.0.0-M15.jar:/Users/seven/software/maven/repository/org/apache/directory/api/api-asn1-api/1.0.0-M20/api-asn1-api-1.0.0-M20.jar:/Users/seven/software/maven/repository/org/apache/directory/api/api-util/1.0.0-M20/api-util-1.0.0-M20.jar:/Users/seven/software/maven/repository/org/apache/curator/curator-framework/2.7.1/curator-framework-2.7.1.jar:/Users/seven/software/maven/repository/com/github/stephenc/findbugs/findbugs-annotations/1.3.9-1/findbugs-annotations-1.3.9-1.jar:/Users/seven/software/maven/repository/junit/junit/4.12/junit-4.12.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-server/1.2.0-cdh5.7.0/hbase-server-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-procedure/1.2.0-cdh5.7.0/hbase-procedure-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-common/1.2.0-cdh5.7.0/hbase-common-1.2.0-cdh5.7.0-tests.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-prefix-tree/1.2.0-cdh5.7.0/hbase-prefix-tree-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar:/Users/seven/software/maven/repository/commons-collections/commons-collections/3.2.2/commons-collections-3.2.2.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-hadoop-compat/1.2.0-cdh5.7.0/hbase-hadoop-compat-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-hadoop2-compat/1.2.0-cdh5.7.0/hbase-hadoop2-compat-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/com/sun/jersey/jersey-core/1.9/jersey-core-1.9.jar:/Users/seven/software/maven/repository/com/sun/jersey/jersey-server/1.9/jersey-server-1.9.jar:/Users/seven/software/maven/repository/asm/asm/3.1/asm-3.1.jar:/Users/seven/software/maven/repository/commons-cli/commons-cli/1.2/commons-cli-1.2.jar:/Users/seven/software/maven/repository/com/github/stephenc/high-scale-lib/high-scale-lib/1.1.1/high-scale-lib-1.1.1.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-math/2.1/commons-math-2.1.jar:/Users/seven/software/maven/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jetty/6.1.26.cloudera.4/jetty-6.1.26.cloudera.4.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jetty-util/6.1.26.cloudera.4/jetty-util-6.1.26.cloudera.4.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jetty-sslengine/6.1.26.cloudera.4/jetty-sslengine-6.1.26.cloudera.4.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jsp-2.1/6.1.14/jsp-2.1-6.1.14.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jsp-api-2.1/6.1.14/jsp-api-2.1-6.1.14.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/servlet-api-2.5/6.1.14/servlet-api-2.5-6.1.14.jar:/Users/seven/software/maven/repository/org/codehaus/jackson/jackson-core-asl/1.8.8/jackson-core-asl-1.8.8.jar:/Users/seven/software/maven/repository/org/codehaus/jackson/jackson-jaxrs/1.8.8/jackson-jaxrs-1.8.8.jar:/Users/seven/software/maven/repository/tomcat/jasper-compiler/5.5.23/jasper-compiler-5.5.23.jar:/Users/seven/software/maven/repository/tomcat/jasper-runtime/5.5.23/jasper-runtime-5.5.23.jar:/Users/seven/software/maven/repository/commons-el/commons-el/1.0/commons-el-1.0.jar:/Users/seven/software/maven/repository/org/jamon/jamon-runtime/2.4.1/jamon-runtime-2.4.1.jar:/Users/seven/software/maven/repository/com/lmax/disruptor/3.3.0/disruptor-3.3.0.jar:/Users/seven/software/maven/repository/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar:/Users/seven/software/maven/repository/org/owasp/esapi/esapi/2.1.0/esapi-2.1.0.jar:/Users/seven/software/maven/repository/commons-beanutils/commons-beanutils-core/1.7.0/commons-beanutils-core-1.7.0.jar:/Users/seven/software/maven/repository/commons-fileupload/commons-fileupload/1.2/commons-fileupload-1.2.jar:/Users/seven/software/maven/repository/xom/xom/1.2.5/xom-1.2.5.jar:/Users/seven/software/maven/repository/xalan/xalan/2.7.0/xalan-2.7.0.jar:/Users/seven/software/maven/repository/org/beanshell/bsh-core/2.0b4/bsh-core-2.0b4.jar:/Users/seven/software/maven/repository/org/owasp/antisamy/antisamy/1.4.3/antisamy-1.4.3.jar:/Users/seven/software/maven/repository/org/apache/xmlgraphics/batik-css/1.7/batik-css-1.7.jar:/Users/seven/software/maven/repository/org/apache/xmlgraphics/batik-ext/1.7/batik-ext-1.7.jar:/Users/seven/software/maven/repository/org/apache/xmlgraphics/batik-util/1.7/batik-util-1.7.jar:/Users/seven/software/maven/repository/xml-apis/xml-apis-ext/1.3.04/xml-apis-ext-1.3.04.jar:/Users/seven/software/maven/repository/net/sourceforge/nekohtml/nekohtml/1.9.12/nekohtml-1.9.12.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-core/2.6.0-mr1-cdh5.7.0/hadoop-core-2.6.0-mr1-cdh5.7.0.jar:/Users/seven/software/maven/repository/javax/servlet/jsp/jsp-api/2.1/jsp-api-2.1.jar:/Users/seven/software/maven/repository/net/java/dev/jets3t/jets3t/0.6.1/jets3t-0.6.1.jar:/Users/seven/software/maven/repository/javax/servlet/servlet-api/2.5/servlet-api-2.5.jar:/Users/seven/software/maven/repository/hsqldb/hsqldb/1.8.0.10/hsqldb-1.8.0.10.jar:/Users/seven/software/maven/repository/org/eclipse/jdt/core/3.1.1/core-3.1.1.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-hdfs/2.6.0-cdh5.7.0/hadoop-hdfs-2.6.0-cdh5.7.0-tests.jar:/Users/seven/software/maven/repository/commons-daemon/commons-daemon/1.0.13/commons-daemon-1.0.13.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-lang3/3.5/commons-lang3-3.5.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-pool2/2.4.2/commons-pool2-2.4.2.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-mllib_2.11/2.2.0/spark-mllib_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-core_2.11/2.2.0/spark-core_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/avro/avro-mapred/1.7.7/avro-mapred-1.7.7-hadoop2.jar:/Users/seven/software/maven/repository/org/apache/avro/avro-ipc/1.7.7/avro-ipc-1.7.7-tests.jar:/Users/seven/software/maven/repository/com/twitter/chill_2.11/0.8.0/chill_2.11-0.8.0.jar:/Users/seven/software/maven/repository/com/esotericsoftware/kryo-shaded/3.0.3/kryo-shaded-3.0.3.jar:/Users/seven/software/maven/repository/com/esotericsoftware/minlog/1.3.0/minlog-1.3.0.jar:/Users/seven/software/maven/repository/org/objenesis/objenesis/2.1/objenesis-2.1.jar:/Users/seven/software/maven/repository/com/twitter/chill-java/0.8.0/chill-java-0.8.0.jar:/Users/seven/software/maven/repository/org/apache/xbean/xbean-asm5-shaded/4.4/xbean-asm5-shaded-4.4.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-launcher_2.11/2.2.0/spark-launcher_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-network-common_2.11/2.2.0/spark-network-common_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-network-shuffle_2.11/2.2.0/spark-network-shuffle_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-unsafe_2.11/2.2.0/spark-unsafe_2.11-2.2.0.jar:/Users/seven/software/maven/repository/javax/servlet/javax.servlet-api/3.1.0/javax.servlet-api-3.1.0.jar:/Users/seven/software/maven/repository/org/slf4j/jul-to-slf4j/1.7.16/jul-to-slf4j-1.7.16.jar:/Users/seven/software/maven/repository/org/slf4j/jcl-over-slf4j/1.7.16/jcl-over-slf4j-1.7.16.jar:/Users/seven/software/maven/repository/com/ning/compress-lzf/1.0.3/compress-lzf-1.0.3.jar:/Users/seven/software/maven/repository/org/xerial/snappy/snappy-java/1.1.2.6/snappy-java-1.1.2.6.jar:/Users/seven/software/maven/repository/org/roaringbitmap/RoaringBitmap/0.5.11/RoaringBitmap-0.5.11.jar:/Users/seven/software/maven/repository/org/json4s/json4s-jackson_2.11/3.2.11/json4s-jackson_2.11-3.2.11.jar:/Users/seven/software/maven/repository/org/json4s/json4s-core_2.11/3.2.11/json4s-core_2.11-3.2.11.jar:/Users/seven/software/maven/repository/org/json4s/json4s-ast_2.11/3.2.11/json4s-ast_2.11-3.2.11.jar:/Users/seven/software/maven/repository/org/scala-lang/scalap/2.11.0/scalap-2.11.0.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/core/jersey-client/2.22.2/jersey-client-2.22.2.jar:/Users/seven/software/maven/repository/javax/ws/rs/javax.ws.rs-api/2.0.1/javax.ws.rs-api-2.0.1.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/hk2-api/2.4.0-b34/hk2-api-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/hk2-utils/2.4.0-b34/hk2-utils-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/external/aopalliance-repackaged/2.4.0-b34/aopalliance-repackaged-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/external/javax.inject/2.4.0-b34/javax.inject-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/hk2-locator/2.4.0-b34/hk2-locator-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/javassist/javassist/3.18.1-GA/javassist-3.18.1-GA.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/core/jersey-common/2.22.2/jersey-common-2.22.2.jar:/Users/seven/software/maven/repository/javax/annotation/javax.annotation-api/1.2/javax.annotation-api-1.2.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/bundles/repackaged/jersey-guava/2.22.2/jersey-guava-2.22.2.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/osgi-resource-locator/1.0.1/osgi-resource-locator-1.0.1.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/core/jersey-server/2.22.2/jersey-server-2.22.2.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/media/jersey-media-jaxb/2.22.2/jersey-media-jaxb-2.22.2.jar:/Users/seven/software/maven/repository/javax/validation/validation-api/1.1.0.Final/validation-api-1.1.0.Final.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/containers/jersey-container-servlet/2.22.2/jersey-container-servlet-2.22.2.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/containers/jersey-container-servlet-core/2.22.2/jersey-container-servlet-core-2.22.2.jar:/Users/seven/software/maven/repository/com/clearspring/analytics/stream/2.7.0/stream-2.7.0.jar:/Users/seven/software/maven/repository/io/dropwizard/metrics/metrics-core/3.1.2/metrics-core-3.1.2.jar:/Users/seven/software/maven/repository/io/dropwizard/metrics/metrics-jvm/3.1.2/metrics-jvm-3.1.2.jar:/Users/seven/software/maven/repository/io/dropwizard/metrics/metrics-json/3.1.2/metrics-json-3.1.2.jar:/Users/seven/software/maven/repository/io/dropwizard/metrics/metrics-graphite/3.1.2/metrics-graphite-3.1.2.jar:/Users/seven/software/maven/repository/org/apache/ivy/ivy/2.4.0/ivy-2.4.0.jar:/Users/seven/software/maven/repository/oro/oro/2.0.8/oro-2.0.8.jar:/Users/seven/software/maven/repository/net/razorvine/pyrolite/4.13/pyrolite-4.13.jar:/Users/seven/software/maven/repository/net/sf/py4j/py4j/0.10.4/py4j-0.10.4.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-crypto/1.0.0/commons-crypto-1.0.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-streaming_2.11/2.2.0/spark-streaming_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-sql_2.11/2.2.0/spark-sql_2.11-2.2.0.jar:/Users/seven/software/maven/repository/com/univocity/univocity-parsers/2.2.1/univocity-parsers-2.2.1.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-sketch_2.11/2.2.0/spark-sketch_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-catalyst_2.11/2.2.0/spark-catalyst_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/codehaus/janino/janino/3.0.0/janino-3.0.0.jar:/Users/seven/software/maven/repository/org/codehaus/janino/commons-compiler/3.0.0/commons-compiler-3.0.0.jar:/Users/seven/software/maven/repository/org/antlr/antlr4-runtime/4.5.3/antlr4-runtime-4.5.3.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-column/1.8.2/parquet-column-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-common/1.8.2/parquet-common-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-encoding/1.8.2/parquet-encoding-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-hadoop/1.8.2/parquet-hadoop-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-format/2.3.1/parquet-format-2.3.1.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-jackson/1.8.2/parquet-jackson-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-graphx_2.11/2.2.0/spark-graphx_2.11-2.2.0.jar:/Users/seven/software/maven/repository/com/github/fommil/netlib/core/1.1.2/core-1.1.2.jar:/Users/seven/software/maven/repository/net/sourceforge/f2j/arpack_combined_all/0.1/arpack_combined_all-0.1.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-mllib-local_2.11/2.2.0/spark-mllib-local_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/scalanlp/breeze_2.11/0.13.1/breeze_2.11-0.13.1.jar:/Users/seven/software/maven/repository/org/scalanlp/breeze-macros_2.11/0.13.1/breeze-macros_2.11-0.13.1.jar:/Users/seven/software/maven/repository/net/sf/opencsv/opencsv/2.3/opencsv-2.3.jar:/Users/seven/software/maven/repository/com/github/rwl/jtransforms/2.4.0/jtransforms-2.4.0.jar:/Users/seven/software/maven/repository/org/spire-math/spire_2.11/0.13.0/spire_2.11-0.13.0.jar:/Users/seven/software/maven/repository/org/spire-math/spire-macros_2.11/0.13.0/spire-macros_2.11-0.13.0.jar:/Users/seven/software/maven/repository/org/typelevel/machinist_2.11/0.6.1/machinist_2.11-0.6.1.jar:/Users/seven/software/maven/repository/com/chuusai/shapeless_2.11/2.3.2/shapeless_2.11-2.3.2.jar:/Users/seven/software/maven/repository/org/typelevel/macro-compat_2.11/1.1.1/macro-compat_2.11-1.1.1.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-math3/3.4.1/commons-math3-3.4.1.jar:/Users/seven/software/maven/repository/org/jpmml/pmml-model/1.2.15/pmml-model-1.2.15.jar:/Users/seven/software/maven/repository/org/jpmml/pmml-schema/1.2.15/pmml-schema-1.2.15.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-tags_2.11/2.2.0/spark-tags_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-streaming-kafka-0-8_2.11/2.2.0/spark-streaming-kafka-0-8_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/kafka/kafka_2.11/0.8.2.1/kafka_2.11-0.8.2.1.jar:/Users/seven/software/maven/repository/com/101tec/zkclient/0.3/zkclient-0.3.jar:/Users/seven/software/maven/repository/org/apache/kafka/kafka-clients/0.8.2.1/kafka-clients-0.8.2.1.jar:/Users/seven/software/maven/repository/mysql/mysql-connector-java/5.1.31/mysql-connector-java-5.1.31.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/module/jackson-module-scala_2.11/2.6.5/jackson-module-scala_2.11-2.6.5.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/core/jackson-core/2.6.5/jackson-core-2.6.5.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/core/jackson-annotations/2.6.5/jackson-annotations-2.6.5.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/core/jackson-databind/2.6.5/jackson-databind-2.6.5.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/module/jackson-module-paranamer/2.6.5/jackson-module-paranamer-2.6.5.jar:/Users/seven/software/maven/repository/com/thoughtworks/paranamer/paranamer/2.6/paranamer-2.6.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-streaming-flume_2.11/2.2.0/spark-streaming-flume_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/flume/flume-ng-core/1.6.0/flume-ng-core-1.6.0.jar:/Users/seven/software/maven/repository/org/apache/flume/flume-ng-configuration/1.6.0/flume-ng-configuration-1.6.0.jar:/Users/seven/software/maven/repository/org/apache/avro/avro-ipc/1.7.4/avro-ipc-1.7.4.jar:/Users/seven/software/maven/repository/org/apache/velocity/velocity/1.7/velocity-1.7.jar:/Users/seven/software/maven/repository/joda-time/joda-time/2.1/joda-time-2.1.jar:/Users/seven/software/maven/repository/org/apache/mina/mina-core/2.0.4/mina-core-2.0.4.jar:/Users/seven/software/maven/repository/org/apache/flume/flume-ng-sdk/1.6.0/flume-ng-sdk-1.6.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-streaming-flume-sink_2.11/2.2.0/spark-streaming-flume-sink_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/flume/flume-ng-clients/flume-ng-log4jappender/1.6.0/flume-ng-log4jappender-1.6.0.jar:/Users/seven/software/maven/repository/net/jpountz/lz4/lz4/1.3.0/lz4-1.3.0.jar:/Applications/IntelliJ IDEA.app/Contents/lib/idea_rt.jar
[INFO][2018-05-24 19:58:10,676][org.apache.zookeeper.ZooKeeper]Client environment:java.library.path=/Users/seven/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:.
[INFO][2018-05-24 19:58:10,676][org.apache.zookeeper.ZooKeeper]Client environment:java.io.tmpdir=/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/
[INFO][2018-05-24 19:58:10,676][org.apache.zookeeper.ZooKeeper]Client environment:java.compiler=<NA>
[INFO][2018-05-24 19:58:10,677][org.apache.zookeeper.ZooKeeper]Client environment:os.name=Mac OS X
[INFO][2018-05-24 19:58:10,677][org.apache.zookeeper.ZooKeeper]Client environment:os.arch=x86_64
[INFO][2018-05-24 19:58:10,677][org.apache.zookeeper.ZooKeeper]Client environment:os.version=10.13.4
[INFO][2018-05-24 19:58:10,677][org.apache.zookeeper.ZooKeeper]Client environment:user.name=seven
[INFO][2018-05-24 19:58:10,677][org.apache.zookeeper.ZooKeeper]Client environment:user.home=/Users/seven
[INFO][2018-05-24 19:58:10,677][org.apache.zookeeper.ZooKeeper]Client environment:user.dir=/Users/seven/project/scala/dataMining
[INFO][2018-05-24 19:58:10,678][org.apache.zookeeper.ZooKeeper]Initiating client connection, connectString=vm-xaj-bigdata-da-d01:2181,vm-xaj-bigdata-da-d02:2181,vm-xaj-bigdata-da-d03:2181 sessionTimeout=60000 watcher=hconnection-0x68404ee20x0, quorum=vm-xaj-bigdata-da-d01:2181,vm-xaj-bigdata-da-d02:2181,vm-xaj-bigdata-da-d03:2181, baseZNode=/hbase
[INFO][2018-05-24 19:58:15,703][org.apache.zookeeper.ClientCnxn]Opening socket connection to server vm-xaj-bigdata-da-d03/10.213.4.27:2181. Will not attempt to authenticate using SASL (unknown error)
[INFO][2018-05-24 19:58:15,733][org.apache.zookeeper.ClientCnxn]Socket connection established, initiating session, client: /10.0.8.39:49190, server: vm-xaj-bigdata-da-d03/10.213.4.27:2181
[INFO][2018-05-24 19:58:15,765][org.apache.zookeeper.ClientCnxn]Session establishment complete on server vm-xaj-bigdata-da-d03/10.213.4.27:2181, sessionid = 0x362b50d75ec5f18, negotiated timeout = 60000
[INFO][2018-05-24 19:58:15,814][org.apache.spark.SparkContext]Running Spark version 2.2.0
[WARN][2018-05-24 19:58:16,804][org.apache.hadoop.util.NativeCodeLoader]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO][2018-05-24 19:58:16,953][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 19:58:16,957][org.apache.spark.SparkContext]Submitted application: SimulationKafkaSendOutData$
[INFO][2018-05-24 19:58:16,973][org.apache.spark.executor.Executor]Finished task 0.0 in stage 0.0 (TID 0). 794 bytes result sent to driver
[INFO][2018-05-24 19:58:16,980][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 0.0 (TID 0) in 6522 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 19:58:16,982][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 19:58:16,985][org.apache.spark.scheduler.DAGScheduler]ResultStage 0 (foreachPartition at ReceiveKafkaData.scala:64) finished in 6.535 s
[INFO][2018-05-24 19:58:16,990][org.apache.spark.scheduler.DAGScheduler]Job 0 finished: foreachPartition at ReceiveKafkaData.scala:64, took 6.777496 s
[INFO][2018-05-24 19:58:16,994][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527163080000 ms.0 from job set of time 1527163080000 ms
[INFO][2018-05-24 19:58:16,995][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 16.994 s for time 1527163080000 ms (execution: 6.829 s)
[INFO][2018-05-24 19:58:17,000][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 19:58:17,004][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 
[INFO][2018-05-24 19:58:17,008][org.apache.spark.SecurityManager]Changing view acls to: seven
[INFO][2018-05-24 19:58:17,009][org.apache.spark.SecurityManager]Changing modify acls to: seven
[INFO][2018-05-24 19:58:17,009][org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO][2018-05-24 19:58:17,010][org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO][2018-05-24 19:58:17,011][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(seven); groups with view permissions: Set(); users  with modify permissions: Set(seven); groups with modify permissions: Set()
[INFO][2018-05-24 19:58:17,326][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 49191.
[INFO][2018-05-24 19:58:17,348][org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO][2018-05-24 19:58:17,363][org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO][2018-05-24 19:58:17,365][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2018-05-24 19:58:17,365][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
[INFO][2018-05-24 19:58:17,375][org.apache.spark.storage.DiskBlockManager]Created local directory at /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/blockmgr-54a13771-ec4c-4e7e-89a9-74630e394be9
[INFO][2018-05-24 19:58:17,393][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 912.3 MB
[INFO][2018-05-24 19:58:17,466][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO][2018-05-24 19:58:17,555][org.spark_project.jetty.util.log]Logging initialized @2904ms
[INFO][2018-05-24 19:58:17,642][org.spark_project.jetty.server.Server]jetty-9.3.z-SNAPSHOT
[INFO][2018-05-24 19:58:17,660][org.spark_project.jetty.server.Server]Started @3011ms
[WARN][2018-05-24 19:58:17,678][org.apache.spark.util.Utils]Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[INFO][2018-05-24 19:58:17,684][org.spark_project.jetty.server.AbstractConnector]Started ServerConnector@60ca0db6{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}
[INFO][2018-05-24 19:58:17,684][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4041.
[INFO][2018-05-24 19:58:17,706][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@180e6ac4{/jobs,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:58:17,707][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@c7a975a{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:58:17,707][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@757d6814{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:58:17,708][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4c168660{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:58:17,709][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@fd0e5b6{/stages,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:58:17,709][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@36b0fcd5{/stages/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:58:17,710][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@475835b1{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:58:17,711][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@77192705{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:58:17,712][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7e809b79{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:58:17,712][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@625e134e{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:58:17,713][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@89c10b7{/storage,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:58:17,714][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4fe89c24{/storage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:58:17,714][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3d08f3f5{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:58:17,715][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1a1da881{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:58:17,716][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7fd4acee{/environment,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:58:17,717][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6175619b{/environment/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:58:17,718][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@756cf158{/executors,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:58:17,719][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@751d3241{/executors/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:58:17,720][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@64337702{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:58:17,720][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@30ea8c23{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:58:17,727][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4e76dac{/static,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:58:17,728][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@f9d87b{/,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:58:17,729][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@26fb628{/api,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:58:17,730][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5ac86ba5{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:58:17,731][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2c9399a4{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:58:17,737][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://192.168.0.102:4041
[INFO][2018-05-24 19:58:17,860][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
[INFO][2018-05-24 19:58:17,881][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 49192.
[INFO][2018-05-24 19:58:17,884][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 192.168.0.102:49192
[INFO][2018-05-24 19:58:17,887][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2018-05-24 19:58:17,888][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 192.168.0.102, 49192, None)
[INFO][2018-05-24 19:58:17,891][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 192.168.0.102:49192 with 912.3 MB RAM, BlockManagerId(driver, 192.168.0.102, 49192, None)
[INFO][2018-05-24 19:58:17,895][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 192.168.0.102, 49192, None)
[INFO][2018-05-24 19:58:17,895][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 192.168.0.102, 49192, None)
[INFO][2018-05-24 19:58:18,101][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@405325cf{/metrics/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:58:18,687][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0 stored as values in memory (estimated size 228.1 KB, free 912.1 MB)
[INFO][2018-05-24 19:58:18,753][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0_piece0 stored as bytes in memory (estimated size 22.1 KB, free 912.1 MB)
[INFO][2018-05-24 19:58:18,755][org.apache.spark.storage.BlockManagerInfo]Added broadcast_0_piece0 in memory on 192.168.0.102:49192 (size: 22.1 KB, free: 912.3 MB)
[INFO][2018-05-24 19:58:18,761][org.apache.spark.SparkContext]Created broadcast 0 from textFile at SimulationKafkaSendOutData.scala:25
[WARN][2018-05-24 19:58:24,178][org.apache.hadoop.util.NativeCodeLoader]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO][2018-05-24 19:58:24,327][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 1
[INFO][2018-05-24 19:58:24,418][org.apache.spark.SparkContext]Starting job: collect at SimulationKafkaSendOutData.scala:25
[INFO][2018-05-24 19:58:24,430][org.apache.spark.scheduler.DAGScheduler]Got job 0 (collect at SimulationKafkaSendOutData.scala:25) with 2 output partitions
[INFO][2018-05-24 19:58:24,430][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 0 (collect at SimulationKafkaSendOutData.scala:25)
[INFO][2018-05-24 19:58:24,431][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 19:58:24,432][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 19:58:24,439][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 0 (hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/* MapPartitionsRDD[1] at textFile at SimulationKafkaSendOutData.scala:25), which has no missing parents
[INFO][2018-05-24 19:58:24,456][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1 stored as values in memory (estimated size 3.3 KB, free 912.1 MB)
[INFO][2018-05-24 19:58:24,457][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1_piece0 stored as bytes in memory (estimated size 1969.0 B, free 912.1 MB)
[INFO][2018-05-24 19:58:24,458][org.apache.spark.storage.BlockManagerInfo]Added broadcast_1_piece0 in memory on 192.168.0.102:49192 (size: 1969.0 B, free: 912.3 MB)
[INFO][2018-05-24 19:58:24,459][org.apache.spark.SparkContext]Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 19:58:24,473][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ResultStage 0 (hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/* MapPartitionsRDD[1] at textFile at SimulationKafkaSendOutData.scala:25) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2018-05-24 19:58:24,474][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 0.0 with 2 tasks
[INFO][2018-05-24 19:58:24,512][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 4883 bytes)
[INFO][2018-05-24 19:58:24,517][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 4883 bytes)
[INFO][2018-05-24 19:58:24,524][org.apache.spark.executor.Executor]Running task 0.0 in stage 0.0 (TID 0)
[INFO][2018-05-24 19:58:24,524][org.apache.spark.executor.Executor]Running task 1.0 in stage 0.0 (TID 1)
[INFO][2018-05-24 19:58:24,586][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/part-00000:0+10717435
[INFO][2018-05-24 19:58:24,587][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/part-00000:10717435+10717436
[INFO][2018-05-24 19:58:34,831][org.apache.spark.storage.memory.MemoryStore]Block taskresult_0 stored as bytes in memory (estimated size 10.4 MB, free 901.7 MB)
[INFO][2018-05-24 19:58:34,833][org.apache.spark.storage.BlockManagerInfo]Added taskresult_0 in memory on 192.168.0.102:49192 (size: 10.4 MB, free: 901.9 MB)
[INFO][2018-05-24 19:58:34,834][org.apache.spark.executor.Executor]Finished task 0.0 in stage 0.0 (TID 0). 10856349 bytes result sent via BlockManager)
[INFO][2018-05-24 19:58:34,866][org.apache.spark.network.client.TransportClientFactory]Successfully created connection to /192.168.0.102:49192 after 18 ms (0 ms spent in bootstraps)
[INFO][2018-05-24 19:58:35,139][org.apache.spark.storage.BlockManagerInfo]Removed taskresult_0 on 192.168.0.102:49192 in memory (size: 10.4 MB, free: 912.3 MB)
[INFO][2018-05-24 19:58:35,139][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 0.0 (TID 0) in 10638 ms on localhost (executor driver) (1/2)
[INFO][2018-05-24 19:58:35,736][org.apache.spark.storage.memory.MemoryStore]Block taskresult_1 stored as bytes in memory (estimated size 10.4 MB, free 901.7 MB)
[INFO][2018-05-24 19:58:35,737][org.apache.spark.storage.BlockManagerInfo]Added taskresult_1 in memory on 192.168.0.102:49192 (size: 10.4 MB, free: 901.9 MB)
[INFO][2018-05-24 19:58:35,737][org.apache.spark.executor.Executor]Finished task 1.0 in stage 0.0 (TID 1). 10855899 bytes result sent via BlockManager)
[INFO][2018-05-24 19:58:35,839][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 0.0 (TID 1) in 11322 ms on localhost (executor driver) (2/2)
[INFO][2018-05-24 19:58:35,839][org.apache.spark.storage.BlockManagerInfo]Removed taskresult_1 on 192.168.0.102:49192 in memory (size: 10.4 MB, free: 912.3 MB)
[INFO][2018-05-24 19:58:35,840][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 19:58:35,841][org.apache.spark.scheduler.DAGScheduler]ResultStage 0 (collect at SimulationKafkaSendOutData.scala:25) finished in 11.351 s
[INFO][2018-05-24 19:58:35,845][org.apache.spark.scheduler.DAGScheduler]Job 0 finished: collect at SimulationKafkaSendOutData.scala:25, took 11.426325 s
[INFO][2018-05-24 19:58:35,904][org.spark_project.jetty.server.AbstractConnector]Stopped Spark@60ca0db6{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}
[INFO][2018-05-24 19:58:35,907][org.apache.spark.ui.SparkUI]Stopped Spark web UI at http://192.168.0.102:4041
[INFO][2018-05-24 19:58:35,915][org.apache.spark.MapOutputTrackerMasterEndpoint]MapOutputTrackerMasterEndpoint stopped!
[INFO][2018-05-24 19:58:35,927][org.apache.spark.storage.memory.MemoryStore]MemoryStore cleared
[INFO][2018-05-24 19:58:35,928][org.apache.spark.storage.BlockManager]BlockManager stopped
[INFO][2018-05-24 19:58:35,987][org.apache.spark.storage.BlockManagerMaster]BlockManagerMaster stopped
[INFO][2018-05-24 19:58:35,989][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint]OutputCommitCoordinator stopped!
[INFO][2018-05-24 19:58:35,990][org.apache.spark.SparkContext]Successfully stopped SparkContext
[INFO][2018-05-24 19:58:36,005][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	compression.type = none
	metric.reporters = []
	metadata.max.age.ms = 300000
	metadata.fetch.timeout.ms = 60000
	acks = all
	batch.size = 16384
	reconnect.backoff.ms = 10
	bootstrap.servers = [vm-xaj-bigdata-da-d01:9092, vm-xaj-bigdata-da-d02:9092, vm-xaj-bigdata-da-d03:9092, vm-xaj-bigdata-da-d04:9092, vm-xaj-bigdata-da-d05:9092, vm-xaj-bigdata-da-d06:9092, vm-xaj-bigdata-da-d07:9092]
	receive.buffer.bytes = 32768
	retry.backoff.ms = 100
	buffer.memory = 33554432
	timeout.ms = 30000
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	retries = 0
	max.request.size = 1048576
	block.on.buffer.full = true
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
	metrics.sample.window.ms = 30000
	send.buffer.bytes = 131072
	max.in.flight.requests.per.connection = 5
	metrics.num.samples = 2
	linger.ms = 1
	client.id = 

[INFO][2018-05-24 19:59:10,098][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527163140000 ms
[INFO][2018-05-24 19:59:10,100][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527163140000 ms.0 from job set of time 1527163140000 ms
[INFO][2018-05-24 19:59:10,120][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:64
[INFO][2018-05-24 19:59:10,122][org.apache.spark.scheduler.DAGScheduler]Got job 1 (foreachPartition at ReceiveKafkaData.scala:64) with 1 output partitions
[INFO][2018-05-24 19:59:10,122][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 1 (foreachPartition at ReceiveKafkaData.scala:64)
[INFO][2018-05-24 19:59:10,122][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 19:59:10,123][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 19:59:10,123][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 1 (MapPartitionsRDD[3] at map at ReceiveKafkaData.scala:62), which has no missing parents
[INFO][2018-05-24 19:59:10,129][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-24 19:59:10,137][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1_piece0 stored as bytes in memory (estimated size 2008.0 B, free 912.3 MB)
[INFO][2018-05-24 19:59:10,138][org.apache.spark.storage.BlockManagerInfo]Added broadcast_1_piece0 in memory on 192.168.0.102:49171 (size: 2008.0 B, free: 912.3 MB)
[INFO][2018-05-24 19:59:10,139][org.apache.spark.SparkContext]Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 19:59:10,141][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at map at ReceiveKafkaData.scala:62) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 19:59:10,141][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 1.0 with 1 tasks
[INFO][2018-05-24 19:59:10,144][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 19:59:10,145][org.apache.spark.executor.Executor]Running task 0.0 in stage 1.0 (TID 1)
[INFO][2018-05-24 19:59:10,217][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 11976 is the same as ending offset skipping seven 0
[INFO][2018-05-24 19:59:10,219][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 19:59:10,221][org.apache.spark.executor.Executor]Finished task 0.0 in stage 1.0 (TID 1). 708 bytes result sent to driver
[INFO][2018-05-24 19:59:10,224][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 1.0 (TID 1) in 81 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 19:59:10,224][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 19:59:10,228][org.apache.spark.scheduler.DAGScheduler]ResultStage 1 (foreachPartition at ReceiveKafkaData.scala:64) finished in 0.084 s
[INFO][2018-05-24 19:59:10,232][org.apache.spark.scheduler.DAGScheduler]Job 1 finished: foreachPartition at ReceiveKafkaData.scala:64, took 0.111857 s
[INFO][2018-05-24 19:59:10,234][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527163140000 ms.0 from job set of time 1527163140000 ms
[INFO][2018-05-24 19:59:10,235][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 10.234 s for time 1527163140000 ms (execution: 0.135 s)
[INFO][2018-05-24 19:59:10,236][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 1 from persistence list
[INFO][2018-05-24 19:59:10,251][org.apache.spark.storage.BlockManager]Removing RDD 1
[INFO][2018-05-24 19:59:10,256][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 0 from persistence list
[INFO][2018-05-24 19:59:10,260][org.apache.spark.storage.BlockManager]Removing RDD 0
[INFO][2018-05-24 19:59:10,261][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 19:59:10,261][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 
[INFO][2018-05-24 20:00:05,074][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527163200000 ms
[INFO][2018-05-24 20:00:05,075][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527163200000 ms.0 from job set of time 1527163200000 ms
[INFO][2018-05-24 20:00:05,095][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:64
[INFO][2018-05-24 20:00:05,096][org.apache.spark.scheduler.DAGScheduler]Got job 2 (foreachPartition at ReceiveKafkaData.scala:64) with 1 output partitions
[INFO][2018-05-24 20:00:05,096][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 2 (foreachPartition at ReceiveKafkaData.scala:64)
[INFO][2018-05-24 20:00:05,096][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:00:05,096][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:00:05,099][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 2 (MapPartitionsRDD[5] at map at ReceiveKafkaData.scala:62), which has no missing parents
[INFO][2018-05-24 20:00:05,103][org.apache.spark.storage.memory.MemoryStore]Block broadcast_2 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-24 20:00:05,113][org.apache.spark.storage.memory.MemoryStore]Block broadcast_2_piece0 stored as bytes in memory (estimated size 2008.0 B, free 912.3 MB)
[INFO][2018-05-24 20:00:05,115][org.apache.spark.storage.BlockManagerInfo]Added broadcast_2_piece0 in memory on 192.168.0.102:49171 (size: 2008.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:00:05,116][org.apache.spark.SparkContext]Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:00:05,117][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[5] at map at ReceiveKafkaData.scala:62) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:00:05,117][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 2.0 with 1 tasks
[INFO][2018-05-24 20:00:05,118][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:00:05,118][org.apache.spark.executor.Executor]Running task 0.0 in stage 2.0 (TID 2)
[INFO][2018-05-24 20:00:05,127][org.apache.spark.streaming.kafka.KafkaRDD]Computing topic seven, partition 0 offsets 11976 -> 12030
[INFO][2018-05-24 20:00:05,128][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 20:00:05,128][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 20:00:05,128][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 20:00:10,726][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:00:10,727][org.apache.spark.executor.Executor]Finished task 0.0 in stage 2.0 (TID 2). 751 bytes result sent to driver
[INFO][2018-05-24 20:00:10,730][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 2.0 (TID 2) in 5612 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:00:10,730][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:00:10,732][org.apache.spark.scheduler.DAGScheduler]ResultStage 2 (foreachPartition at ReceiveKafkaData.scala:64) finished in 5.614 s
[INFO][2018-05-24 20:00:10,734][org.apache.spark.scheduler.DAGScheduler]Job 2 finished: foreachPartition at ReceiveKafkaData.scala:64, took 5.638829 s
[INFO][2018-05-24 20:00:10,742][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527163200000 ms.0 from job set of time 1527163200000 ms
[INFO][2018-05-24 20:00:10,743][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 10.742 s for time 1527163200000 ms (execution: 5.667 s)
[INFO][2018-05-24 20:00:10,743][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 3 from persistence list
[INFO][2018-05-24 20:00:10,745][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_1_piece0 on 192.168.0.102:49171 in memory (size: 2008.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:00:10,747][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 2 from persistence list
[INFO][2018-05-24 20:00:10,747][org.apache.spark.storage.BlockManager]Removing RDD 3
[INFO][2018-05-24 20:00:10,748][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:00:10,748][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527163080000 ms
[INFO][2018-05-24 20:00:10,749][org.apache.spark.storage.BlockManager]Removing RDD 2
[INFO][2018-05-24 20:00:51,726][org.apache.spark.util.ShutdownHookManager]Shutdown hook called
[INFO][2018-05-24 20:00:51,729][org.apache.spark.util.ShutdownHookManager]Deleting directory /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/spark-629a02a3-0cd7-4fef-989d-b0b80b9c30d6
[INFO][2018-05-24 20:01:10,078][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527163260000 ms
[INFO][2018-05-24 20:01:10,079][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527163260000 ms.0 from job set of time 1527163260000 ms
[INFO][2018-05-24 20:01:10,089][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:64
[INFO][2018-05-24 20:01:10,090][org.apache.spark.scheduler.DAGScheduler]Got job 3 (foreachPartition at ReceiveKafkaData.scala:64) with 1 output partitions
[INFO][2018-05-24 20:01:10,091][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 3 (foreachPartition at ReceiveKafkaData.scala:64)
[INFO][2018-05-24 20:01:10,091][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:01:10,091][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:01:10,091][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 3 (MapPartitionsRDD[7] at map at ReceiveKafkaData.scala:62), which has no missing parents
[INFO][2018-05-24 20:01:10,095][org.apache.spark.storage.memory.MemoryStore]Block broadcast_3 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-24 20:01:10,099][org.apache.spark.storage.memory.MemoryStore]Block broadcast_3_piece0 stored as bytes in memory (estimated size 2008.0 B, free 912.3 MB)
[INFO][2018-05-24 20:01:10,100][org.apache.spark.storage.BlockManagerInfo]Added broadcast_3_piece0 in memory on 192.168.0.102:49171 (size: 2008.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:01:10,100][org.apache.spark.SparkContext]Created broadcast 3 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:01:10,101][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[7] at map at ReceiveKafkaData.scala:62) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:01:10,101][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 3.0 with 1 tasks
[INFO][2018-05-24 20:01:10,102][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 3.0 (TID 3, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:01:10,102][org.apache.spark.executor.Executor]Running task 0.0 in stage 3.0 (TID 3)
[INFO][2018-05-24 20:01:10,105][org.apache.spark.streaming.kafka.KafkaRDD]Computing topic seven, partition 0 offsets 12030 -> 12076
[INFO][2018-05-24 20:01:10,106][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 20:01:10,106][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 20:01:10,106][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 20:01:15,228][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:01:15,229][org.apache.spark.executor.Executor]Finished task 0.0 in stage 3.0 (TID 3). 708 bytes result sent to driver
[INFO][2018-05-24 20:01:15,231][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 3.0 (TID 3) in 5129 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:01:15,231][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 3.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:01:15,232][org.apache.spark.scheduler.DAGScheduler]ResultStage 3 (foreachPartition at ReceiveKafkaData.scala:64) finished in 5.131 s
[INFO][2018-05-24 20:01:15,232][org.apache.spark.scheduler.DAGScheduler]Job 3 finished: foreachPartition at ReceiveKafkaData.scala:64, took 5.142500 s
[INFO][2018-05-24 20:01:15,233][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527163260000 ms.0 from job set of time 1527163260000 ms
[INFO][2018-05-24 20:01:15,233][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 15.233 s for time 1527163260000 ms (execution: 5.154 s)
[INFO][2018-05-24 20:01:15,233][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 5 from persistence list
[INFO][2018-05-24 20:01:15,234][org.apache.spark.storage.BlockManager]Removing RDD 5
[INFO][2018-05-24 20:01:15,234][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 4 from persistence list
[INFO][2018-05-24 20:01:15,235][org.apache.spark.storage.BlockManager]Removing RDD 4
[INFO][2018-05-24 20:01:15,235][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:01:15,235][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527163140000 ms
[INFO][2018-05-24 20:02:10,091][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527163320000 ms
[INFO][2018-05-24 20:02:10,091][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527163320000 ms.0 from job set of time 1527163320000 ms
[INFO][2018-05-24 20:02:10,099][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:64
[INFO][2018-05-24 20:02:10,101][org.apache.spark.scheduler.DAGScheduler]Got job 4 (foreachPartition at ReceiveKafkaData.scala:64) with 1 output partitions
[INFO][2018-05-24 20:02:10,101][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 4 (foreachPartition at ReceiveKafkaData.scala:64)
[INFO][2018-05-24 20:02:10,101][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:02:10,102][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:02:10,102][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 4 (MapPartitionsRDD[9] at map at ReceiveKafkaData.scala:62), which has no missing parents
[INFO][2018-05-24 20:02:10,104][org.apache.spark.storage.memory.MemoryStore]Block broadcast_4 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-24 20:02:10,109][org.apache.spark.storage.memory.MemoryStore]Block broadcast_4_piece0 stored as bytes in memory (estimated size 2008.0 B, free 912.3 MB)
[INFO][2018-05-24 20:02:10,109][org.apache.spark.storage.BlockManagerInfo]Added broadcast_4_piece0 in memory on 192.168.0.102:49171 (size: 2008.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:02:10,110][org.apache.spark.SparkContext]Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:02:10,110][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[9] at map at ReceiveKafkaData.scala:62) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:02:10,111][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 4.0 with 1 tasks
[INFO][2018-05-24 20:02:10,111][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 4.0 (TID 4, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:02:10,112][org.apache.spark.executor.Executor]Running task 0.0 in stage 4.0 (TID 4)
[INFO][2018-05-24 20:02:10,115][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12076 is the same as ending offset skipping seven 0
[INFO][2018-05-24 20:02:10,115][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:02:10,116][org.apache.spark.executor.Executor]Finished task 0.0 in stage 4.0 (TID 4). 708 bytes result sent to driver
[INFO][2018-05-24 20:02:10,117][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 4.0 (TID 4) in 6 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:02:10,117][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 4.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:02:10,117][org.apache.spark.scheduler.DAGScheduler]ResultStage 4 (foreachPartition at ReceiveKafkaData.scala:64) finished in 0.006 s
[INFO][2018-05-24 20:02:10,118][org.apache.spark.scheduler.DAGScheduler]Job 4 finished: foreachPartition at ReceiveKafkaData.scala:64, took 0.018234 s
[INFO][2018-05-24 20:02:10,118][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527163320000 ms.0 from job set of time 1527163320000 ms
[INFO][2018-05-24 20:02:10,119][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 10.118 s for time 1527163320000 ms (execution: 0.027 s)
[INFO][2018-05-24 20:02:10,119][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 7 from persistence list
[INFO][2018-05-24 20:02:10,119][org.apache.spark.storage.BlockManager]Removing RDD 7
[INFO][2018-05-24 20:02:10,119][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 6 from persistence list
[INFO][2018-05-24 20:02:10,119][org.apache.spark.storage.BlockManager]Removing RDD 6
[INFO][2018-05-24 20:02:10,119][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:02:10,120][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527163200000 ms
[INFO][2018-05-24 20:02:45,885][org.apache.spark.streaming.StreamingContext]Invoking stop(stopGracefully=false) from shutdown hook
[INFO][2018-05-24 20:02:45,889][org.apache.spark.streaming.scheduler.ReceiverTracker]ReceiverTracker stopped
[INFO][2018-05-24 20:02:45,890][org.apache.spark.streaming.scheduler.JobGenerator]Stopping JobGenerator immediately
[INFO][2018-05-24 20:02:45,891][org.apache.spark.streaming.util.RecurringTimer]Stopped timer for JobGenerator after time 1527163320000
[INFO][2018-05-24 20:02:45,893][org.apache.spark.streaming.scheduler.JobGenerator]Stopped JobGenerator
[INFO][2018-05-24 20:02:45,894][org.apache.spark.streaming.scheduler.JobScheduler]Stopped JobScheduler
[INFO][2018-05-24 20:02:45,907][org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@57a48985{/streaming,null,UNAVAILABLE,@Spark}
[INFO][2018-05-24 20:02:45,908][org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@3f672204{/streaming/batch,null,UNAVAILABLE,@Spark}
[INFO][2018-05-24 20:02:45,909][org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@2d000e80{/static/streaming,null,UNAVAILABLE,@Spark}
[INFO][2018-05-24 20:02:45,910][org.apache.spark.streaming.StreamingContext]StreamingContext stopped successfully
[INFO][2018-05-24 20:02:45,910][org.apache.spark.SparkContext]Invoking stop() from shutdown hook
[INFO][2018-05-24 20:02:45,919][org.spark_project.jetty.server.AbstractConnector]Stopped Spark@2090b9b9{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-05-24 20:02:45,922][org.apache.spark.ui.SparkUI]Stopped Spark web UI at http://192.168.0.102:4040
[INFO][2018-05-24 20:02:45,933][org.apache.spark.MapOutputTrackerMasterEndpoint]MapOutputTrackerMasterEndpoint stopped!
[INFO][2018-05-24 20:02:45,947][org.apache.spark.storage.memory.MemoryStore]MemoryStore cleared
[INFO][2018-05-24 20:02:45,947][org.apache.spark.storage.BlockManager]BlockManager stopped
[INFO][2018-05-24 20:02:45,948][org.apache.spark.storage.BlockManagerMaster]BlockManagerMaster stopped
[INFO][2018-05-24 20:02:45,950][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint]OutputCommitCoordinator stopped!
[INFO][2018-05-24 20:02:45,951][org.apache.spark.SparkContext]Successfully stopped SparkContext
[INFO][2018-05-24 20:02:45,952][org.apache.spark.util.ShutdownHookManager]Shutdown hook called
[INFO][2018-05-24 20:02:45,952][org.apache.spark.util.ShutdownHookManager]Deleting directory /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/spark-d6cfb298-1773-407a-bfe4-3e85d59fe166
[INFO][2018-05-24 20:10:13,570][org.apache.spark.SparkContext]Running Spark version 2.2.0
[INFO][2018-05-24 20:10:14,564][org.apache.spark.SparkContext]Submitted application: ReceiveKafkaData$
[INFO][2018-05-24 20:10:14,591][org.apache.spark.SecurityManager]Changing view acls to: seven
[INFO][2018-05-24 20:10:14,592][org.apache.spark.SecurityManager]Changing modify acls to: seven
[INFO][2018-05-24 20:10:14,592][org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO][2018-05-24 20:10:14,593][org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO][2018-05-24 20:10:14,600][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(seven); groups with view permissions: Set(); users  with modify permissions: Set(seven); groups with modify permissions: Set()
[INFO][2018-05-24 20:10:14,987][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 49457.
[INFO][2018-05-24 20:10:15,018][org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO][2018-05-24 20:10:15,053][org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO][2018-05-24 20:10:15,056][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2018-05-24 20:10:15,057][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
[INFO][2018-05-24 20:10:15,070][org.apache.spark.storage.DiskBlockManager]Created local directory at /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/blockmgr-fe1d0352-8474-4361-8dbb-40c093df3b65
[INFO][2018-05-24 20:10:15,085][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 912.3 MB
[INFO][2018-05-24 20:10:15,158][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO][2018-05-24 20:10:15,259][org.spark_project.jetty.util.log]Logging initialized @2960ms
[INFO][2018-05-24 20:10:15,330][org.spark_project.jetty.server.Server]jetty-9.3.z-SNAPSHOT
[INFO][2018-05-24 20:10:15,353][org.spark_project.jetty.server.Server]Started @3056ms
[INFO][2018-05-24 20:10:15,388][org.spark_project.jetty.server.AbstractConnector]Started ServerConnector@24faea88{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-05-24 20:10:15,388][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4040.
[INFO][2018-05-24 20:10:15,437][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@649725e3{/jobs,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:10:15,438][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1a1da881{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:10:15,443][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7fd4acee{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:10:15,446][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2f058b8a{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:10:15,447][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3f2ef586{/stages,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:10:15,448][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@76c7beb3{/stages/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:10:15,450][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2cf92cc7{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:10:15,454][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@611df6e3{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:10:15,454][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6273c5a4{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:10:15,455][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@53e211ee{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:10:15,471][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3d8bbcdc{/storage,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:10:15,480][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@117e0fe5{/storage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:10:15,481][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@78aea4b9{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:10:15,482][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4b85880b{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:10:15,483][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4215838f{/environment,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:10:15,487][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2289aca5{/environment/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:10:15,488][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@184497d1{/executors,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:10:15,488][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6ffab045{/executors/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:10:15,492][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3e2943ab{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:10:15,493][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4a9f80d3{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:10:15,508][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@41fe9859{/static,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:10:15,512][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4602c2a9{/,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:10:15,514][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3e2822{/api,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:10:15,518][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5fcacc0{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:10:15,520][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6d1d4d7{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:10:15,526][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://192.168.0.102:4040
[INFO][2018-05-24 20:10:15,662][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
[INFO][2018-05-24 20:10:15,731][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 49458.
[INFO][2018-05-24 20:10:15,733][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 192.168.0.102:49458
[INFO][2018-05-24 20:10:15,735][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2018-05-24 20:10:15,739][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 192.168.0.102, 49458, None)
[INFO][2018-05-24 20:10:15,745][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 192.168.0.102:49458 with 912.3 MB RAM, BlockManagerId(driver, 192.168.0.102, 49458, None)
[INFO][2018-05-24 20:10:15,754][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 192.168.0.102, 49458, None)
[INFO][2018-05-24 20:10:15,755][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 192.168.0.102, 49458, None)
[INFO][2018-05-24 20:10:16,085][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@18ca3c62{/metrics/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:10:16,244][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 20:10:16,250][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 20:10:16,251][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 20:10:31,715][org.apache.spark.streaming.kafka.DirectKafkaInputDStream]Slide time = 10000 ms
[INFO][2018-05-24 20:10:31,716][org.apache.spark.streaming.kafka.DirectKafkaInputDStream]Storage level = Serialized 1x Replicated
[INFO][2018-05-24 20:10:31,716][org.apache.spark.streaming.kafka.DirectKafkaInputDStream]Checkpoint interval = null
[INFO][2018-05-24 20:10:31,717][org.apache.spark.streaming.kafka.DirectKafkaInputDStream]Remember interval = 10000 ms
[INFO][2018-05-24 20:10:31,718][org.apache.spark.streaming.kafka.DirectKafkaInputDStream]Initialized and validated org.apache.spark.streaming.kafka.DirectKafkaInputDStream@748e565d
[INFO][2018-05-24 20:10:31,718][org.apache.spark.streaming.dstream.MappedDStream]Slide time = 10000 ms
[INFO][2018-05-24 20:10:31,718][org.apache.spark.streaming.dstream.MappedDStream]Storage level = Serialized 1x Replicated
[INFO][2018-05-24 20:10:31,718][org.apache.spark.streaming.dstream.MappedDStream]Checkpoint interval = null
[INFO][2018-05-24 20:10:31,718][org.apache.spark.streaming.dstream.MappedDStream]Remember interval = 10000 ms
[INFO][2018-05-24 20:10:31,718][org.apache.spark.streaming.dstream.MappedDStream]Initialized and validated org.apache.spark.streaming.dstream.MappedDStream@d3c017d
[INFO][2018-05-24 20:10:31,719][org.apache.spark.streaming.dstream.ForEachDStream]Slide time = 10000 ms
[INFO][2018-05-24 20:10:31,719][org.apache.spark.streaming.dstream.ForEachDStream]Storage level = Serialized 1x Replicated
[INFO][2018-05-24 20:10:31,719][org.apache.spark.streaming.dstream.ForEachDStream]Checkpoint interval = null
[INFO][2018-05-24 20:10:31,719][org.apache.spark.streaming.dstream.ForEachDStream]Remember interval = 10000 ms
[INFO][2018-05-24 20:10:31,719][org.apache.spark.streaming.dstream.ForEachDStream]Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@56a7fc14
[INFO][2018-05-24 20:10:31,778][org.apache.spark.streaming.util.RecurringTimer]Started timer for JobGenerator at time 1527163840000
[INFO][2018-05-24 20:10:31,779][org.apache.spark.streaming.scheduler.JobGenerator]Started JobGenerator at 1527163840000 ms
[INFO][2018-05-24 20:10:31,780][org.apache.spark.streaming.scheduler.JobScheduler]Started JobScheduler
[INFO][2018-05-24 20:10:31,784][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1ab6718{/streaming,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:10:31,785][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@460510aa{/streaming/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:10:31,786][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@78b41097{/streaming/batch,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:10:31,786][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@327c7bea{/streaming/batch/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:10:31,787][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7cf283e1{/static/streaming,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:10:31,788][org.apache.spark.streaming.StreamingContext]StreamingContext started
[INFO][2018-05-24 20:10:38,308][org.apache.spark.SparkContext]Running Spark version 2.2.0
[INFO][2018-05-24 20:10:39,124][org.apache.spark.SparkContext]Submitted application: SimulationKafkaSendOutData$
[INFO][2018-05-24 20:10:39,145][org.apache.spark.SecurityManager]Changing view acls to: seven
[INFO][2018-05-24 20:10:39,145][org.apache.spark.SecurityManager]Changing modify acls to: seven
[INFO][2018-05-24 20:10:39,146][org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO][2018-05-24 20:10:39,146][org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO][2018-05-24 20:10:39,147][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(seven); groups with view permissions: Set(); users  with modify permissions: Set(seven); groups with modify permissions: Set()
[INFO][2018-05-24 20:10:39,429][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 49472.
[INFO][2018-05-24 20:10:39,449][org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO][2018-05-24 20:10:39,464][org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO][2018-05-24 20:10:39,468][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2018-05-24 20:10:39,468][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
[INFO][2018-05-24 20:10:39,478][org.apache.spark.storage.DiskBlockManager]Created local directory at /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/blockmgr-0a997afd-72ad-4ca4-b2cc-4f5e1fa9c153
[INFO][2018-05-24 20:10:39,500][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 912.3 MB
[INFO][2018-05-24 20:10:39,603][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO][2018-05-24 20:10:39,706][org.spark_project.jetty.util.log]Logging initialized @2477ms
[INFO][2018-05-24 20:10:39,785][org.spark_project.jetty.server.Server]jetty-9.3.z-SNAPSHOT
[INFO][2018-05-24 20:10:39,799][org.spark_project.jetty.server.Server]Started @2571ms
[WARN][2018-05-24 20:10:39,827][org.apache.spark.util.Utils]Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[INFO][2018-05-24 20:10:39,834][org.spark_project.jetty.server.AbstractConnector]Started ServerConnector@1643d68f{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}
[INFO][2018-05-24 20:10:39,835][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4041.
[INFO][2018-05-24 20:10:39,863][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2a39fe6a{/jobs,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:10:39,864][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@649725e3{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:10:39,864][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4c168660{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:10:39,866][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4eed46ee{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:10:39,867][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4fad94a7{/stages,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:10:39,867][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6326d182{/stages/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:10:39,869][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@716a7124{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:10:39,871][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5cc126dc{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:10:39,871][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@72bd06ca{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:10:39,872][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5dbe30be{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:10:39,873][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@55795845{/storage,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:10:39,873][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@119f1f2a{/storage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:10:39,873][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5b970f7{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:10:39,874][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@165b8a71{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:10:39,875][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2f058b8a{/environment,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:10:39,876][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3f2ef586{/environment/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:10:39,876][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@76c7beb3{/executors,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:10:39,877][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2cf92cc7{/executors/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:10:39,877][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7b139eab{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:10:39,878][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@611df6e3{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:10:39,884][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6273c5a4{/static,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:10:39,884][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3e2943ab{/,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:10:39,886][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4a9f80d3{/api,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:10:39,886][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@191ae03f{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:10:39,887][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@53ab0286{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:10:39,895][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://192.168.0.102:4041
[INFO][2018-05-24 20:10:39,993][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
[INFO][2018-05-24 20:10:40,011][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 49473.
[INFO][2018-05-24 20:10:40,012][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 192.168.0.102:49473
[INFO][2018-05-24 20:10:40,013][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2018-05-24 20:10:40,015][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 192.168.0.102, 49473, None)
[INFO][2018-05-24 20:10:40,019][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 192.168.0.102:49473 with 912.3 MB RAM, BlockManagerId(driver, 192.168.0.102, 49473, None)
[INFO][2018-05-24 20:10:40,028][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 192.168.0.102, 49473, None)
[INFO][2018-05-24 20:10:40,030][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 192.168.0.102, 49473, None)
[INFO][2018-05-24 20:10:40,092][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 20:10:40,092][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 20:10:40,092][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 20:10:40,295][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@63034ed1{/metrics/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:10:40,849][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0 stored as values in memory (estimated size 228.1 KB, free 912.1 MB)
[INFO][2018-05-24 20:10:40,923][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0_piece0 stored as bytes in memory (estimated size 22.1 KB, free 912.1 MB)
[INFO][2018-05-24 20:10:40,925][org.apache.spark.storage.BlockManagerInfo]Added broadcast_0_piece0 in memory on 192.168.0.102:49473 (size: 22.1 KB, free: 912.3 MB)
[INFO][2018-05-24 20:10:40,931][org.apache.spark.SparkContext]Created broadcast 0 from textFile at SimulationKafkaSendOutData.scala:25
[INFO][2018-05-24 20:10:45,213][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527163840000 ms
[INFO][2018-05-24 20:10:45,219][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527163840000 ms.0 from job set of time 1527163840000 ms
[INFO][2018-05-24 20:10:45,279][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:64
[INFO][2018-05-24 20:10:45,309][org.apache.spark.scheduler.DAGScheduler]Got job 0 (foreachPartition at ReceiveKafkaData.scala:64) with 1 output partitions
[INFO][2018-05-24 20:10:45,311][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 0 (foreachPartition at ReceiveKafkaData.scala:64)
[INFO][2018-05-24 20:10:45,311][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:10:45,312][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:10:45,326][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 0 (MapPartitionsRDD[1] at map at ReceiveKafkaData.scala:62), which has no missing parents
[INFO][2018-05-24 20:10:45,522][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-24 20:10:45,565][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0_piece0 stored as bytes in memory (estimated size 2007.0 B, free 912.3 MB)
[INFO][2018-05-24 20:10:45,566][org.apache.spark.storage.BlockManagerInfo]Added broadcast_0_piece0 in memory on 192.168.0.102:49458 (size: 2007.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:10:45,572][org.apache.spark.SparkContext]Created broadcast 0 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:10:45,596][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at map at ReceiveKafkaData.scala:62) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:10:45,597][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 0.0 with 1 tasks
[INFO][2018-05-24 20:10:45,642][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:10:45,650][org.apache.spark.executor.Executor]Running task 0.0 in stage 0.0 (TID 0)
[INFO][2018-05-24 20:10:45,682][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12076 is the same as ending offset skipping seven 0
[INFO][2018-05-24 20:10:45,872][org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper]Process identifier=hconnection-0x24a26cd4 connecting to ZooKeeper ensemble=vm-xaj-bigdata-da-d01:2181,vm-xaj-bigdata-da-d02:2181,vm-xaj-bigdata-da-d03:2181
[INFO][2018-05-24 20:10:45,878][org.apache.zookeeper.ZooKeeper]Client environment:zookeeper.version=3.4.5-cdh5.7.0--1, built on 04/06/2016 14:24 GMT
[INFO][2018-05-24 20:10:45,878][org.apache.zookeeper.ZooKeeper]Client environment:host.name=192.168.0.102
[INFO][2018-05-24 20:10:45,878][org.apache.zookeeper.ZooKeeper]Client environment:java.version=1.8.0_161
[INFO][2018-05-24 20:10:45,878][org.apache.zookeeper.ZooKeeper]Client environment:java.vendor=Oracle Corporation
[INFO][2018-05-24 20:10:45,878][org.apache.zookeeper.ZooKeeper]Client environment:java.home=/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre
[INFO][2018-05-24 20:10:45,878][org.apache.zookeeper.ZooKeeper]Client environment:java.class.path=/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/charsets.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/deploy.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/cldrdata.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/dnsns.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/jaccess.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/jfxrt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/localedata.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/nashorn.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/sunec.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/sunjce_provider.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/sunpkcs11.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/zipfs.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/javaws.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/jce.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/jfr.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/jfxswt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/jsse.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/management-agent.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/plugin.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/resources.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/rt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/ant-javafx.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/dt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/javafx-mx.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/jconsole.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/packager.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/sa-jdi.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/tools.jar:/Users/seven/project/scala/dataMining/target/classes:/Users/seven/software/maven/repository/org/scala-lang/scala-library/2.11.12/scala-library-2.11.12.jar:/Users/seven/software/maven/repository/org/scala-lang/scala-compiler/2.11.12/scala-compiler-2.11.12.jar:/Users/seven/software/maven/repository/org/scala-lang/modules/scala-xml_2.11/1.0.5/scala-xml_2.11-1.0.5.jar:/Users/seven/software/maven/repository/org/scala-lang/modules/scala-parser-combinators_2.11/1.0.4/scala-parser-combinators_2.11-1.0.4.jar:/Users/seven/software/maven/repository/org/scala-lang/scala-reflect/2.11.12/scala-reflect-2.11.12.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-client/2.6.0-cdh5.7.0/hadoop-client-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-common/2.6.0-cdh5.7.0/hadoop-common-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/xmlenc/xmlenc/0.52/xmlenc-0.52.jar:/Users/seven/software/maven/repository/commons-net/commons-net/3.1/commons-net-3.1.jar:/Users/seven/software/maven/repository/commons-configuration/commons-configuration/1.6/commons-configuration-1.6.jar:/Users/seven/software/maven/repository/commons-digester/commons-digester/1.8/commons-digester-1.8.jar:/Users/seven/software/maven/repository/commons-beanutils/commons-beanutils/1.7.0/commons-beanutils-1.7.0.jar:/Users/seven/software/maven/repository/org/slf4j/slf4j-api/1.7.5/slf4j-api-1.7.5.jar:/Users/seven/software/maven/repository/org/slf4j/slf4j-log4j12/1.7.5/slf4j-log4j12-1.7.5.jar:/Users/seven/software/maven/repository/org/apache/avro/avro/1.7.6-cdh5.7.0/avro-1.7.6-cdh5.7.0.jar:/Users/seven/software/maven/repository/com/google/code/gson/gson/2.2.4/gson-2.2.4.jar:/Users/seven/software/maven/repository/org/apache/curator/curator-client/2.7.1/curator-client-2.7.1.jar:/Users/seven/software/maven/repository/org/apache/curator/curator-recipes/2.7.1/curator-recipes-2.7.1.jar:/Users/seven/software/maven/repository/com/google/code/findbugs/jsr305/3.0.0/jsr305-3.0.0.jar:/Users/seven/software/maven/repository/org/apache/htrace/htrace-core4/4.0.1-incubating/htrace-core4-4.0.1-incubating.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-compress/1.4.1/commons-compress-1.4.1.jar:/Users/seven/software/maven/repository/org/tukaani/xz/1.0/xz-1.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-hdfs/2.6.0-cdh5.7.0/hadoop-hdfs-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/io/netty/netty/3.6.2.Final/netty-3.6.2.Final.jar:/Users/seven/software/maven/repository/xerces/xercesImpl/2.9.1/xercesImpl-2.9.1.jar:/Users/seven/software/maven/repository/xml-apis/xml-apis/1.3.04/xml-apis-1.3.04.jar:/Users/seven/software/maven/repository/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-app/2.6.0-cdh5.7.0/hadoop-mapreduce-client-app-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-common/2.6.0-cdh5.7.0/hadoop-mapreduce-client-common-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-yarn-client/2.6.0-cdh5.7.0/hadoop-yarn-client-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-yarn-server-common/2.6.0-cdh5.7.0/hadoop-yarn-server-common-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-shuffle/2.6.0-cdh5.7.0/hadoop-mapreduce-client-shuffle-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-yarn-api/2.6.0-cdh5.7.0/hadoop-yarn-api-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-core/2.6.0-cdh5.7.0/hadoop-mapreduce-client-core-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-yarn-common/2.6.0-cdh5.7.0/hadoop-yarn-common-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/javax/xml/bind/jaxb-api/2.2.2/jaxb-api-2.2.2.jar:/Users/seven/software/maven/repository/javax/xml/stream/stax-api/1.0-2/stax-api-1.0-2.jar:/Users/seven/software/maven/repository/javax/activation/activation/1.1/activation-1.1.jar:/Users/seven/software/maven/repository/com/sun/jersey/jersey-client/1.9/jersey-client-1.9.jar:/Users/seven/software/maven/repository/org/codehaus/jackson/jackson-xc/1.8.8/jackson-xc-1.8.8.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.6.0-cdh5.7.0/hadoop-mapreduce-client-jobclient-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-aws/2.6.0-cdh5.7.0/hadoop-aws-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/com/amazonaws/aws-java-sdk-s3/1.10.6/aws-java-sdk-s3-1.10.6.jar:/Users/seven/software/maven/repository/com/amazonaws/aws-java-sdk-kms/1.10.6/aws-java-sdk-kms-1.10.6.jar:/Users/seven/software/maven/repository/com/amazonaws/aws-java-sdk-core/1.10.6/aws-java-sdk-core-1.10.6.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-annotations/2.6.0-cdh5.7.0/hadoop-annotations-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-client/1.2.0-cdh5.7.0/hbase-client-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-annotations/1.2.0-cdh5.7.0/hbase-annotations-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-common/1.2.0-cdh5.7.0/hbase-common-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-protocol/1.2.0-cdh5.7.0/hbase-protocol-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/commons-codec/commons-codec/1.9/commons-codec-1.9.jar:/Users/seven/software/maven/repository/commons-io/commons-io/2.4/commons-io-2.4.jar:/Users/seven/software/maven/repository/commons-lang/commons-lang/2.6/commons-lang-2.6.jar:/Users/seven/software/maven/repository/commons-logging/commons-logging/1.2/commons-logging-1.2.jar:/Users/seven/software/maven/repository/com/google/guava/guava/12.0.1/guava-12.0.1.jar:/Users/seven/software/maven/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/Users/seven/software/maven/repository/io/netty/netty-all/4.0.23.Final/netty-all-4.0.23.Final.jar:/Users/seven/software/maven/repository/org/apache/zookeeper/zookeeper/3.4.5-cdh5.7.0/zookeeper-3.4.5-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/htrace/htrace-core/3.2.0-incubating/htrace-core-3.2.0-incubating.jar:/Users/seven/software/maven/repository/org/codehaus/jackson/jackson-mapper-asl/1.8.8/jackson-mapper-asl-1.8.8.jar:/Users/seven/software/maven/repository/org/jruby/jcodings/jcodings/1.0.8/jcodings-1.0.8.jar:/Users/seven/software/maven/repository/org/jruby/joni/joni/2.1.2/joni-2.1.2.jar:/Users/seven/software/maven/repository/com/yammer/metrics/metrics-core/2.2.0/metrics-core-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-auth/2.6.0-cdh5.7.0/hadoop-auth-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/httpcomponents/httpclient/4.2.5/httpclient-4.2.5.jar:/Users/seven/software/maven/repository/org/apache/httpcomponents/httpcore/4.2.4/httpcore-4.2.4.jar:/Users/seven/software/maven/repository/org/apache/directory/server/apacheds-kerberos-codec/2.0.0-M15/apacheds-kerberos-codec-2.0.0-M15.jar:/Users/seven/software/maven/repository/org/apache/directory/server/apacheds-i18n/2.0.0-M15/apacheds-i18n-2.0.0-M15.jar:/Users/seven/software/maven/repository/org/apache/directory/api/api-asn1-api/1.0.0-M20/api-asn1-api-1.0.0-M20.jar:/Users/seven/software/maven/repository/org/apache/directory/api/api-util/1.0.0-M20/api-util-1.0.0-M20.jar:/Users/seven/software/maven/repository/org/apache/curator/curator-framework/2.7.1/curator-framework-2.7.1.jar:/Users/seven/software/maven/repository/com/github/stephenc/findbugs/findbugs-annotations/1.3.9-1/findbugs-annotations-1.3.9-1.jar:/Users/seven/software/maven/repository/junit/junit/4.12/junit-4.12.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-server/1.2.0-cdh5.7.0/hbase-server-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-procedure/1.2.0-cdh5.7.0/hbase-procedure-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-common/1.2.0-cdh5.7.0/hbase-common-1.2.0-cdh5.7.0-tests.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-prefix-tree/1.2.0-cdh5.7.0/hbase-prefix-tree-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar:/Users/seven/software/maven/repository/commons-collections/commons-collections/3.2.2/commons-collections-3.2.2.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-hadoop-compat/1.2.0-cdh5.7.0/hbase-hadoop-compat-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-hadoop2-compat/1.2.0-cdh5.7.0/hbase-hadoop2-compat-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/com/sun/jersey/jersey-core/1.9/jersey-core-1.9.jar:/Users/seven/software/maven/repository/com/sun/jersey/jersey-server/1.9/jersey-server-1.9.jar:/Users/seven/software/maven/repository/asm/asm/3.1/asm-3.1.jar:/Users/seven/software/maven/repository/commons-cli/commons-cli/1.2/commons-cli-1.2.jar:/Users/seven/software/maven/repository/com/github/stephenc/high-scale-lib/high-scale-lib/1.1.1/high-scale-lib-1.1.1.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-math/2.1/commons-math-2.1.jar:/Users/seven/software/maven/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jetty/6.1.26.cloudera.4/jetty-6.1.26.cloudera.4.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jetty-util/6.1.26.cloudera.4/jetty-util-6.1.26.cloudera.4.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jetty-sslengine/6.1.26.cloudera.4/jetty-sslengine-6.1.26.cloudera.4.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jsp-2.1/6.1.14/jsp-2.1-6.1.14.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jsp-api-2.1/6.1.14/jsp-api-2.1-6.1.14.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/servlet-api-2.5/6.1.14/servlet-api-2.5-6.1.14.jar:/Users/seven/software/maven/repository/org/codehaus/jackson/jackson-core-asl/1.8.8/jackson-core-asl-1.8.8.jar:/Users/seven/software/maven/repository/org/codehaus/jackson/jackson-jaxrs/1.8.8/jackson-jaxrs-1.8.8.jar:/Users/seven/software/maven/repository/tomcat/jasper-compiler/5.5.23/jasper-compiler-5.5.23.jar:/Users/seven/software/maven/repository/tomcat/jasper-runtime/5.5.23/jasper-runtime-5.5.23.jar:/Users/seven/software/maven/repository/commons-el/commons-el/1.0/commons-el-1.0.jar:/Users/seven/software/maven/repository/org/jamon/jamon-runtime/2.4.1/jamon-runtime-2.4.1.jar:/Users/seven/software/maven/repository/com/lmax/disruptor/3.3.0/disruptor-3.3.0.jar:/Users/seven/software/maven/repository/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar:/Users/seven/software/maven/repository/org/owasp/esapi/esapi/2.1.0/esapi-2.1.0.jar:/Users/seven/software/maven/repository/commons-beanutils/commons-beanutils-core/1.7.0/commons-beanutils-core-1.7.0.jar:/Users/seven/software/maven/repository/commons-fileupload/commons-fileupload/1.2/commons-fileupload-1.2.jar:/Users/seven/software/maven/repository/xom/xom/1.2.5/xom-1.2.5.jar:/Users/seven/software/maven/repository/xalan/xalan/2.7.0/xalan-2.7.0.jar:/Users/seven/software/maven/repository/org/beanshell/bsh-core/2.0b4/bsh-core-2.0b4.jar:/Users/seven/software/maven/repository/org/owasp/antisamy/antisamy/1.4.3/antisamy-1.4.3.jar:/Users/seven/software/maven/repository/org/apache/xmlgraphics/batik-css/1.7/batik-css-1.7.jar:/Users/seven/software/maven/repository/org/apache/xmlgraphics/batik-ext/1.7/batik-ext-1.7.jar:/Users/seven/software/maven/repository/org/apache/xmlgraphics/batik-util/1.7/batik-util-1.7.jar:/Users/seven/software/maven/repository/xml-apis/xml-apis-ext/1.3.04/xml-apis-ext-1.3.04.jar:/Users/seven/software/maven/repository/net/sourceforge/nekohtml/nekohtml/1.9.12/nekohtml-1.9.12.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-core/2.6.0-mr1-cdh5.7.0/hadoop-core-2.6.0-mr1-cdh5.7.0.jar:/Users/seven/software/maven/repository/javax/servlet/jsp/jsp-api/2.1/jsp-api-2.1.jar:/Users/seven/software/maven/repository/net/java/dev/jets3t/jets3t/0.6.1/jets3t-0.6.1.jar:/Users/seven/software/maven/repository/javax/servlet/servlet-api/2.5/servlet-api-2.5.jar:/Users/seven/software/maven/repository/hsqldb/hsqldb/1.8.0.10/hsqldb-1.8.0.10.jar:/Users/seven/software/maven/repository/org/eclipse/jdt/core/3.1.1/core-3.1.1.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-hdfs/2.6.0-cdh5.7.0/hadoop-hdfs-2.6.0-cdh5.7.0-tests.jar:/Users/seven/software/maven/repository/commons-daemon/commons-daemon/1.0.13/commons-daemon-1.0.13.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-lang3/3.5/commons-lang3-3.5.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-pool2/2.4.2/commons-pool2-2.4.2.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-mllib_2.11/2.2.0/spark-mllib_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-core_2.11/2.2.0/spark-core_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/avro/avro-mapred/1.7.7/avro-mapred-1.7.7-hadoop2.jar:/Users/seven/software/maven/repository/org/apache/avro/avro-ipc/1.7.7/avro-ipc-1.7.7-tests.jar:/Users/seven/software/maven/repository/com/twitter/chill_2.11/0.8.0/chill_2.11-0.8.0.jar:/Users/seven/software/maven/repository/com/esotericsoftware/kryo-shaded/3.0.3/kryo-shaded-3.0.3.jar:/Users/seven/software/maven/repository/com/esotericsoftware/minlog/1.3.0/minlog-1.3.0.jar:/Users/seven/software/maven/repository/org/objenesis/objenesis/2.1/objenesis-2.1.jar:/Users/seven/software/maven/repository/com/twitter/chill-java/0.8.0/chill-java-0.8.0.jar:/Users/seven/software/maven/repository/org/apache/xbean/xbean-asm5-shaded/4.4/xbean-asm5-shaded-4.4.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-launcher_2.11/2.2.0/spark-launcher_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-network-common_2.11/2.2.0/spark-network-common_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-network-shuffle_2.11/2.2.0/spark-network-shuffle_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-unsafe_2.11/2.2.0/spark-unsafe_2.11-2.2.0.jar:/Users/seven/software/maven/repository/javax/servlet/javax.servlet-api/3.1.0/javax.servlet-api-3.1.0.jar:/Users/seven/software/maven/repository/org/slf4j/jul-to-slf4j/1.7.16/jul-to-slf4j-1.7.16.jar:/Users/seven/software/maven/repository/org/slf4j/jcl-over-slf4j/1.7.16/jcl-over-slf4j-1.7.16.jar:/Users/seven/software/maven/repository/com/ning/compress-lzf/1.0.3/compress-lzf-1.0.3.jar:/Users/seven/software/maven/repository/org/xerial/snappy/snappy-java/1.1.2.6/snappy-java-1.1.2.6.jar:/Users/seven/software/maven/repository/org/roaringbitmap/RoaringBitmap/0.5.11/RoaringBitmap-0.5.11.jar:/Users/seven/software/maven/repository/org/json4s/json4s-jackson_2.11/3.2.11/json4s-jackson_2.11-3.2.11.jar:/Users/seven/software/maven/repository/org/json4s/json4s-core_2.11/3.2.11/json4s-core_2.11-3.2.11.jar:/Users/seven/software/maven/repository/org/json4s/json4s-ast_2.11/3.2.11/json4s-ast_2.11-3.2.11.jar:/Users/seven/software/maven/repository/org/scala-lang/scalap/2.11.0/scalap-2.11.0.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/core/jersey-client/2.22.2/jersey-client-2.22.2.jar:/Users/seven/software/maven/repository/javax/ws/rs/javax.ws.rs-api/2.0.1/javax.ws.rs-api-2.0.1.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/hk2-api/2.4.0-b34/hk2-api-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/hk2-utils/2.4.0-b34/hk2-utils-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/external/aopalliance-repackaged/2.4.0-b34/aopalliance-repackaged-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/external/javax.inject/2.4.0-b34/javax.inject-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/hk2-locator/2.4.0-b34/hk2-locator-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/javassist/javassist/3.18.1-GA/javassist-3.18.1-GA.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/core/jersey-common/2.22.2/jersey-common-2.22.2.jar:/Users/seven/software/maven/repository/javax/annotation/javax.annotation-api/1.2/javax.annotation-api-1.2.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/bundles/repackaged/jersey-guava/2.22.2/jersey-guava-2.22.2.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/osgi-resource-locator/1.0.1/osgi-resource-locator-1.0.1.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/core/jersey-server/2.22.2/jersey-server-2.22.2.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/media/jersey-media-jaxb/2.22.2/jersey-media-jaxb-2.22.2.jar:/Users/seven/software/maven/repository/javax/validation/validation-api/1.1.0.Final/validation-api-1.1.0.Final.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/containers/jersey-container-servlet/2.22.2/jersey-container-servlet-2.22.2.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/containers/jersey-container-servlet-core/2.22.2/jersey-container-servlet-core-2.22.2.jar:/Users/seven/software/maven/repository/com/clearspring/analytics/stream/2.7.0/stream-2.7.0.jar:/Users/seven/software/maven/repository/io/dropwizard/metrics/metrics-core/3.1.2/metrics-core-3.1.2.jar:/Users/seven/software/maven/repository/io/dropwizard/metrics/metrics-jvm/3.1.2/metrics-jvm-3.1.2.jar:/Users/seven/software/maven/repository/io/dropwizard/metrics/metrics-json/3.1.2/metrics-json-3.1.2.jar:/Users/seven/software/maven/repository/io/dropwizard/metrics/metrics-graphite/3.1.2/metrics-graphite-3.1.2.jar:/Users/seven/software/maven/repository/org/apache/ivy/ivy/2.4.0/ivy-2.4.0.jar:/Users/seven/software/maven/repository/oro/oro/2.0.8/oro-2.0.8.jar:/Users/seven/software/maven/repository/net/razorvine/pyrolite/4.13/pyrolite-4.13.jar:/Users/seven/software/maven/repository/net/sf/py4j/py4j/0.10.4/py4j-0.10.4.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-crypto/1.0.0/commons-crypto-1.0.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-streaming_2.11/2.2.0/spark-streaming_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-sql_2.11/2.2.0/spark-sql_2.11-2.2.0.jar:/Users/seven/software/maven/repository/com/univocity/univocity-parsers/2.2.1/univocity-parsers-2.2.1.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-sketch_2.11/2.2.0/spark-sketch_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-catalyst_2.11/2.2.0/spark-catalyst_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/codehaus/janino/janino/3.0.0/janino-3.0.0.jar:/Users/seven/software/maven/repository/org/codehaus/janino/commons-compiler/3.0.0/commons-compiler-3.0.0.jar:/Users/seven/software/maven/repository/org/antlr/antlr4-runtime/4.5.3/antlr4-runtime-4.5.3.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-column/1.8.2/parquet-column-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-common/1.8.2/parquet-common-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-encoding/1.8.2/parquet-encoding-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-hadoop/1.8.2/parquet-hadoop-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-format/2.3.1/parquet-format-2.3.1.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-jackson/1.8.2/parquet-jackson-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-graphx_2.11/2.2.0/spark-graphx_2.11-2.2.0.jar:/Users/seven/software/maven/repository/com/github/fommil/netlib/core/1.1.2/core-1.1.2.jar:/Users/seven/software/maven/repository/net/sourceforge/f2j/arpack_combined_all/0.1/arpack_combined_all-0.1.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-mllib-local_2.11/2.2.0/spark-mllib-local_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/scalanlp/breeze_2.11/0.13.1/breeze_2.11-0.13.1.jar:/Users/seven/software/maven/repository/org/scalanlp/breeze-macros_2.11/0.13.1/breeze-macros_2.11-0.13.1.jar:/Users/seven/software/maven/repository/net/sf/opencsv/opencsv/2.3/opencsv-2.3.jar:/Users/seven/software/maven/repository/com/github/rwl/jtransforms/2.4.0/jtransforms-2.4.0.jar:/Users/seven/software/maven/repository/org/spire-math/spire_2.11/0.13.0/spire_2.11-0.13.0.jar:/Users/seven/software/maven/repository/org/spire-math/spire-macros_2.11/0.13.0/spire-macros_2.11-0.13.0.jar:/Users/seven/software/maven/repository/org/typelevel/machinist_2.11/0.6.1/machinist_2.11-0.6.1.jar:/Users/seven/software/maven/repository/com/chuusai/shapeless_2.11/2.3.2/shapeless_2.11-2.3.2.jar:/Users/seven/software/maven/repository/org/typelevel/macro-compat_2.11/1.1.1/macro-compat_2.11-1.1.1.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-math3/3.4.1/commons-math3-3.4.1.jar:/Users/seven/software/maven/repository/org/jpmml/pmml-model/1.2.15/pmml-model-1.2.15.jar:/Users/seven/software/maven/repository/org/jpmml/pmml-schema/1.2.15/pmml-schema-1.2.15.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-tags_2.11/2.2.0/spark-tags_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-streaming-kafka-0-8_2.11/2.2.0/spark-streaming-kafka-0-8_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/kafka/kafka_2.11/0.8.2.1/kafka_2.11-0.8.2.1.jar:/Users/seven/software/maven/repository/com/101tec/zkclient/0.3/zkclient-0.3.jar:/Users/seven/software/maven/repository/org/apache/kafka/kafka-clients/0.8.2.1/kafka-clients-0.8.2.1.jar:/Users/seven/software/maven/repository/mysql/mysql-connector-java/5.1.31/mysql-connector-java-5.1.31.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/module/jackson-module-scala_2.11/2.6.5/jackson-module-scala_2.11-2.6.5.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/core/jackson-core/2.6.5/jackson-core-2.6.5.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/core/jackson-annotations/2.6.5/jackson-annotations-2.6.5.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/core/jackson-databind/2.6.5/jackson-databind-2.6.5.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/module/jackson-module-paranamer/2.6.5/jackson-module-paranamer-2.6.5.jar:/Users/seven/software/maven/repository/com/thoughtworks/paranamer/paranamer/2.6/paranamer-2.6.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-streaming-flume_2.11/2.2.0/spark-streaming-flume_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/flume/flume-ng-core/1.6.0/flume-ng-core-1.6.0.jar:/Users/seven/software/maven/repository/org/apache/flume/flume-ng-configuration/1.6.0/flume-ng-configuration-1.6.0.jar:/Users/seven/software/maven/repository/org/apache/avro/avro-ipc/1.7.4/avro-ipc-1.7.4.jar:/Users/seven/software/maven/repository/org/apache/velocity/velocity/1.7/velocity-1.7.jar:/Users/seven/software/maven/repository/joda-time/joda-time/2.1/joda-time-2.1.jar:/Users/seven/software/maven/repository/org/apache/mina/mina-core/2.0.4/mina-core-2.0.4.jar:/Users/seven/software/maven/repository/org/apache/flume/flume-ng-sdk/1.6.0/flume-ng-sdk-1.6.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-streaming-flume-sink_2.11/2.2.0/spark-streaming-flume-sink_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/flume/flume-ng-clients/flume-ng-log4jappender/1.6.0/flume-ng-log4jappender-1.6.0.jar:/Users/seven/software/maven/repository/net/jpountz/lz4/lz4/1.3.0/lz4-1.3.0.jar:/Applications/IntelliJ IDEA.app/Contents/lib/idea_rt.jar
[INFO][2018-05-24 20:10:45,879][org.apache.zookeeper.ZooKeeper]Client environment:java.library.path=/Users/seven/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:.
[INFO][2018-05-24 20:10:45,879][org.apache.zookeeper.ZooKeeper]Client environment:java.io.tmpdir=/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/
[INFO][2018-05-24 20:10:45,879][org.apache.zookeeper.ZooKeeper]Client environment:java.compiler=<NA>
[INFO][2018-05-24 20:10:45,879][org.apache.zookeeper.ZooKeeper]Client environment:os.name=Mac OS X
[INFO][2018-05-24 20:10:45,879][org.apache.zookeeper.ZooKeeper]Client environment:os.arch=x86_64
[INFO][2018-05-24 20:10:45,879][org.apache.zookeeper.ZooKeeper]Client environment:os.version=10.13.4
[INFO][2018-05-24 20:10:45,879][org.apache.zookeeper.ZooKeeper]Client environment:user.name=seven
[INFO][2018-05-24 20:10:45,879][org.apache.zookeeper.ZooKeeper]Client environment:user.home=/Users/seven
[INFO][2018-05-24 20:10:45,879][org.apache.zookeeper.ZooKeeper]Client environment:user.dir=/Users/seven/project/scala/dataMining
[INFO][2018-05-24 20:10:45,880][org.apache.zookeeper.ZooKeeper]Initiating client connection, connectString=vm-xaj-bigdata-da-d01:2181,vm-xaj-bigdata-da-d02:2181,vm-xaj-bigdata-da-d03:2181 sessionTimeout=60000 watcher=hconnection-0x24a26cd40x0, quorum=vm-xaj-bigdata-da-d01:2181,vm-xaj-bigdata-da-d02:2181,vm-xaj-bigdata-da-d03:2181, baseZNode=/hbase
[WARN][2018-05-24 20:10:46,380][org.apache.hadoop.util.NativeCodeLoader]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO][2018-05-24 20:10:46,515][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 1
[INFO][2018-05-24 20:10:46,617][org.apache.spark.SparkContext]Starting job: collect at SimulationKafkaSendOutData.scala:25
[INFO][2018-05-24 20:10:46,645][org.apache.spark.scheduler.DAGScheduler]Got job 0 (collect at SimulationKafkaSendOutData.scala:25) with 2 output partitions
[INFO][2018-05-24 20:10:46,646][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 0 (collect at SimulationKafkaSendOutData.scala:25)
[INFO][2018-05-24 20:10:46,646][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:10:46,649][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:10:46,659][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 0 (hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/* MapPartitionsRDD[1] at textFile at SimulationKafkaSendOutData.scala:25), which has no missing parents
[INFO][2018-05-24 20:10:46,680][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1 stored as values in memory (estimated size 3.3 KB, free 912.1 MB)
[INFO][2018-05-24 20:10:46,682][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1_piece0 stored as bytes in memory (estimated size 1969.0 B, free 912.1 MB)
[INFO][2018-05-24 20:10:46,683][org.apache.spark.storage.BlockManagerInfo]Added broadcast_1_piece0 in memory on 192.168.0.102:49473 (size: 1969.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:10:46,684][org.apache.spark.SparkContext]Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:10:46,699][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ResultStage 0 (hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/* MapPartitionsRDD[1] at textFile at SimulationKafkaSendOutData.scala:25) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2018-05-24 20:10:46,702][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 0.0 with 2 tasks
[INFO][2018-05-24 20:10:46,739][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 4883 bytes)
[INFO][2018-05-24 20:10:46,743][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 4883 bytes)
[INFO][2018-05-24 20:10:46,753][org.apache.spark.executor.Executor]Running task 1.0 in stage 0.0 (TID 1)
[INFO][2018-05-24 20:10:46,753][org.apache.spark.executor.Executor]Running task 0.0 in stage 0.0 (TID 0)
[INFO][2018-05-24 20:10:46,842][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/part-00000:10717435+10717436
[INFO][2018-05-24 20:10:46,843][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/part-00000:0+10717435
[INFO][2018-05-24 20:10:50,340][org.apache.spark.storage.memory.MemoryStore]Block taskresult_0 stored as bytes in memory (estimated size 10.4 MB, free 901.7 MB)
[INFO][2018-05-24 20:10:50,341][org.apache.spark.storage.BlockManagerInfo]Added taskresult_0 in memory on 192.168.0.102:49473 (size: 10.4 MB, free: 901.9 MB)
[INFO][2018-05-24 20:10:50,342][org.apache.spark.executor.Executor]Finished task 0.0 in stage 0.0 (TID 0). 10856349 bytes result sent via BlockManager)
[INFO][2018-05-24 20:10:50,423][org.apache.spark.network.client.TransportClientFactory]Successfully created connection to /192.168.0.102:49473 after 63 ms (0 ms spent in bootstraps)
[INFO][2018-05-24 20:10:50,560][org.apache.spark.storage.memory.MemoryStore]Block taskresult_1 stored as bytes in memory (estimated size 10.4 MB, free 891.3 MB)
[INFO][2018-05-24 20:10:50,561][org.apache.spark.storage.BlockManagerInfo]Added taskresult_1 in memory on 192.168.0.102:49473 (size: 10.4 MB, free: 891.6 MB)
[INFO][2018-05-24 20:10:50,575][org.apache.spark.executor.Executor]Finished task 1.0 in stage 0.0 (TID 1). 10855899 bytes result sent via BlockManager)
[INFO][2018-05-24 20:10:50,827][org.apache.spark.storage.BlockManagerInfo]Removed taskresult_0 on 192.168.0.102:49473 in memory (size: 10.4 MB, free: 901.9 MB)
[INFO][2018-05-24 20:10:50,827][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 0.0 (TID 0) in 4099 ms on localhost (executor driver) (1/2)
[INFO][2018-05-24 20:10:50,844][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 0.0 (TID 1) in 4102 ms on localhost (executor driver) (2/2)
[INFO][2018-05-24 20:10:50,845][org.apache.spark.storage.BlockManagerInfo]Removed taskresult_1 on 192.168.0.102:49473 in memory (size: 10.4 MB, free: 912.3 MB)
[INFO][2018-05-24 20:10:50,846][org.apache.spark.scheduler.DAGScheduler]ResultStage 0 (collect at SimulationKafkaSendOutData.scala:25) finished in 4.129 s
[INFO][2018-05-24 20:10:50,847][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:10:50,852][org.apache.spark.scheduler.DAGScheduler]Job 0 finished: collect at SimulationKafkaSendOutData.scala:25, took 4.234651 s
[INFO][2018-05-24 20:10:50,940][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527163850000 ms
[INFO][2018-05-24 20:10:50,992][org.spark_project.jetty.server.AbstractConnector]Stopped Spark@1643d68f{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}
[INFO][2018-05-24 20:10:50,994][org.apache.spark.ui.SparkUI]Stopped Spark web UI at http://192.168.0.102:4041
[INFO][2018-05-24 20:10:51,003][org.apache.spark.MapOutputTrackerMasterEndpoint]MapOutputTrackerMasterEndpoint stopped!
[INFO][2018-05-24 20:10:51,017][org.apache.spark.storage.memory.MemoryStore]MemoryStore cleared
[INFO][2018-05-24 20:10:51,017][org.apache.spark.storage.BlockManager]BlockManager stopped
[INFO][2018-05-24 20:10:51,018][org.apache.spark.storage.BlockManagerMaster]BlockManagerMaster stopped
[INFO][2018-05-24 20:10:51,020][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint]OutputCommitCoordinator stopped!
[INFO][2018-05-24 20:10:51,021][org.apache.spark.SparkContext]Successfully stopped SparkContext
[INFO][2018-05-24 20:10:51,052][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	compression.type = none
	metric.reporters = []
	metadata.max.age.ms = 300000
	metadata.fetch.timeout.ms = 60000
	acks = all
	batch.size = 16384
	reconnect.backoff.ms = 10
	bootstrap.servers = [vm-xaj-bigdata-da-d01:9092, vm-xaj-bigdata-da-d02:9092, vm-xaj-bigdata-da-d03:9092, vm-xaj-bigdata-da-d04:9092, vm-xaj-bigdata-da-d05:9092, vm-xaj-bigdata-da-d06:9092, vm-xaj-bigdata-da-d07:9092]
	receive.buffer.bytes = 32768
	retry.backoff.ms = 100
	buffer.memory = 33554432
	timeout.ms = 30000
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	retries = 0
	max.request.size = 1048576
	block.on.buffer.full = true
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
	metrics.sample.window.ms = 30000
	send.buffer.bytes = 131072
	max.in.flight.requests.per.connection = 5
	metrics.num.samples = 2
	linger.ms = 1
	client.id = 

[INFO][2018-05-24 20:10:55,926][org.apache.zookeeper.ClientCnxn]Opening socket connection to server master/10.213.4.25:2181. Will not attempt to authenticate using SASL (unknown error)
[INFO][2018-05-24 20:10:55,939][org.apache.zookeeper.ClientCnxn]Socket connection established, initiating session, client: /10.0.8.39:49485, server: master/10.213.4.25:2181
[INFO][2018-05-24 20:10:55,959][org.apache.zookeeper.ClientCnxn]Session establishment complete on server master/10.213.4.25:2181, sessionid = 0x162b4dc56095e8c, negotiated timeout = 60000
[WARN][2018-05-24 20:10:56,427][org.apache.hadoop.util.NativeCodeLoader]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO][2018-05-24 20:10:56,504][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:10:56,519][org.apache.spark.executor.Executor]Finished task 0.0 in stage 0.0 (TID 0). 794 bytes result sent to driver
[INFO][2018-05-24 20:10:56,526][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 0.0 (TID 0) in 10895 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:10:56,529][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:10:56,532][org.apache.spark.scheduler.DAGScheduler]ResultStage 0 (foreachPartition at ReceiveKafkaData.scala:64) finished in 10.914 s
[INFO][2018-05-24 20:10:56,538][org.apache.spark.scheduler.DAGScheduler]Job 0 finished: foreachPartition at ReceiveKafkaData.scala:64, took 11.257978 s
[INFO][2018-05-24 20:10:56,542][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527163840000 ms.0 from job set of time 1527163840000 ms
[INFO][2018-05-24 20:10:56,543][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 16.541 s for time 1527163840000 ms (execution: 11.325 s)
[INFO][2018-05-24 20:10:56,543][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527163850000 ms.0 from job set of time 1527163850000 ms
[INFO][2018-05-24 20:10:56,549][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:10:56,551][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:64
[INFO][2018-05-24 20:10:56,553][org.apache.spark.scheduler.DAGScheduler]Got job 1 (foreachPartition at ReceiveKafkaData.scala:64) with 1 output partitions
[INFO][2018-05-24 20:10:56,553][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 1 (foreachPartition at ReceiveKafkaData.scala:64)
[INFO][2018-05-24 20:10:56,553][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:10:56,553][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:10:56,553][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 1 (MapPartitionsRDD[3] at map at ReceiveKafkaData.scala:62), which has no missing parents
[INFO][2018-05-24 20:10:56,555][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 
[INFO][2018-05-24 20:10:56,559][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-24 20:10:56,561][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1_piece0 stored as bytes in memory (estimated size 2008.0 B, free 912.3 MB)
[INFO][2018-05-24 20:10:56,561][org.apache.spark.storage.BlockManagerInfo]Added broadcast_1_piece0 in memory on 192.168.0.102:49458 (size: 2008.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:10:56,562][org.apache.spark.SparkContext]Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:10:56,563][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at map at ReceiveKafkaData.scala:62) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:10:56,563][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 1.0 with 1 tasks
[INFO][2018-05-24 20:10:56,564][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:10:56,567][org.apache.spark.executor.Executor]Running task 0.0 in stage 1.0 (TID 1)
[INFO][2018-05-24 20:10:56,601][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12076 is the same as ending offset skipping seven 0
[INFO][2018-05-24 20:10:56,601][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:10:56,603][org.apache.spark.executor.Executor]Finished task 0.0 in stage 1.0 (TID 1). 708 bytes result sent to driver
[INFO][2018-05-24 20:10:56,604][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 1.0 (TID 1) in 40 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:10:56,604][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:10:56,605][org.apache.spark.scheduler.DAGScheduler]ResultStage 1 (foreachPartition at ReceiveKafkaData.scala:64) finished in 0.041 s
[INFO][2018-05-24 20:10:56,605][org.apache.spark.scheduler.DAGScheduler]Job 1 finished: foreachPartition at ReceiveKafkaData.scala:64, took 0.054102 s
[INFO][2018-05-24 20:10:56,606][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527163850000 ms.0 from job set of time 1527163850000 ms
[INFO][2018-05-24 20:10:56,607][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 6.606 s for time 1527163850000 ms (execution: 0.063 s)
[INFO][2018-05-24 20:10:56,607][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 1 from persistence list
[INFO][2018-05-24 20:10:56,612][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 0 from persistence list
[INFO][2018-05-24 20:10:56,613][org.apache.spark.storage.BlockManager]Removing RDD 1
[INFO][2018-05-24 20:10:56,613][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:10:56,613][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 
[INFO][2018-05-24 20:10:56,613][org.apache.spark.storage.BlockManager]Removing RDD 0
[INFO][2018-05-24 20:11:00,060][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527163860000 ms
[INFO][2018-05-24 20:11:00,061][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527163860000 ms.0 from job set of time 1527163860000 ms
[INFO][2018-05-24 20:11:00,070][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:64
[INFO][2018-05-24 20:11:00,071][org.apache.spark.scheduler.DAGScheduler]Got job 2 (foreachPartition at ReceiveKafkaData.scala:64) with 1 output partitions
[INFO][2018-05-24 20:11:00,071][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 2 (foreachPartition at ReceiveKafkaData.scala:64)
[INFO][2018-05-24 20:11:00,071][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:11:00,072][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:11:00,072][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 2 (MapPartitionsRDD[5] at map at ReceiveKafkaData.scala:62), which has no missing parents
[INFO][2018-05-24 20:11:00,076][org.apache.spark.storage.memory.MemoryStore]Block broadcast_2 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-24 20:11:00,078][org.apache.spark.storage.memory.MemoryStore]Block broadcast_2_piece0 stored as bytes in memory (estimated size 2008.0 B, free 912.3 MB)
[INFO][2018-05-24 20:11:00,078][org.apache.spark.storage.BlockManagerInfo]Added broadcast_2_piece0 in memory on 192.168.0.102:49458 (size: 2008.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:11:00,079][org.apache.spark.SparkContext]Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:11:00,080][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[5] at map at ReceiveKafkaData.scala:62) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:11:00,080][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 2.0 with 1 tasks
[INFO][2018-05-24 20:11:00,081][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:11:00,081][org.apache.spark.executor.Executor]Running task 0.0 in stage 2.0 (TID 2)
[INFO][2018-05-24 20:11:00,085][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12076 is the same as ending offset skipping seven 0
[INFO][2018-05-24 20:11:00,085][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:11:00,086][org.apache.spark.executor.Executor]Finished task 0.0 in stage 2.0 (TID 2). 751 bytes result sent to driver
[INFO][2018-05-24 20:11:00,087][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 2.0 (TID 2) in 6 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:11:00,087][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:11:00,088][org.apache.spark.scheduler.DAGScheduler]ResultStage 2 (foreachPartition at ReceiveKafkaData.scala:64) finished in 0.007 s
[INFO][2018-05-24 20:11:00,089][org.apache.spark.scheduler.DAGScheduler]Job 2 finished: foreachPartition at ReceiveKafkaData.scala:64, took 0.018639 s
[INFO][2018-05-24 20:11:00,089][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527163860000 ms.0 from job set of time 1527163860000 ms
[INFO][2018-05-24 20:11:00,090][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 3 from persistence list
[INFO][2018-05-24 20:11:00,090][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.089 s for time 1527163860000 ms (execution: 0.028 s)
[INFO][2018-05-24 20:11:00,090][org.apache.spark.storage.BlockManager]Removing RDD 3
[INFO][2018-05-24 20:11:00,090][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 2 from persistence list
[INFO][2018-05-24 20:11:00,090][org.apache.spark.storage.BlockManager]Removing RDD 2
[INFO][2018-05-24 20:11:00,091][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:11:00,091][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527163840000 ms
[INFO][2018-05-24 20:11:15,061][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527163870000 ms
[INFO][2018-05-24 20:11:15,061][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527163870000 ms.0 from job set of time 1527163870000 ms
[INFO][2018-05-24 20:11:15,074][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:64
[INFO][2018-05-24 20:11:15,075][org.apache.spark.scheduler.DAGScheduler]Got job 3 (foreachPartition at ReceiveKafkaData.scala:64) with 1 output partitions
[INFO][2018-05-24 20:11:15,075][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 3 (foreachPartition at ReceiveKafkaData.scala:64)
[INFO][2018-05-24 20:11:15,075][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:11:15,075][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:11:15,077][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 3 (MapPartitionsRDD[7] at map at ReceiveKafkaData.scala:62), which has no missing parents
[INFO][2018-05-24 20:11:15,080][org.apache.spark.storage.memory.MemoryStore]Block broadcast_3 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-24 20:11:15,089][org.apache.spark.storage.memory.MemoryStore]Block broadcast_3_piece0 stored as bytes in memory (estimated size 2008.0 B, free 912.3 MB)
[INFO][2018-05-24 20:11:15,090][org.apache.spark.storage.BlockManagerInfo]Added broadcast_3_piece0 in memory on 192.168.0.102:49458 (size: 2008.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:11:15,091][org.apache.spark.SparkContext]Created broadcast 3 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:11:15,093][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[7] at map at ReceiveKafkaData.scala:62) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:11:15,093][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 3.0 with 1 tasks
[INFO][2018-05-24 20:11:15,094][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 3.0 (TID 3, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:11:15,095][org.apache.spark.executor.Executor]Running task 0.0 in stage 3.0 (TID 3)
[INFO][2018-05-24 20:11:15,100][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12076 is the same as ending offset skipping seven 0
[INFO][2018-05-24 20:11:15,101][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:11:15,103][org.apache.spark.executor.Executor]Finished task 0.0 in stage 3.0 (TID 3). 708 bytes result sent to driver
[INFO][2018-05-24 20:11:15,106][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 3.0 (TID 3) in 12 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:11:15,106][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 3.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:11:15,107][org.apache.spark.scheduler.DAGScheduler]ResultStage 3 (foreachPartition at ReceiveKafkaData.scala:64) finished in 0.013 s
[INFO][2018-05-24 20:11:15,108][org.apache.spark.scheduler.DAGScheduler]Job 3 finished: foreachPartition at ReceiveKafkaData.scala:64, took 0.033555 s
[INFO][2018-05-24 20:11:15,108][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527163870000 ms.0 from job set of time 1527163870000 ms
[INFO][2018-05-24 20:11:15,108][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 5.108 s for time 1527163870000 ms (execution: 0.047 s)
[INFO][2018-05-24 20:11:15,108][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 5 from persistence list
[INFO][2018-05-24 20:11:15,109][org.apache.spark.storage.BlockManager]Removing RDD 5
[INFO][2018-05-24 20:11:15,109][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 4 from persistence list
[INFO][2018-05-24 20:11:15,109][org.apache.spark.storage.BlockManager]Removing RDD 4
[INFO][2018-05-24 20:11:15,110][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:11:15,110][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527163850000 ms
[INFO][2018-05-24 20:11:20,066][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527163880000 ms
[INFO][2018-05-24 20:11:20,066][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527163880000 ms.0 from job set of time 1527163880000 ms
[INFO][2018-05-24 20:11:20,074][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:64
[INFO][2018-05-24 20:11:20,076][org.apache.spark.scheduler.DAGScheduler]Got job 4 (foreachPartition at ReceiveKafkaData.scala:64) with 1 output partitions
[INFO][2018-05-24 20:11:20,076][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 4 (foreachPartition at ReceiveKafkaData.scala:64)
[INFO][2018-05-24 20:11:20,076][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:11:20,076][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:11:20,076][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 4 (MapPartitionsRDD[9] at map at ReceiveKafkaData.scala:62), which has no missing parents
[INFO][2018-05-24 20:11:20,078][org.apache.spark.storage.memory.MemoryStore]Block broadcast_4 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-24 20:11:20,082][org.apache.spark.storage.memory.MemoryStore]Block broadcast_4_piece0 stored as bytes in memory (estimated size 2008.0 B, free 912.3 MB)
[INFO][2018-05-24 20:11:20,083][org.apache.spark.storage.BlockManagerInfo]Added broadcast_4_piece0 in memory on 192.168.0.102:49458 (size: 2008.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:11:20,083][org.apache.spark.SparkContext]Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:11:20,084][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[9] at map at ReceiveKafkaData.scala:62) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:11:20,084][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 4.0 with 1 tasks
[INFO][2018-05-24 20:11:20,085][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 4.0 (TID 4, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:11:20,085][org.apache.spark.executor.Executor]Running task 0.0 in stage 4.0 (TID 4)
[INFO][2018-05-24 20:11:20,088][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12076 is the same as ending offset skipping seven 0
[INFO][2018-05-24 20:11:20,089][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:11:20,089][org.apache.spark.executor.Executor]Finished task 0.0 in stage 4.0 (TID 4). 708 bytes result sent to driver
[INFO][2018-05-24 20:11:20,091][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 4.0 (TID 4) in 6 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:11:20,091][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 4.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:11:20,092][org.apache.spark.scheduler.DAGScheduler]ResultStage 4 (foreachPartition at ReceiveKafkaData.scala:64) finished in 0.006 s
[INFO][2018-05-24 20:11:20,092][org.apache.spark.scheduler.DAGScheduler]Job 4 finished: foreachPartition at ReceiveKafkaData.scala:64, took 0.017755 s
[INFO][2018-05-24 20:11:20,092][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527163880000 ms.0 from job set of time 1527163880000 ms
[INFO][2018-05-24 20:11:20,093][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.092 s for time 1527163880000 ms (execution: 0.026 s)
[INFO][2018-05-24 20:11:20,093][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 7 from persistence list
[INFO][2018-05-24 20:11:20,093][org.apache.spark.storage.BlockManager]Removing RDD 7
[INFO][2018-05-24 20:11:20,094][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 6 from persistence list
[INFO][2018-05-24 20:11:20,094][org.apache.spark.storage.BlockManager]Removing RDD 6
[INFO][2018-05-24 20:11:20,094][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:11:20,095][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527163860000 ms
[INFO][2018-05-24 20:11:35,070][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527163890000 ms
[INFO][2018-05-24 20:11:35,071][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527163890000 ms.0 from job set of time 1527163890000 ms
[INFO][2018-05-24 20:11:35,079][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:64
[INFO][2018-05-24 20:11:35,080][org.apache.spark.scheduler.DAGScheduler]Got job 5 (foreachPartition at ReceiveKafkaData.scala:64) with 1 output partitions
[INFO][2018-05-24 20:11:35,080][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 5 (foreachPartition at ReceiveKafkaData.scala:64)
[INFO][2018-05-24 20:11:35,080][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:11:35,080][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:11:35,081][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 5 (MapPartitionsRDD[11] at map at ReceiveKafkaData.scala:62), which has no missing parents
[INFO][2018-05-24 20:11:35,083][org.apache.spark.storage.memory.MemoryStore]Block broadcast_5 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-24 20:11:35,087][org.apache.spark.storage.memory.MemoryStore]Block broadcast_5_piece0 stored as bytes in memory (estimated size 2008.0 B, free 912.3 MB)
[INFO][2018-05-24 20:11:35,087][org.apache.spark.storage.BlockManagerInfo]Added broadcast_5_piece0 in memory on 192.168.0.102:49458 (size: 2008.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:11:35,088][org.apache.spark.SparkContext]Created broadcast 5 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:11:35,088][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[11] at map at ReceiveKafkaData.scala:62) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:11:35,088][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 5.0 with 1 tasks
[INFO][2018-05-24 20:11:35,089][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 5.0 (TID 5, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:11:35,089][org.apache.spark.executor.Executor]Running task 0.0 in stage 5.0 (TID 5)
[INFO][2018-05-24 20:11:35,096][org.apache.spark.streaming.kafka.KafkaRDD]Computing topic seven, partition 0 offsets 12076 -> 12082
[INFO][2018-05-24 20:11:35,096][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 20:11:35,097][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 20:11:35,097][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 20:11:35,297][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_2_piece0 on 192.168.0.102:49458 in memory (size: 2008.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:11:35,300][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_3_piece0 on 192.168.0.102:49458 in memory (size: 2008.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:11:35,307][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_4_piece0 on 192.168.0.102:49458 in memory (size: 2008.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:11:35,312][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_1_piece0 on 192.168.0.102:49458 in memory (size: 2008.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:11:40,580][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:11:40,582][org.apache.spark.executor.Executor]Finished task 0.0 in stage 5.0 (TID 5). 751 bytes result sent to driver
[INFO][2018-05-24 20:11:40,583][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 5.0 (TID 5) in 5493 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:11:40,583][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 5.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:11:40,583][org.apache.spark.scheduler.DAGScheduler]ResultStage 5 (foreachPartition at ReceiveKafkaData.scala:64) finished in 5.494 s
[INFO][2018-05-24 20:11:40,584][org.apache.spark.scheduler.DAGScheduler]Job 5 finished: foreachPartition at ReceiveKafkaData.scala:64, took 5.504302 s
[INFO][2018-05-24 20:11:40,584][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527163890000 ms.0 from job set of time 1527163890000 ms
[INFO][2018-05-24 20:11:40,585][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 10.584 s for time 1527163890000 ms (execution: 5.513 s)
[INFO][2018-05-24 20:11:50,068][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527163900000 ms
[INFO][2018-05-24 20:11:50,068][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 9 from persistence list
[INFO][2018-05-24 20:11:50,069][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527163900000 ms.0 from job set of time 1527163900000 ms
[INFO][2018-05-24 20:11:50,069][org.apache.spark.storage.BlockManager]Removing RDD 9
[INFO][2018-05-24 20:11:50,069][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 8 from persistence list
[INFO][2018-05-24 20:11:50,070][org.apache.spark.storage.BlockManager]Removing RDD 8
[INFO][2018-05-24 20:11:50,070][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:11:50,070][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527163870000 ms
[INFO][2018-05-24 20:11:50,075][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:64
[INFO][2018-05-24 20:11:50,077][org.apache.spark.scheduler.DAGScheduler]Got job 6 (foreachPartition at ReceiveKafkaData.scala:64) with 1 output partitions
[INFO][2018-05-24 20:11:50,077][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 6 (foreachPartition at ReceiveKafkaData.scala:64)
[INFO][2018-05-24 20:11:50,077][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:11:50,077][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:11:50,077][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 6 (MapPartitionsRDD[13] at map at ReceiveKafkaData.scala:62), which has no missing parents
[INFO][2018-05-24 20:11:50,079][org.apache.spark.storage.memory.MemoryStore]Block broadcast_6 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-24 20:11:50,080][org.apache.spark.storage.memory.MemoryStore]Block broadcast_6_piece0 stored as bytes in memory (estimated size 2004.0 B, free 912.3 MB)
[INFO][2018-05-24 20:11:50,081][org.apache.spark.storage.BlockManagerInfo]Added broadcast_6_piece0 in memory on 192.168.0.102:49458 (size: 2004.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:11:50,082][org.apache.spark.SparkContext]Created broadcast 6 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:11:50,082][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[13] at map at ReceiveKafkaData.scala:62) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:11:50,082][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 6.0 with 1 tasks
[INFO][2018-05-24 20:11:50,083][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 6.0 (TID 6, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:11:50,084][org.apache.spark.executor.Executor]Running task 0.0 in stage 6.0 (TID 6)
[INFO][2018-05-24 20:11:50,086][org.apache.spark.streaming.kafka.KafkaRDD]Computing topic seven, partition 0 offsets 12082 -> 12088
[INFO][2018-05-24 20:11:50,086][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 20:11:50,086][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 20:11:50,086][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 20:11:50,123][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527163910000 ms
[INFO][2018-05-24 20:11:50,165][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:11:50,166][org.apache.spark.executor.Executor]Finished task 0.0 in stage 6.0 (TID 6). 708 bytes result sent to driver
[INFO][2018-05-24 20:11:50,169][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 6.0 (TID 6) in 86 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:11:50,170][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 6.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:11:50,171][org.apache.spark.scheduler.DAGScheduler]ResultStage 6 (foreachPartition at ReceiveKafkaData.scala:64) finished in 0.088 s
[INFO][2018-05-24 20:11:50,172][org.apache.spark.scheduler.DAGScheduler]Job 6 finished: foreachPartition at ReceiveKafkaData.scala:64, took 0.096566 s
[INFO][2018-05-24 20:11:50,173][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527163900000 ms.0 from job set of time 1527163900000 ms
[INFO][2018-05-24 20:11:50,173][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 10.173 s for time 1527163900000 ms (execution: 0.104 s)
[INFO][2018-05-24 20:11:50,173][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527163910000 ms.0 from job set of time 1527163910000 ms
[INFO][2018-05-24 20:11:50,174][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 11 from persistence list
[INFO][2018-05-24 20:11:50,177][org.apache.spark.storage.BlockManager]Removing RDD 11
[INFO][2018-05-24 20:11:50,177][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 10 from persistence list
[INFO][2018-05-24 20:11:50,179][org.apache.spark.storage.BlockManager]Removing RDD 10
[INFO][2018-05-24 20:11:50,179][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:11:50,179][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527163880000 ms
[INFO][2018-05-24 20:11:50,181][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:64
[INFO][2018-05-24 20:11:50,182][org.apache.spark.scheduler.DAGScheduler]Got job 7 (foreachPartition at ReceiveKafkaData.scala:64) with 1 output partitions
[INFO][2018-05-24 20:11:50,182][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 7 (foreachPartition at ReceiveKafkaData.scala:64)
[INFO][2018-05-24 20:11:50,182][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:11:50,182][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:11:50,182][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 7 (MapPartitionsRDD[15] at map at ReceiveKafkaData.scala:62), which has no missing parents
[INFO][2018-05-24 20:11:50,185][org.apache.spark.storage.memory.MemoryStore]Block broadcast_7 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-24 20:11:50,186][org.apache.spark.storage.memory.MemoryStore]Block broadcast_7_piece0 stored as bytes in memory (estimated size 2005.0 B, free 912.3 MB)
[INFO][2018-05-24 20:11:50,186][org.apache.spark.storage.BlockManagerInfo]Added broadcast_7_piece0 in memory on 192.168.0.102:49458 (size: 2005.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:11:50,187][org.apache.spark.SparkContext]Created broadcast 7 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:11:50,188][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[15] at map at ReceiveKafkaData.scala:62) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:11:50,188][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 7.0 with 1 tasks
[INFO][2018-05-24 20:11:50,189][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 7.0 (TID 7, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:11:50,190][org.apache.spark.executor.Executor]Running task 0.0 in stage 7.0 (TID 7)
[INFO][2018-05-24 20:11:50,194][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12088 is the same as ending offset skipping seven 0
[INFO][2018-05-24 20:11:50,194][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:11:50,195][org.apache.spark.executor.Executor]Finished task 0.0 in stage 7.0 (TID 7). 751 bytes result sent to driver
[INFO][2018-05-24 20:11:50,196][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 7.0 (TID 7) in 7 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:11:50,196][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 7.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:11:50,198][org.apache.spark.scheduler.DAGScheduler]ResultStage 7 (foreachPartition at ReceiveKafkaData.scala:64) finished in 0.008 s
[INFO][2018-05-24 20:11:50,198][org.apache.spark.scheduler.DAGScheduler]Job 7 finished: foreachPartition at ReceiveKafkaData.scala:64, took 0.017265 s
[INFO][2018-05-24 20:11:50,199][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527163910000 ms.0 from job set of time 1527163910000 ms
[INFO][2018-05-24 20:11:50,199][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 13 from persistence list
[INFO][2018-05-24 20:11:50,200][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.199 s for time 1527163910000 ms (execution: 0.026 s)
[INFO][2018-05-24 20:11:50,200][org.apache.spark.storage.BlockManager]Removing RDD 13
[INFO][2018-05-24 20:11:50,200][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 12 from persistence list
[INFO][2018-05-24 20:11:50,200][org.apache.spark.storage.BlockManager]Removing RDD 12
[INFO][2018-05-24 20:11:50,200][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:11:50,201][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527163890000 ms
[INFO][2018-05-24 20:12:00,065][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527163920000 ms
[INFO][2018-05-24 20:12:00,065][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527163920000 ms.0 from job set of time 1527163920000 ms
[INFO][2018-05-24 20:12:00,071][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:64
[INFO][2018-05-24 20:12:00,071][org.apache.spark.scheduler.DAGScheduler]Got job 8 (foreachPartition at ReceiveKafkaData.scala:64) with 1 output partitions
[INFO][2018-05-24 20:12:00,072][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 8 (foreachPartition at ReceiveKafkaData.scala:64)
[INFO][2018-05-24 20:12:00,072][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:12:00,072][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:12:00,072][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 8 (MapPartitionsRDD[17] at map at ReceiveKafkaData.scala:62), which has no missing parents
[INFO][2018-05-24 20:12:00,074][org.apache.spark.storage.memory.MemoryStore]Block broadcast_8 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-24 20:12:00,076][org.apache.spark.storage.memory.MemoryStore]Block broadcast_8_piece0 stored as bytes in memory (estimated size 2005.0 B, free 912.3 MB)
[INFO][2018-05-24 20:12:00,077][org.apache.spark.storage.BlockManagerInfo]Added broadcast_8_piece0 in memory on 192.168.0.102:49458 (size: 2005.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:12:00,077][org.apache.spark.SparkContext]Created broadcast 8 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:12:00,078][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[17] at map at ReceiveKafkaData.scala:62) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:12:00,078][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 8.0 with 1 tasks
[INFO][2018-05-24 20:12:00,079][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 8.0 (TID 8, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:12:00,079][org.apache.spark.executor.Executor]Running task 0.0 in stage 8.0 (TID 8)
[INFO][2018-05-24 20:12:00,081][org.apache.spark.streaming.kafka.KafkaRDD]Computing topic seven, partition 0 offsets 12088 -> 12092
[INFO][2018-05-24 20:12:00,081][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 20:12:00,082][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 20:12:00,082][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 20:12:00,150][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:12:00,151][org.apache.spark.executor.Executor]Finished task 0.0 in stage 8.0 (TID 8). 708 bytes result sent to driver
[INFO][2018-05-24 20:12:00,152][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 8.0 (TID 8) in 74 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:12:00,152][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 8.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:12:00,153][org.apache.spark.scheduler.DAGScheduler]ResultStage 8 (foreachPartition at ReceiveKafkaData.scala:64) finished in 0.074 s
[INFO][2018-05-24 20:12:00,153][org.apache.spark.scheduler.DAGScheduler]Job 8 finished: foreachPartition at ReceiveKafkaData.scala:64, took 0.082526 s
[INFO][2018-05-24 20:12:00,154][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527163920000 ms.0 from job set of time 1527163920000 ms
[INFO][2018-05-24 20:12:00,154][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 15 from persistence list
[INFO][2018-05-24 20:12:00,154][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.154 s for time 1527163920000 ms (execution: 0.089 s)
[INFO][2018-05-24 20:12:00,155][org.apache.spark.storage.BlockManager]Removing RDD 15
[INFO][2018-05-24 20:12:00,155][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 14 from persistence list
[INFO][2018-05-24 20:12:00,155][org.apache.spark.storage.BlockManager]Removing RDD 14
[INFO][2018-05-24 20:12:00,155][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:12:00,155][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527163900000 ms
[INFO][2018-05-24 20:12:10,059][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527163930000 ms
[INFO][2018-05-24 20:12:10,059][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527163930000 ms.0 from job set of time 1527163930000 ms
[INFO][2018-05-24 20:12:10,065][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:64
[INFO][2018-05-24 20:12:10,066][org.apache.spark.scheduler.DAGScheduler]Got job 9 (foreachPartition at ReceiveKafkaData.scala:64) with 1 output partitions
[INFO][2018-05-24 20:12:10,066][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 9 (foreachPartition at ReceiveKafkaData.scala:64)
[INFO][2018-05-24 20:12:10,066][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:12:10,067][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:12:10,067][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 9 (MapPartitionsRDD[19] at map at ReceiveKafkaData.scala:62), which has no missing parents
[INFO][2018-05-24 20:12:10,069][org.apache.spark.storage.memory.MemoryStore]Block broadcast_9 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-24 20:12:10,070][org.apache.spark.storage.memory.MemoryStore]Block broadcast_9_piece0 stored as bytes in memory (estimated size 2008.0 B, free 912.3 MB)
[INFO][2018-05-24 20:12:10,071][org.apache.spark.storage.BlockManagerInfo]Added broadcast_9_piece0 in memory on 192.168.0.102:49458 (size: 2008.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:12:10,071][org.apache.spark.SparkContext]Created broadcast 9 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:12:10,072][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[19] at map at ReceiveKafkaData.scala:62) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:12:10,072][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 9.0 with 1 tasks
[INFO][2018-05-24 20:12:10,072][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 9.0 (TID 9, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:12:10,073][org.apache.spark.executor.Executor]Running task 0.0 in stage 9.0 (TID 9)
[INFO][2018-05-24 20:12:10,074][org.apache.spark.streaming.kafka.KafkaRDD]Computing topic seven, partition 0 offsets 12092 -> 12096
[INFO][2018-05-24 20:12:10,075][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 20:12:10,075][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 20:12:10,075][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 20:12:10,157][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:12:10,158][org.apache.spark.executor.Executor]Finished task 0.0 in stage 9.0 (TID 9). 665 bytes result sent to driver
[INFO][2018-05-24 20:12:10,159][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 9.0 (TID 9) in 87 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:12:10,159][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 9.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:12:10,160][org.apache.spark.scheduler.DAGScheduler]ResultStage 9 (foreachPartition at ReceiveKafkaData.scala:64) finished in 0.088 s
[INFO][2018-05-24 20:12:10,160][org.apache.spark.scheduler.DAGScheduler]Job 9 finished: foreachPartition at ReceiveKafkaData.scala:64, took 0.094654 s
[INFO][2018-05-24 20:12:10,161][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527163930000 ms.0 from job set of time 1527163930000 ms
[INFO][2018-05-24 20:12:10,161][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.161 s for time 1527163930000 ms (execution: 0.102 s)
[INFO][2018-05-24 20:12:10,161][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 17 from persistence list
[INFO][2018-05-24 20:12:10,162][org.apache.spark.storage.BlockManager]Removing RDD 17
[INFO][2018-05-24 20:12:10,162][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 16 from persistence list
[INFO][2018-05-24 20:12:10,162][org.apache.spark.storage.BlockManager]Removing RDD 16
[INFO][2018-05-24 20:12:10,162][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:12:10,162][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527163910000 ms
[INFO][2018-05-24 20:12:30,074][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527163940000 ms
[INFO][2018-05-24 20:12:30,080][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527163940000 ms.0 from job set of time 1527163940000 ms
[INFO][2018-05-24 20:12:30,100][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:64
[INFO][2018-05-24 20:12:30,101][org.apache.spark.scheduler.DAGScheduler]Got job 10 (foreachPartition at ReceiveKafkaData.scala:64) with 1 output partitions
[INFO][2018-05-24 20:12:30,101][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 10 (foreachPartition at ReceiveKafkaData.scala:64)
[INFO][2018-05-24 20:12:30,101][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:12:30,101][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:12:30,102][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 10 (MapPartitionsRDD[21] at map at ReceiveKafkaData.scala:62), which has no missing parents
[INFO][2018-05-24 20:12:30,103][org.apache.spark.storage.memory.MemoryStore]Block broadcast_10 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-24 20:12:30,105][org.apache.spark.storage.memory.MemoryStore]Block broadcast_10_piece0 stored as bytes in memory (estimated size 2008.0 B, free 912.3 MB)
[INFO][2018-05-24 20:12:30,106][org.apache.spark.storage.BlockManagerInfo]Added broadcast_10_piece0 in memory on 192.168.0.102:49458 (size: 2008.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:12:30,106][org.apache.spark.SparkContext]Created broadcast 10 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:12:30,107][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[21] at map at ReceiveKafkaData.scala:62) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:12:30,107][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 10.0 with 1 tasks
[INFO][2018-05-24 20:12:30,108][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 10.0 (TID 10, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:12:30,108][org.apache.spark.executor.Executor]Running task 0.0 in stage 10.0 (TID 10)
[INFO][2018-05-24 20:12:30,110][org.apache.spark.streaming.kafka.KafkaRDD]Computing topic seven, partition 0 offsets 12096 -> 12104
[INFO][2018-05-24 20:12:30,110][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 20:12:30,110][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 20:12:30,110][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 20:12:32,383][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527163950000 ms
[INFO][2018-05-24 20:12:35,196][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:12:35,198][org.apache.spark.executor.Executor]Finished task 0.0 in stage 10.0 (TID 10). 708 bytes result sent to driver
[INFO][2018-05-24 20:12:35,198][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 10.0 (TID 10) in 5090 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:12:35,198][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 10.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:12:35,199][org.apache.spark.scheduler.DAGScheduler]ResultStage 10 (foreachPartition at ReceiveKafkaData.scala:64) finished in 5.092 s
[INFO][2018-05-24 20:12:35,200][org.apache.spark.scheduler.DAGScheduler]Job 10 finished: foreachPartition at ReceiveKafkaData.scala:64, took 5.098949 s
[INFO][2018-05-24 20:12:35,201][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527163940000 ms.0 from job set of time 1527163940000 ms
[INFO][2018-05-24 20:12:35,201][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 15.200 s for time 1527163940000 ms (execution: 5.123 s)
[INFO][2018-05-24 20:12:35,201][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 19 from persistence list
[INFO][2018-05-24 20:12:35,201][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527163950000 ms.0 from job set of time 1527163950000 ms
[INFO][2018-05-24 20:12:35,202][org.apache.spark.storage.BlockManager]Removing RDD 19
[INFO][2018-05-24 20:12:35,202][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 18 from persistence list
[INFO][2018-05-24 20:12:35,203][org.apache.spark.storage.BlockManager]Removing RDD 18
[INFO][2018-05-24 20:12:35,203][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:12:35,203][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527163920000 ms
[INFO][2018-05-24 20:12:35,207][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:64
[INFO][2018-05-24 20:12:35,208][org.apache.spark.scheduler.DAGScheduler]Got job 11 (foreachPartition at ReceiveKafkaData.scala:64) with 1 output partitions
[INFO][2018-05-24 20:12:35,208][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 11 (foreachPartition at ReceiveKafkaData.scala:64)
[INFO][2018-05-24 20:12:35,208][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:12:35,208][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:12:35,209][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 11 (MapPartitionsRDD[23] at map at ReceiveKafkaData.scala:62), which has no missing parents
[INFO][2018-05-24 20:12:35,211][org.apache.spark.storage.memory.MemoryStore]Block broadcast_11 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-24 20:12:35,212][org.apache.spark.storage.memory.MemoryStore]Block broadcast_11_piece0 stored as bytes in memory (estimated size 2008.0 B, free 912.3 MB)
[INFO][2018-05-24 20:12:35,213][org.apache.spark.storage.BlockManagerInfo]Added broadcast_11_piece0 in memory on 192.168.0.102:49458 (size: 2008.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:12:35,213][org.apache.spark.SparkContext]Created broadcast 11 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:12:35,214][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[23] at map at ReceiveKafkaData.scala:62) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:12:35,214][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 11.0 with 1 tasks
[INFO][2018-05-24 20:12:35,214][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 11.0 (TID 11, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:12:35,215][org.apache.spark.executor.Executor]Running task 0.0 in stage 11.0 (TID 11)
[INFO][2018-05-24 20:12:35,216][org.apache.spark.streaming.kafka.KafkaRDD]Computing topic seven, partition 0 offsets 12104 -> 12105
[INFO][2018-05-24 20:12:35,216][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 20:12:35,217][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 20:12:35,217][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 20:12:35,270][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:12:35,271][org.apache.spark.executor.Executor]Finished task 0.0 in stage 11.0 (TID 11). 708 bytes result sent to driver
[INFO][2018-05-24 20:12:35,271][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 11.0 (TID 11) in 57 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:12:35,272][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 11.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:12:35,274][org.apache.spark.scheduler.DAGScheduler]ResultStage 11 (foreachPartition at ReceiveKafkaData.scala:64) finished in 0.059 s
[INFO][2018-05-24 20:12:35,276][org.apache.spark.scheduler.DAGScheduler]Job 11 finished: foreachPartition at ReceiveKafkaData.scala:64, took 0.068504 s
[INFO][2018-05-24 20:12:35,277][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527163950000 ms.0 from job set of time 1527163950000 ms
[INFO][2018-05-24 20:12:35,278][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 5.277 s for time 1527163950000 ms (execution: 0.076 s)
[INFO][2018-05-24 20:12:35,278][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 21 from persistence list
[INFO][2018-05-24 20:12:35,279][org.apache.spark.storage.BlockManager]Removing RDD 21
[INFO][2018-05-24 20:12:35,280][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 20 from persistence list
[INFO][2018-05-24 20:12:35,280][org.apache.spark.storage.BlockManager]Removing RDD 20
[INFO][2018-05-24 20:12:35,283][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:12:35,283][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527163930000 ms
[INFO][2018-05-24 20:12:40,062][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527163960000 ms
[INFO][2018-05-24 20:12:40,063][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527163960000 ms.0 from job set of time 1527163960000 ms
[INFO][2018-05-24 20:12:40,068][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:64
[INFO][2018-05-24 20:12:40,069][org.apache.spark.scheduler.DAGScheduler]Got job 12 (foreachPartition at ReceiveKafkaData.scala:64) with 1 output partitions
[INFO][2018-05-24 20:12:40,069][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 12 (foreachPartition at ReceiveKafkaData.scala:64)
[INFO][2018-05-24 20:12:40,069][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:12:40,069][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:12:40,069][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 12 (MapPartitionsRDD[25] at map at ReceiveKafkaData.scala:62), which has no missing parents
[INFO][2018-05-24 20:12:40,071][org.apache.spark.storage.memory.MemoryStore]Block broadcast_12 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-24 20:12:40,073][org.apache.spark.storage.memory.MemoryStore]Block broadcast_12_piece0 stored as bytes in memory (estimated size 2008.0 B, free 912.3 MB)
[INFO][2018-05-24 20:12:40,073][org.apache.spark.storage.BlockManagerInfo]Added broadcast_12_piece0 in memory on 192.168.0.102:49458 (size: 2008.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:12:40,074][org.apache.spark.SparkContext]Created broadcast 12 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:12:40,074][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[25] at map at ReceiveKafkaData.scala:62) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:12:40,074][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 12.0 with 1 tasks
[INFO][2018-05-24 20:12:40,075][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 12.0 (TID 12, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:12:40,075][org.apache.spark.executor.Executor]Running task 0.0 in stage 12.0 (TID 12)
[INFO][2018-05-24 20:12:40,077][org.apache.spark.streaming.kafka.KafkaRDD]Computing topic seven, partition 0 offsets 12105 -> 12108
[INFO][2018-05-24 20:12:40,077][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 20:12:40,078][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 20:12:40,078][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 20:12:40,150][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:12:40,151][org.apache.spark.executor.Executor]Finished task 0.0 in stage 12.0 (TID 12). 708 bytes result sent to driver
[INFO][2018-05-24 20:12:40,151][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 12.0 (TID 12) in 76 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:12:40,151][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 12.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:12:40,152][org.apache.spark.scheduler.DAGScheduler]ResultStage 12 (foreachPartition at ReceiveKafkaData.scala:64) finished in 0.077 s
[INFO][2018-05-24 20:12:40,152][org.apache.spark.scheduler.DAGScheduler]Job 12 finished: foreachPartition at ReceiveKafkaData.scala:64, took 0.084080 s
[INFO][2018-05-24 20:12:40,153][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527163960000 ms.0 from job set of time 1527163960000 ms
[INFO][2018-05-24 20:12:40,153][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 23 from persistence list
[INFO][2018-05-24 20:12:40,153][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.153 s for time 1527163960000 ms (execution: 0.090 s)
[INFO][2018-05-24 20:12:40,153][org.apache.spark.storage.BlockManager]Removing RDD 23
[INFO][2018-05-24 20:12:40,153][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 22 from persistence list
[INFO][2018-05-24 20:12:40,154][org.apache.spark.storage.BlockManager]Removing RDD 22
[INFO][2018-05-24 20:12:40,154][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:12:40,154][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527163940000 ms
[INFO][2018-05-24 20:12:50,076][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527163970000 ms
[INFO][2018-05-24 20:12:50,077][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527163970000 ms.0 from job set of time 1527163970000 ms
[INFO][2018-05-24 20:12:50,084][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:64
[INFO][2018-05-24 20:12:50,084][org.apache.spark.scheduler.DAGScheduler]Got job 13 (foreachPartition at ReceiveKafkaData.scala:64) with 1 output partitions
[INFO][2018-05-24 20:12:50,084][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 13 (foreachPartition at ReceiveKafkaData.scala:64)
[INFO][2018-05-24 20:12:50,084][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:12:50,085][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:12:50,085][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 13 (MapPartitionsRDD[27] at map at ReceiveKafkaData.scala:62), which has no missing parents
[INFO][2018-05-24 20:12:50,087][org.apache.spark.storage.memory.MemoryStore]Block broadcast_13 stored as values in memory (estimated size 3.4 KB, free 912.2 MB)
[INFO][2018-05-24 20:12:50,088][org.apache.spark.storage.memory.MemoryStore]Block broadcast_13_piece0 stored as bytes in memory (estimated size 2008.0 B, free 912.2 MB)
[INFO][2018-05-24 20:12:50,089][org.apache.spark.storage.BlockManagerInfo]Added broadcast_13_piece0 in memory on 192.168.0.102:49458 (size: 2008.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:12:50,089][org.apache.spark.SparkContext]Created broadcast 13 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:12:50,090][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[27] at map at ReceiveKafkaData.scala:62) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:12:50,090][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 13.0 with 1 tasks
[INFO][2018-05-24 20:12:50,090][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 13.0 (TID 13, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:12:50,092][org.apache.spark.executor.Executor]Running task 0.0 in stage 13.0 (TID 13)
[INFO][2018-05-24 20:12:50,094][org.apache.spark.streaming.kafka.KafkaRDD]Computing topic seven, partition 0 offsets 12108 -> 12112
[INFO][2018-05-24 20:12:50,094][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 20:12:50,094][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 20:12:50,094][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 20:12:50,153][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:12:50,154][org.apache.spark.executor.Executor]Finished task 0.0 in stage 13.0 (TID 13). 708 bytes result sent to driver
[INFO][2018-05-24 20:12:50,154][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 13.0 (TID 13) in 64 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:12:50,155][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 13.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:12:50,156][org.apache.spark.scheduler.DAGScheduler]ResultStage 13 (foreachPartition at ReceiveKafkaData.scala:64) finished in 0.063 s
[INFO][2018-05-24 20:12:50,157][org.apache.spark.scheduler.DAGScheduler]Job 13 finished: foreachPartition at ReceiveKafkaData.scala:64, took 0.073166 s
[INFO][2018-05-24 20:12:50,157][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527163970000 ms.0 from job set of time 1527163970000 ms
[INFO][2018-05-24 20:12:50,157][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 25 from persistence list
[INFO][2018-05-24 20:12:50,157][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.157 s for time 1527163970000 ms (execution: 0.080 s)
[INFO][2018-05-24 20:12:50,158][org.apache.spark.storage.BlockManager]Removing RDD 25
[INFO][2018-05-24 20:12:50,158][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 24 from persistence list
[INFO][2018-05-24 20:12:50,158][org.apache.spark.storage.BlockManager]Removing RDD 24
[INFO][2018-05-24 20:12:50,159][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:12:50,159][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527163950000 ms
[INFO][2018-05-24 20:13:10,078][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527163980000 ms
[INFO][2018-05-24 20:13:10,078][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527163980000 ms.0 from job set of time 1527163980000 ms
[INFO][2018-05-24 20:13:10,086][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:64
[INFO][2018-05-24 20:13:10,087][org.apache.spark.scheduler.DAGScheduler]Got job 14 (foreachPartition at ReceiveKafkaData.scala:64) with 1 output partitions
[INFO][2018-05-24 20:13:10,087][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 14 (foreachPartition at ReceiveKafkaData.scala:64)
[INFO][2018-05-24 20:13:10,087][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:13:10,087][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:13:10,087][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 14 (MapPartitionsRDD[29] at map at ReceiveKafkaData.scala:62), which has no missing parents
[INFO][2018-05-24 20:13:10,089][org.apache.spark.storage.memory.MemoryStore]Block broadcast_14 stored as values in memory (estimated size 3.4 KB, free 912.2 MB)
[INFO][2018-05-24 20:13:10,090][org.apache.spark.storage.memory.MemoryStore]Block broadcast_14_piece0 stored as bytes in memory (estimated size 2008.0 B, free 912.2 MB)
[INFO][2018-05-24 20:13:10,091][org.apache.spark.storage.BlockManagerInfo]Added broadcast_14_piece0 in memory on 192.168.0.102:49458 (size: 2008.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:13:10,092][org.apache.spark.SparkContext]Created broadcast 14 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:13:10,093][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[29] at map at ReceiveKafkaData.scala:62) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:13:10,093][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 14.0 with 1 tasks
[INFO][2018-05-24 20:13:10,101][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 14.0 (TID 14, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:13:10,102][org.apache.spark.executor.Executor]Running task 0.0 in stage 14.0 (TID 14)
[INFO][2018-05-24 20:13:10,104][org.apache.spark.streaming.kafka.KafkaRDD]Computing topic seven, partition 0 offsets 12112 -> 12120
[INFO][2018-05-24 20:13:10,105][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 20:13:10,105][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 20:13:10,105][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 20:13:15,144][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:13:15,148][org.apache.spark.executor.Executor]Finished task 0.0 in stage 14.0 (TID 14). 708 bytes result sent to driver
[INFO][2018-05-24 20:13:15,152][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 14.0 (TID 14) in 5054 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:13:15,153][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 14.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:13:15,153][org.apache.spark.scheduler.DAGScheduler]ResultStage 14 (foreachPartition at ReceiveKafkaData.scala:64) finished in 5.059 s
[INFO][2018-05-24 20:13:15,158][org.apache.spark.scheduler.DAGScheduler]Job 14 finished: foreachPartition at ReceiveKafkaData.scala:64, took 5.071614 s
[INFO][2018-05-24 20:13:15,165][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527163980000 ms.0 from job set of time 1527163980000 ms
[INFO][2018-05-24 20:13:15,166][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 15.165 s for time 1527163980000 ms (execution: 5.087 s)
[INFO][2018-05-24 20:13:15,203][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527163990000 ms
[INFO][2018-05-24 20:13:15,203][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 27 from persistence list
[INFO][2018-05-24 20:13:15,203][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527163990000 ms.0 from job set of time 1527163990000 ms
[INFO][2018-05-24 20:13:15,204][org.apache.spark.storage.BlockManager]Removing RDD 27
[INFO][2018-05-24 20:13:15,204][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 26 from persistence list
[INFO][2018-05-24 20:13:15,205][org.apache.spark.storage.BlockManager]Removing RDD 26
[INFO][2018-05-24 20:13:15,205][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:13:15,205][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527163960000 ms
[INFO][2018-05-24 20:13:15,209][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:64
[INFO][2018-05-24 20:13:15,209][org.apache.spark.scheduler.DAGScheduler]Got job 15 (foreachPartition at ReceiveKafkaData.scala:64) with 1 output partitions
[INFO][2018-05-24 20:13:15,209][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 15 (foreachPartition at ReceiveKafkaData.scala:64)
[INFO][2018-05-24 20:13:15,209][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:13:15,209][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:13:15,210][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 15 (MapPartitionsRDD[31] at map at ReceiveKafkaData.scala:62), which has no missing parents
[INFO][2018-05-24 20:13:15,211][org.apache.spark.storage.memory.MemoryStore]Block broadcast_15 stored as values in memory (estimated size 3.4 KB, free 912.2 MB)
[INFO][2018-05-24 20:13:15,212][org.apache.spark.storage.memory.MemoryStore]Block broadcast_15_piece0 stored as bytes in memory (estimated size 2008.0 B, free 912.2 MB)
[INFO][2018-05-24 20:13:15,212][org.apache.spark.storage.BlockManagerInfo]Added broadcast_15_piece0 in memory on 192.168.0.102:49458 (size: 2008.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:13:15,213][org.apache.spark.SparkContext]Created broadcast 15 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:13:15,214][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 15 (MapPartitionsRDD[31] at map at ReceiveKafkaData.scala:62) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:13:15,214][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 15.0 with 1 tasks
[INFO][2018-05-24 20:13:15,215][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 15.0 (TID 15, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:13:15,216][org.apache.spark.executor.Executor]Running task 0.0 in stage 15.0 (TID 15)
[INFO][2018-05-24 20:13:15,224][org.apache.spark.streaming.kafka.KafkaRDD]Computing topic seven, partition 0 offsets 12120 -> 12122
[INFO][2018-05-24 20:13:15,224][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 20:13:15,224][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 20:13:15,224][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 20:13:15,287][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:13:15,288][org.apache.spark.executor.Executor]Finished task 0.0 in stage 15.0 (TID 15). 708 bytes result sent to driver
[INFO][2018-05-24 20:13:15,289][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 15.0 (TID 15) in 74 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:13:15,289][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 15.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:13:15,290][org.apache.spark.scheduler.DAGScheduler]ResultStage 15 (foreachPartition at ReceiveKafkaData.scala:64) finished in 0.075 s
[INFO][2018-05-24 20:13:15,290][org.apache.spark.scheduler.DAGScheduler]Job 15 finished: foreachPartition at ReceiveKafkaData.scala:64, took 0.081573 s
[INFO][2018-05-24 20:13:15,291][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527163990000 ms.0 from job set of time 1527163990000 ms
[INFO][2018-05-24 20:13:15,291][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 5.291 s for time 1527163990000 ms (execution: 0.088 s)
[INFO][2018-05-24 20:13:15,291][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 29 from persistence list
[INFO][2018-05-24 20:13:15,292][org.apache.spark.storage.BlockManager]Removing RDD 29
[INFO][2018-05-24 20:13:15,292][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 28 from persistence list
[INFO][2018-05-24 20:13:15,293][org.apache.spark.storage.BlockManager]Removing RDD 28
[INFO][2018-05-24 20:13:15,293][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:13:15,293][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527163970000 ms
[INFO][2018-05-24 20:13:19,153][org.apache.spark.util.ShutdownHookManager]Shutdown hook called
[INFO][2018-05-24 20:13:19,154][org.apache.spark.util.ShutdownHookManager]Deleting directory /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/spark-0eb2ed87-71f8-40b0-9e40-60e066627c4f
[INFO][2018-05-24 20:13:19,184][org.apache.spark.streaming.StreamingContext]Invoking stop(stopGracefully=false) from shutdown hook
[INFO][2018-05-24 20:13:19,186][org.apache.spark.streaming.scheduler.ReceiverTracker]ReceiverTracker stopped
[INFO][2018-05-24 20:13:19,189][org.apache.spark.streaming.scheduler.JobGenerator]Stopping JobGenerator immediately
[INFO][2018-05-24 20:13:19,190][org.apache.spark.streaming.util.RecurringTimer]Stopped timer for JobGenerator after time 1527163990000
[INFO][2018-05-24 20:13:19,191][org.apache.spark.streaming.scheduler.JobGenerator]Stopped JobGenerator
[INFO][2018-05-24 20:13:19,195][org.apache.spark.streaming.scheduler.JobScheduler]Stopped JobScheduler
[INFO][2018-05-24 20:13:19,205][org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@1ab6718{/streaming,null,UNAVAILABLE,@Spark}
[INFO][2018-05-24 20:13:19,206][org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@78b41097{/streaming/batch,null,UNAVAILABLE,@Spark}
[INFO][2018-05-24 20:13:19,207][org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@7cf283e1{/static/streaming,null,UNAVAILABLE,@Spark}
[INFO][2018-05-24 20:13:19,208][org.apache.spark.streaming.StreamingContext]StreamingContext stopped successfully
[INFO][2018-05-24 20:13:19,208][org.apache.spark.SparkContext]Invoking stop() from shutdown hook
[INFO][2018-05-24 20:13:19,220][org.spark_project.jetty.server.AbstractConnector]Stopped Spark@24faea88{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-05-24 20:13:19,222][org.apache.spark.ui.SparkUI]Stopped Spark web UI at http://192.168.0.102:4040
[INFO][2018-05-24 20:13:19,230][org.apache.spark.MapOutputTrackerMasterEndpoint]MapOutputTrackerMasterEndpoint stopped!
[INFO][2018-05-24 20:13:19,250][org.apache.spark.storage.memory.MemoryStore]MemoryStore cleared
[INFO][2018-05-24 20:13:19,250][org.apache.spark.storage.BlockManager]BlockManager stopped
[INFO][2018-05-24 20:13:19,251][org.apache.spark.storage.BlockManagerMaster]BlockManagerMaster stopped
[INFO][2018-05-24 20:13:19,253][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint]OutputCommitCoordinator stopped!
[INFO][2018-05-24 20:13:19,254][org.apache.spark.SparkContext]Successfully stopped SparkContext
[INFO][2018-05-24 20:13:19,255][org.apache.spark.util.ShutdownHookManager]Shutdown hook called
[INFO][2018-05-24 20:13:19,255][org.apache.spark.util.ShutdownHookManager]Deleting directory /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/spark-45cc9e91-7750-42f9-8a7e-77159e5673a1
[INFO][2018-05-24 20:22:58,037][org.apache.spark.SparkContext]Running Spark version 2.2.0
[INFO][2018-05-24 20:22:58,681][org.apache.spark.SparkContext]Submitted application: ReceiveKafkaData$
[INFO][2018-05-24 20:22:58,706][org.apache.spark.SecurityManager]Changing view acls to: seven
[INFO][2018-05-24 20:22:58,706][org.apache.spark.SecurityManager]Changing modify acls to: seven
[INFO][2018-05-24 20:22:58,709][org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO][2018-05-24 20:22:58,710][org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO][2018-05-24 20:22:58,711][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(seven); groups with view permissions: Set(); users  with modify permissions: Set(seven); groups with modify permissions: Set()
[INFO][2018-05-24 20:22:58,986][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 49676.
[INFO][2018-05-24 20:22:59,008][org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO][2018-05-24 20:22:59,022][org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO][2018-05-24 20:22:59,025][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2018-05-24 20:22:59,025][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
[INFO][2018-05-24 20:22:59,034][org.apache.spark.storage.DiskBlockManager]Created local directory at /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/blockmgr-1eb23118-d589-4d33-9478-e40c03e97963
[INFO][2018-05-24 20:22:59,047][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 912.3 MB
[INFO][2018-05-24 20:22:59,117][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO][2018-05-24 20:22:59,179][org.spark_project.jetty.util.log]Logging initialized @1989ms
[INFO][2018-05-24 20:22:59,236][org.spark_project.jetty.server.Server]jetty-9.3.z-SNAPSHOT
[INFO][2018-05-24 20:22:59,250][org.spark_project.jetty.server.Server]Started @2060ms
[INFO][2018-05-24 20:22:59,273][org.spark_project.jetty.server.AbstractConnector]Started ServerConnector@6efa953f{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-05-24 20:22:59,273][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4040.
[INFO][2018-05-24 20:22:59,301][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@c7a975a{/jobs,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:22:59,302][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@55795845{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:22:59,302][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@119f1f2a{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:22:59,304][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7fd4acee{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:22:59,304][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6175619b{/stages,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:22:59,305][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@756cf158{/stages/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:22:59,306][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@751d3241{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:22:59,307][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@30ea8c23{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:22:59,308][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4e76dac{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:22:59,308][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5f2f577{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:22:59,309][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5d465e4b{/storage,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:22:59,310][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@41a90fa8{/storage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:22:59,310][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@52500920{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:22:59,311][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@18a3962d{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:22:59,312][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2a65bb85{/environment,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:22:59,318][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4f936da8{/environment/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:22:59,321][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@452ba1db{/executors,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:22:59,322][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@76a36b71{/executors/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:22:59,322][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@f9d87b{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:22:59,323][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@26fb628{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:22:59,331][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@70dd7e15{/static,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:22:59,332][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@60d8c0dc{/,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:22:59,333][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6a62689d{/api,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:22:59,334][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1849db1a{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:22:59,335][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1ca25c47{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:22:59,341][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://192.168.0.102:4040
[INFO][2018-05-24 20:22:59,476][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
[INFO][2018-05-24 20:22:59,512][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 49677.
[INFO][2018-05-24 20:22:59,516][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 192.168.0.102:49677
[INFO][2018-05-24 20:22:59,522][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2018-05-24 20:22:59,524][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 192.168.0.102, 49677, None)
[INFO][2018-05-24 20:22:59,527][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 192.168.0.102:49677 with 912.3 MB RAM, BlockManagerId(driver, 192.168.0.102, 49677, None)
[INFO][2018-05-24 20:22:59,530][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 192.168.0.102, 49677, None)
[INFO][2018-05-24 20:22:59,531][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 192.168.0.102, 49677, None)
[INFO][2018-05-24 20:22:59,805][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@17f460bb{/metrics/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:22:59,917][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 20:22:59,920][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 20:22:59,920][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 20:23:10,433][org.apache.spark.streaming.kafka.DirectKafkaInputDStream]Slide time = 10000 ms
[INFO][2018-05-24 20:23:10,434][org.apache.spark.streaming.kafka.DirectKafkaInputDStream]Storage level = Serialized 1x Replicated
[INFO][2018-05-24 20:23:10,435][org.apache.spark.streaming.kafka.DirectKafkaInputDStream]Checkpoint interval = null
[INFO][2018-05-24 20:23:10,435][org.apache.spark.streaming.kafka.DirectKafkaInputDStream]Remember interval = 10000 ms
[INFO][2018-05-24 20:23:10,436][org.apache.spark.streaming.kafka.DirectKafkaInputDStream]Initialized and validated org.apache.spark.streaming.kafka.DirectKafkaInputDStream@36a8cfad
[INFO][2018-05-24 20:23:10,436][org.apache.spark.streaming.dstream.MappedDStream]Slide time = 10000 ms
[INFO][2018-05-24 20:23:10,436][org.apache.spark.streaming.dstream.MappedDStream]Storage level = Serialized 1x Replicated
[INFO][2018-05-24 20:23:10,436][org.apache.spark.streaming.dstream.MappedDStream]Checkpoint interval = null
[INFO][2018-05-24 20:23:10,436][org.apache.spark.streaming.dstream.MappedDStream]Remember interval = 10000 ms
[INFO][2018-05-24 20:23:10,436][org.apache.spark.streaming.dstream.MappedDStream]Initialized and validated org.apache.spark.streaming.dstream.MappedDStream@228e988d
[INFO][2018-05-24 20:23:10,436][org.apache.spark.streaming.dstream.ForEachDStream]Slide time = 10000 ms
[INFO][2018-05-24 20:23:10,436][org.apache.spark.streaming.dstream.ForEachDStream]Storage level = Serialized 1x Replicated
[INFO][2018-05-24 20:23:10,436][org.apache.spark.streaming.dstream.ForEachDStream]Checkpoint interval = null
[INFO][2018-05-24 20:23:10,437][org.apache.spark.streaming.dstream.ForEachDStream]Remember interval = 10000 ms
[INFO][2018-05-24 20:23:10,437][org.apache.spark.streaming.dstream.ForEachDStream]Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@3025b3ba
[INFO][2018-05-24 20:23:10,477][org.apache.spark.streaming.util.RecurringTimer]Started timer for JobGenerator at time 1527164600000
[INFO][2018-05-24 20:23:10,477][org.apache.spark.streaming.scheduler.JobGenerator]Started JobGenerator at 1527164600000 ms
[INFO][2018-05-24 20:23:10,478][org.apache.spark.streaming.scheduler.JobScheduler]Started JobScheduler
[INFO][2018-05-24 20:23:10,483][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@57a48985{/streaming,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:23:10,486][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3910fe11{/streaming/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:23:10,487][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3f672204{/streaming/batch,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:23:10,487][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2c2db130{/streaming/batch/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:23:10,488][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2d000e80{/static/streaming,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:23:10,489][org.apache.spark.streaming.StreamingContext]StreamingContext started
[INFO][2018-05-24 20:23:20,054][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 20:23:20,055][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 20:23:20,056][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 20:23:22,267][org.apache.spark.SparkContext]Running Spark version 2.2.0
[INFO][2018-05-24 20:23:23,112][org.apache.spark.SparkContext]Submitted application: SimulationKafkaSendOutData$
[INFO][2018-05-24 20:23:23,139][org.apache.spark.SecurityManager]Changing view acls to: seven
[INFO][2018-05-24 20:23:23,140][org.apache.spark.SecurityManager]Changing modify acls to: seven
[INFO][2018-05-24 20:23:23,140][org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO][2018-05-24 20:23:23,141][org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO][2018-05-24 20:23:23,141][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(seven); groups with view permissions: Set(); users  with modify permissions: Set(seven); groups with modify permissions: Set()
[INFO][2018-05-24 20:23:23,514][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 49685.
[INFO][2018-05-24 20:23:23,539][org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO][2018-05-24 20:23:23,559][org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO][2018-05-24 20:23:23,562][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2018-05-24 20:23:23,562][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
[INFO][2018-05-24 20:23:23,574][org.apache.spark.storage.DiskBlockManager]Created local directory at /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/blockmgr-400bcae9-fa25-4046-ac65-bc389d2a4d25
[INFO][2018-05-24 20:23:23,597][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 912.3 MB
[INFO][2018-05-24 20:23:23,705][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO][2018-05-24 20:23:23,818][org.spark_project.jetty.util.log]Logging initialized @2593ms
[INFO][2018-05-24 20:23:23,917][org.spark_project.jetty.server.Server]jetty-9.3.z-SNAPSHOT
[INFO][2018-05-24 20:23:23,939][org.spark_project.jetty.server.Server]Started @2714ms
[WARN][2018-05-24 20:23:23,959][org.apache.spark.util.Utils]Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[INFO][2018-05-24 20:23:23,967][org.spark_project.jetty.server.AbstractConnector]Started ServerConnector@60ca0db6{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}
[INFO][2018-05-24 20:23:23,967][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4041.
[INFO][2018-05-24 20:23:23,998][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@180e6ac4{/jobs,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:23:23,999][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@c7a975a{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:23:23,999][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@757d6814{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:23:24,003][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4c168660{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:23:24,004][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@fd0e5b6{/stages,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:23:24,004][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@36b0fcd5{/stages/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:23:24,005][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@475835b1{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:23:24,009][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@77192705{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:23:24,010][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7e809b79{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:23:24,011][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@625e134e{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:23:24,012][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@89c10b7{/storage,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:23:24,012][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4fe89c24{/storage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:23:24,013][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3d08f3f5{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:23:24,015][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1a1da881{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:23:24,016][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7fd4acee{/environment,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:23:24,019][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6175619b{/environment/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:23:24,020][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@756cf158{/executors,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:23:24,021][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@751d3241{/executors/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:23:24,021][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@64337702{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:23:24,022][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@30ea8c23{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:23:24,031][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4e76dac{/static,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:23:24,032][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@f9d87b{/,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:23:24,036][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@26fb628{/api,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:23:24,038][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5ac86ba5{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:23:24,040][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2c9399a4{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:23:24,051][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://192.168.0.102:4041
[INFO][2018-05-24 20:23:24,200][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
[INFO][2018-05-24 20:23:24,232][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 49686.
[INFO][2018-05-24 20:23:24,233][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 192.168.0.102:49686
[INFO][2018-05-24 20:23:24,269][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2018-05-24 20:23:24,271][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 192.168.0.102, 49686, None)
[INFO][2018-05-24 20:23:24,274][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 192.168.0.102:49686 with 912.3 MB RAM, BlockManagerId(driver, 192.168.0.102, 49686, None)
[INFO][2018-05-24 20:23:24,278][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 192.168.0.102, 49686, None)
[INFO][2018-05-24 20:23:24,278][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 192.168.0.102, 49686, None)
[INFO][2018-05-24 20:23:24,614][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@405325cf{/metrics/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:23:25,153][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527164600000 ms
[INFO][2018-05-24 20:23:25,156][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527164600000 ms.0 from job set of time 1527164600000 ms
[INFO][2018-05-24 20:23:25,190][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:65
[INFO][2018-05-24 20:23:25,207][org.apache.spark.scheduler.DAGScheduler]Got job 0 (foreachPartition at ReceiveKafkaData.scala:65) with 1 output partitions
[INFO][2018-05-24 20:23:25,207][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 0 (foreachPartition at ReceiveKafkaData.scala:65)
[INFO][2018-05-24 20:23:25,208][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:23:25,209][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:23:25,219][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 0 (MapPartitionsRDD[1] at map at ReceiveKafkaData.scala:63), which has no missing parents
[INFO][2018-05-24 20:23:25,323][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0 stored as values in memory (estimated size 228.1 KB, free 912.1 MB)
[INFO][2018-05-24 20:23:25,471][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 20:23:25,515][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0_piece0 stored as bytes in memory (estimated size 22.1 KB, free 912.1 MB)
[INFO][2018-05-24 20:23:25,518][org.apache.spark.storage.BlockManagerInfo]Added broadcast_0_piece0 in memory on 192.168.0.102:49686 (size: 22.1 KB, free: 912.3 MB)
[INFO][2018-05-24 20:23:25,519][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0_piece0 stored as bytes in memory (estimated size 1877.0 B, free 912.3 MB)
[INFO][2018-05-24 20:23:25,521][org.apache.spark.storage.BlockManagerInfo]Added broadcast_0_piece0 in memory on 192.168.0.102:49677 (size: 1877.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:23:25,524][org.apache.spark.SparkContext]Created broadcast 0 from textFile at SimulationKafkaSendOutData.scala:25
[INFO][2018-05-24 20:23:25,526][org.apache.spark.SparkContext]Created broadcast 0 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:23:25,555][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at map at ReceiveKafkaData.scala:63) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:23:25,556][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 0.0 with 1 tasks
[INFO][2018-05-24 20:23:25,597][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:23:25,608][org.apache.spark.executor.Executor]Running task 0.0 in stage 0.0 (TID 0)
[INFO][2018-05-24 20:23:25,639][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12124 is the same as ending offset skipping seven 0
[INFO][2018-05-24 20:23:25,918][org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper]Process identifier=hconnection-0x2020b19 connecting to ZooKeeper ensemble=vm-xaj-bigdata-da-d01:2181,vm-xaj-bigdata-da-d02:2181,vm-xaj-bigdata-da-d03:2181
[INFO][2018-05-24 20:23:25,925][org.apache.zookeeper.ZooKeeper]Client environment:zookeeper.version=3.4.5-cdh5.7.0--1, built on 04/06/2016 14:24 GMT
[INFO][2018-05-24 20:23:25,925][org.apache.zookeeper.ZooKeeper]Client environment:host.name=192.168.0.102
[INFO][2018-05-24 20:23:25,925][org.apache.zookeeper.ZooKeeper]Client environment:java.version=1.8.0_161
[INFO][2018-05-24 20:23:25,925][org.apache.zookeeper.ZooKeeper]Client environment:java.vendor=Oracle Corporation
[INFO][2018-05-24 20:23:25,925][org.apache.zookeeper.ZooKeeper]Client environment:java.home=/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre
[INFO][2018-05-24 20:23:25,925][org.apache.zookeeper.ZooKeeper]Client environment:java.class.path=/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/charsets.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/deploy.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/cldrdata.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/dnsns.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/jaccess.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/jfxrt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/localedata.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/nashorn.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/sunec.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/sunjce_provider.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/sunpkcs11.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/zipfs.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/javaws.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/jce.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/jfr.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/jfxswt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/jsse.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/management-agent.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/plugin.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/resources.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/rt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/ant-javafx.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/dt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/javafx-mx.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/jconsole.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/packager.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/sa-jdi.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/tools.jar:/Users/seven/project/scala/dataMining/target/classes:/Users/seven/software/maven/repository/org/scala-lang/scala-library/2.11.12/scala-library-2.11.12.jar:/Users/seven/software/maven/repository/org/scala-lang/scala-compiler/2.11.12/scala-compiler-2.11.12.jar:/Users/seven/software/maven/repository/org/scala-lang/modules/scala-xml_2.11/1.0.5/scala-xml_2.11-1.0.5.jar:/Users/seven/software/maven/repository/org/scala-lang/modules/scala-parser-combinators_2.11/1.0.4/scala-parser-combinators_2.11-1.0.4.jar:/Users/seven/software/maven/repository/org/scala-lang/scala-reflect/2.11.12/scala-reflect-2.11.12.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-client/2.6.0-cdh5.7.0/hadoop-client-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-common/2.6.0-cdh5.7.0/hadoop-common-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/xmlenc/xmlenc/0.52/xmlenc-0.52.jar:/Users/seven/software/maven/repository/commons-net/commons-net/3.1/commons-net-3.1.jar:/Users/seven/software/maven/repository/commons-configuration/commons-configuration/1.6/commons-configuration-1.6.jar:/Users/seven/software/maven/repository/commons-digester/commons-digester/1.8/commons-digester-1.8.jar:/Users/seven/software/maven/repository/commons-beanutils/commons-beanutils/1.7.0/commons-beanutils-1.7.0.jar:/Users/seven/software/maven/repository/org/slf4j/slf4j-api/1.7.5/slf4j-api-1.7.5.jar:/Users/seven/software/maven/repository/org/slf4j/slf4j-log4j12/1.7.5/slf4j-log4j12-1.7.5.jar:/Users/seven/software/maven/repository/org/apache/avro/avro/1.7.6-cdh5.7.0/avro-1.7.6-cdh5.7.0.jar:/Users/seven/software/maven/repository/com/google/code/gson/gson/2.2.4/gson-2.2.4.jar:/Users/seven/software/maven/repository/org/apache/curator/curator-client/2.7.1/curator-client-2.7.1.jar:/Users/seven/software/maven/repository/org/apache/curator/curator-recipes/2.7.1/curator-recipes-2.7.1.jar:/Users/seven/software/maven/repository/com/google/code/findbugs/jsr305/3.0.0/jsr305-3.0.0.jar:/Users/seven/software/maven/repository/org/apache/htrace/htrace-core4/4.0.1-incubating/htrace-core4-4.0.1-incubating.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-compress/1.4.1/commons-compress-1.4.1.jar:/Users/seven/software/maven/repository/org/tukaani/xz/1.0/xz-1.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-hdfs/2.6.0-cdh5.7.0/hadoop-hdfs-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/io/netty/netty/3.6.2.Final/netty-3.6.2.Final.jar:/Users/seven/software/maven/repository/xerces/xercesImpl/2.9.1/xercesImpl-2.9.1.jar:/Users/seven/software/maven/repository/xml-apis/xml-apis/1.3.04/xml-apis-1.3.04.jar:/Users/seven/software/maven/repository/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-app/2.6.0-cdh5.7.0/hadoop-mapreduce-client-app-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-common/2.6.0-cdh5.7.0/hadoop-mapreduce-client-common-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-yarn-client/2.6.0-cdh5.7.0/hadoop-yarn-client-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-yarn-server-common/2.6.0-cdh5.7.0/hadoop-yarn-server-common-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-shuffle/2.6.0-cdh5.7.0/hadoop-mapreduce-client-shuffle-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-yarn-api/2.6.0-cdh5.7.0/hadoop-yarn-api-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-core/2.6.0-cdh5.7.0/hadoop-mapreduce-client-core-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-yarn-common/2.6.0-cdh5.7.0/hadoop-yarn-common-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/javax/xml/bind/jaxb-api/2.2.2/jaxb-api-2.2.2.jar:/Users/seven/software/maven/repository/javax/xml/stream/stax-api/1.0-2/stax-api-1.0-2.jar:/Users/seven/software/maven/repository/javax/activation/activation/1.1/activation-1.1.jar:/Users/seven/software/maven/repository/com/sun/jersey/jersey-client/1.9/jersey-client-1.9.jar:/Users/seven/software/maven/repository/org/codehaus/jackson/jackson-xc/1.8.8/jackson-xc-1.8.8.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.6.0-cdh5.7.0/hadoop-mapreduce-client-jobclient-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-aws/2.6.0-cdh5.7.0/hadoop-aws-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/com/amazonaws/aws-java-sdk-s3/1.10.6/aws-java-sdk-s3-1.10.6.jar:/Users/seven/software/maven/repository/com/amazonaws/aws-java-sdk-kms/1.10.6/aws-java-sdk-kms-1.10.6.jar:/Users/seven/software/maven/repository/com/amazonaws/aws-java-sdk-core/1.10.6/aws-java-sdk-core-1.10.6.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-annotations/2.6.0-cdh5.7.0/hadoop-annotations-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-client/1.2.0-cdh5.7.0/hbase-client-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-annotations/1.2.0-cdh5.7.0/hbase-annotations-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-common/1.2.0-cdh5.7.0/hbase-common-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-protocol/1.2.0-cdh5.7.0/hbase-protocol-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/commons-codec/commons-codec/1.9/commons-codec-1.9.jar:/Users/seven/software/maven/repository/commons-io/commons-io/2.4/commons-io-2.4.jar:/Users/seven/software/maven/repository/commons-lang/commons-lang/2.6/commons-lang-2.6.jar:/Users/seven/software/maven/repository/commons-logging/commons-logging/1.2/commons-logging-1.2.jar:/Users/seven/software/maven/repository/com/google/guava/guava/12.0.1/guava-12.0.1.jar:/Users/seven/software/maven/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/Users/seven/software/maven/repository/io/netty/netty-all/4.0.23.Final/netty-all-4.0.23.Final.jar:/Users/seven/software/maven/repository/org/apache/zookeeper/zookeeper/3.4.5-cdh5.7.0/zookeeper-3.4.5-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/htrace/htrace-core/3.2.0-incubating/htrace-core-3.2.0-incubating.jar:/Users/seven/software/maven/repository/org/codehaus/jackson/jackson-mapper-asl/1.8.8/jackson-mapper-asl-1.8.8.jar:/Users/seven/software/maven/repository/org/jruby/jcodings/jcodings/1.0.8/jcodings-1.0.8.jar:/Users/seven/software/maven/repository/org/jruby/joni/joni/2.1.2/joni-2.1.2.jar:/Users/seven/software/maven/repository/com/yammer/metrics/metrics-core/2.2.0/metrics-core-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-auth/2.6.0-cdh5.7.0/hadoop-auth-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/httpcomponents/httpclient/4.2.5/httpclient-4.2.5.jar:/Users/seven/software/maven/repository/org/apache/httpcomponents/httpcore/4.2.4/httpcore-4.2.4.jar:/Users/seven/software/maven/repository/org/apache/directory/server/apacheds-kerberos-codec/2.0.0-M15/apacheds-kerberos-codec-2.0.0-M15.jar:/Users/seven/software/maven/repository/org/apache/directory/server/apacheds-i18n/2.0.0-M15/apacheds-i18n-2.0.0-M15.jar:/Users/seven/software/maven/repository/org/apache/directory/api/api-asn1-api/1.0.0-M20/api-asn1-api-1.0.0-M20.jar:/Users/seven/software/maven/repository/org/apache/directory/api/api-util/1.0.0-M20/api-util-1.0.0-M20.jar:/Users/seven/software/maven/repository/org/apache/curator/curator-framework/2.7.1/curator-framework-2.7.1.jar:/Users/seven/software/maven/repository/com/github/stephenc/findbugs/findbugs-annotations/1.3.9-1/findbugs-annotations-1.3.9-1.jar:/Users/seven/software/maven/repository/junit/junit/4.12/junit-4.12.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-server/1.2.0-cdh5.7.0/hbase-server-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-procedure/1.2.0-cdh5.7.0/hbase-procedure-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-common/1.2.0-cdh5.7.0/hbase-common-1.2.0-cdh5.7.0-tests.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-prefix-tree/1.2.0-cdh5.7.0/hbase-prefix-tree-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar:/Users/seven/software/maven/repository/commons-collections/commons-collections/3.2.2/commons-collections-3.2.2.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-hadoop-compat/1.2.0-cdh5.7.0/hbase-hadoop-compat-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-hadoop2-compat/1.2.0-cdh5.7.0/hbase-hadoop2-compat-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/com/sun/jersey/jersey-core/1.9/jersey-core-1.9.jar:/Users/seven/software/maven/repository/com/sun/jersey/jersey-server/1.9/jersey-server-1.9.jar:/Users/seven/software/maven/repository/asm/asm/3.1/asm-3.1.jar:/Users/seven/software/maven/repository/commons-cli/commons-cli/1.2/commons-cli-1.2.jar:/Users/seven/software/maven/repository/com/github/stephenc/high-scale-lib/high-scale-lib/1.1.1/high-scale-lib-1.1.1.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-math/2.1/commons-math-2.1.jar:/Users/seven/software/maven/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jetty/6.1.26.cloudera.4/jetty-6.1.26.cloudera.4.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jetty-util/6.1.26.cloudera.4/jetty-util-6.1.26.cloudera.4.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jetty-sslengine/6.1.26.cloudera.4/jetty-sslengine-6.1.26.cloudera.4.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jsp-2.1/6.1.14/jsp-2.1-6.1.14.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jsp-api-2.1/6.1.14/jsp-api-2.1-6.1.14.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/servlet-api-2.5/6.1.14/servlet-api-2.5-6.1.14.jar:/Users/seven/software/maven/repository/org/codehaus/jackson/jackson-core-asl/1.8.8/jackson-core-asl-1.8.8.jar:/Users/seven/software/maven/repository/org/codehaus/jackson/jackson-jaxrs/1.8.8/jackson-jaxrs-1.8.8.jar:/Users/seven/software/maven/repository/tomcat/jasper-compiler/5.5.23/jasper-compiler-5.5.23.jar:/Users/seven/software/maven/repository/tomcat/jasper-runtime/5.5.23/jasper-runtime-5.5.23.jar:/Users/seven/software/maven/repository/commons-el/commons-el/1.0/commons-el-1.0.jar:/Users/seven/software/maven/repository/org/jamon/jamon-runtime/2.4.1/jamon-runtime-2.4.1.jar:/Users/seven/software/maven/repository/com/lmax/disruptor/3.3.0/disruptor-3.3.0.jar:/Users/seven/software/maven/repository/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar:/Users/seven/software/maven/repository/org/owasp/esapi/esapi/2.1.0/esapi-2.1.0.jar:/Users/seven/software/maven/repository/commons-beanutils/commons-beanutils-core/1.7.0/commons-beanutils-core-1.7.0.jar:/Users/seven/software/maven/repository/commons-fileupload/commons-fileupload/1.2/commons-fileupload-1.2.jar:/Users/seven/software/maven/repository/xom/xom/1.2.5/xom-1.2.5.jar:/Users/seven/software/maven/repository/xalan/xalan/2.7.0/xalan-2.7.0.jar:/Users/seven/software/maven/repository/org/beanshell/bsh-core/2.0b4/bsh-core-2.0b4.jar:/Users/seven/software/maven/repository/org/owasp/antisamy/antisamy/1.4.3/antisamy-1.4.3.jar:/Users/seven/software/maven/repository/org/apache/xmlgraphics/batik-css/1.7/batik-css-1.7.jar:/Users/seven/software/maven/repository/org/apache/xmlgraphics/batik-ext/1.7/batik-ext-1.7.jar:/Users/seven/software/maven/repository/org/apache/xmlgraphics/batik-util/1.7/batik-util-1.7.jar:/Users/seven/software/maven/repository/xml-apis/xml-apis-ext/1.3.04/xml-apis-ext-1.3.04.jar:/Users/seven/software/maven/repository/net/sourceforge/nekohtml/nekohtml/1.9.12/nekohtml-1.9.12.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-core/2.6.0-mr1-cdh5.7.0/hadoop-core-2.6.0-mr1-cdh5.7.0.jar:/Users/seven/software/maven/repository/javax/servlet/jsp/jsp-api/2.1/jsp-api-2.1.jar:/Users/seven/software/maven/repository/net/java/dev/jets3t/jets3t/0.6.1/jets3t-0.6.1.jar:/Users/seven/software/maven/repository/javax/servlet/servlet-api/2.5/servlet-api-2.5.jar:/Users/seven/software/maven/repository/hsqldb/hsqldb/1.8.0.10/hsqldb-1.8.0.10.jar:/Users/seven/software/maven/repository/org/eclipse/jdt/core/3.1.1/core-3.1.1.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-hdfs/2.6.0-cdh5.7.0/hadoop-hdfs-2.6.0-cdh5.7.0-tests.jar:/Users/seven/software/maven/repository/commons-daemon/commons-daemon/1.0.13/commons-daemon-1.0.13.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-lang3/3.5/commons-lang3-3.5.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-pool2/2.4.2/commons-pool2-2.4.2.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-mllib_2.11/2.2.0/spark-mllib_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-core_2.11/2.2.0/spark-core_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/avro/avro-mapred/1.7.7/avro-mapred-1.7.7-hadoop2.jar:/Users/seven/software/maven/repository/org/apache/avro/avro-ipc/1.7.7/avro-ipc-1.7.7-tests.jar:/Users/seven/software/maven/repository/com/twitter/chill_2.11/0.8.0/chill_2.11-0.8.0.jar:/Users/seven/software/maven/repository/com/esotericsoftware/kryo-shaded/3.0.3/kryo-shaded-3.0.3.jar:/Users/seven/software/maven/repository/com/esotericsoftware/minlog/1.3.0/minlog-1.3.0.jar:/Users/seven/software/maven/repository/org/objenesis/objenesis/2.1/objenesis-2.1.jar:/Users/seven/software/maven/repository/com/twitter/chill-java/0.8.0/chill-java-0.8.0.jar:/Users/seven/software/maven/repository/org/apache/xbean/xbean-asm5-shaded/4.4/xbean-asm5-shaded-4.4.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-launcher_2.11/2.2.0/spark-launcher_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-network-common_2.11/2.2.0/spark-network-common_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-network-shuffle_2.11/2.2.0/spark-network-shuffle_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-unsafe_2.11/2.2.0/spark-unsafe_2.11-2.2.0.jar:/Users/seven/software/maven/repository/javax/servlet/javax.servlet-api/3.1.0/javax.servlet-api-3.1.0.jar:/Users/seven/software/maven/repository/org/slf4j/jul-to-slf4j/1.7.16/jul-to-slf4j-1.7.16.jar:/Users/seven/software/maven/repository/org/slf4j/jcl-over-slf4j/1.7.16/jcl-over-slf4j-1.7.16.jar:/Users/seven/software/maven/repository/com/ning/compress-lzf/1.0.3/compress-lzf-1.0.3.jar:/Users/seven/software/maven/repository/org/xerial/snappy/snappy-java/1.1.2.6/snappy-java-1.1.2.6.jar:/Users/seven/software/maven/repository/org/roaringbitmap/RoaringBitmap/0.5.11/RoaringBitmap-0.5.11.jar:/Users/seven/software/maven/repository/org/json4s/json4s-jackson_2.11/3.2.11/json4s-jackson_2.11-3.2.11.jar:/Users/seven/software/maven/repository/org/json4s/json4s-core_2.11/3.2.11/json4s-core_2.11-3.2.11.jar:/Users/seven/software/maven/repository/org/json4s/json4s-ast_2.11/3.2.11/json4s-ast_2.11-3.2.11.jar:/Users/seven/software/maven/repository/org/scala-lang/scalap/2.11.0/scalap-2.11.0.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/core/jersey-client/2.22.2/jersey-client-2.22.2.jar:/Users/seven/software/maven/repository/javax/ws/rs/javax.ws.rs-api/2.0.1/javax.ws.rs-api-2.0.1.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/hk2-api/2.4.0-b34/hk2-api-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/hk2-utils/2.4.0-b34/hk2-utils-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/external/aopalliance-repackaged/2.4.0-b34/aopalliance-repackaged-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/external/javax.inject/2.4.0-b34/javax.inject-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/hk2-locator/2.4.0-b34/hk2-locator-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/javassist/javassist/3.18.1-GA/javassist-3.18.1-GA.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/core/jersey-common/2.22.2/jersey-common-2.22.2.jar:/Users/seven/software/maven/repository/javax/annotation/javax.annotation-api/1.2/javax.annotation-api-1.2.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/bundles/repackaged/jersey-guava/2.22.2/jersey-guava-2.22.2.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/osgi-resource-locator/1.0.1/osgi-resource-locator-1.0.1.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/core/jersey-server/2.22.2/jersey-server-2.22.2.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/media/jersey-media-jaxb/2.22.2/jersey-media-jaxb-2.22.2.jar:/Users/seven/software/maven/repository/javax/validation/validation-api/1.1.0.Final/validation-api-1.1.0.Final.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/containers/jersey-container-servlet/2.22.2/jersey-container-servlet-2.22.2.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/containers/jersey-container-servlet-core/2.22.2/jersey-container-servlet-core-2.22.2.jar:/Users/seven/software/maven/repository/com/clearspring/analytics/stream/2.7.0/stream-2.7.0.jar:/Users/seven/software/maven/repository/io/dropwizard/metrics/metrics-core/3.1.2/metrics-core-3.1.2.jar:/Users/seven/software/maven/repository/io/dropwizard/metrics/metrics-jvm/3.1.2/metrics-jvm-3.1.2.jar:/Users/seven/software/maven/repository/io/dropwizard/metrics/metrics-json/3.1.2/metrics-json-3.1.2.jar:/Users/seven/software/maven/repository/io/dropwizard/metrics/metrics-graphite/3.1.2/metrics-graphite-3.1.2.jar:/Users/seven/software/maven/repository/org/apache/ivy/ivy/2.4.0/ivy-2.4.0.jar:/Users/seven/software/maven/repository/oro/oro/2.0.8/oro-2.0.8.jar:/Users/seven/software/maven/repository/net/razorvine/pyrolite/4.13/pyrolite-4.13.jar:/Users/seven/software/maven/repository/net/sf/py4j/py4j/0.10.4/py4j-0.10.4.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-crypto/1.0.0/commons-crypto-1.0.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-streaming_2.11/2.2.0/spark-streaming_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-sql_2.11/2.2.0/spark-sql_2.11-2.2.0.jar:/Users/seven/software/maven/repository/com/univocity/univocity-parsers/2.2.1/univocity-parsers-2.2.1.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-sketch_2.11/2.2.0/spark-sketch_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-catalyst_2.11/2.2.0/spark-catalyst_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/codehaus/janino/janino/3.0.0/janino-3.0.0.jar:/Users/seven/software/maven/repository/org/codehaus/janino/commons-compiler/3.0.0/commons-compiler-3.0.0.jar:/Users/seven/software/maven/repository/org/antlr/antlr4-runtime/4.5.3/antlr4-runtime-4.5.3.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-column/1.8.2/parquet-column-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-common/1.8.2/parquet-common-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-encoding/1.8.2/parquet-encoding-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-hadoop/1.8.2/parquet-hadoop-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-format/2.3.1/parquet-format-2.3.1.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-jackson/1.8.2/parquet-jackson-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-graphx_2.11/2.2.0/spark-graphx_2.11-2.2.0.jar:/Users/seven/software/maven/repository/com/github/fommil/netlib/core/1.1.2/core-1.1.2.jar:/Users/seven/software/maven/repository/net/sourceforge/f2j/arpack_combined_all/0.1/arpack_combined_all-0.1.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-mllib-local_2.11/2.2.0/spark-mllib-local_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/scalanlp/breeze_2.11/0.13.1/breeze_2.11-0.13.1.jar:/Users/seven/software/maven/repository/org/scalanlp/breeze-macros_2.11/0.13.1/breeze-macros_2.11-0.13.1.jar:/Users/seven/software/maven/repository/net/sf/opencsv/opencsv/2.3/opencsv-2.3.jar:/Users/seven/software/maven/repository/com/github/rwl/jtransforms/2.4.0/jtransforms-2.4.0.jar:/Users/seven/software/maven/repository/org/spire-math/spire_2.11/0.13.0/spire_2.11-0.13.0.jar:/Users/seven/software/maven/repository/org/spire-math/spire-macros_2.11/0.13.0/spire-macros_2.11-0.13.0.jar:/Users/seven/software/maven/repository/org/typelevel/machinist_2.11/0.6.1/machinist_2.11-0.6.1.jar:/Users/seven/software/maven/repository/com/chuusai/shapeless_2.11/2.3.2/shapeless_2.11-2.3.2.jar:/Users/seven/software/maven/repository/org/typelevel/macro-compat_2.11/1.1.1/macro-compat_2.11-1.1.1.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-math3/3.4.1/commons-math3-3.4.1.jar:/Users/seven/software/maven/repository/org/jpmml/pmml-model/1.2.15/pmml-model-1.2.15.jar:/Users/seven/software/maven/repository/org/jpmml/pmml-schema/1.2.15/pmml-schema-1.2.15.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-tags_2.11/2.2.0/spark-tags_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-streaming-kafka-0-8_2.11/2.2.0/spark-streaming-kafka-0-8_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/kafka/kafka_2.11/0.8.2.1/kafka_2.11-0.8.2.1.jar:/Users/seven/software/maven/repository/com/101tec/zkclient/0.3/zkclient-0.3.jar:/Users/seven/software/maven/repository/org/apache/kafka/kafka-clients/0.8.2.1/kafka-clients-0.8.2.1.jar:/Users/seven/software/maven/repository/mysql/mysql-connector-java/5.1.31/mysql-connector-java-5.1.31.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/module/jackson-module-scala_2.11/2.6.5/jackson-module-scala_2.11-2.6.5.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/core/jackson-core/2.6.5/jackson-core-2.6.5.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/core/jackson-annotations/2.6.5/jackson-annotations-2.6.5.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/core/jackson-databind/2.6.5/jackson-databind-2.6.5.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/module/jackson-module-paranamer/2.6.5/jackson-module-paranamer-2.6.5.jar:/Users/seven/software/maven/repository/com/thoughtworks/paranamer/paranamer/2.6/paranamer-2.6.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-streaming-flume_2.11/2.2.0/spark-streaming-flume_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/flume/flume-ng-core/1.6.0/flume-ng-core-1.6.0.jar:/Users/seven/software/maven/repository/org/apache/flume/flume-ng-configuration/1.6.0/flume-ng-configuration-1.6.0.jar:/Users/seven/software/maven/repository/org/apache/avro/avro-ipc/1.7.4/avro-ipc-1.7.4.jar:/Users/seven/software/maven/repository/org/apache/velocity/velocity/1.7/velocity-1.7.jar:/Users/seven/software/maven/repository/joda-time/joda-time/2.1/joda-time-2.1.jar:/Users/seven/software/maven/repository/org/apache/mina/mina-core/2.0.4/mina-core-2.0.4.jar:/Users/seven/software/maven/repository/org/apache/flume/flume-ng-sdk/1.6.0/flume-ng-sdk-1.6.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-streaming-flume-sink_2.11/2.2.0/spark-streaming-flume-sink_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/flume/flume-ng-clients/flume-ng-log4jappender/1.6.0/flume-ng-log4jappender-1.6.0.jar:/Users/seven/software/maven/repository/net/jpountz/lz4/lz4/1.3.0/lz4-1.3.0.jar:/Applications/IntelliJ IDEA.app/Contents/lib/idea_rt.jar
[INFO][2018-05-24 20:23:25,926][org.apache.zookeeper.ZooKeeper]Client environment:java.library.path=/Users/seven/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:.
[INFO][2018-05-24 20:23:25,926][org.apache.zookeeper.ZooKeeper]Client environment:java.io.tmpdir=/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/
[INFO][2018-05-24 20:23:25,926][org.apache.zookeeper.ZooKeeper]Client environment:java.compiler=<NA>
[INFO][2018-05-24 20:23:25,926][org.apache.zookeeper.ZooKeeper]Client environment:os.name=Mac OS X
[INFO][2018-05-24 20:23:25,926][org.apache.zookeeper.ZooKeeper]Client environment:os.arch=x86_64
[INFO][2018-05-24 20:23:25,927][org.apache.zookeeper.ZooKeeper]Client environment:os.version=10.13.4
[INFO][2018-05-24 20:23:25,927][org.apache.zookeeper.ZooKeeper]Client environment:user.name=seven
[INFO][2018-05-24 20:23:25,927][org.apache.zookeeper.ZooKeeper]Client environment:user.home=/Users/seven
[INFO][2018-05-24 20:23:25,927][org.apache.zookeeper.ZooKeeper]Client environment:user.dir=/Users/seven/project/scala/dataMining
[INFO][2018-05-24 20:23:25,928][org.apache.zookeeper.ZooKeeper]Initiating client connection, connectString=vm-xaj-bigdata-da-d01:2181,vm-xaj-bigdata-da-d02:2181,vm-xaj-bigdata-da-d03:2181 sessionTimeout=60000 watcher=hconnection-0x2020b190x0, quorum=vm-xaj-bigdata-da-d01:2181,vm-xaj-bigdata-da-d02:2181,vm-xaj-bigdata-da-d03:2181, baseZNode=/hbase
[INFO][2018-05-24 20:23:30,066][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527164610000 ms
[WARN][2018-05-24 20:23:31,179][org.apache.hadoop.util.NativeCodeLoader]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO][2018-05-24 20:23:31,333][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 1
[INFO][2018-05-24 20:23:31,433][org.apache.spark.SparkContext]Starting job: collect at SimulationKafkaSendOutData.scala:25
[INFO][2018-05-24 20:23:31,443][org.apache.spark.scheduler.DAGScheduler]Got job 0 (collect at SimulationKafkaSendOutData.scala:25) with 2 output partitions
[INFO][2018-05-24 20:23:31,444][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 0 (collect at SimulationKafkaSendOutData.scala:25)
[INFO][2018-05-24 20:23:31,444][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:23:31,445][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:23:31,452][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 0 (hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/* MapPartitionsRDD[1] at textFile at SimulationKafkaSendOutData.scala:25), which has no missing parents
[INFO][2018-05-24 20:23:31,471][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1 stored as values in memory (estimated size 3.3 KB, free 912.1 MB)
[INFO][2018-05-24 20:23:31,473][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1_piece0 stored as bytes in memory (estimated size 1969.0 B, free 912.1 MB)
[INFO][2018-05-24 20:23:31,474][org.apache.spark.storage.BlockManagerInfo]Added broadcast_1_piece0 in memory on 192.168.0.102:49686 (size: 1969.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:23:31,474][org.apache.spark.SparkContext]Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:23:31,487][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ResultStage 0 (hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/* MapPartitionsRDD[1] at textFile at SimulationKafkaSendOutData.scala:25) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2018-05-24 20:23:31,488][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 0.0 with 2 tasks
[INFO][2018-05-24 20:23:31,525][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 4883 bytes)
[INFO][2018-05-24 20:23:31,528][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 4883 bytes)
[INFO][2018-05-24 20:23:31,535][org.apache.spark.executor.Executor]Running task 0.0 in stage 0.0 (TID 0)
[INFO][2018-05-24 20:23:31,536][org.apache.spark.executor.Executor]Running task 1.0 in stage 0.0 (TID 1)
[INFO][2018-05-24 20:23:31,593][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/part-00000:10717435+10717436
[INFO][2018-05-24 20:23:31,593][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/part-00000:0+10717435
[INFO][2018-05-24 20:23:35,952][org.apache.zookeeper.ClientCnxn]Opening socket connection to server master/10.213.4.25:2181. Will not attempt to authenticate using SASL (unknown error)
[INFO][2018-05-24 20:23:35,969][org.apache.zookeeper.ClientCnxn]Socket connection established, initiating session, client: /10.0.8.39:49696, server: master/10.213.4.25:2181
[INFO][2018-05-24 20:23:35,998][org.apache.zookeeper.ClientCnxn]Session establishment complete on server master/10.213.4.25:2181, sessionid = 0x162b4dc56095e9c, negotiated timeout = 60000
[WARN][2018-05-24 20:23:36,538][org.apache.hadoop.util.NativeCodeLoader]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO][2018-05-24 20:23:36,632][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:23:36,647][org.apache.spark.executor.Executor]Finished task 0.0 in stage 0.0 (TID 0). 751 bytes result sent to driver
[INFO][2018-05-24 20:23:36,654][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 0.0 (TID 0) in 11067 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:23:36,656][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:23:36,659][org.apache.spark.scheduler.DAGScheduler]ResultStage 0 (foreachPartition at ReceiveKafkaData.scala:65) finished in 11.082 s
[INFO][2018-05-24 20:23:36,666][org.apache.spark.scheduler.DAGScheduler]Job 0 finished: foreachPartition at ReceiveKafkaData.scala:65, took 11.474881 s
[INFO][2018-05-24 20:23:36,670][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527164600000 ms.0 from job set of time 1527164600000 ms
[INFO][2018-05-24 20:23:36,672][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 16.670 s for time 1527164600000 ms (execution: 11.515 s)
[INFO][2018-05-24 20:23:36,672][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527164610000 ms.0 from job set of time 1527164610000 ms
[INFO][2018-05-24 20:23:36,679][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:23:36,681][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:65
[INFO][2018-05-24 20:23:36,682][org.apache.spark.scheduler.DAGScheduler]Got job 1 (foreachPartition at ReceiveKafkaData.scala:65) with 1 output partitions
[INFO][2018-05-24 20:23:36,682][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 1 (foreachPartition at ReceiveKafkaData.scala:65)
[INFO][2018-05-24 20:23:36,683][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:23:36,683][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:23:36,683][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 1 (MapPartitionsRDD[3] at map at ReceiveKafkaData.scala:63), which has no missing parents
[INFO][2018-05-24 20:23:36,684][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 
[INFO][2018-05-24 20:23:36,686][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 20:23:36,690][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1_piece0 stored as bytes in memory (estimated size 1878.0 B, free 912.3 MB)
[INFO][2018-05-24 20:23:36,691][org.apache.spark.storage.BlockManagerInfo]Added broadcast_1_piece0 in memory on 192.168.0.102:49677 (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:23:36,692][org.apache.spark.SparkContext]Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:23:36,693][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at map at ReceiveKafkaData.scala:63) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:23:36,693][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 1.0 with 1 tasks
[INFO][2018-05-24 20:23:36,694][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:23:36,694][org.apache.spark.executor.Executor]Running task 0.0 in stage 1.0 (TID 1)
[INFO][2018-05-24 20:23:36,735][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12124 is the same as ending offset skipping seven 0
[INFO][2018-05-24 20:23:36,735][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:23:36,737][org.apache.spark.executor.Executor]Finished task 0.0 in stage 1.0 (TID 1). 708 bytes result sent to driver
[INFO][2018-05-24 20:23:36,738][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 1.0 (TID 1) in 44 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:23:36,739][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:23:36,739][org.apache.spark.scheduler.DAGScheduler]ResultStage 1 (foreachPartition at ReceiveKafkaData.scala:65) finished in 0.046 s
[INFO][2018-05-24 20:23:36,740][org.apache.spark.scheduler.DAGScheduler]Job 1 finished: foreachPartition at ReceiveKafkaData.scala:65, took 0.058596 s
[INFO][2018-05-24 20:23:36,741][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527164610000 ms.0 from job set of time 1527164610000 ms
[INFO][2018-05-24 20:23:36,741][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 6.741 s for time 1527164610000 ms (execution: 0.069 s)
[INFO][2018-05-24 20:23:36,742][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 1 from persistence list
[INFO][2018-05-24 20:23:36,749][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 0 from persistence list
[INFO][2018-05-24 20:23:36,750][org.apache.spark.storage.BlockManager]Removing RDD 1
[INFO][2018-05-24 20:23:36,750][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:23:36,750][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 
[INFO][2018-05-24 20:23:36,750][org.apache.spark.storage.BlockManager]Removing RDD 0
[INFO][2018-05-24 20:23:36,788][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_1_piece0 on 192.168.0.102:49677 in memory (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:23:36,791][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_0_piece0 on 192.168.0.102:49677 in memory (size: 1877.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:23:40,305][org.apache.spark.storage.memory.MemoryStore]Block taskresult_0 stored as bytes in memory (estimated size 10.4 MB, free 901.7 MB)
[INFO][2018-05-24 20:23:40,306][org.apache.spark.storage.BlockManagerInfo]Added taskresult_0 in memory on 192.168.0.102:49686 (size: 10.4 MB, free: 901.9 MB)
[INFO][2018-05-24 20:23:40,307][org.apache.spark.executor.Executor]Finished task 0.0 in stage 0.0 (TID 0). 10856349 bytes result sent via BlockManager)
[INFO][2018-05-24 20:23:40,345][org.apache.spark.network.client.TransportClientFactory]Successfully created connection to /192.168.0.102:49686 after 21 ms (0 ms spent in bootstraps)
[INFO][2018-05-24 20:23:40,658][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 0.0 (TID 0) in 9145 ms on localhost (executor driver) (1/2)
[INFO][2018-05-24 20:23:40,661][org.apache.spark.storage.BlockManagerInfo]Removed taskresult_0 on 192.168.0.102:49686 in memory (size: 10.4 MB, free: 912.3 MB)
[INFO][2018-05-24 20:23:41,853][org.apache.spark.storage.memory.MemoryStore]Block taskresult_1 stored as bytes in memory (estimated size 10.4 MB, free 901.7 MB)
[INFO][2018-05-24 20:23:41,854][org.apache.spark.storage.BlockManagerInfo]Added taskresult_1 in memory on 192.168.0.102:49686 (size: 10.4 MB, free: 901.9 MB)
[INFO][2018-05-24 20:23:41,855][org.apache.spark.executor.Executor]Finished task 1.0 in stage 0.0 (TID 1). 10855899 bytes result sent via BlockManager)
[INFO][2018-05-24 20:23:41,927][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 0.0 (TID 1) in 10400 ms on localhost (executor driver) (2/2)
[INFO][2018-05-24 20:23:41,928][org.apache.spark.storage.BlockManagerInfo]Removed taskresult_1 on 192.168.0.102:49686 in memory (size: 10.4 MB, free: 912.3 MB)
[INFO][2018-05-24 20:23:41,929][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:23:41,930][org.apache.spark.scheduler.DAGScheduler]ResultStage 0 (collect at SimulationKafkaSendOutData.scala:25) finished in 10.427 s
[INFO][2018-05-24 20:23:41,935][org.apache.spark.scheduler.DAGScheduler]Job 0 finished: collect at SimulationKafkaSendOutData.scala:25, took 10.501356 s
[INFO][2018-05-24 20:23:42,207][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_0_piece0 on 192.168.0.102:49686 in memory (size: 22.1 KB, free: 912.3 MB)
[INFO][2018-05-24 20:23:42,213][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_1_piece0 on 192.168.0.102:49686 in memory (size: 1969.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:23:42,217][org.spark_project.jetty.server.AbstractConnector]Stopped Spark@60ca0db6{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}
[INFO][2018-05-24 20:23:42,219][org.apache.spark.ui.SparkUI]Stopped Spark web UI at http://192.168.0.102:4041
[INFO][2018-05-24 20:23:42,227][org.apache.spark.MapOutputTrackerMasterEndpoint]MapOutputTrackerMasterEndpoint stopped!
[INFO][2018-05-24 20:23:42,240][org.apache.spark.storage.memory.MemoryStore]MemoryStore cleared
[INFO][2018-05-24 20:23:42,240][org.apache.spark.storage.BlockManager]BlockManager stopped
[INFO][2018-05-24 20:23:42,241][org.apache.spark.storage.BlockManagerMaster]BlockManagerMaster stopped
[INFO][2018-05-24 20:23:42,243][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint]OutputCommitCoordinator stopped!
[INFO][2018-05-24 20:23:42,244][org.apache.spark.SparkContext]Successfully stopped SparkContext
[INFO][2018-05-24 20:23:42,254][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	compression.type = none
	metric.reporters = []
	metadata.max.age.ms = 300000
	metadata.fetch.timeout.ms = 60000
	acks = all
	batch.size = 16384
	reconnect.backoff.ms = 10
	bootstrap.servers = [vm-xaj-bigdata-da-d01:9092, vm-xaj-bigdata-da-d02:9092, vm-xaj-bigdata-da-d03:9092, vm-xaj-bigdata-da-d04:9092, vm-xaj-bigdata-da-d05:9092, vm-xaj-bigdata-da-d06:9092, vm-xaj-bigdata-da-d07:9092]
	receive.buffer.bytes = 32768
	retry.backoff.ms = 100
	buffer.memory = 33554432
	timeout.ms = 30000
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	retries = 0
	max.request.size = 1048576
	block.on.buffer.full = true
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
	metrics.sample.window.ms = 30000
	send.buffer.bytes = 131072
	max.in.flight.requests.per.connection = 5
	metrics.num.samples = 2
	linger.ms = 1
	client.id = 

[INFO][2018-05-24 20:23:50,082][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527164620000 ms
[INFO][2018-05-24 20:23:50,084][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527164620000 ms.0 from job set of time 1527164620000 ms
[INFO][2018-05-24 20:23:50,109][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:65
[INFO][2018-05-24 20:23:50,111][org.apache.spark.scheduler.DAGScheduler]Got job 2 (foreachPartition at ReceiveKafkaData.scala:65) with 1 output partitions
[INFO][2018-05-24 20:23:50,111][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 2 (foreachPartition at ReceiveKafkaData.scala:65)
[INFO][2018-05-24 20:23:50,111][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:23:50,111][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:23:50,112][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 2 (MapPartitionsRDD[5] at map at ReceiveKafkaData.scala:63), which has no missing parents
[INFO][2018-05-24 20:23:50,115][org.apache.spark.storage.memory.MemoryStore]Block broadcast_2 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 20:23:50,119][org.apache.spark.storage.memory.MemoryStore]Block broadcast_2_piece0 stored as bytes in memory (estimated size 1878.0 B, free 912.3 MB)
[INFO][2018-05-24 20:23:50,120][org.apache.spark.storage.BlockManagerInfo]Added broadcast_2_piece0 in memory on 192.168.0.102:49677 (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:23:50,121][org.apache.spark.SparkContext]Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:23:50,122][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[5] at map at ReceiveKafkaData.scala:63) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:23:50,122][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 2.0 with 1 tasks
[INFO][2018-05-24 20:23:50,125][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:23:50,127][org.apache.spark.executor.Executor]Running task 0.0 in stage 2.0 (TID 2)
[INFO][2018-05-24 20:23:50,134][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12124 is the same as ending offset skipping seven 0
[INFO][2018-05-24 20:23:50,134][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:23:50,135][org.apache.spark.executor.Executor]Finished task 0.0 in stage 2.0 (TID 2). 751 bytes result sent to driver
[INFO][2018-05-24 20:23:50,137][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 2.0 (TID 2) in 14 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:23:50,138][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:23:50,143][org.apache.spark.scheduler.DAGScheduler]ResultStage 2 (foreachPartition at ReceiveKafkaData.scala:65) finished in 0.019 s
[INFO][2018-05-24 20:23:50,145][org.apache.spark.scheduler.DAGScheduler]Job 2 finished: foreachPartition at ReceiveKafkaData.scala:65, took 0.034766 s
[INFO][2018-05-24 20:23:50,146][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527164620000 ms.0 from job set of time 1527164620000 ms
[INFO][2018-05-24 20:23:50,147][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 10.145 s for time 1527164620000 ms (execution: 0.061 s)
[INFO][2018-05-24 20:23:50,149][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527164630000 ms
[INFO][2018-05-24 20:23:50,149][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 3 from persistence list
[INFO][2018-05-24 20:23:50,149][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527164630000 ms.0 from job set of time 1527164630000 ms
[INFO][2018-05-24 20:23:50,150][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 2 from persistence list
[INFO][2018-05-24 20:23:50,150][org.apache.spark.storage.BlockManager]Removing RDD 3
[INFO][2018-05-24 20:23:50,151][org.apache.spark.storage.BlockManager]Removing RDD 2
[INFO][2018-05-24 20:23:50,151][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:23:50,151][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527164600000 ms
[INFO][2018-05-24 20:23:50,166][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:65
[INFO][2018-05-24 20:23:50,167][org.apache.spark.scheduler.DAGScheduler]Got job 3 (foreachPartition at ReceiveKafkaData.scala:65) with 1 output partitions
[INFO][2018-05-24 20:23:50,167][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 3 (foreachPartition at ReceiveKafkaData.scala:65)
[INFO][2018-05-24 20:23:50,167][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:23:50,167][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:23:50,168][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 3 (MapPartitionsRDD[7] at map at ReceiveKafkaData.scala:63), which has no missing parents
[INFO][2018-05-24 20:23:50,172][org.apache.spark.storage.memory.MemoryStore]Block broadcast_3 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 20:23:50,181][org.apache.spark.storage.memory.MemoryStore]Block broadcast_3_piece0 stored as bytes in memory (estimated size 1878.0 B, free 912.3 MB)
[INFO][2018-05-24 20:23:50,183][org.apache.spark.storage.BlockManagerInfo]Added broadcast_3_piece0 in memory on 192.168.0.102:49677 (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:23:50,184][org.apache.spark.SparkContext]Created broadcast 3 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:23:50,185][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[7] at map at ReceiveKafkaData.scala:63) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:23:50,185][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 3.0 with 1 tasks
[INFO][2018-05-24 20:23:50,186][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 3.0 (TID 3, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:23:50,187][org.apache.spark.executor.Executor]Running task 0.0 in stage 3.0 (TID 3)
[INFO][2018-05-24 20:23:50,193][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12124 is the same as ending offset skipping seven 0
[INFO][2018-05-24 20:23:50,195][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:23:50,198][org.apache.spark.executor.Executor]Finished task 0.0 in stage 3.0 (TID 3). 708 bytes result sent to driver
[INFO][2018-05-24 20:23:50,200][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 3.0 (TID 3) in 14 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:23:50,201][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 3.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:23:50,203][org.apache.spark.scheduler.DAGScheduler]ResultStage 3 (foreachPartition at ReceiveKafkaData.scala:65) finished in 0.017 s
[INFO][2018-05-24 20:23:50,203][org.apache.spark.scheduler.DAGScheduler]Job 3 finished: foreachPartition at ReceiveKafkaData.scala:65, took 0.037327 s
[INFO][2018-05-24 20:23:50,204][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527164630000 ms.0 from job set of time 1527164630000 ms
[INFO][2018-05-24 20:23:50,204][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.204 s for time 1527164630000 ms (execution: 0.055 s)
[INFO][2018-05-24 20:23:50,205][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 5 from persistence list
[INFO][2018-05-24 20:23:50,205][org.apache.spark.storage.BlockManager]Removing RDD 5
[INFO][2018-05-24 20:23:50,206][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 4 from persistence list
[INFO][2018-05-24 20:23:50,206][org.apache.spark.storage.BlockManager]Removing RDD 4
[INFO][2018-05-24 20:23:50,207][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:23:50,208][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527164610000 ms
[INFO][2018-05-24 20:24:05,062][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527164640000 ms
[INFO][2018-05-24 20:24:05,062][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527164640000 ms.0 from job set of time 1527164640000 ms
[INFO][2018-05-24 20:24:05,069][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:65
[INFO][2018-05-24 20:24:05,071][org.apache.spark.scheduler.DAGScheduler]Got job 4 (foreachPartition at ReceiveKafkaData.scala:65) with 1 output partitions
[INFO][2018-05-24 20:24:05,071][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 4 (foreachPartition at ReceiveKafkaData.scala:65)
[INFO][2018-05-24 20:24:05,071][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:24:05,071][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:24:05,072][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 4 (MapPartitionsRDD[9] at map at ReceiveKafkaData.scala:63), which has no missing parents
[INFO][2018-05-24 20:24:05,074][org.apache.spark.storage.memory.MemoryStore]Block broadcast_4 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 20:24:05,075][org.apache.spark.storage.memory.MemoryStore]Block broadcast_4_piece0 stored as bytes in memory (estimated size 1878.0 B, free 912.3 MB)
[INFO][2018-05-24 20:24:05,076][org.apache.spark.storage.BlockManagerInfo]Added broadcast_4_piece0 in memory on 192.168.0.102:49677 (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:24:05,077][org.apache.spark.SparkContext]Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:24:05,078][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[9] at map at ReceiveKafkaData.scala:63) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:24:05,078][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 4.0 with 1 tasks
[INFO][2018-05-24 20:24:05,080][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 4.0 (TID 4, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:24:05,081][org.apache.spark.executor.Executor]Running task 0.0 in stage 4.0 (TID 4)
[INFO][2018-05-24 20:24:05,085][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12124 is the same as ending offset skipping seven 0
[INFO][2018-05-24 20:24:05,085][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:24:05,086][org.apache.spark.executor.Executor]Finished task 0.0 in stage 4.0 (TID 4). 708 bytes result sent to driver
[INFO][2018-05-24 20:24:05,088][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 4.0 (TID 4) in 8 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:24:05,088][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 4.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:24:05,089][org.apache.spark.scheduler.DAGScheduler]ResultStage 4 (foreachPartition at ReceiveKafkaData.scala:65) finished in 0.009 s
[INFO][2018-05-24 20:24:05,089][org.apache.spark.scheduler.DAGScheduler]Job 4 finished: foreachPartition at ReceiveKafkaData.scala:65, took 0.019718 s
[INFO][2018-05-24 20:24:05,090][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527164640000 ms.0 from job set of time 1527164640000 ms
[INFO][2018-05-24 20:24:05,090][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 5.090 s for time 1527164640000 ms (execution: 0.028 s)
[INFO][2018-05-24 20:24:05,090][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 7 from persistence list
[INFO][2018-05-24 20:24:05,090][org.apache.spark.storage.BlockManager]Removing RDD 7
[INFO][2018-05-24 20:24:05,091][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 6 from persistence list
[INFO][2018-05-24 20:24:05,092][org.apache.spark.storage.BlockManager]Removing RDD 6
[INFO][2018-05-24 20:24:05,092][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:24:05,092][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527164620000 ms
[INFO][2018-05-24 20:24:10,058][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527164650000 ms
[INFO][2018-05-24 20:24:10,059][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527164650000 ms.0 from job set of time 1527164650000 ms
[INFO][2018-05-24 20:24:10,065][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:65
[INFO][2018-05-24 20:24:10,066][org.apache.spark.scheduler.DAGScheduler]Got job 5 (foreachPartition at ReceiveKafkaData.scala:65) with 1 output partitions
[INFO][2018-05-24 20:24:10,066][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 5 (foreachPartition at ReceiveKafkaData.scala:65)
[INFO][2018-05-24 20:24:10,066][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:24:10,066][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:24:10,067][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 5 (MapPartitionsRDD[11] at map at ReceiveKafkaData.scala:63), which has no missing parents
[INFO][2018-05-24 20:24:10,068][org.apache.spark.storage.memory.MemoryStore]Block broadcast_5 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 20:24:10,070][org.apache.spark.storage.memory.MemoryStore]Block broadcast_5_piece0 stored as bytes in memory (estimated size 1878.0 B, free 912.3 MB)
[INFO][2018-05-24 20:24:10,072][org.apache.spark.storage.BlockManagerInfo]Added broadcast_5_piece0 in memory on 192.168.0.102:49677 (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:24:10,073][org.apache.spark.SparkContext]Created broadcast 5 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:24:10,074][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[11] at map at ReceiveKafkaData.scala:63) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:24:10,074][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 5.0 with 1 tasks
[INFO][2018-05-24 20:24:10,075][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 5.0 (TID 5, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:24:10,075][org.apache.spark.executor.Executor]Running task 0.0 in stage 5.0 (TID 5)
[INFO][2018-05-24 20:24:10,078][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12124 is the same as ending offset skipping seven 0
[INFO][2018-05-24 20:24:10,078][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:24:10,079][org.apache.spark.executor.Executor]Finished task 0.0 in stage 5.0 (TID 5). 665 bytes result sent to driver
[INFO][2018-05-24 20:24:10,080][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 5.0 (TID 5) in 6 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:24:10,080][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 5.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:24:10,080][org.apache.spark.scheduler.DAGScheduler]ResultStage 5 (foreachPartition at ReceiveKafkaData.scala:65) finished in 0.006 s
[INFO][2018-05-24 20:24:10,081][org.apache.spark.scheduler.DAGScheduler]Job 5 finished: foreachPartition at ReceiveKafkaData.scala:65, took 0.015288 s
[INFO][2018-05-24 20:24:10,081][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527164650000 ms.0 from job set of time 1527164650000 ms
[INFO][2018-05-24 20:24:10,081][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 9 from persistence list
[INFO][2018-05-24 20:24:10,082][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.081 s for time 1527164650000 ms (execution: 0.022 s)
[INFO][2018-05-24 20:24:10,082][org.apache.spark.storage.BlockManager]Removing RDD 9
[INFO][2018-05-24 20:24:10,082][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 8 from persistence list
[INFO][2018-05-24 20:24:10,083][org.apache.spark.storage.BlockManager]Removing RDD 8
[INFO][2018-05-24 20:24:10,083][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:24:10,083][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527164630000 ms
[INFO][2018-05-24 20:24:30,082][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527164660000 ms
[INFO][2018-05-24 20:24:30,084][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527164660000 ms.0 from job set of time 1527164660000 ms
[INFO][2018-05-24 20:24:30,090][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:65
[INFO][2018-05-24 20:24:30,091][org.apache.spark.scheduler.DAGScheduler]Got job 6 (foreachPartition at ReceiveKafkaData.scala:65) with 1 output partitions
[INFO][2018-05-24 20:24:30,091][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 6 (foreachPartition at ReceiveKafkaData.scala:65)
[INFO][2018-05-24 20:24:30,091][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:24:30,091][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:24:30,092][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 6 (MapPartitionsRDD[13] at map at ReceiveKafkaData.scala:63), which has no missing parents
[INFO][2018-05-24 20:24:30,094][org.apache.spark.storage.memory.MemoryStore]Block broadcast_6 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 20:24:30,096][org.apache.spark.storage.memory.MemoryStore]Block broadcast_6_piece0 stored as bytes in memory (estimated size 1878.0 B, free 912.3 MB)
[INFO][2018-05-24 20:24:30,098][org.apache.spark.storage.BlockManagerInfo]Added broadcast_6_piece0 in memory on 192.168.0.102:49677 (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:24:30,100][org.apache.spark.SparkContext]Created broadcast 6 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:24:30,101][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[13] at map at ReceiveKafkaData.scala:63) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:24:30,101][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 6.0 with 1 tasks
[INFO][2018-05-24 20:24:30,102][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 6.0 (TID 6, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:24:30,102][org.apache.spark.executor.Executor]Running task 0.0 in stage 6.0 (TID 6)
[INFO][2018-05-24 20:24:30,109][org.apache.spark.streaming.kafka.KafkaRDD]Computing topic seven, partition 0 offsets 12124 -> 12131
[INFO][2018-05-24 20:24:30,109][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 20:24:30,110][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 20:24:30,110][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 20:24:30,154][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527164670000 ms
[INFO][2018-05-24 20:24:30,536][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:24:30,537][org.apache.spark.executor.Executor]Finished task 0.0 in stage 6.0 (TID 6). 708 bytes result sent to driver
[INFO][2018-05-24 20:24:30,538][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 6.0 (TID 6) in 437 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:24:30,538][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 6.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:24:30,539][org.apache.spark.scheduler.DAGScheduler]ResultStage 6 (foreachPartition at ReceiveKafkaData.scala:65) finished in 0.438 s
[INFO][2018-05-24 20:24:30,539][org.apache.spark.scheduler.DAGScheduler]Job 6 finished: foreachPartition at ReceiveKafkaData.scala:65, took 0.448867 s
[INFO][2018-05-24 20:24:30,540][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527164660000 ms.0 from job set of time 1527164660000 ms
[INFO][2018-05-24 20:24:30,540][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 10.540 s for time 1527164660000 ms (execution: 0.456 s)
[INFO][2018-05-24 20:24:30,541][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527164670000 ms.0 from job set of time 1527164670000 ms
[INFO][2018-05-24 20:24:30,541][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 11 from persistence list
[INFO][2018-05-24 20:24:30,542][org.apache.spark.storage.BlockManager]Removing RDD 11
[INFO][2018-05-24 20:24:30,542][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 10 from persistence list
[INFO][2018-05-24 20:24:30,543][org.apache.spark.storage.BlockManager]Removing RDD 10
[INFO][2018-05-24 20:24:30,544][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:24:30,544][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527164640000 ms
[INFO][2018-05-24 20:24:30,549][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:65
[INFO][2018-05-24 20:24:30,550][org.apache.spark.scheduler.DAGScheduler]Got job 7 (foreachPartition at ReceiveKafkaData.scala:65) with 1 output partitions
[INFO][2018-05-24 20:24:30,550][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 7 (foreachPartition at ReceiveKafkaData.scala:65)
[INFO][2018-05-24 20:24:30,550][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:24:30,550][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:24:30,551][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 7 (MapPartitionsRDD[15] at map at ReceiveKafkaData.scala:63), which has no missing parents
[INFO][2018-05-24 20:24:30,554][org.apache.spark.storage.memory.MemoryStore]Block broadcast_7 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 20:24:30,556][org.apache.spark.storage.memory.MemoryStore]Block broadcast_7_piece0 stored as bytes in memory (estimated size 1878.0 B, free 912.3 MB)
[INFO][2018-05-24 20:24:30,557][org.apache.spark.storage.BlockManagerInfo]Added broadcast_7_piece0 in memory on 192.168.0.102:49677 (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:24:30,557][org.apache.spark.SparkContext]Created broadcast 7 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:24:30,558][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[15] at map at ReceiveKafkaData.scala:63) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:24:30,558][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 7.0 with 1 tasks
[INFO][2018-05-24 20:24:30,559][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 7.0 (TID 7, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:24:30,559][org.apache.spark.executor.Executor]Running task 0.0 in stage 7.0 (TID 7)
[INFO][2018-05-24 20:24:30,564][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12131 is the same as ending offset skipping seven 0
[INFO][2018-05-24 20:24:30,565][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:24:30,566][org.apache.spark.executor.Executor]Finished task 0.0 in stage 7.0 (TID 7). 708 bytes result sent to driver
[INFO][2018-05-24 20:24:30,567][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 7.0 (TID 7) in 8 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:24:30,567][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 7.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:24:30,567][org.apache.spark.scheduler.DAGScheduler]ResultStage 7 (foreachPartition at ReceiveKafkaData.scala:65) finished in 0.008 s
[INFO][2018-05-24 20:24:30,568][org.apache.spark.scheduler.DAGScheduler]Job 7 finished: foreachPartition at ReceiveKafkaData.scala:65, took 0.018430 s
[INFO][2018-05-24 20:24:30,568][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527164670000 ms.0 from job set of time 1527164670000 ms
[INFO][2018-05-24 20:24:30,569][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.568 s for time 1527164670000 ms (execution: 0.027 s)
[INFO][2018-05-24 20:24:30,569][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 13 from persistence list
[INFO][2018-05-24 20:24:30,569][org.apache.spark.storage.BlockManager]Removing RDD 13
[INFO][2018-05-24 20:24:30,569][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 12 from persistence list
[INFO][2018-05-24 20:24:30,570][org.apache.spark.storage.BlockManager]Removing RDD 12
[INFO][2018-05-24 20:24:30,570][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:24:30,570][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527164650000 ms
[INFO][2018-05-24 20:24:45,073][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527164680000 ms
[INFO][2018-05-24 20:24:45,073][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527164680000 ms.0 from job set of time 1527164680000 ms
[INFO][2018-05-24 20:24:45,078][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:65
[INFO][2018-05-24 20:24:45,079][org.apache.spark.scheduler.DAGScheduler]Got job 8 (foreachPartition at ReceiveKafkaData.scala:65) with 1 output partitions
[INFO][2018-05-24 20:24:45,079][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 8 (foreachPartition at ReceiveKafkaData.scala:65)
[INFO][2018-05-24 20:24:45,079][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:24:45,079][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:24:45,080][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 8 (MapPartitionsRDD[17] at map at ReceiveKafkaData.scala:63), which has no missing parents
[INFO][2018-05-24 20:24:45,082][org.apache.spark.storage.memory.MemoryStore]Block broadcast_8 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 20:24:45,084][org.apache.spark.storage.memory.MemoryStore]Block broadcast_8_piece0 stored as bytes in memory (estimated size 1878.0 B, free 912.3 MB)
[INFO][2018-05-24 20:24:45,084][org.apache.spark.storage.BlockManagerInfo]Added broadcast_8_piece0 in memory on 192.168.0.102:49677 (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:24:45,085][org.apache.spark.SparkContext]Created broadcast 8 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:24:45,086][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[17] at map at ReceiveKafkaData.scala:63) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:24:45,086][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 8.0 with 1 tasks
[INFO][2018-05-24 20:24:45,087][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 8.0 (TID 8, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:24:45,089][org.apache.spark.executor.Executor]Running task 0.0 in stage 8.0 (TID 8)
[INFO][2018-05-24 20:24:45,091][org.apache.spark.streaming.kafka.KafkaRDD]Computing topic seven, partition 0 offsets 12131 -> 12137
[INFO][2018-05-24 20:24:45,091][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 20:24:45,092][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 20:24:45,092][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 20:24:45,206][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:24:45,208][org.apache.spark.executor.Executor]Finished task 0.0 in stage 8.0 (TID 8). 665 bytes result sent to driver
[INFO][2018-05-24 20:24:45,209][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 8.0 (TID 8) in 122 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:24:45,209][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 8.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:24:45,209][org.apache.spark.scheduler.DAGScheduler]ResultStage 8 (foreachPartition at ReceiveKafkaData.scala:65) finished in 0.123 s
[INFO][2018-05-24 20:24:45,210][org.apache.spark.scheduler.DAGScheduler]Job 8 finished: foreachPartition at ReceiveKafkaData.scala:65, took 0.131346 s
[INFO][2018-05-24 20:24:45,211][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527164680000 ms.0 from job set of time 1527164680000 ms
[INFO][2018-05-24 20:24:45,211][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 5.211 s for time 1527164680000 ms (execution: 0.138 s)
[INFO][2018-05-24 20:24:45,211][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 15 from persistence list
[INFO][2018-05-24 20:24:45,212][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 14 from persistence list
[INFO][2018-05-24 20:24:45,213][org.apache.spark.storage.BlockManager]Removing RDD 15
[INFO][2018-05-24 20:24:45,213][org.apache.spark.storage.BlockManager]Removing RDD 14
[INFO][2018-05-24 20:24:45,214][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:24:45,214][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527164660000 ms
[INFO][2018-05-24 20:24:55,058][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527164690000 ms
[INFO][2018-05-24 20:24:55,058][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527164690000 ms.0 from job set of time 1527164690000 ms
[INFO][2018-05-24 20:24:55,064][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:65
[INFO][2018-05-24 20:24:55,066][org.apache.spark.scheduler.DAGScheduler]Got job 9 (foreachPartition at ReceiveKafkaData.scala:65) with 1 output partitions
[INFO][2018-05-24 20:24:55,066][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 9 (foreachPartition at ReceiveKafkaData.scala:65)
[INFO][2018-05-24 20:24:55,066][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:24:55,066][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:24:55,067][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 9 (MapPartitionsRDD[19] at map at ReceiveKafkaData.scala:63), which has no missing parents
[INFO][2018-05-24 20:24:55,068][org.apache.spark.storage.memory.MemoryStore]Block broadcast_9 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 20:24:55,070][org.apache.spark.storage.memory.MemoryStore]Block broadcast_9_piece0 stored as bytes in memory (estimated size 1878.0 B, free 912.3 MB)
[INFO][2018-05-24 20:24:55,071][org.apache.spark.storage.BlockManagerInfo]Added broadcast_9_piece0 in memory on 192.168.0.102:49677 (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:24:55,071][org.apache.spark.SparkContext]Created broadcast 9 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:24:55,072][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[19] at map at ReceiveKafkaData.scala:63) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:24:55,072][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 9.0 with 1 tasks
[INFO][2018-05-24 20:24:55,073][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 9.0 (TID 9, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:24:55,073][org.apache.spark.executor.Executor]Running task 0.0 in stage 9.0 (TID 9)
[INFO][2018-05-24 20:24:55,076][org.apache.spark.streaming.kafka.KafkaRDD]Computing topic seven, partition 0 offsets 12137 -> 12141
[INFO][2018-05-24 20:24:55,076][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 20:24:55,076][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 20:24:55,076][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 20:25:00,164][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:25:00,165][org.apache.spark.executor.Executor]Finished task 0.0 in stage 9.0 (TID 9). 708 bytes result sent to driver
[INFO][2018-05-24 20:25:00,166][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 9.0 (TID 9) in 5093 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:25:00,166][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 9.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:25:00,167][org.apache.spark.scheduler.DAGScheduler]ResultStage 9 (foreachPartition at ReceiveKafkaData.scala:65) finished in 5.095 s
[INFO][2018-05-24 20:25:00,168][org.apache.spark.scheduler.DAGScheduler]Job 9 finished: foreachPartition at ReceiveKafkaData.scala:65, took 5.103660 s
[INFO][2018-05-24 20:25:00,168][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527164690000 ms.0 from job set of time 1527164690000 ms
[INFO][2018-05-24 20:25:00,169][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 10.168 s for time 1527164690000 ms (execution: 5.110 s)
[INFO][2018-05-24 20:25:10,076][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527164700000 ms
[INFO][2018-05-24 20:25:10,076][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 17 from persistence list
[INFO][2018-05-24 20:25:10,077][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527164700000 ms.0 from job set of time 1527164700000 ms
[INFO][2018-05-24 20:25:10,077][org.apache.spark.storage.BlockManager]Removing RDD 17
[INFO][2018-05-24 20:25:10,077][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 16 from persistence list
[INFO][2018-05-24 20:25:10,078][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:25:10,079][org.apache.spark.storage.BlockManager]Removing RDD 16
[INFO][2018-05-24 20:25:10,079][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527164670000 ms
[INFO][2018-05-24 20:25:10,086][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:65
[INFO][2018-05-24 20:25:10,087][org.apache.spark.scheduler.DAGScheduler]Got job 10 (foreachPartition at ReceiveKafkaData.scala:65) with 1 output partitions
[INFO][2018-05-24 20:25:10,087][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 10 (foreachPartition at ReceiveKafkaData.scala:65)
[INFO][2018-05-24 20:25:10,087][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:25:10,087][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:25:10,088][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 10 (MapPartitionsRDD[21] at map at ReceiveKafkaData.scala:63), which has no missing parents
[INFO][2018-05-24 20:25:10,090][org.apache.spark.storage.memory.MemoryStore]Block broadcast_10 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 20:25:10,097][org.apache.spark.storage.memory.MemoryStore]Block broadcast_10_piece0 stored as bytes in memory (estimated size 1878.0 B, free 912.3 MB)
[INFO][2018-05-24 20:25:10,097][org.apache.spark.storage.BlockManagerInfo]Added broadcast_10_piece0 in memory on 192.168.0.102:49677 (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:25:10,099][org.apache.spark.SparkContext]Created broadcast 10 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:25:10,099][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[21] at map at ReceiveKafkaData.scala:63) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:25:10,100][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 10.0 with 1 tasks
[INFO][2018-05-24 20:25:10,100][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 10.0 (TID 10, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:25:10,101][org.apache.spark.executor.Executor]Running task 0.0 in stage 10.0 (TID 10)
[INFO][2018-05-24 20:25:10,102][org.apache.spark.streaming.kafka.KafkaRDD]Computing topic seven, partition 0 offsets 12141 -> 12147
[INFO][2018-05-24 20:25:10,103][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 20:25:10,103][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 20:25:10,104][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 20:25:10,133][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527164710000 ms
[INFO][2018-05-24 20:25:10,168][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:25:10,169][org.apache.spark.executor.Executor]Finished task 0.0 in stage 10.0 (TID 10). 665 bytes result sent to driver
[INFO][2018-05-24 20:25:10,169][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 10.0 (TID 10) in 69 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:25:10,170][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 10.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:25:10,170][org.apache.spark.scheduler.DAGScheduler]ResultStage 10 (foreachPartition at ReceiveKafkaData.scala:65) finished in 0.070 s
[INFO][2018-05-24 20:25:10,170][org.apache.spark.scheduler.DAGScheduler]Job 10 finished: foreachPartition at ReceiveKafkaData.scala:65, took 0.084291 s
[INFO][2018-05-24 20:25:10,171][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527164700000 ms.0 from job set of time 1527164700000 ms
[INFO][2018-05-24 20:25:10,171][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 10.171 s for time 1527164700000 ms (execution: 0.095 s)
[INFO][2018-05-24 20:25:10,171][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527164710000 ms.0 from job set of time 1527164710000 ms
[INFO][2018-05-24 20:25:10,172][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 19 from persistence list
[INFO][2018-05-24 20:25:10,172][org.apache.spark.storage.BlockManager]Removing RDD 19
[INFO][2018-05-24 20:25:10,173][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 18 from persistence list
[INFO][2018-05-24 20:25:10,173][org.apache.spark.storage.BlockManager]Removing RDD 18
[INFO][2018-05-24 20:25:10,174][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:25:10,174][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527164680000 ms
[INFO][2018-05-24 20:25:10,180][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:65
[INFO][2018-05-24 20:25:10,181][org.apache.spark.scheduler.DAGScheduler]Got job 11 (foreachPartition at ReceiveKafkaData.scala:65) with 1 output partitions
[INFO][2018-05-24 20:25:10,181][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 11 (foreachPartition at ReceiveKafkaData.scala:65)
[INFO][2018-05-24 20:25:10,182][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:25:10,182][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:25:10,183][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 11 (MapPartitionsRDD[23] at map at ReceiveKafkaData.scala:63), which has no missing parents
[INFO][2018-05-24 20:25:10,185][org.apache.spark.storage.memory.MemoryStore]Block broadcast_11 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 20:25:10,193][org.apache.spark.storage.memory.MemoryStore]Block broadcast_11_piece0 stored as bytes in memory (estimated size 1878.0 B, free 912.3 MB)
[INFO][2018-05-24 20:25:10,194][org.apache.spark.storage.BlockManagerInfo]Added broadcast_11_piece0 in memory on 192.168.0.102:49677 (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:25:10,195][org.apache.spark.SparkContext]Created broadcast 11 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:25:10,196][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[23] at map at ReceiveKafkaData.scala:63) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:25:10,196][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 11.0 with 1 tasks
[INFO][2018-05-24 20:25:10,197][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 11.0 (TID 11, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:25:10,197][org.apache.spark.executor.Executor]Running task 0.0 in stage 11.0 (TID 11)
[INFO][2018-05-24 20:25:10,199][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12147 is the same as ending offset skipping seven 0
[INFO][2018-05-24 20:25:10,199][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:25:10,200][org.apache.spark.executor.Executor]Finished task 0.0 in stage 11.0 (TID 11). 665 bytes result sent to driver
[INFO][2018-05-24 20:25:10,200][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 11.0 (TID 11) in 3 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:25:10,200][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 11.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:25:10,201][org.apache.spark.scheduler.DAGScheduler]ResultStage 11 (foreachPartition at ReceiveKafkaData.scala:65) finished in 0.005 s
[INFO][2018-05-24 20:25:10,201][org.apache.spark.scheduler.DAGScheduler]Job 11 finished: foreachPartition at ReceiveKafkaData.scala:65, took 0.020663 s
[INFO][2018-05-24 20:25:10,202][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527164710000 ms.0 from job set of time 1527164710000 ms
[INFO][2018-05-24 20:25:10,202][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 21 from persistence list
[INFO][2018-05-24 20:25:10,202][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.202 s for time 1527164710000 ms (execution: 0.031 s)
[INFO][2018-05-24 20:25:10,203][org.apache.spark.storage.BlockManager]Removing RDD 21
[INFO][2018-05-24 20:25:10,203][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 20 from persistence list
[INFO][2018-05-24 20:25:10,203][org.apache.spark.storage.BlockManager]Removing RDD 20
[INFO][2018-05-24 20:25:10,203][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:25:10,203][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527164690000 ms
[INFO][2018-05-24 20:25:25,065][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527164720000 ms
[INFO][2018-05-24 20:25:25,066][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527164720000 ms.0 from job set of time 1527164720000 ms
[INFO][2018-05-24 20:25:25,074][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:65
[INFO][2018-05-24 20:25:25,075][org.apache.spark.scheduler.DAGScheduler]Got job 12 (foreachPartition at ReceiveKafkaData.scala:65) with 1 output partitions
[INFO][2018-05-24 20:25:25,075][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 12 (foreachPartition at ReceiveKafkaData.scala:65)
[INFO][2018-05-24 20:25:25,075][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:25:25,075][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:25:25,076][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 12 (MapPartitionsRDD[25] at map at ReceiveKafkaData.scala:63), which has no missing parents
[INFO][2018-05-24 20:25:25,078][org.apache.spark.storage.memory.MemoryStore]Block broadcast_12 stored as values in memory (estimated size 3.3 KB, free 912.2 MB)
[INFO][2018-05-24 20:25:25,084][org.apache.spark.storage.memory.MemoryStore]Block broadcast_12_piece0 stored as bytes in memory (estimated size 1878.0 B, free 912.2 MB)
[INFO][2018-05-24 20:25:25,084][org.apache.spark.storage.BlockManagerInfo]Added broadcast_12_piece0 in memory on 192.168.0.102:49677 (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:25:25,085][org.apache.spark.SparkContext]Created broadcast 12 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:25:25,085][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[25] at map at ReceiveKafkaData.scala:63) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:25:25,085][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 12.0 with 1 tasks
[INFO][2018-05-24 20:25:25,086][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 12.0 (TID 12, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:25:25,087][org.apache.spark.executor.Executor]Running task 0.0 in stage 12.0 (TID 12)
[INFO][2018-05-24 20:25:25,088][org.apache.spark.streaming.kafka.KafkaRDD]Computing topic seven, partition 0 offsets 12147 -> 12153
[INFO][2018-05-24 20:25:25,088][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 20:25:25,089][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 20:25:25,089][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 20:25:25,153][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:25:25,153][org.apache.spark.executor.Executor]Finished task 0.0 in stage 12.0 (TID 12). 708 bytes result sent to driver
[INFO][2018-05-24 20:25:25,154][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 12.0 (TID 12) in 68 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:25:25,154][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 12.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:25:25,155][org.apache.spark.scheduler.DAGScheduler]ResultStage 12 (foreachPartition at ReceiveKafkaData.scala:65) finished in 0.069 s
[INFO][2018-05-24 20:25:25,156][org.apache.spark.scheduler.DAGScheduler]Job 12 finished: foreachPartition at ReceiveKafkaData.scala:65, took 0.081653 s
[INFO][2018-05-24 20:25:25,156][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527164720000 ms.0 from job set of time 1527164720000 ms
[INFO][2018-05-24 20:25:25,157][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 23 from persistence list
[INFO][2018-05-24 20:25:25,157][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 5.156 s for time 1527164720000 ms (execution: 0.090 s)
[INFO][2018-05-24 20:25:25,157][org.apache.spark.storage.BlockManager]Removing RDD 23
[INFO][2018-05-24 20:25:25,157][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 22 from persistence list
[INFO][2018-05-24 20:25:25,158][org.apache.spark.storage.BlockManager]Removing RDD 22
[INFO][2018-05-24 20:25:25,158][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:25:25,158][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527164700000 ms
[INFO][2018-05-24 20:25:35,062][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527164730000 ms
[INFO][2018-05-24 20:25:35,062][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527164730000 ms.0 from job set of time 1527164730000 ms
[INFO][2018-05-24 20:25:35,070][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:65
[INFO][2018-05-24 20:25:35,071][org.apache.spark.scheduler.DAGScheduler]Got job 13 (foreachPartition at ReceiveKafkaData.scala:65) with 1 output partitions
[INFO][2018-05-24 20:25:35,071][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 13 (foreachPartition at ReceiveKafkaData.scala:65)
[INFO][2018-05-24 20:25:35,071][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:25:35,071][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:25:35,072][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 13 (MapPartitionsRDD[27] at map at ReceiveKafkaData.scala:63), which has no missing parents
[INFO][2018-05-24 20:25:35,074][org.apache.spark.storage.memory.MemoryStore]Block broadcast_13 stored as values in memory (estimated size 3.3 KB, free 912.2 MB)
[INFO][2018-05-24 20:25:35,077][org.apache.spark.storage.memory.MemoryStore]Block broadcast_13_piece0 stored as bytes in memory (estimated size 1876.0 B, free 912.2 MB)
[INFO][2018-05-24 20:25:35,077][org.apache.spark.storage.BlockManagerInfo]Added broadcast_13_piece0 in memory on 192.168.0.102:49677 (size: 1876.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:25:35,078][org.apache.spark.SparkContext]Created broadcast 13 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:25:35,078][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[27] at map at ReceiveKafkaData.scala:63) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:25:35,078][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 13.0 with 1 tasks
[INFO][2018-05-24 20:25:35,079][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 13.0 (TID 13, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:25:35,079][org.apache.spark.executor.Executor]Running task 0.0 in stage 13.0 (TID 13)
[INFO][2018-05-24 20:25:35,080][org.apache.spark.streaming.kafka.KafkaRDD]Computing topic seven, partition 0 offsets 12153 -> 12157
[INFO][2018-05-24 20:25:35,081][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 20:25:35,081][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 20:25:35,081][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 20:25:40,157][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:25:40,159][org.apache.spark.executor.Executor]Finished task 0.0 in stage 13.0 (TID 13). 708 bytes result sent to driver
[INFO][2018-05-24 20:25:40,160][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 13.0 (TID 13) in 5081 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:25:40,160][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 13.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:25:40,161][org.apache.spark.scheduler.DAGScheduler]ResultStage 13 (foreachPartition at ReceiveKafkaData.scala:65) finished in 5.082 s
[INFO][2018-05-24 20:25:40,161][org.apache.spark.scheduler.DAGScheduler]Job 13 finished: foreachPartition at ReceiveKafkaData.scala:65, took 5.090816 s
[INFO][2018-05-24 20:25:40,162][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527164730000 ms.0 from job set of time 1527164730000 ms
[INFO][2018-05-24 20:25:40,162][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 10.162 s for time 1527164730000 ms (execution: 5.100 s)
[INFO][2018-05-24 20:25:45,100][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527164740000 ms
[INFO][2018-05-24 20:25:45,100][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 25 from persistence list
[INFO][2018-05-24 20:25:45,100][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527164740000 ms.0 from job set of time 1527164740000 ms
[INFO][2018-05-24 20:25:45,101][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 24 from persistence list
[INFO][2018-05-24 20:25:45,101][org.apache.spark.storage.BlockManager]Removing RDD 25
[INFO][2018-05-24 20:25:45,101][org.apache.spark.storage.BlockManager]Removing RDD 24
[INFO][2018-05-24 20:25:45,101][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:25:45,102][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527164710000 ms
[INFO][2018-05-24 20:25:45,105][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:65
[INFO][2018-05-24 20:25:45,106][org.apache.spark.scheduler.DAGScheduler]Got job 14 (foreachPartition at ReceiveKafkaData.scala:65) with 1 output partitions
[INFO][2018-05-24 20:25:45,106][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 14 (foreachPartition at ReceiveKafkaData.scala:65)
[INFO][2018-05-24 20:25:45,106][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:25:45,106][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:25:45,106][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 14 (MapPartitionsRDD[29] at map at ReceiveKafkaData.scala:63), which has no missing parents
[INFO][2018-05-24 20:25:45,108][org.apache.spark.storage.memory.MemoryStore]Block broadcast_14 stored as values in memory (estimated size 3.3 KB, free 912.2 MB)
[INFO][2018-05-24 20:25:45,111][org.apache.spark.storage.memory.MemoryStore]Block broadcast_14_piece0 stored as bytes in memory (estimated size 1878.0 B, free 912.2 MB)
[INFO][2018-05-24 20:25:45,112][org.apache.spark.storage.BlockManagerInfo]Added broadcast_14_piece0 in memory on 192.168.0.102:49677 (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:25:45,112][org.apache.spark.SparkContext]Created broadcast 14 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:25:45,113][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[29] at map at ReceiveKafkaData.scala:63) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:25:45,113][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 14.0 with 1 tasks
[INFO][2018-05-24 20:25:45,113][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 14.0 (TID 14, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:25:45,113][org.apache.spark.executor.Executor]Running task 0.0 in stage 14.0 (TID 14)
[INFO][2018-05-24 20:25:45,114][org.apache.spark.streaming.kafka.KafkaRDD]Computing topic seven, partition 0 offsets 12157 -> 12161
[INFO][2018-05-24 20:25:45,114][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 20:25:45,115][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 20:25:45,115][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 20:25:45,181][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:25:45,183][org.apache.spark.executor.Executor]Finished task 0.0 in stage 14.0 (TID 14). 708 bytes result sent to driver
[INFO][2018-05-24 20:25:45,184][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 14.0 (TID 14) in 71 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:25:45,184][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 14.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:25:45,184][org.apache.spark.scheduler.DAGScheduler]ResultStage 14 (foreachPartition at ReceiveKafkaData.scala:65) finished in 0.071 s
[INFO][2018-05-24 20:25:45,185][org.apache.spark.scheduler.DAGScheduler]Job 14 finished: foreachPartition at ReceiveKafkaData.scala:65, took 0.079871 s
[INFO][2018-05-24 20:25:45,185][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527164740000 ms.0 from job set of time 1527164740000 ms
[INFO][2018-05-24 20:25:45,186][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 5.185 s for time 1527164740000 ms (execution: 0.085 s)
[INFO][2018-05-24 20:25:45,186][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 27 from persistence list
[INFO][2018-05-24 20:25:45,186][org.apache.spark.storage.BlockManager]Removing RDD 27
[INFO][2018-05-24 20:25:45,187][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 26 from persistence list
[INFO][2018-05-24 20:25:45,188][org.apache.spark.storage.BlockManager]Removing RDD 26
[INFO][2018-05-24 20:25:45,188][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:25:45,188][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527164720000 ms
[INFO][2018-05-24 20:25:50,069][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527164750000 ms
[INFO][2018-05-24 20:25:50,070][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527164750000 ms.0 from job set of time 1527164750000 ms
[INFO][2018-05-24 20:25:50,082][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:65
[INFO][2018-05-24 20:25:50,084][org.apache.spark.scheduler.DAGScheduler]Got job 15 (foreachPartition at ReceiveKafkaData.scala:65) with 1 output partitions
[INFO][2018-05-24 20:25:50,084][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 15 (foreachPartition at ReceiveKafkaData.scala:65)
[INFO][2018-05-24 20:25:50,084][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:25:50,084][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:25:50,085][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 15 (MapPartitionsRDD[31] at map at ReceiveKafkaData.scala:63), which has no missing parents
[INFO][2018-05-24 20:25:50,086][org.apache.spark.storage.memory.MemoryStore]Block broadcast_15 stored as values in memory (estimated size 3.3 KB, free 912.2 MB)
[INFO][2018-05-24 20:25:50,093][org.apache.spark.storage.memory.MemoryStore]Block broadcast_15_piece0 stored as bytes in memory (estimated size 1878.0 B, free 912.2 MB)
[INFO][2018-05-24 20:25:50,093][org.apache.spark.storage.BlockManagerInfo]Added broadcast_15_piece0 in memory on 192.168.0.102:49677 (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:25:50,093][org.apache.spark.SparkContext]Created broadcast 15 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:25:50,094][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 15 (MapPartitionsRDD[31] at map at ReceiveKafkaData.scala:63) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:25:50,095][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 15.0 with 1 tasks
[INFO][2018-05-24 20:25:50,095][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 15.0 (TID 15, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:25:50,096][org.apache.spark.executor.Executor]Running task 0.0 in stage 15.0 (TID 15)
[INFO][2018-05-24 20:25:50,098][org.apache.spark.streaming.kafka.KafkaRDD]Computing topic seven, partition 0 offsets 12161 -> 12163
[INFO][2018-05-24 20:25:50,098][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 20:25:50,098][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 20:25:50,099][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 20:25:50,161][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:25:50,162][org.apache.spark.executor.Executor]Finished task 0.0 in stage 15.0 (TID 15). 708 bytes result sent to driver
[INFO][2018-05-24 20:25:50,164][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 15.0 (TID 15) in 69 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:25:50,164][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 15.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:25:50,165][org.apache.spark.scheduler.DAGScheduler]ResultStage 15 (foreachPartition at ReceiveKafkaData.scala:65) finished in 0.069 s
[INFO][2018-05-24 20:25:50,166][org.apache.spark.scheduler.DAGScheduler]Job 15 finished: foreachPartition at ReceiveKafkaData.scala:65, took 0.082892 s
[INFO][2018-05-24 20:25:50,166][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527164750000 ms.0 from job set of time 1527164750000 ms
[INFO][2018-05-24 20:25:50,167][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 29 from persistence list
[INFO][2018-05-24 20:25:50,167][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.166 s for time 1527164750000 ms (execution: 0.096 s)
[INFO][2018-05-24 20:25:50,167][org.apache.spark.storage.BlockManager]Removing RDD 29
[INFO][2018-05-24 20:25:50,167][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 28 from persistence list
[INFO][2018-05-24 20:25:50,168][org.apache.spark.storage.BlockManager]Removing RDD 28
[INFO][2018-05-24 20:25:50,168][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:25:50,169][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527164730000 ms
[INFO][2018-05-24 20:26:00,339][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527164760000 ms
[INFO][2018-05-24 20:26:00,339][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527164760000 ms.0 from job set of time 1527164760000 ms
[INFO][2018-05-24 20:26:00,346][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:65
[INFO][2018-05-24 20:26:00,347][org.apache.spark.scheduler.DAGScheduler]Got job 16 (foreachPartition at ReceiveKafkaData.scala:65) with 1 output partitions
[INFO][2018-05-24 20:26:00,347][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 16 (foreachPartition at ReceiveKafkaData.scala:65)
[INFO][2018-05-24 20:26:00,347][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:26:00,348][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:26:00,348][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 16 (MapPartitionsRDD[33] at map at ReceiveKafkaData.scala:63), which has no missing parents
[INFO][2018-05-24 20:26:00,350][org.apache.spark.storage.memory.MemoryStore]Block broadcast_16 stored as values in memory (estimated size 3.3 KB, free 912.2 MB)
[INFO][2018-05-24 20:26:00,357][org.apache.spark.storage.memory.MemoryStore]Block broadcast_16_piece0 stored as bytes in memory (estimated size 1878.0 B, free 912.2 MB)
[INFO][2018-05-24 20:26:00,358][org.apache.spark.storage.BlockManagerInfo]Added broadcast_16_piece0 in memory on 192.168.0.102:49677 (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:26:00,358][org.apache.spark.SparkContext]Created broadcast 16 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:26:00,359][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 16 (MapPartitionsRDD[33] at map at ReceiveKafkaData.scala:63) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:26:00,359][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 16.0 with 1 tasks
[INFO][2018-05-24 20:26:00,360][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 16.0 (TID 16, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:26:00,360][org.apache.spark.executor.Executor]Running task 0.0 in stage 16.0 (TID 16)
[INFO][2018-05-24 20:26:00,362][org.apache.spark.streaming.kafka.KafkaRDD]Computing topic seven, partition 0 offsets 12163 -> 12168
[INFO][2018-05-24 20:26:00,362][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 20:26:00,362][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 20:26:00,363][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 20:26:00,433][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:26:00,433][org.apache.spark.executor.Executor]Finished task 0.0 in stage 16.0 (TID 16). 708 bytes result sent to driver
[INFO][2018-05-24 20:26:00,434][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 16.0 (TID 16) in 74 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:26:00,434][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 16.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:26:00,434][org.apache.spark.scheduler.DAGScheduler]ResultStage 16 (foreachPartition at ReceiveKafkaData.scala:65) finished in 0.075 s
[INFO][2018-05-24 20:26:00,435][org.apache.spark.scheduler.DAGScheduler]Job 16 finished: foreachPartition at ReceiveKafkaData.scala:65, took 0.088039 s
[INFO][2018-05-24 20:26:00,435][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527164760000 ms.0 from job set of time 1527164760000 ms
[INFO][2018-05-24 20:26:00,435][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 31 from persistence list
[INFO][2018-05-24 20:26:00,436][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.435 s for time 1527164760000 ms (execution: 0.096 s)
[INFO][2018-05-24 20:26:00,436][org.apache.spark.storage.BlockManager]Removing RDD 31
[INFO][2018-05-24 20:26:00,436][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 30 from persistence list
[INFO][2018-05-24 20:26:00,437][org.apache.spark.storage.BlockManager]Removing RDD 30
[INFO][2018-05-24 20:26:00,437][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:26:00,437][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527164740000 ms
[INFO][2018-05-24 20:26:10,065][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527164770000 ms
[INFO][2018-05-24 20:26:10,067][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527164770000 ms.0 from job set of time 1527164770000 ms
[INFO][2018-05-24 20:26:10,079][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:65
[INFO][2018-05-24 20:26:10,079][org.apache.spark.scheduler.DAGScheduler]Got job 17 (foreachPartition at ReceiveKafkaData.scala:65) with 1 output partitions
[INFO][2018-05-24 20:26:10,079][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 17 (foreachPartition at ReceiveKafkaData.scala:65)
[INFO][2018-05-24 20:26:10,079][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:26:10,080][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:26:10,080][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 17 (MapPartitionsRDD[35] at map at ReceiveKafkaData.scala:63), which has no missing parents
[INFO][2018-05-24 20:26:10,081][org.apache.spark.storage.memory.MemoryStore]Block broadcast_17 stored as values in memory (estimated size 3.3 KB, free 912.2 MB)
[INFO][2018-05-24 20:26:10,090][org.apache.spark.storage.memory.MemoryStore]Block broadcast_17_piece0 stored as bytes in memory (estimated size 1878.0 B, free 912.2 MB)
[INFO][2018-05-24 20:26:10,091][org.apache.spark.storage.BlockManagerInfo]Added broadcast_17_piece0 in memory on 192.168.0.102:49677 (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:26:10,092][org.apache.spark.SparkContext]Created broadcast 17 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:26:10,093][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[35] at map at ReceiveKafkaData.scala:63) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:26:10,093][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 17.0 with 1 tasks
[INFO][2018-05-24 20:26:10,093][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 17.0 (TID 17, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:26:10,094][org.apache.spark.executor.Executor]Running task 0.0 in stage 17.0 (TID 17)
[INFO][2018-05-24 20:26:10,095][org.apache.spark.streaming.kafka.KafkaRDD]Computing topic seven, partition 0 offsets 12168 -> 12171
[INFO][2018-05-24 20:26:10,095][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 20:26:10,096][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 20:26:10,096][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 20:26:15,171][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:26:15,172][org.apache.spark.executor.Executor]Finished task 0.0 in stage 17.0 (TID 17). 665 bytes result sent to driver
[INFO][2018-05-24 20:26:15,172][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 17.0 (TID 17) in 5079 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:26:15,172][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 17.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:26:15,173][org.apache.spark.scheduler.DAGScheduler]ResultStage 17 (foreachPartition at ReceiveKafkaData.scala:65) finished in 5.080 s
[INFO][2018-05-24 20:26:15,173][org.apache.spark.scheduler.DAGScheduler]Job 17 finished: foreachPartition at ReceiveKafkaData.scala:65, took 5.094222 s
[INFO][2018-05-24 20:26:15,174][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527164770000 ms.0 from job set of time 1527164770000 ms
[INFO][2018-05-24 20:26:15,174][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 33 from persistence list
[INFO][2018-05-24 20:26:15,174][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 5.174 s for time 1527164770000 ms (execution: 5.108 s)
[INFO][2018-05-24 20:26:15,174][org.apache.spark.storage.BlockManager]Removing RDD 33
[INFO][2018-05-24 20:26:15,175][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 32 from persistence list
[INFO][2018-05-24 20:26:15,175][org.apache.spark.storage.BlockManager]Removing RDD 32
[INFO][2018-05-24 20:26:15,175][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:26:15,175][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527164750000 ms
[INFO][2018-05-24 20:26:17,815][org.apache.spark.util.ShutdownHookManager]Shutdown hook called
[INFO][2018-05-24 20:26:17,816][org.apache.spark.util.ShutdownHookManager]Deleting directory /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/spark-56730831-b55e-48e3-b817-1960173255d3
[INFO][2018-05-24 20:26:25,081][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527164780000 ms
[INFO][2018-05-24 20:26:25,082][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527164780000 ms.0 from job set of time 1527164780000 ms
[INFO][2018-05-24 20:26:25,088][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:65
[INFO][2018-05-24 20:26:25,089][org.apache.spark.scheduler.DAGScheduler]Got job 18 (foreachPartition at ReceiveKafkaData.scala:65) with 1 output partitions
[INFO][2018-05-24 20:26:25,089][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 18 (foreachPartition at ReceiveKafkaData.scala:65)
[INFO][2018-05-24 20:26:25,089][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:26:25,089][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:26:25,089][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 18 (MapPartitionsRDD[37] at map at ReceiveKafkaData.scala:63), which has no missing parents
[INFO][2018-05-24 20:26:25,091][org.apache.spark.storage.memory.MemoryStore]Block broadcast_18 stored as values in memory (estimated size 3.3 KB, free 912.2 MB)
[INFO][2018-05-24 20:26:25,097][org.apache.spark.storage.memory.MemoryStore]Block broadcast_18_piece0 stored as bytes in memory (estimated size 1878.0 B, free 912.2 MB)
[INFO][2018-05-24 20:26:25,098][org.apache.spark.storage.BlockManagerInfo]Added broadcast_18_piece0 in memory on 192.168.0.102:49677 (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:26:25,099][org.apache.spark.SparkContext]Created broadcast 18 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:26:25,099][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 18 (MapPartitionsRDD[37] at map at ReceiveKafkaData.scala:63) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:26:25,099][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 18.0 with 1 tasks
[INFO][2018-05-24 20:26:25,100][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 18.0 (TID 18, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:26:25,102][org.apache.spark.executor.Executor]Running task 0.0 in stage 18.0 (TID 18)
[INFO][2018-05-24 20:26:25,105][org.apache.spark.streaming.kafka.KafkaRDD]Computing topic seven, partition 0 offsets 12171 -> 12174
[INFO][2018-05-24 20:26:25,106][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 20:26:25,106][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 20:26:25,107][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 20:26:25,178][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:26:25,180][org.apache.spark.executor.Executor]Finished task 0.0 in stage 18.0 (TID 18). 708 bytes result sent to driver
[INFO][2018-05-24 20:26:25,182][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 18.0 (TID 18) in 82 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:26:25,182][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 18.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:26:25,184][org.apache.spark.scheduler.DAGScheduler]ResultStage 18 (foreachPartition at ReceiveKafkaData.scala:65) finished in 0.083 s
[INFO][2018-05-24 20:26:25,185][org.apache.spark.scheduler.DAGScheduler]Job 18 finished: foreachPartition at ReceiveKafkaData.scala:65, took 0.096651 s
[INFO][2018-05-24 20:26:25,187][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527164780000 ms.0 from job set of time 1527164780000 ms
[INFO][2018-05-24 20:26:25,188][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 5.187 s for time 1527164780000 ms (execution: 0.106 s)
[INFO][2018-05-24 20:26:25,188][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 35 from persistence list
[INFO][2018-05-24 20:26:25,189][org.apache.spark.storage.BlockManager]Removing RDD 35
[INFO][2018-05-24 20:26:25,189][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 34 from persistence list
[INFO][2018-05-24 20:26:25,189][org.apache.spark.storage.BlockManager]Removing RDD 34
[INFO][2018-05-24 20:26:25,190][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:26:25,191][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527164760000 ms
[INFO][2018-05-24 20:26:35,061][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527164790000 ms
[INFO][2018-05-24 20:26:35,062][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527164790000 ms.0 from job set of time 1527164790000 ms
[INFO][2018-05-24 20:26:35,068][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:65
[INFO][2018-05-24 20:26:35,069][org.apache.spark.scheduler.DAGScheduler]Got job 19 (foreachPartition at ReceiveKafkaData.scala:65) with 1 output partitions
[INFO][2018-05-24 20:26:35,069][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 19 (foreachPartition at ReceiveKafkaData.scala:65)
[INFO][2018-05-24 20:26:35,069][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:26:35,069][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:26:35,070][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 19 (MapPartitionsRDD[39] at map at ReceiveKafkaData.scala:63), which has no missing parents
[INFO][2018-05-24 20:26:35,071][org.apache.spark.storage.memory.MemoryStore]Block broadcast_19 stored as values in memory (estimated size 3.3 KB, free 912.2 MB)
[INFO][2018-05-24 20:26:35,078][org.apache.spark.storage.memory.MemoryStore]Block broadcast_19_piece0 stored as bytes in memory (estimated size 1878.0 B, free 912.2 MB)
[INFO][2018-05-24 20:26:35,078][org.apache.spark.storage.BlockManagerInfo]Added broadcast_19_piece0 in memory on 192.168.0.102:49677 (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:26:35,079][org.apache.spark.SparkContext]Created broadcast 19 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:26:35,080][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 19 (MapPartitionsRDD[39] at map at ReceiveKafkaData.scala:63) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:26:35,080][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 19.0 with 1 tasks
[INFO][2018-05-24 20:26:35,080][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 19.0 (TID 19, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:26:35,080][org.apache.spark.executor.Executor]Running task 0.0 in stage 19.0 (TID 19)
[INFO][2018-05-24 20:26:35,082][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12174 is the same as ending offset skipping seven 0
[INFO][2018-05-24 20:26:35,082][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:26:35,082][org.apache.spark.executor.Executor]Finished task 0.0 in stage 19.0 (TID 19). 665 bytes result sent to driver
[INFO][2018-05-24 20:26:35,083][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 19.0 (TID 19) in 3 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:26:35,083][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 19.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:26:35,084][org.apache.spark.scheduler.DAGScheduler]ResultStage 19 (foreachPartition at ReceiveKafkaData.scala:65) finished in 0.003 s
[INFO][2018-05-24 20:26:35,084][org.apache.spark.scheduler.DAGScheduler]Job 19 finished: foreachPartition at ReceiveKafkaData.scala:65, took 0.015479 s
[INFO][2018-05-24 20:26:35,084][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527164790000 ms.0 from job set of time 1527164790000 ms
[INFO][2018-05-24 20:26:35,084][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 5.084 s for time 1527164790000 ms (execution: 0.022 s)
[INFO][2018-05-24 20:26:35,084][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 37 from persistence list
[INFO][2018-05-24 20:26:35,085][org.apache.spark.storage.BlockManager]Removing RDD 37
[INFO][2018-05-24 20:26:35,085][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 36 from persistence list
[INFO][2018-05-24 20:26:35,085][org.apache.spark.storage.BlockManager]Removing RDD 36
[INFO][2018-05-24 20:26:35,085][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:26:35,085][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527164770000 ms
[INFO][2018-05-24 20:26:45,062][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527164800000 ms
[INFO][2018-05-24 20:26:45,063][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527164800000 ms.0 from job set of time 1527164800000 ms
[INFO][2018-05-24 20:26:45,070][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:65
[INFO][2018-05-24 20:26:45,070][org.apache.spark.scheduler.DAGScheduler]Got job 20 (foreachPartition at ReceiveKafkaData.scala:65) with 1 output partitions
[INFO][2018-05-24 20:26:45,071][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 20 (foreachPartition at ReceiveKafkaData.scala:65)
[INFO][2018-05-24 20:26:45,071][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:26:45,071][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:26:45,071][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 20 (MapPartitionsRDD[41] at map at ReceiveKafkaData.scala:63), which has no missing parents
[INFO][2018-05-24 20:26:45,073][org.apache.spark.storage.memory.MemoryStore]Block broadcast_20 stored as values in memory (estimated size 3.3 KB, free 912.2 MB)
[INFO][2018-05-24 20:26:45,077][org.apache.spark.storage.memory.MemoryStore]Block broadcast_20_piece0 stored as bytes in memory (estimated size 1878.0 B, free 912.2 MB)
[INFO][2018-05-24 20:26:45,078][org.apache.spark.storage.BlockManagerInfo]Added broadcast_20_piece0 in memory on 192.168.0.102:49677 (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:26:45,078][org.apache.spark.SparkContext]Created broadcast 20 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:26:45,078][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 20 (MapPartitionsRDD[41] at map at ReceiveKafkaData.scala:63) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:26:45,078][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 20.0 with 1 tasks
[INFO][2018-05-24 20:26:45,079][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 20.0 (TID 20, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:26:45,079][org.apache.spark.executor.Executor]Running task 0.0 in stage 20.0 (TID 20)
[INFO][2018-05-24 20:26:45,080][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12174 is the same as ending offset skipping seven 0
[INFO][2018-05-24 20:26:45,081][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:26:45,081][org.apache.spark.executor.Executor]Finished task 0.0 in stage 20.0 (TID 20). 708 bytes result sent to driver
[INFO][2018-05-24 20:26:45,081][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 20.0 (TID 20) in 2 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:26:45,081][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 20.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:26:45,082][org.apache.spark.scheduler.DAGScheduler]ResultStage 20 (foreachPartition at ReceiveKafkaData.scala:65) finished in 0.003 s
[INFO][2018-05-24 20:26:45,082][org.apache.spark.scheduler.DAGScheduler]Job 20 finished: foreachPartition at ReceiveKafkaData.scala:65, took 0.012678 s
[INFO][2018-05-24 20:26:45,083][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527164800000 ms.0 from job set of time 1527164800000 ms
[INFO][2018-05-24 20:26:45,083][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 5.083 s for time 1527164800000 ms (execution: 0.020 s)
[INFO][2018-05-24 20:26:45,083][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 39 from persistence list
[INFO][2018-05-24 20:26:45,083][org.apache.spark.storage.BlockManager]Removing RDD 39
[INFO][2018-05-24 20:26:45,084][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 38 from persistence list
[INFO][2018-05-24 20:26:45,084][org.apache.spark.storage.BlockManager]Removing RDD 38
[INFO][2018-05-24 20:26:45,084][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:26:45,084][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527164780000 ms
[INFO][2018-05-24 20:27:00,111][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527164810000 ms
[INFO][2018-05-24 20:27:00,112][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527164810000 ms.0 from job set of time 1527164810000 ms
[INFO][2018-05-24 20:27:00,129][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_12_piece0 on 192.168.0.102:49677 in memory (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:27:00,129][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:65
[INFO][2018-05-24 20:27:00,130][org.apache.spark.scheduler.DAGScheduler]Got job 21 (foreachPartition at ReceiveKafkaData.scala:65) with 1 output partitions
[INFO][2018-05-24 20:27:00,130][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_17_piece0 on 192.168.0.102:49677 in memory (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:27:00,130][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 21 (foreachPartition at ReceiveKafkaData.scala:65)
[INFO][2018-05-24 20:27:00,130][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:27:00,130][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:27:00,130][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 21 (MapPartitionsRDD[43] at map at ReceiveKafkaData.scala:63), which has no missing parents
[INFO][2018-05-24 20:27:00,131][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_20_piece0 on 192.168.0.102:49677 in memory (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:27:00,131][org.apache.spark.storage.memory.MemoryStore]Block broadcast_21 stored as values in memory (estimated size 3.3 KB, free 912.2 MB)
[INFO][2018-05-24 20:27:00,132][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_6_piece0 on 192.168.0.102:49677 in memory (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:27:00,132][org.apache.spark.storage.memory.MemoryStore]Block broadcast_21_piece0 stored as bytes in memory (estimated size 1878.0 B, free 912.2 MB)
[INFO][2018-05-24 20:27:00,133][org.apache.spark.storage.BlockManagerInfo]Added broadcast_21_piece0 in memory on 192.168.0.102:49677 (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:27:00,133][org.apache.spark.SparkContext]Created broadcast 21 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:27:00,134][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_2_piece0 on 192.168.0.102:49677 in memory (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:27:00,134][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 21 (MapPartitionsRDD[43] at map at ReceiveKafkaData.scala:63) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:27:00,134][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 21.0 with 1 tasks
[INFO][2018-05-24 20:27:00,134][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 21.0 (TID 21, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:27:00,135][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_8_piece0 on 192.168.0.102:49677 in memory (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:27:00,135][org.apache.spark.executor.Executor]Running task 0.0 in stage 21.0 (TID 21)
[INFO][2018-05-24 20:27:00,136][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12174 is the same as ending offset skipping seven 0
[INFO][2018-05-24 20:27:00,136][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:27:00,137][org.apache.spark.executor.Executor]Finished task 0.0 in stage 21.0 (TID 21). 665 bytes result sent to driver
[INFO][2018-05-24 20:27:00,137][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_9_piece0 on 192.168.0.102:49677 in memory (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:27:00,138][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 21.0 (TID 21) in 4 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:27:00,138][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 21.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:27:00,139][org.apache.spark.scheduler.DAGScheduler]ResultStage 21 (foreachPartition at ReceiveKafkaData.scala:65) finished in 0.004 s
[INFO][2018-05-24 20:27:00,139][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_14_piece0 on 192.168.0.102:49677 in memory (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:27:00,139][org.apache.spark.scheduler.DAGScheduler]Job 21 finished: foreachPartition at ReceiveKafkaData.scala:65, took 0.009671 s
[INFO][2018-05-24 20:27:00,140][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527164810000 ms.0 from job set of time 1527164810000 ms
[INFO][2018-05-24 20:27:00,140][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 10.140 s for time 1527164810000 ms (execution: 0.029 s)
[INFO][2018-05-24 20:27:00,142][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_19_piece0 on 192.168.0.102:49677 in memory (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:27:00,144][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_4_piece0 on 192.168.0.102:49677 in memory (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:27:00,145][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_15_piece0 on 192.168.0.102:49677 in memory (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:27:00,149][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_3_piece0 on 192.168.0.102:49677 in memory (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:27:00,154][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_11_piece0 on 192.168.0.102:49677 in memory (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:27:00,157][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_7_piece0 on 192.168.0.102:49677 in memory (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:27:00,159][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_16_piece0 on 192.168.0.102:49677 in memory (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:27:00,162][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_13_piece0 on 192.168.0.102:49677 in memory (size: 1876.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:27:00,163][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_10_piece0 on 192.168.0.102:49677 in memory (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:27:00,164][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_5_piece0 on 192.168.0.102:49677 in memory (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:27:00,165][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_18_piece0 on 192.168.0.102:49677 in memory (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:27:05,182][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527164820000 ms
[INFO][2018-05-24 20:27:05,182][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 41 from persistence list
[INFO][2018-05-24 20:27:05,182][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527164820000 ms.0 from job set of time 1527164820000 ms
[INFO][2018-05-24 20:27:05,183][org.apache.spark.storage.BlockManager]Removing RDD 41
[INFO][2018-05-24 20:27:05,183][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 40 from persistence list
[INFO][2018-05-24 20:27:05,184][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:27:05,184][org.apache.spark.storage.BlockManager]Removing RDD 40
[INFO][2018-05-24 20:27:05,184][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527164790000 ms
[INFO][2018-05-24 20:27:05,189][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:65
[INFO][2018-05-24 20:27:05,191][org.apache.spark.scheduler.DAGScheduler]Got job 22 (foreachPartition at ReceiveKafkaData.scala:65) with 1 output partitions
[INFO][2018-05-24 20:27:05,191][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 22 (foreachPartition at ReceiveKafkaData.scala:65)
[INFO][2018-05-24 20:27:05,191][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:27:05,191][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:27:05,192][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 22 (MapPartitionsRDD[45] at map at ReceiveKafkaData.scala:63), which has no missing parents
[INFO][2018-05-24 20:27:05,198][org.apache.spark.storage.memory.MemoryStore]Block broadcast_22 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 20:27:05,202][org.apache.spark.storage.memory.MemoryStore]Block broadcast_22_piece0 stored as bytes in memory (estimated size 1878.0 B, free 912.3 MB)
[INFO][2018-05-24 20:27:05,203][org.apache.spark.storage.BlockManagerInfo]Added broadcast_22_piece0 in memory on 192.168.0.102:49677 (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:27:05,203][org.apache.spark.SparkContext]Created broadcast 22 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:27:05,204][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 22 (MapPartitionsRDD[45] at map at ReceiveKafkaData.scala:63) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:27:05,204][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 22.0 with 1 tasks
[INFO][2018-05-24 20:27:05,204][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 22.0 (TID 22, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:27:05,205][org.apache.spark.executor.Executor]Running task 0.0 in stage 22.0 (TID 22)
[INFO][2018-05-24 20:27:05,206][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12174 is the same as ending offset skipping seven 0
[INFO][2018-05-24 20:27:05,206][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:27:05,207][org.apache.spark.executor.Executor]Finished task 0.0 in stage 22.0 (TID 22). 665 bytes result sent to driver
[INFO][2018-05-24 20:27:05,208][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 22.0 (TID 22) in 4 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:27:05,208][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 22.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:27:05,208][org.apache.spark.scheduler.DAGScheduler]ResultStage 22 (foreachPartition at ReceiveKafkaData.scala:65) finished in 0.004 s
[INFO][2018-05-24 20:27:05,208][org.apache.spark.scheduler.DAGScheduler]Job 22 finished: foreachPartition at ReceiveKafkaData.scala:65, took 0.018614 s
[INFO][2018-05-24 20:27:05,209][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527164820000 ms.0 from job set of time 1527164820000 ms
[INFO][2018-05-24 20:27:05,209][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 5.209 s for time 1527164820000 ms (execution: 0.027 s)
[INFO][2018-05-24 20:27:05,209][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 43 from persistence list
[INFO][2018-05-24 20:27:05,209][org.apache.spark.storage.BlockManager]Removing RDD 43
[INFO][2018-05-24 20:27:05,209][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 42 from persistence list
[INFO][2018-05-24 20:27:05,210][org.apache.spark.storage.BlockManager]Removing RDD 42
[INFO][2018-05-24 20:27:05,210][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:27:05,210][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527164800000 ms
[INFO][2018-05-24 20:27:10,062][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527164830000 ms
[INFO][2018-05-24 20:27:10,063][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527164830000 ms.0 from job set of time 1527164830000 ms
[INFO][2018-05-24 20:27:10,069][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:65
[INFO][2018-05-24 20:27:10,071][org.apache.spark.scheduler.DAGScheduler]Got job 23 (foreachPartition at ReceiveKafkaData.scala:65) with 1 output partitions
[INFO][2018-05-24 20:27:10,071][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 23 (foreachPartition at ReceiveKafkaData.scala:65)
[INFO][2018-05-24 20:27:10,071][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:27:10,071][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:27:10,071][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 23 (MapPartitionsRDD[47] at map at ReceiveKafkaData.scala:63), which has no missing parents
[INFO][2018-05-24 20:27:10,073][org.apache.spark.storage.memory.MemoryStore]Block broadcast_23 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 20:27:10,074][org.apache.spark.storage.memory.MemoryStore]Block broadcast_23_piece0 stored as bytes in memory (estimated size 1877.0 B, free 912.3 MB)
[INFO][2018-05-24 20:27:10,077][org.apache.spark.storage.BlockManagerInfo]Added broadcast_23_piece0 in memory on 192.168.0.102:49677 (size: 1877.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:27:10,077][org.apache.spark.SparkContext]Created broadcast 23 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:27:10,078][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 23 (MapPartitionsRDD[47] at map at ReceiveKafkaData.scala:63) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:27:10,078][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 23.0 with 1 tasks
[INFO][2018-05-24 20:27:10,078][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 23.0 (TID 23, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:27:10,079][org.apache.spark.executor.Executor]Running task 0.0 in stage 23.0 (TID 23)
[INFO][2018-05-24 20:27:10,080][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12174 is the same as ending offset skipping seven 0
[INFO][2018-05-24 20:27:10,081][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:27:10,081][org.apache.spark.executor.Executor]Finished task 0.0 in stage 23.0 (TID 23). 665 bytes result sent to driver
[INFO][2018-05-24 20:27:10,082][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 23.0 (TID 23) in 4 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:27:10,082][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 23.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:27:10,083][org.apache.spark.scheduler.DAGScheduler]ResultStage 23 (foreachPartition at ReceiveKafkaData.scala:65) finished in 0.005 s
[INFO][2018-05-24 20:27:10,084][org.apache.spark.scheduler.DAGScheduler]Job 23 finished: foreachPartition at ReceiveKafkaData.scala:65, took 0.014735 s
[INFO][2018-05-24 20:27:10,085][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527164830000 ms.0 from job set of time 1527164830000 ms
[INFO][2018-05-24 20:27:10,086][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.085 s for time 1527164830000 ms (execution: 0.023 s)
[INFO][2018-05-24 20:27:10,086][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 45 from persistence list
[INFO][2018-05-24 20:27:10,087][org.apache.spark.storage.BlockManager]Removing RDD 45
[INFO][2018-05-24 20:27:10,087][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 44 from persistence list
[INFO][2018-05-24 20:27:10,088][org.apache.spark.storage.BlockManager]Removing RDD 44
[INFO][2018-05-24 20:27:10,088][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:27:10,088][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527164810000 ms
[INFO][2018-05-24 20:27:25,076][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527164840000 ms
[INFO][2018-05-24 20:27:25,077][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527164840000 ms.0 from job set of time 1527164840000 ms
[INFO][2018-05-24 20:27:25,081][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:65
[INFO][2018-05-24 20:27:25,081][org.apache.spark.scheduler.DAGScheduler]Got job 24 (foreachPartition at ReceiveKafkaData.scala:65) with 1 output partitions
[INFO][2018-05-24 20:27:25,081][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 24 (foreachPartition at ReceiveKafkaData.scala:65)
[INFO][2018-05-24 20:27:25,081][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:27:25,081][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:27:25,081][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 24 (MapPartitionsRDD[49] at map at ReceiveKafkaData.scala:63), which has no missing parents
[INFO][2018-05-24 20:27:25,083][org.apache.spark.storage.memory.MemoryStore]Block broadcast_24 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 20:27:25,084][org.apache.spark.storage.memory.MemoryStore]Block broadcast_24_piece0 stored as bytes in memory (estimated size 1878.0 B, free 912.3 MB)
[INFO][2018-05-24 20:27:25,084][org.apache.spark.storage.BlockManagerInfo]Added broadcast_24_piece0 in memory on 192.168.0.102:49677 (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:27:25,085][org.apache.spark.SparkContext]Created broadcast 24 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:27:25,085][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 24 (MapPartitionsRDD[49] at map at ReceiveKafkaData.scala:63) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:27:25,085][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 24.0 with 1 tasks
[INFO][2018-05-24 20:27:25,085][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 24.0 (TID 24, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:27:25,086][org.apache.spark.executor.Executor]Running task 0.0 in stage 24.0 (TID 24)
[INFO][2018-05-24 20:27:25,087][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12174 is the same as ending offset skipping seven 0
[INFO][2018-05-24 20:27:25,087][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:27:25,088][org.apache.spark.executor.Executor]Finished task 0.0 in stage 24.0 (TID 24). 665 bytes result sent to driver
[INFO][2018-05-24 20:27:25,088][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 24.0 (TID 24) in 3 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:27:25,088][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 24.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:27:25,088][org.apache.spark.scheduler.DAGScheduler]ResultStage 24 (foreachPartition at ReceiveKafkaData.scala:65) finished in 0.003 s
[INFO][2018-05-24 20:27:25,089][org.apache.spark.scheduler.DAGScheduler]Job 24 finished: foreachPartition at ReceiveKafkaData.scala:65, took 0.008001 s
[INFO][2018-05-24 20:27:25,089][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527164840000 ms.0 from job set of time 1527164840000 ms
[INFO][2018-05-24 20:27:25,089][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 5.089 s for time 1527164840000 ms (execution: 0.012 s)
[INFO][2018-05-24 20:27:25,089][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 47 from persistence list
[INFO][2018-05-24 20:27:25,089][org.apache.spark.storage.BlockManager]Removing RDD 47
[INFO][2018-05-24 20:27:25,090][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 46 from persistence list
[INFO][2018-05-24 20:27:25,090][org.apache.spark.storage.BlockManager]Removing RDD 46
[INFO][2018-05-24 20:27:25,090][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:27:25,090][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527164820000 ms
[INFO][2018-05-24 20:27:40,082][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527164850000 ms
[INFO][2018-05-24 20:27:40,082][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527164850000 ms.0 from job set of time 1527164850000 ms
[INFO][2018-05-24 20:27:40,087][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:65
[INFO][2018-05-24 20:27:40,088][org.apache.spark.scheduler.DAGScheduler]Got job 25 (foreachPartition at ReceiveKafkaData.scala:65) with 1 output partitions
[INFO][2018-05-24 20:27:40,088][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 25 (foreachPartition at ReceiveKafkaData.scala:65)
[INFO][2018-05-24 20:27:40,088][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:27:40,088][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:27:40,088][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 25 (MapPartitionsRDD[51] at map at ReceiveKafkaData.scala:63), which has no missing parents
[INFO][2018-05-24 20:27:40,090][org.apache.spark.storage.memory.MemoryStore]Block broadcast_25 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 20:27:40,091][org.apache.spark.storage.memory.MemoryStore]Block broadcast_25_piece0 stored as bytes in memory (estimated size 1878.0 B, free 912.3 MB)
[INFO][2018-05-24 20:27:40,092][org.apache.spark.storage.BlockManagerInfo]Added broadcast_25_piece0 in memory on 192.168.0.102:49677 (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:27:40,092][org.apache.spark.SparkContext]Created broadcast 25 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:27:40,093][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 25 (MapPartitionsRDD[51] at map at ReceiveKafkaData.scala:63) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:27:40,093][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 25.0 with 1 tasks
[INFO][2018-05-24 20:27:40,094][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 25.0 (TID 25, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:27:40,094][org.apache.spark.executor.Executor]Running task 0.0 in stage 25.0 (TID 25)
[INFO][2018-05-24 20:27:40,095][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12174 is the same as ending offset skipping seven 0
[INFO][2018-05-24 20:27:40,096][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:27:40,096][org.apache.spark.executor.Executor]Finished task 0.0 in stage 25.0 (TID 25). 665 bytes result sent to driver
[INFO][2018-05-24 20:27:40,096][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 25.0 (TID 25) in 3 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:27:40,096][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 25.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:27:40,097][org.apache.spark.scheduler.DAGScheduler]ResultStage 25 (foreachPartition at ReceiveKafkaData.scala:65) finished in 0.004 s
[INFO][2018-05-24 20:27:40,097][org.apache.spark.scheduler.DAGScheduler]Job 25 finished: foreachPartition at ReceiveKafkaData.scala:65, took 0.009705 s
[INFO][2018-05-24 20:27:40,097][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527164850000 ms.0 from job set of time 1527164850000 ms
[INFO][2018-05-24 20:27:40,097][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 10.097 s for time 1527164850000 ms (execution: 0.015 s)
[INFO][2018-05-24 20:27:45,169][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527164860000 ms
[INFO][2018-05-24 20:27:45,169][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 49 from persistence list
[INFO][2018-05-24 20:27:45,169][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527164860000 ms.0 from job set of time 1527164860000 ms
[INFO][2018-05-24 20:27:45,169][org.apache.spark.storage.BlockManager]Removing RDD 49
[INFO][2018-05-24 20:27:45,170][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 48 from persistence list
[INFO][2018-05-24 20:27:45,170][org.apache.spark.storage.BlockManager]Removing RDD 48
[INFO][2018-05-24 20:27:45,171][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:27:45,171][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527164830000 ms
[INFO][2018-05-24 20:27:45,175][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:65
[INFO][2018-05-24 20:27:45,175][org.apache.spark.scheduler.DAGScheduler]Got job 26 (foreachPartition at ReceiveKafkaData.scala:65) with 1 output partitions
[INFO][2018-05-24 20:27:45,175][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 26 (foreachPartition at ReceiveKafkaData.scala:65)
[INFO][2018-05-24 20:27:45,175][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:27:45,175][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:27:45,175][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 26 (MapPartitionsRDD[53] at map at ReceiveKafkaData.scala:63), which has no missing parents
[INFO][2018-05-24 20:27:45,177][org.apache.spark.storage.memory.MemoryStore]Block broadcast_26 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 20:27:45,178][org.apache.spark.storage.memory.MemoryStore]Block broadcast_26_piece0 stored as bytes in memory (estimated size 1878.0 B, free 912.3 MB)
[INFO][2018-05-24 20:27:45,179][org.apache.spark.storage.BlockManagerInfo]Added broadcast_26_piece0 in memory on 192.168.0.102:49677 (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:27:45,180][org.apache.spark.SparkContext]Created broadcast 26 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:27:45,180][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 26 (MapPartitionsRDD[53] at map at ReceiveKafkaData.scala:63) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:27:45,180][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 26.0 with 1 tasks
[INFO][2018-05-24 20:27:45,181][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 26.0 (TID 26, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:27:45,181][org.apache.spark.executor.Executor]Running task 0.0 in stage 26.0 (TID 26)
[INFO][2018-05-24 20:27:45,183][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12174 is the same as ending offset skipping seven 0
[INFO][2018-05-24 20:27:45,183][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:27:45,184][org.apache.spark.executor.Executor]Finished task 0.0 in stage 26.0 (TID 26). 708 bytes result sent to driver
[INFO][2018-05-24 20:27:45,186][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 26.0 (TID 26) in 5 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:27:45,186][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 26.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:27:45,186][org.apache.spark.scheduler.DAGScheduler]ResultStage 26 (foreachPartition at ReceiveKafkaData.scala:65) finished in 0.005 s
[INFO][2018-05-24 20:27:45,187][org.apache.spark.scheduler.DAGScheduler]Job 26 finished: foreachPartition at ReceiveKafkaData.scala:65, took 0.012194 s
[INFO][2018-05-24 20:27:45,188][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527164860000 ms.0 from job set of time 1527164860000 ms
[INFO][2018-05-24 20:27:45,188][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 5.187 s for time 1527164860000 ms (execution: 0.018 s)
[INFO][2018-05-24 20:27:45,188][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 51 from persistence list
[INFO][2018-05-24 20:27:45,188][org.apache.spark.storage.BlockManager]Removing RDD 51
[INFO][2018-05-24 20:27:45,189][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 50 from persistence list
[INFO][2018-05-24 20:27:45,189][org.apache.spark.storage.BlockManager]Removing RDD 50
[INFO][2018-05-24 20:27:45,189][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:27:45,189][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527164840000 ms
[INFO][2018-05-24 20:27:55,065][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527164870000 ms
[INFO][2018-05-24 20:27:55,066][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527164870000 ms.0 from job set of time 1527164870000 ms
[INFO][2018-05-24 20:27:55,071][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:65
[INFO][2018-05-24 20:27:55,071][org.apache.spark.scheduler.DAGScheduler]Got job 27 (foreachPartition at ReceiveKafkaData.scala:65) with 1 output partitions
[INFO][2018-05-24 20:27:55,071][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 27 (foreachPartition at ReceiveKafkaData.scala:65)
[INFO][2018-05-24 20:27:55,071][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:27:55,071][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:27:55,071][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 27 (MapPartitionsRDD[55] at map at ReceiveKafkaData.scala:63), which has no missing parents
[INFO][2018-05-24 20:27:55,073][org.apache.spark.storage.memory.MemoryStore]Block broadcast_27 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 20:27:55,075][org.apache.spark.storage.memory.MemoryStore]Block broadcast_27_piece0 stored as bytes in memory (estimated size 1878.0 B, free 912.3 MB)
[INFO][2018-05-24 20:27:55,075][org.apache.spark.storage.BlockManagerInfo]Added broadcast_27_piece0 in memory on 192.168.0.102:49677 (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:27:55,075][org.apache.spark.SparkContext]Created broadcast 27 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:27:55,076][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 27 (MapPartitionsRDD[55] at map at ReceiveKafkaData.scala:63) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:27:55,076][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 27.0 with 1 tasks
[INFO][2018-05-24 20:27:55,077][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 27.0 (TID 27, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:27:55,077][org.apache.spark.executor.Executor]Running task 0.0 in stage 27.0 (TID 27)
[INFO][2018-05-24 20:27:55,078][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12174 is the same as ending offset skipping seven 0
[INFO][2018-05-24 20:27:55,078][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:27:55,078][org.apache.spark.executor.Executor]Finished task 0.0 in stage 27.0 (TID 27). 665 bytes result sent to driver
[INFO][2018-05-24 20:27:55,079][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 27.0 (TID 27) in 3 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:27:55,079][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 27.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:27:55,079][org.apache.spark.scheduler.DAGScheduler]ResultStage 27 (foreachPartition at ReceiveKafkaData.scala:65) finished in 0.003 s
[INFO][2018-05-24 20:27:55,083][org.apache.spark.scheduler.DAGScheduler]Job 27 finished: foreachPartition at ReceiveKafkaData.scala:65, took 0.010014 s
[INFO][2018-05-24 20:27:55,083][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527164870000 ms.0 from job set of time 1527164870000 ms
[INFO][2018-05-24 20:27:55,083][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 53 from persistence list
[INFO][2018-05-24 20:27:55,083][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 5.083 s for time 1527164870000 ms (execution: 0.017 s)
[INFO][2018-05-24 20:27:55,084][org.apache.spark.storage.BlockManager]Removing RDD 53
[INFO][2018-05-24 20:27:55,084][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 52 from persistence list
[INFO][2018-05-24 20:27:55,084][org.apache.spark.storage.BlockManager]Removing RDD 52
[INFO][2018-05-24 20:27:55,084][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:27:55,084][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527164850000 ms
[INFO][2018-05-24 20:27:55,574][org.apache.spark.streaming.StreamingContext]Invoking stop(stopGracefully=false) from shutdown hook
[INFO][2018-05-24 20:27:55,578][org.apache.spark.streaming.scheduler.ReceiverTracker]ReceiverTracker stopped
[INFO][2018-05-24 20:27:55,578][org.apache.spark.streaming.scheduler.JobGenerator]Stopping JobGenerator immediately
[INFO][2018-05-24 20:27:55,579][org.apache.spark.streaming.util.RecurringTimer]Stopped timer for JobGenerator after time 1527164870000
[INFO][2018-05-24 20:27:55,580][org.apache.spark.streaming.scheduler.JobGenerator]Stopped JobGenerator
[INFO][2018-05-24 20:27:55,581][org.apache.spark.streaming.scheduler.JobScheduler]Stopped JobScheduler
[INFO][2018-05-24 20:27:55,586][org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@57a48985{/streaming,null,UNAVAILABLE,@Spark}
[INFO][2018-05-24 20:27:55,586][org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@3f672204{/streaming/batch,null,UNAVAILABLE,@Spark}
[INFO][2018-05-24 20:27:55,587][org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@2d000e80{/static/streaming,null,UNAVAILABLE,@Spark}
[INFO][2018-05-24 20:27:55,588][org.apache.spark.streaming.StreamingContext]StreamingContext stopped successfully
[INFO][2018-05-24 20:27:55,588][org.apache.spark.SparkContext]Invoking stop() from shutdown hook
[INFO][2018-05-24 20:27:55,597][org.spark_project.jetty.server.AbstractConnector]Stopped Spark@6efa953f{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-05-24 20:27:55,598][org.apache.spark.ui.SparkUI]Stopped Spark web UI at http://192.168.0.102:4040
[INFO][2018-05-24 20:27:55,606][org.apache.spark.MapOutputTrackerMasterEndpoint]MapOutputTrackerMasterEndpoint stopped!
[INFO][2018-05-24 20:27:55,624][org.apache.spark.storage.memory.MemoryStore]MemoryStore cleared
[INFO][2018-05-24 20:27:55,625][org.apache.spark.storage.BlockManager]BlockManager stopped
[INFO][2018-05-24 20:27:55,625][org.apache.spark.storage.BlockManagerMaster]BlockManagerMaster stopped
[INFO][2018-05-24 20:27:55,630][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint]OutputCommitCoordinator stopped!
[INFO][2018-05-24 20:27:55,631][org.apache.spark.SparkContext]Successfully stopped SparkContext
[INFO][2018-05-24 20:27:55,631][org.apache.spark.util.ShutdownHookManager]Shutdown hook called
[INFO][2018-05-24 20:27:55,634][org.apache.spark.util.ShutdownHookManager]Deleting directory /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/spark-5f32ab1b-0bfe-40e6-b1ba-8e583b6261ee
[INFO][2018-05-24 20:28:46,928][org.apache.spark.SparkContext]Running Spark version 2.2.0
[INFO][2018-05-24 20:28:47,695][org.apache.spark.SparkContext]Submitted application: ReceiveKafkaData$
[INFO][2018-05-24 20:28:47,743][org.apache.spark.SecurityManager]Changing view acls to: seven
[INFO][2018-05-24 20:28:47,744][org.apache.spark.SecurityManager]Changing modify acls to: seven
[INFO][2018-05-24 20:28:47,749][org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO][2018-05-24 20:28:47,751][org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO][2018-05-24 20:28:47,752][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(seven); groups with view permissions: Set(); users  with modify permissions: Set(seven); groups with modify permissions: Set()
[INFO][2018-05-24 20:28:48,044][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 49838.
[INFO][2018-05-24 20:28:48,077][org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO][2018-05-24 20:28:48,094][org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO][2018-05-24 20:28:48,099][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2018-05-24 20:28:48,099][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
[INFO][2018-05-24 20:28:48,109][org.apache.spark.storage.DiskBlockManager]Created local directory at /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/blockmgr-529e64c9-ba60-4cb3-904e-31a647fe1d1b
[INFO][2018-05-24 20:28:48,124][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 912.3 MB
[INFO][2018-05-24 20:28:48,194][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO][2018-05-24 20:28:48,284][org.spark_project.jetty.util.log]Logging initialized @2311ms
[INFO][2018-05-24 20:28:48,355][org.spark_project.jetty.server.Server]jetty-9.3.z-SNAPSHOT
[INFO][2018-05-24 20:28:48,367][org.spark_project.jetty.server.Server]Started @2399ms
[INFO][2018-05-24 20:28:48,391][org.spark_project.jetty.server.AbstractConnector]Started ServerConnector@6d366c9b{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-05-24 20:28:48,391][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4040.
[INFO][2018-05-24 20:28:48,424][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2c1b9e4b{/jobs,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:28:48,425][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3d08f3f5{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:28:48,426][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1a1da881{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:28:48,430][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@165b8a71{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:28:48,431][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2f058b8a{/stages,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:28:48,433][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3f2ef586{/stages/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:28:48,434][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@76c7beb3{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:28:48,436][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7b139eab{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:28:48,440][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@611df6e3{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:28:48,441][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6273c5a4{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:28:48,442][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@53e211ee{/storage,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:28:48,447][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3d8bbcdc{/storage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:28:48,448][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@117e0fe5{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:28:48,449][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@78aea4b9{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:28:48,450][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4b85880b{/environment,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:28:48,452][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4215838f{/environment/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:28:48,453][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2289aca5{/executors,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:28:48,454][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@184497d1{/executors/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:28:48,455][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6ffab045{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:28:48,456][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3e2943ab{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:28:48,468][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4a9f80d3{/static,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:28:48,471][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4204541c{/,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:28:48,474][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4602c2a9{/api,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:28:48,477][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@69c79f09{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:28:48,478][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5fcacc0{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:28:48,481][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://192.168.0.102:4040
[INFO][2018-05-24 20:28:48,580][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
[INFO][2018-05-24 20:28:48,598][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 49839.
[INFO][2018-05-24 20:28:48,602][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 192.168.0.102:49839
[INFO][2018-05-24 20:28:48,610][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2018-05-24 20:28:48,612][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 192.168.0.102, 49839, None)
[INFO][2018-05-24 20:28:48,623][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 192.168.0.102:49839 with 912.3 MB RAM, BlockManagerId(driver, 192.168.0.102, 49839, None)
[INFO][2018-05-24 20:28:48,631][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 192.168.0.102, 49839, None)
[INFO][2018-05-24 20:28:48,634][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 192.168.0.102, 49839, None)
[INFO][2018-05-24 20:28:48,837][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@64a1923a{/metrics/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:28:48,995][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 20:28:48,998][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 20:28:48,998][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 20:29:04,466][org.apache.spark.streaming.kafka.DirectKafkaInputDStream]Slide time = 10000 ms
[INFO][2018-05-24 20:29:04,466][org.apache.spark.streaming.kafka.DirectKafkaInputDStream]Storage level = Serialized 1x Replicated
[INFO][2018-05-24 20:29:04,467][org.apache.spark.streaming.kafka.DirectKafkaInputDStream]Checkpoint interval = null
[INFO][2018-05-24 20:29:04,468][org.apache.spark.streaming.kafka.DirectKafkaInputDStream]Remember interval = 10000 ms
[INFO][2018-05-24 20:29:04,468][org.apache.spark.streaming.kafka.DirectKafkaInputDStream]Initialized and validated org.apache.spark.streaming.kafka.DirectKafkaInputDStream@d9b4795
[INFO][2018-05-24 20:29:04,468][org.apache.spark.streaming.dstream.MappedDStream]Slide time = 10000 ms
[INFO][2018-05-24 20:29:04,469][org.apache.spark.streaming.dstream.MappedDStream]Storage level = Serialized 1x Replicated
[INFO][2018-05-24 20:29:04,469][org.apache.spark.streaming.dstream.MappedDStream]Checkpoint interval = null
[INFO][2018-05-24 20:29:04,469][org.apache.spark.streaming.dstream.MappedDStream]Remember interval = 10000 ms
[INFO][2018-05-24 20:29:04,469][org.apache.spark.streaming.dstream.MappedDStream]Initialized and validated org.apache.spark.streaming.dstream.MappedDStream@4ed04674
[INFO][2018-05-24 20:29:04,469][org.apache.spark.streaming.dstream.ForEachDStream]Slide time = 10000 ms
[INFO][2018-05-24 20:29:04,469][org.apache.spark.streaming.dstream.ForEachDStream]Storage level = Serialized 1x Replicated
[INFO][2018-05-24 20:29:04,469][org.apache.spark.streaming.dstream.ForEachDStream]Checkpoint interval = null
[INFO][2018-05-24 20:29:04,470][org.apache.spark.streaming.dstream.ForEachDStream]Remember interval = 10000 ms
[INFO][2018-05-24 20:29:04,470][org.apache.spark.streaming.dstream.ForEachDStream]Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@2ddf9d3b
[INFO][2018-05-24 20:29:04,512][org.apache.spark.streaming.util.RecurringTimer]Started timer for JobGenerator at time 1527164950000
[INFO][2018-05-24 20:29:04,513][org.apache.spark.streaming.scheduler.JobGenerator]Started JobGenerator at 1527164950000 ms
[INFO][2018-05-24 20:29:04,514][org.apache.spark.streaming.scheduler.JobScheduler]Started JobScheduler
[INFO][2018-05-24 20:29:04,520][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6972c30a{/streaming,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:29:04,520][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1ab6718{/streaming/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:29:04,521][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5109e8cf{/streaming/batch,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:29:04,522][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@78b41097{/streaming/batch/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:29:04,523][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6c65860d{/static/streaming,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:29:04,524][org.apache.spark.streaming.StreamingContext]StreamingContext started
[INFO][2018-05-24 20:29:10,050][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 20:29:10,050][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 20:29:10,050][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 20:29:15,157][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527164950000 ms
[INFO][2018-05-24 20:29:15,160][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527164950000 ms.0 from job set of time 1527164950000 ms
[INFO][2018-05-24 20:29:15,201][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:65
[INFO][2018-05-24 20:29:15,219][org.apache.spark.scheduler.DAGScheduler]Got job 0 (foreachPartition at ReceiveKafkaData.scala:65) with 1 output partitions
[INFO][2018-05-24 20:29:15,220][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 0 (foreachPartition at ReceiveKafkaData.scala:65)
[INFO][2018-05-24 20:29:15,221][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:29:15,223][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:29:15,245][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 0 (MapPartitionsRDD[1] at map at ReceiveKafkaData.scala:63), which has no missing parents
[INFO][2018-05-24 20:29:15,362][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 20:29:15,397][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0_piece0 stored as bytes in memory (estimated size 1877.0 B, free 912.3 MB)
[INFO][2018-05-24 20:29:15,400][org.apache.spark.storage.BlockManagerInfo]Added broadcast_0_piece0 in memory on 192.168.0.102:49839 (size: 1877.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:29:15,404][org.apache.spark.SparkContext]Created broadcast 0 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:29:15,420][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at map at ReceiveKafkaData.scala:63) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:29:15,421][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 0.0 with 1 tasks
[INFO][2018-05-24 20:29:15,496][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:29:15,506][org.apache.spark.executor.Executor]Running task 0.0 in stage 0.0 (TID 0)
[INFO][2018-05-24 20:29:15,545][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12174 is the same as ending offset skipping seven 0
[INFO][2018-05-24 20:29:15,840][org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper]Process identifier=hconnection-0x361504d6 connecting to ZooKeeper ensemble=vm-xaj-bigdata-da-d01:2181,vm-xaj-bigdata-da-d02:2181,vm-xaj-bigdata-da-d03:2181
[INFO][2018-05-24 20:29:15,847][org.apache.zookeeper.ZooKeeper]Client environment:zookeeper.version=3.4.5-cdh5.7.0--1, built on 04/06/2016 14:24 GMT
[INFO][2018-05-24 20:29:15,847][org.apache.zookeeper.ZooKeeper]Client environment:host.name=192.168.0.102
[INFO][2018-05-24 20:29:15,847][org.apache.zookeeper.ZooKeeper]Client environment:java.version=1.8.0_161
[INFO][2018-05-24 20:29:15,847][org.apache.zookeeper.ZooKeeper]Client environment:java.vendor=Oracle Corporation
[INFO][2018-05-24 20:29:15,848][org.apache.zookeeper.ZooKeeper]Client environment:java.home=/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre
[INFO][2018-05-24 20:29:15,848][org.apache.zookeeper.ZooKeeper]Client environment:java.class.path=/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/charsets.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/deploy.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/cldrdata.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/dnsns.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/jaccess.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/jfxrt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/localedata.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/nashorn.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/sunec.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/sunjce_provider.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/sunpkcs11.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/zipfs.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/javaws.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/jce.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/jfr.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/jfxswt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/jsse.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/management-agent.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/plugin.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/resources.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/rt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/ant-javafx.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/dt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/javafx-mx.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/jconsole.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/packager.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/sa-jdi.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/tools.jar:/Users/seven/project/scala/dataMining/target/classes:/Users/seven/software/maven/repository/org/scala-lang/scala-library/2.11.12/scala-library-2.11.12.jar:/Users/seven/software/maven/repository/org/scala-lang/scala-compiler/2.11.12/scala-compiler-2.11.12.jar:/Users/seven/software/maven/repository/org/scala-lang/modules/scala-xml_2.11/1.0.5/scala-xml_2.11-1.0.5.jar:/Users/seven/software/maven/repository/org/scala-lang/modules/scala-parser-combinators_2.11/1.0.4/scala-parser-combinators_2.11-1.0.4.jar:/Users/seven/software/maven/repository/org/scala-lang/scala-reflect/2.11.12/scala-reflect-2.11.12.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-client/2.6.0-cdh5.7.0/hadoop-client-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-common/2.6.0-cdh5.7.0/hadoop-common-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/xmlenc/xmlenc/0.52/xmlenc-0.52.jar:/Users/seven/software/maven/repository/commons-net/commons-net/3.1/commons-net-3.1.jar:/Users/seven/software/maven/repository/commons-configuration/commons-configuration/1.6/commons-configuration-1.6.jar:/Users/seven/software/maven/repository/commons-digester/commons-digester/1.8/commons-digester-1.8.jar:/Users/seven/software/maven/repository/commons-beanutils/commons-beanutils/1.7.0/commons-beanutils-1.7.0.jar:/Users/seven/software/maven/repository/org/slf4j/slf4j-api/1.7.5/slf4j-api-1.7.5.jar:/Users/seven/software/maven/repository/org/slf4j/slf4j-log4j12/1.7.5/slf4j-log4j12-1.7.5.jar:/Users/seven/software/maven/repository/org/apache/avro/avro/1.7.6-cdh5.7.0/avro-1.7.6-cdh5.7.0.jar:/Users/seven/software/maven/repository/com/google/code/gson/gson/2.2.4/gson-2.2.4.jar:/Users/seven/software/maven/repository/org/apache/curator/curator-client/2.7.1/curator-client-2.7.1.jar:/Users/seven/software/maven/repository/org/apache/curator/curator-recipes/2.7.1/curator-recipes-2.7.1.jar:/Users/seven/software/maven/repository/com/google/code/findbugs/jsr305/3.0.0/jsr305-3.0.0.jar:/Users/seven/software/maven/repository/org/apache/htrace/htrace-core4/4.0.1-incubating/htrace-core4-4.0.1-incubating.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-compress/1.4.1/commons-compress-1.4.1.jar:/Users/seven/software/maven/repository/org/tukaani/xz/1.0/xz-1.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-hdfs/2.6.0-cdh5.7.0/hadoop-hdfs-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/io/netty/netty/3.6.2.Final/netty-3.6.2.Final.jar:/Users/seven/software/maven/repository/xerces/xercesImpl/2.9.1/xercesImpl-2.9.1.jar:/Users/seven/software/maven/repository/xml-apis/xml-apis/1.3.04/xml-apis-1.3.04.jar:/Users/seven/software/maven/repository/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-app/2.6.0-cdh5.7.0/hadoop-mapreduce-client-app-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-common/2.6.0-cdh5.7.0/hadoop-mapreduce-client-common-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-yarn-client/2.6.0-cdh5.7.0/hadoop-yarn-client-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-yarn-server-common/2.6.0-cdh5.7.0/hadoop-yarn-server-common-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-shuffle/2.6.0-cdh5.7.0/hadoop-mapreduce-client-shuffle-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-yarn-api/2.6.0-cdh5.7.0/hadoop-yarn-api-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-core/2.6.0-cdh5.7.0/hadoop-mapreduce-client-core-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-yarn-common/2.6.0-cdh5.7.0/hadoop-yarn-common-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/javax/xml/bind/jaxb-api/2.2.2/jaxb-api-2.2.2.jar:/Users/seven/software/maven/repository/javax/xml/stream/stax-api/1.0-2/stax-api-1.0-2.jar:/Users/seven/software/maven/repository/javax/activation/activation/1.1/activation-1.1.jar:/Users/seven/software/maven/repository/com/sun/jersey/jersey-client/1.9/jersey-client-1.9.jar:/Users/seven/software/maven/repository/org/codehaus/jackson/jackson-xc/1.8.8/jackson-xc-1.8.8.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.6.0-cdh5.7.0/hadoop-mapreduce-client-jobclient-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-aws/2.6.0-cdh5.7.0/hadoop-aws-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/com/amazonaws/aws-java-sdk-s3/1.10.6/aws-java-sdk-s3-1.10.6.jar:/Users/seven/software/maven/repository/com/amazonaws/aws-java-sdk-kms/1.10.6/aws-java-sdk-kms-1.10.6.jar:/Users/seven/software/maven/repository/com/amazonaws/aws-java-sdk-core/1.10.6/aws-java-sdk-core-1.10.6.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-annotations/2.6.0-cdh5.7.0/hadoop-annotations-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-client/1.2.0-cdh5.7.0/hbase-client-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-annotations/1.2.0-cdh5.7.0/hbase-annotations-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-common/1.2.0-cdh5.7.0/hbase-common-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-protocol/1.2.0-cdh5.7.0/hbase-protocol-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/commons-codec/commons-codec/1.9/commons-codec-1.9.jar:/Users/seven/software/maven/repository/commons-io/commons-io/2.4/commons-io-2.4.jar:/Users/seven/software/maven/repository/commons-lang/commons-lang/2.6/commons-lang-2.6.jar:/Users/seven/software/maven/repository/commons-logging/commons-logging/1.2/commons-logging-1.2.jar:/Users/seven/software/maven/repository/com/google/guava/guava/12.0.1/guava-12.0.1.jar:/Users/seven/software/maven/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/Users/seven/software/maven/repository/io/netty/netty-all/4.0.23.Final/netty-all-4.0.23.Final.jar:/Users/seven/software/maven/repository/org/apache/zookeeper/zookeeper/3.4.5-cdh5.7.0/zookeeper-3.4.5-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/htrace/htrace-core/3.2.0-incubating/htrace-core-3.2.0-incubating.jar:/Users/seven/software/maven/repository/org/codehaus/jackson/jackson-mapper-asl/1.8.8/jackson-mapper-asl-1.8.8.jar:/Users/seven/software/maven/repository/org/jruby/jcodings/jcodings/1.0.8/jcodings-1.0.8.jar:/Users/seven/software/maven/repository/org/jruby/joni/joni/2.1.2/joni-2.1.2.jar:/Users/seven/software/maven/repository/com/yammer/metrics/metrics-core/2.2.0/metrics-core-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-auth/2.6.0-cdh5.7.0/hadoop-auth-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/httpcomponents/httpclient/4.2.5/httpclient-4.2.5.jar:/Users/seven/software/maven/repository/org/apache/httpcomponents/httpcore/4.2.4/httpcore-4.2.4.jar:/Users/seven/software/maven/repository/org/apache/directory/server/apacheds-kerberos-codec/2.0.0-M15/apacheds-kerberos-codec-2.0.0-M15.jar:/Users/seven/software/maven/repository/org/apache/directory/server/apacheds-i18n/2.0.0-M15/apacheds-i18n-2.0.0-M15.jar:/Users/seven/software/maven/repository/org/apache/directory/api/api-asn1-api/1.0.0-M20/api-asn1-api-1.0.0-M20.jar:/Users/seven/software/maven/repository/org/apache/directory/api/api-util/1.0.0-M20/api-util-1.0.0-M20.jar:/Users/seven/software/maven/repository/org/apache/curator/curator-framework/2.7.1/curator-framework-2.7.1.jar:/Users/seven/software/maven/repository/com/github/stephenc/findbugs/findbugs-annotations/1.3.9-1/findbugs-annotations-1.3.9-1.jar:/Users/seven/software/maven/repository/junit/junit/4.12/junit-4.12.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-server/1.2.0-cdh5.7.0/hbase-server-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-procedure/1.2.0-cdh5.7.0/hbase-procedure-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-common/1.2.0-cdh5.7.0/hbase-common-1.2.0-cdh5.7.0-tests.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-prefix-tree/1.2.0-cdh5.7.0/hbase-prefix-tree-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar:/Users/seven/software/maven/repository/commons-collections/commons-collections/3.2.2/commons-collections-3.2.2.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-hadoop-compat/1.2.0-cdh5.7.0/hbase-hadoop-compat-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-hadoop2-compat/1.2.0-cdh5.7.0/hbase-hadoop2-compat-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/com/sun/jersey/jersey-core/1.9/jersey-core-1.9.jar:/Users/seven/software/maven/repository/com/sun/jersey/jersey-server/1.9/jersey-server-1.9.jar:/Users/seven/software/maven/repository/asm/asm/3.1/asm-3.1.jar:/Users/seven/software/maven/repository/commons-cli/commons-cli/1.2/commons-cli-1.2.jar:/Users/seven/software/maven/repository/com/github/stephenc/high-scale-lib/high-scale-lib/1.1.1/high-scale-lib-1.1.1.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-math/2.1/commons-math-2.1.jar:/Users/seven/software/maven/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jetty/6.1.26.cloudera.4/jetty-6.1.26.cloudera.4.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jetty-util/6.1.26.cloudera.4/jetty-util-6.1.26.cloudera.4.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jetty-sslengine/6.1.26.cloudera.4/jetty-sslengine-6.1.26.cloudera.4.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jsp-2.1/6.1.14/jsp-2.1-6.1.14.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jsp-api-2.1/6.1.14/jsp-api-2.1-6.1.14.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/servlet-api-2.5/6.1.14/servlet-api-2.5-6.1.14.jar:/Users/seven/software/maven/repository/org/codehaus/jackson/jackson-core-asl/1.8.8/jackson-core-asl-1.8.8.jar:/Users/seven/software/maven/repository/org/codehaus/jackson/jackson-jaxrs/1.8.8/jackson-jaxrs-1.8.8.jar:/Users/seven/software/maven/repository/tomcat/jasper-compiler/5.5.23/jasper-compiler-5.5.23.jar:/Users/seven/software/maven/repository/tomcat/jasper-runtime/5.5.23/jasper-runtime-5.5.23.jar:/Users/seven/software/maven/repository/commons-el/commons-el/1.0/commons-el-1.0.jar:/Users/seven/software/maven/repository/org/jamon/jamon-runtime/2.4.1/jamon-runtime-2.4.1.jar:/Users/seven/software/maven/repository/com/lmax/disruptor/3.3.0/disruptor-3.3.0.jar:/Users/seven/software/maven/repository/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar:/Users/seven/software/maven/repository/org/owasp/esapi/esapi/2.1.0/esapi-2.1.0.jar:/Users/seven/software/maven/repository/commons-beanutils/commons-beanutils-core/1.7.0/commons-beanutils-core-1.7.0.jar:/Users/seven/software/maven/repository/commons-fileupload/commons-fileupload/1.2/commons-fileupload-1.2.jar:/Users/seven/software/maven/repository/xom/xom/1.2.5/xom-1.2.5.jar:/Users/seven/software/maven/repository/xalan/xalan/2.7.0/xalan-2.7.0.jar:/Users/seven/software/maven/repository/org/beanshell/bsh-core/2.0b4/bsh-core-2.0b4.jar:/Users/seven/software/maven/repository/org/owasp/antisamy/antisamy/1.4.3/antisamy-1.4.3.jar:/Users/seven/software/maven/repository/org/apache/xmlgraphics/batik-css/1.7/batik-css-1.7.jar:/Users/seven/software/maven/repository/org/apache/xmlgraphics/batik-ext/1.7/batik-ext-1.7.jar:/Users/seven/software/maven/repository/org/apache/xmlgraphics/batik-util/1.7/batik-util-1.7.jar:/Users/seven/software/maven/repository/xml-apis/xml-apis-ext/1.3.04/xml-apis-ext-1.3.04.jar:/Users/seven/software/maven/repository/net/sourceforge/nekohtml/nekohtml/1.9.12/nekohtml-1.9.12.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-core/2.6.0-mr1-cdh5.7.0/hadoop-core-2.6.0-mr1-cdh5.7.0.jar:/Users/seven/software/maven/repository/javax/servlet/jsp/jsp-api/2.1/jsp-api-2.1.jar:/Users/seven/software/maven/repository/net/java/dev/jets3t/jets3t/0.6.1/jets3t-0.6.1.jar:/Users/seven/software/maven/repository/javax/servlet/servlet-api/2.5/servlet-api-2.5.jar:/Users/seven/software/maven/repository/hsqldb/hsqldb/1.8.0.10/hsqldb-1.8.0.10.jar:/Users/seven/software/maven/repository/org/eclipse/jdt/core/3.1.1/core-3.1.1.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-hdfs/2.6.0-cdh5.7.0/hadoop-hdfs-2.6.0-cdh5.7.0-tests.jar:/Users/seven/software/maven/repository/commons-daemon/commons-daemon/1.0.13/commons-daemon-1.0.13.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-lang3/3.5/commons-lang3-3.5.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-pool2/2.4.2/commons-pool2-2.4.2.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-mllib_2.11/2.2.0/spark-mllib_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-core_2.11/2.2.0/spark-core_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/avro/avro-mapred/1.7.7/avro-mapred-1.7.7-hadoop2.jar:/Users/seven/software/maven/repository/org/apache/avro/avro-ipc/1.7.7/avro-ipc-1.7.7-tests.jar:/Users/seven/software/maven/repository/com/twitter/chill_2.11/0.8.0/chill_2.11-0.8.0.jar:/Users/seven/software/maven/repository/com/esotericsoftware/kryo-shaded/3.0.3/kryo-shaded-3.0.3.jar:/Users/seven/software/maven/repository/com/esotericsoftware/minlog/1.3.0/minlog-1.3.0.jar:/Users/seven/software/maven/repository/org/objenesis/objenesis/2.1/objenesis-2.1.jar:/Users/seven/software/maven/repository/com/twitter/chill-java/0.8.0/chill-java-0.8.0.jar:/Users/seven/software/maven/repository/org/apache/xbean/xbean-asm5-shaded/4.4/xbean-asm5-shaded-4.4.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-launcher_2.11/2.2.0/spark-launcher_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-network-common_2.11/2.2.0/spark-network-common_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-network-shuffle_2.11/2.2.0/spark-network-shuffle_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-unsafe_2.11/2.2.0/spark-unsafe_2.11-2.2.0.jar:/Users/seven/software/maven/repository/javax/servlet/javax.servlet-api/3.1.0/javax.servlet-api-3.1.0.jar:/Users/seven/software/maven/repository/org/slf4j/jul-to-slf4j/1.7.16/jul-to-slf4j-1.7.16.jar:/Users/seven/software/maven/repository/org/slf4j/jcl-over-slf4j/1.7.16/jcl-over-slf4j-1.7.16.jar:/Users/seven/software/maven/repository/com/ning/compress-lzf/1.0.3/compress-lzf-1.0.3.jar:/Users/seven/software/maven/repository/org/xerial/snappy/snappy-java/1.1.2.6/snappy-java-1.1.2.6.jar:/Users/seven/software/maven/repository/org/roaringbitmap/RoaringBitmap/0.5.11/RoaringBitmap-0.5.11.jar:/Users/seven/software/maven/repository/org/json4s/json4s-jackson_2.11/3.2.11/json4s-jackson_2.11-3.2.11.jar:/Users/seven/software/maven/repository/org/json4s/json4s-core_2.11/3.2.11/json4s-core_2.11-3.2.11.jar:/Users/seven/software/maven/repository/org/json4s/json4s-ast_2.11/3.2.11/json4s-ast_2.11-3.2.11.jar:/Users/seven/software/maven/repository/org/scala-lang/scalap/2.11.0/scalap-2.11.0.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/core/jersey-client/2.22.2/jersey-client-2.22.2.jar:/Users/seven/software/maven/repository/javax/ws/rs/javax.ws.rs-api/2.0.1/javax.ws.rs-api-2.0.1.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/hk2-api/2.4.0-b34/hk2-api-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/hk2-utils/2.4.0-b34/hk2-utils-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/external/aopalliance-repackaged/2.4.0-b34/aopalliance-repackaged-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/external/javax.inject/2.4.0-b34/javax.inject-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/hk2-locator/2.4.0-b34/hk2-locator-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/javassist/javassist/3.18.1-GA/javassist-3.18.1-GA.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/core/jersey-common/2.22.2/jersey-common-2.22.2.jar:/Users/seven/software/maven/repository/javax/annotation/javax.annotation-api/1.2/javax.annotation-api-1.2.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/bundles/repackaged/jersey-guava/2.22.2/jersey-guava-2.22.2.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/osgi-resource-locator/1.0.1/osgi-resource-locator-1.0.1.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/core/jersey-server/2.22.2/jersey-server-2.22.2.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/media/jersey-media-jaxb/2.22.2/jersey-media-jaxb-2.22.2.jar:/Users/seven/software/maven/repository/javax/validation/validation-api/1.1.0.Final/validation-api-1.1.0.Final.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/containers/jersey-container-servlet/2.22.2/jersey-container-servlet-2.22.2.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/containers/jersey-container-servlet-core/2.22.2/jersey-container-servlet-core-2.22.2.jar:/Users/seven/software/maven/repository/com/clearspring/analytics/stream/2.7.0/stream-2.7.0.jar:/Users/seven/software/maven/repository/io/dropwizard/metrics/metrics-core/3.1.2/metrics-core-3.1.2.jar:/Users/seven/software/maven/repository/io/dropwizard/metrics/metrics-jvm/3.1.2/metrics-jvm-3.1.2.jar:/Users/seven/software/maven/repository/io/dropwizard/metrics/metrics-json/3.1.2/metrics-json-3.1.2.jar:/Users/seven/software/maven/repository/io/dropwizard/metrics/metrics-graphite/3.1.2/metrics-graphite-3.1.2.jar:/Users/seven/software/maven/repository/org/apache/ivy/ivy/2.4.0/ivy-2.4.0.jar:/Users/seven/software/maven/repository/oro/oro/2.0.8/oro-2.0.8.jar:/Users/seven/software/maven/repository/net/razorvine/pyrolite/4.13/pyrolite-4.13.jar:/Users/seven/software/maven/repository/net/sf/py4j/py4j/0.10.4/py4j-0.10.4.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-crypto/1.0.0/commons-crypto-1.0.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-streaming_2.11/2.2.0/spark-streaming_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-sql_2.11/2.2.0/spark-sql_2.11-2.2.0.jar:/Users/seven/software/maven/repository/com/univocity/univocity-parsers/2.2.1/univocity-parsers-2.2.1.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-sketch_2.11/2.2.0/spark-sketch_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-catalyst_2.11/2.2.0/spark-catalyst_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/codehaus/janino/janino/3.0.0/janino-3.0.0.jar:/Users/seven/software/maven/repository/org/codehaus/janino/commons-compiler/3.0.0/commons-compiler-3.0.0.jar:/Users/seven/software/maven/repository/org/antlr/antlr4-runtime/4.5.3/antlr4-runtime-4.5.3.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-column/1.8.2/parquet-column-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-common/1.8.2/parquet-common-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-encoding/1.8.2/parquet-encoding-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-hadoop/1.8.2/parquet-hadoop-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-format/2.3.1/parquet-format-2.3.1.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-jackson/1.8.2/parquet-jackson-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-graphx_2.11/2.2.0/spark-graphx_2.11-2.2.0.jar:/Users/seven/software/maven/repository/com/github/fommil/netlib/core/1.1.2/core-1.1.2.jar:/Users/seven/software/maven/repository/net/sourceforge/f2j/arpack_combined_all/0.1/arpack_combined_all-0.1.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-mllib-local_2.11/2.2.0/spark-mllib-local_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/scalanlp/breeze_2.11/0.13.1/breeze_2.11-0.13.1.jar:/Users/seven/software/maven/repository/org/scalanlp/breeze-macros_2.11/0.13.1/breeze-macros_2.11-0.13.1.jar:/Users/seven/software/maven/repository/net/sf/opencsv/opencsv/2.3/opencsv-2.3.jar:/Users/seven/software/maven/repository/com/github/rwl/jtransforms/2.4.0/jtransforms-2.4.0.jar:/Users/seven/software/maven/repository/org/spire-math/spire_2.11/0.13.0/spire_2.11-0.13.0.jar:/Users/seven/software/maven/repository/org/spire-math/spire-macros_2.11/0.13.0/spire-macros_2.11-0.13.0.jar:/Users/seven/software/maven/repository/org/typelevel/machinist_2.11/0.6.1/machinist_2.11-0.6.1.jar:/Users/seven/software/maven/repository/com/chuusai/shapeless_2.11/2.3.2/shapeless_2.11-2.3.2.jar:/Users/seven/software/maven/repository/org/typelevel/macro-compat_2.11/1.1.1/macro-compat_2.11-1.1.1.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-math3/3.4.1/commons-math3-3.4.1.jar:/Users/seven/software/maven/repository/org/jpmml/pmml-model/1.2.15/pmml-model-1.2.15.jar:/Users/seven/software/maven/repository/org/jpmml/pmml-schema/1.2.15/pmml-schema-1.2.15.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-tags_2.11/2.2.0/spark-tags_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-streaming-kafka-0-8_2.11/2.2.0/spark-streaming-kafka-0-8_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/kafka/kafka_2.11/0.8.2.1/kafka_2.11-0.8.2.1.jar:/Users/seven/software/maven/repository/com/101tec/zkclient/0.3/zkclient-0.3.jar:/Users/seven/software/maven/repository/org/apache/kafka/kafka-clients/0.8.2.1/kafka-clients-0.8.2.1.jar:/Users/seven/software/maven/repository/mysql/mysql-connector-java/5.1.31/mysql-connector-java-5.1.31.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/module/jackson-module-scala_2.11/2.6.5/jackson-module-scala_2.11-2.6.5.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/core/jackson-core/2.6.5/jackson-core-2.6.5.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/core/jackson-annotations/2.6.5/jackson-annotations-2.6.5.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/core/jackson-databind/2.6.5/jackson-databind-2.6.5.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/module/jackson-module-paranamer/2.6.5/jackson-module-paranamer-2.6.5.jar:/Users/seven/software/maven/repository/com/thoughtworks/paranamer/paranamer/2.6/paranamer-2.6.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-streaming-flume_2.11/2.2.0/spark-streaming-flume_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/flume/flume-ng-core/1.6.0/flume-ng-core-1.6.0.jar:/Users/seven/software/maven/repository/org/apache/flume/flume-ng-configuration/1.6.0/flume-ng-configuration-1.6.0.jar:/Users/seven/software/maven/repository/org/apache/avro/avro-ipc/1.7.4/avro-ipc-1.7.4.jar:/Users/seven/software/maven/repository/org/apache/velocity/velocity/1.7/velocity-1.7.jar:/Users/seven/software/maven/repository/joda-time/joda-time/2.1/joda-time-2.1.jar:/Users/seven/software/maven/repository/org/apache/mina/mina-core/2.0.4/mina-core-2.0.4.jar:/Users/seven/software/maven/repository/org/apache/flume/flume-ng-sdk/1.6.0/flume-ng-sdk-1.6.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-streaming-flume-sink_2.11/2.2.0/spark-streaming-flume-sink_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/flume/flume-ng-clients/flume-ng-log4jappender/1.6.0/flume-ng-log4jappender-1.6.0.jar:/Users/seven/software/maven/repository/net/jpountz/lz4/lz4/1.3.0/lz4-1.3.0.jar:/Applications/IntelliJ IDEA.app/Contents/lib/idea_rt.jar
[INFO][2018-05-24 20:29:15,849][org.apache.zookeeper.ZooKeeper]Client environment:java.library.path=/Users/seven/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:.
[INFO][2018-05-24 20:29:15,849][org.apache.zookeeper.ZooKeeper]Client environment:java.io.tmpdir=/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/
[INFO][2018-05-24 20:29:15,849][org.apache.zookeeper.ZooKeeper]Client environment:java.compiler=<NA>
[INFO][2018-05-24 20:29:15,849][org.apache.zookeeper.ZooKeeper]Client environment:os.name=Mac OS X
[INFO][2018-05-24 20:29:15,849][org.apache.zookeeper.ZooKeeper]Client environment:os.arch=x86_64
[INFO][2018-05-24 20:29:15,849][org.apache.zookeeper.ZooKeeper]Client environment:os.version=10.13.4
[INFO][2018-05-24 20:29:15,849][org.apache.zookeeper.ZooKeeper]Client environment:user.name=seven
[INFO][2018-05-24 20:29:15,850][org.apache.zookeeper.ZooKeeper]Client environment:user.home=/Users/seven
[INFO][2018-05-24 20:29:15,850][org.apache.zookeeper.ZooKeeper]Client environment:user.dir=/Users/seven/project/scala/dataMining
[INFO][2018-05-24 20:29:15,851][org.apache.zookeeper.ZooKeeper]Initiating client connection, connectString=vm-xaj-bigdata-da-d01:2181,vm-xaj-bigdata-da-d02:2181,vm-xaj-bigdata-da-d03:2181 sessionTimeout=60000 watcher=hconnection-0x361504d60x0, quorum=vm-xaj-bigdata-da-d01:2181,vm-xaj-bigdata-da-d02:2181,vm-xaj-bigdata-da-d03:2181, baseZNode=/hbase
[INFO][2018-05-24 20:29:20,061][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527164960000 ms
[INFO][2018-05-24 20:29:21,051][org.apache.spark.SparkContext]Running Spark version 2.2.0
[INFO][2018-05-24 20:29:21,839][org.apache.spark.SparkContext]Submitted application: SimulationKafkaSendOutData$
[INFO][2018-05-24 20:29:21,865][org.apache.spark.SecurityManager]Changing view acls to: seven
[INFO][2018-05-24 20:29:21,866][org.apache.spark.SecurityManager]Changing modify acls to: seven
[INFO][2018-05-24 20:29:21,867][org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO][2018-05-24 20:29:21,868][org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO][2018-05-24 20:29:21,868][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(seven); groups with view permissions: Set(); users  with modify permissions: Set(seven); groups with modify permissions: Set()
[INFO][2018-05-24 20:29:22,165][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 49904.
[INFO][2018-05-24 20:29:22,188][org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO][2018-05-24 20:29:22,203][org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO][2018-05-24 20:29:22,206][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2018-05-24 20:29:22,206][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
[INFO][2018-05-24 20:29:22,215][org.apache.spark.storage.DiskBlockManager]Created local directory at /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/blockmgr-97cf478f-6d94-4ba9-a362-be7ae7101bc2
[INFO][2018-05-24 20:29:22,233][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 912.3 MB
[INFO][2018-05-24 20:29:22,310][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO][2018-05-24 20:29:22,383][org.spark_project.jetty.util.log]Logging initialized @2585ms
[INFO][2018-05-24 20:29:22,464][org.spark_project.jetty.server.Server]jetty-9.3.z-SNAPSHOT
[INFO][2018-05-24 20:29:22,485][org.spark_project.jetty.server.Server]Started @2688ms
[WARN][2018-05-24 20:29:22,502][org.apache.spark.util.Utils]Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[INFO][2018-05-24 20:29:22,507][org.spark_project.jetty.server.AbstractConnector]Started ServerConnector@60ca0db6{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}
[INFO][2018-05-24 20:29:22,507][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4041.
[INFO][2018-05-24 20:29:22,529][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@180e6ac4{/jobs,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:29:22,530][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@c7a975a{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:29:22,530][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@757d6814{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:29:22,531][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4c168660{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:29:22,532][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@fd0e5b6{/stages,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:29:22,532][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@36b0fcd5{/stages/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:29:22,533][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@475835b1{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:29:22,534][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@77192705{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:29:22,534][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7e809b79{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:29:22,535][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@625e134e{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:29:22,535][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@89c10b7{/storage,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:29:22,536][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4fe89c24{/storage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:29:22,537][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3d08f3f5{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:29:22,537][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1a1da881{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:29:22,538][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7fd4acee{/environment,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:29:22,539][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6175619b{/environment/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:29:22,539][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@756cf158{/executors,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:29:22,540][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@751d3241{/executors/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:29:22,541][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@64337702{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:29:22,541][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@30ea8c23{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:29:22,546][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4e76dac{/static,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:29:22,547][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@f9d87b{/,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:29:22,548][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@26fb628{/api,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:29:22,549][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5ac86ba5{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:29:22,550][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2c9399a4{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:29:22,552][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://192.168.0.102:4041
[INFO][2018-05-24 20:29:22,640][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
[INFO][2018-05-24 20:29:22,658][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 49905.
[INFO][2018-05-24 20:29:22,658][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 192.168.0.102:49905
[INFO][2018-05-24 20:29:22,660][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2018-05-24 20:29:22,661][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 192.168.0.102, 49905, None)
[INFO][2018-05-24 20:29:22,664][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 192.168.0.102:49905 with 912.3 MB RAM, BlockManagerId(driver, 192.168.0.102, 49905, None)
[INFO][2018-05-24 20:29:22,667][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 192.168.0.102, 49905, None)
[INFO][2018-05-24 20:29:22,668][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 192.168.0.102, 49905, None)
[INFO][2018-05-24 20:29:22,824][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@405325cf{/metrics/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:29:23,429][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0 stored as values in memory (estimated size 228.1 KB, free 912.1 MB)
[INFO][2018-05-24 20:29:23,512][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0_piece0 stored as bytes in memory (estimated size 22.1 KB, free 912.1 MB)
[INFO][2018-05-24 20:29:23,515][org.apache.spark.storage.BlockManagerInfo]Added broadcast_0_piece0 in memory on 192.168.0.102:49905 (size: 22.1 KB, free: 912.3 MB)
[INFO][2018-05-24 20:29:23,519][org.apache.spark.SparkContext]Created broadcast 0 from textFile at SimulationKafkaSendOutData.scala:25
[INFO][2018-05-24 20:29:25,929][org.apache.zookeeper.ClientCnxn]Opening socket connection to server master/10.213.4.25:2181. Will not attempt to authenticate using SASL (unknown error)
[INFO][2018-05-24 20:29:25,941][org.apache.zookeeper.ClientCnxn]Socket connection established, initiating session, client: /10.0.8.39:49906, server: master/10.213.4.25:2181
[INFO][2018-05-24 20:29:25,971][org.apache.zookeeper.ClientCnxn]Session establishment complete on server master/10.213.4.25:2181, sessionid = 0x162b4dc56095ea7, negotiated timeout = 60000
[WARN][2018-05-24 20:29:26,422][org.apache.hadoop.util.NativeCodeLoader]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO][2018-05-24 20:29:26,498][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:29:26,507][org.apache.spark.executor.Executor]Finished task 0.0 in stage 0.0 (TID 0). 794 bytes result sent to driver
[INFO][2018-05-24 20:29:26,514][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 0.0 (TID 0) in 11041 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:29:26,515][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:29:26,518][org.apache.spark.scheduler.DAGScheduler]ResultStage 0 (foreachPartition at ReceiveKafkaData.scala:65) finished in 11.064 s
[INFO][2018-05-24 20:29:26,523][org.apache.spark.scheduler.DAGScheduler]Job 0 finished: foreachPartition at ReceiveKafkaData.scala:65, took 11.321840 s
[INFO][2018-05-24 20:29:26,527][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527164950000 ms.0 from job set of time 1527164950000 ms
[INFO][2018-05-24 20:29:26,528][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 16.526 s for time 1527164950000 ms (execution: 11.367 s)
[INFO][2018-05-24 20:29:26,528][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527164960000 ms.0 from job set of time 1527164960000 ms
[INFO][2018-05-24 20:29:26,534][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:29:26,534][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:65
[INFO][2018-05-24 20:29:26,536][org.apache.spark.scheduler.DAGScheduler]Got job 1 (foreachPartition at ReceiveKafkaData.scala:65) with 1 output partitions
[INFO][2018-05-24 20:29:26,536][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 1 (foreachPartition at ReceiveKafkaData.scala:65)
[INFO][2018-05-24 20:29:26,536][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:29:26,536][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:29:26,536][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 1 (MapPartitionsRDD[3] at map at ReceiveKafkaData.scala:63), which has no missing parents
[INFO][2018-05-24 20:29:26,538][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 
[INFO][2018-05-24 20:29:26,539][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 20:29:26,540][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1_piece0 stored as bytes in memory (estimated size 1878.0 B, free 912.3 MB)
[INFO][2018-05-24 20:29:26,541][org.apache.spark.storage.BlockManagerInfo]Added broadcast_1_piece0 in memory on 192.168.0.102:49839 (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:29:26,541][org.apache.spark.SparkContext]Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:29:26,542][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at map at ReceiveKafkaData.scala:63) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:29:26,542][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 1.0 with 1 tasks
[INFO][2018-05-24 20:29:26,543][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:29:26,543][org.apache.spark.executor.Executor]Running task 0.0 in stage 1.0 (TID 1)
[INFO][2018-05-24 20:29:26,561][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12174 is the same as ending offset skipping seven 0
[INFO][2018-05-24 20:29:26,562][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:29:26,565][org.apache.spark.executor.Executor]Finished task 0.0 in stage 1.0 (TID 1). 751 bytes result sent to driver
[INFO][2018-05-24 20:29:26,567][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 1.0 (TID 1) in 24 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:29:26,567][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:29:26,568][org.apache.spark.scheduler.DAGScheduler]ResultStage 1 (foreachPartition at ReceiveKafkaData.scala:65) finished in 0.026 s
[INFO][2018-05-24 20:29:26,568][org.apache.spark.scheduler.DAGScheduler]Job 1 finished: foreachPartition at ReceiveKafkaData.scala:65, took 0.033455 s
[INFO][2018-05-24 20:29:26,569][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527164960000 ms.0 from job set of time 1527164960000 ms
[INFO][2018-05-24 20:29:26,569][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 6.569 s for time 1527164960000 ms (execution: 0.041 s)
[INFO][2018-05-24 20:29:26,570][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 1 from persistence list
[INFO][2018-05-24 20:29:26,574][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 0 from persistence list
[INFO][2018-05-24 20:29:26,574][org.apache.spark.storage.BlockManager]Removing RDD 1
[INFO][2018-05-24 20:29:26,575][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:29:26,575][org.apache.spark.storage.BlockManager]Removing RDD 0
[INFO][2018-05-24 20:29:26,575][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 
[WARN][2018-05-24 20:29:28,935][org.apache.hadoop.util.NativeCodeLoader]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO][2018-05-24 20:29:29,087][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 1
[INFO][2018-05-24 20:29:29,172][org.apache.spark.SparkContext]Starting job: collect at SimulationKafkaSendOutData.scala:25
[INFO][2018-05-24 20:29:29,184][org.apache.spark.scheduler.DAGScheduler]Got job 0 (collect at SimulationKafkaSendOutData.scala:25) with 2 output partitions
[INFO][2018-05-24 20:29:29,184][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 0 (collect at SimulationKafkaSendOutData.scala:25)
[INFO][2018-05-24 20:29:29,185][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:29:29,186][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:29:29,193][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 0 (hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/* MapPartitionsRDD[1] at textFile at SimulationKafkaSendOutData.scala:25), which has no missing parents
[INFO][2018-05-24 20:29:29,210][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1 stored as values in memory (estimated size 3.3 KB, free 912.1 MB)
[INFO][2018-05-24 20:29:29,212][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1_piece0 stored as bytes in memory (estimated size 1969.0 B, free 912.1 MB)
[INFO][2018-05-24 20:29:29,213][org.apache.spark.storage.BlockManagerInfo]Added broadcast_1_piece0 in memory on 192.168.0.102:49905 (size: 1969.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:29:29,213][org.apache.spark.SparkContext]Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:29:29,232][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ResultStage 0 (hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/* MapPartitionsRDD[1] at textFile at SimulationKafkaSendOutData.scala:25) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2018-05-24 20:29:29,233][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 0.0 with 2 tasks
[INFO][2018-05-24 20:29:29,272][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 4883 bytes)
[INFO][2018-05-24 20:29:29,274][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 4883 bytes)
[INFO][2018-05-24 20:29:29,286][org.apache.spark.executor.Executor]Running task 0.0 in stage 0.0 (TID 0)
[INFO][2018-05-24 20:29:29,286][org.apache.spark.executor.Executor]Running task 1.0 in stage 0.0 (TID 1)
[INFO][2018-05-24 20:29:29,344][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/part-00000:10717435+10717436
[INFO][2018-05-24 20:29:29,346][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/part-00000:0+10717435
[INFO][2018-05-24 20:29:38,759][org.apache.spark.storage.memory.MemoryStore]Block taskresult_0 stored as bytes in memory (estimated size 10.4 MB, free 901.7 MB)
[INFO][2018-05-24 20:29:38,762][org.apache.spark.storage.BlockManagerInfo]Added taskresult_0 in memory on 192.168.0.102:49905 (size: 10.4 MB, free: 901.9 MB)
[INFO][2018-05-24 20:29:38,763][org.apache.spark.executor.Executor]Finished task 0.0 in stage 0.0 (TID 0). 10856349 bytes result sent via BlockManager)
[INFO][2018-05-24 20:29:38,806][org.apache.spark.network.client.TransportClientFactory]Successfully created connection to /192.168.0.102:49905 after 25 ms (0 ms spent in bootstraps)
[INFO][2018-05-24 20:29:39,204][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 0.0 (TID 0) in 9943 ms on localhost (executor driver) (1/2)
[INFO][2018-05-24 20:29:39,209][org.apache.spark.storage.BlockManagerInfo]Removed taskresult_0 on 192.168.0.102:49905 in memory (size: 10.4 MB, free: 912.3 MB)
[INFO][2018-05-24 20:29:39,394][org.apache.spark.storage.memory.MemoryStore]Block taskresult_1 stored as bytes in memory (estimated size 10.4 MB, free 901.7 MB)
[INFO][2018-05-24 20:29:39,395][org.apache.spark.storage.BlockManagerInfo]Added taskresult_1 in memory on 192.168.0.102:49905 (size: 10.4 MB, free: 901.9 MB)
[INFO][2018-05-24 20:29:39,395][org.apache.spark.executor.Executor]Finished task 1.0 in stage 0.0 (TID 1). 10855899 bytes result sent via BlockManager)
[INFO][2018-05-24 20:29:39,464][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 0.0 (TID 1) in 10190 ms on localhost (executor driver) (2/2)
[INFO][2018-05-24 20:29:39,464][org.apache.spark.storage.BlockManagerInfo]Removed taskresult_1 on 192.168.0.102:49905 in memory (size: 10.4 MB, free: 912.3 MB)
[INFO][2018-05-24 20:29:39,465][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:29:39,466][org.apache.spark.scheduler.DAGScheduler]ResultStage 0 (collect at SimulationKafkaSendOutData.scala:25) finished in 10.219 s
[INFO][2018-05-24 20:29:39,470][org.apache.spark.scheduler.DAGScheduler]Job 0 finished: collect at SimulationKafkaSendOutData.scala:25, took 10.297329 s
[INFO][2018-05-24 20:29:39,655][org.spark_project.jetty.server.AbstractConnector]Stopped Spark@60ca0db6{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}
[INFO][2018-05-24 20:29:39,658][org.apache.spark.ui.SparkUI]Stopped Spark web UI at http://192.168.0.102:4041
[INFO][2018-05-24 20:29:39,665][org.apache.spark.MapOutputTrackerMasterEndpoint]MapOutputTrackerMasterEndpoint stopped!
[INFO][2018-05-24 20:29:39,677][org.apache.spark.storage.memory.MemoryStore]MemoryStore cleared
[INFO][2018-05-24 20:29:39,677][org.apache.spark.storage.BlockManager]BlockManager stopped
[INFO][2018-05-24 20:29:39,678][org.apache.spark.storage.BlockManagerMaster]BlockManagerMaster stopped
[INFO][2018-05-24 20:29:39,680][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint]OutputCommitCoordinator stopped!
[INFO][2018-05-24 20:29:39,681][org.apache.spark.SparkContext]Successfully stopped SparkContext
[INFO][2018-05-24 20:29:39,690][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	compression.type = none
	metric.reporters = []
	metadata.max.age.ms = 300000
	metadata.fetch.timeout.ms = 60000
	acks = all
	batch.size = 16384
	reconnect.backoff.ms = 10
	bootstrap.servers = [vm-xaj-bigdata-da-d01:9092, vm-xaj-bigdata-da-d02:9092, vm-xaj-bigdata-da-d03:9092, vm-xaj-bigdata-da-d04:9092, vm-xaj-bigdata-da-d05:9092, vm-xaj-bigdata-da-d06:9092, vm-xaj-bigdata-da-d07:9092]
	receive.buffer.bytes = 32768
	retry.backoff.ms = 100
	buffer.memory = 33554432
	timeout.ms = 30000
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	retries = 0
	max.request.size = 1048576
	block.on.buffer.full = true
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
	metrics.sample.window.ms = 30000
	send.buffer.bytes = 131072
	max.in.flight.requests.per.connection = 5
	metrics.num.samples = 2
	linger.ms = 1
	client.id = 

[INFO][2018-05-24 20:29:40,098][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527164970000 ms
[INFO][2018-05-24 20:29:40,100][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527164970000 ms.0 from job set of time 1527164970000 ms
[INFO][2018-05-24 20:29:40,139][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:65
[INFO][2018-05-24 20:29:40,150][org.apache.spark.scheduler.DAGScheduler]Got job 2 (foreachPartition at ReceiveKafkaData.scala:65) with 1 output partitions
[INFO][2018-05-24 20:29:40,150][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 2 (foreachPartition at ReceiveKafkaData.scala:65)
[INFO][2018-05-24 20:29:40,150][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:29:40,151][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:29:40,153][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 2 (MapPartitionsRDD[5] at map at ReceiveKafkaData.scala:63), which has no missing parents
[INFO][2018-05-24 20:29:40,162][org.apache.spark.storage.memory.MemoryStore]Block broadcast_2 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 20:29:40,177][org.apache.spark.storage.memory.MemoryStore]Block broadcast_2_piece0 stored as bytes in memory (estimated size 1878.0 B, free 912.3 MB)
[INFO][2018-05-24 20:29:40,180][org.apache.spark.storage.BlockManagerInfo]Added broadcast_2_piece0 in memory on 192.168.0.102:49839 (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:29:40,181][org.apache.spark.SparkContext]Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:29:40,182][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[5] at map at ReceiveKafkaData.scala:63) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:29:40,183][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 2.0 with 1 tasks
[INFO][2018-05-24 20:29:40,185][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:29:40,186][org.apache.spark.executor.Executor]Running task 0.0 in stage 2.0 (TID 2)
[INFO][2018-05-24 20:29:40,194][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12174 is the same as ending offset skipping seven 0
[INFO][2018-05-24 20:29:40,195][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:29:40,198][org.apache.spark.executor.Executor]Finished task 0.0 in stage 2.0 (TID 2). 708 bytes result sent to driver
[INFO][2018-05-24 20:29:40,200][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 2.0 (TID 2) in 16 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:29:40,200][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:29:40,201][org.apache.spark.scheduler.DAGScheduler]ResultStage 2 (foreachPartition at ReceiveKafkaData.scala:65) finished in 0.017 s
[INFO][2018-05-24 20:29:40,202][org.apache.spark.scheduler.DAGScheduler]Job 2 finished: foreachPartition at ReceiveKafkaData.scala:65, took 0.062815 s
[INFO][2018-05-24 20:29:40,203][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527164970000 ms.0 from job set of time 1527164970000 ms
[INFO][2018-05-24 20:29:40,203][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 10.203 s for time 1527164970000 ms (execution: 0.103 s)
[INFO][2018-05-24 20:29:45,173][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527164980000 ms
[INFO][2018-05-24 20:29:45,175][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 3 from persistence list
[INFO][2018-05-24 20:29:45,180][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527164980000 ms.0 from job set of time 1527164980000 ms
[INFO][2018-05-24 20:29:45,180][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 2 from persistence list
[INFO][2018-05-24 20:29:45,180][org.apache.spark.storage.BlockManager]Removing RDD 3
[INFO][2018-05-24 20:29:45,182][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:29:45,182][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527164950000 ms
[INFO][2018-05-24 20:29:45,182][org.apache.spark.storage.BlockManager]Removing RDD 2
[INFO][2018-05-24 20:29:45,187][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:65
[INFO][2018-05-24 20:29:45,187][org.apache.spark.scheduler.DAGScheduler]Got job 3 (foreachPartition at ReceiveKafkaData.scala:65) with 1 output partitions
[INFO][2018-05-24 20:29:45,187][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 3 (foreachPartition at ReceiveKafkaData.scala:65)
[INFO][2018-05-24 20:29:45,187][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:29:45,188][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:29:45,188][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 3 (MapPartitionsRDD[7] at map at ReceiveKafkaData.scala:63), which has no missing parents
[INFO][2018-05-24 20:29:45,190][org.apache.spark.storage.memory.MemoryStore]Block broadcast_3 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 20:29:45,199][org.apache.spark.storage.memory.MemoryStore]Block broadcast_3_piece0 stored as bytes in memory (estimated size 1878.0 B, free 912.3 MB)
[INFO][2018-05-24 20:29:45,202][org.apache.spark.storage.BlockManagerInfo]Added broadcast_3_piece0 in memory on 192.168.0.102:49839 (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:29:45,203][org.apache.spark.SparkContext]Created broadcast 3 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:29:45,204][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[7] at map at ReceiveKafkaData.scala:63) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:29:45,204][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 3.0 with 1 tasks
[INFO][2018-05-24 20:29:45,205][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 3.0 (TID 3, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:29:45,205][org.apache.spark.executor.Executor]Running task 0.0 in stage 3.0 (TID 3)
[INFO][2018-05-24 20:29:45,209][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12174 is the same as ending offset skipping seven 0
[INFO][2018-05-24 20:29:45,209][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:29:45,210][org.apache.spark.executor.Executor]Finished task 0.0 in stage 3.0 (TID 3). 708 bytes result sent to driver
[INFO][2018-05-24 20:29:45,211][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 3.0 (TID 3) in 6 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:29:45,211][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 3.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:29:45,212][org.apache.spark.scheduler.DAGScheduler]ResultStage 3 (foreachPartition at ReceiveKafkaData.scala:65) finished in 0.007 s
[INFO][2018-05-24 20:29:45,212][org.apache.spark.scheduler.DAGScheduler]Job 3 finished: foreachPartition at ReceiveKafkaData.scala:65, took 0.025134 s
[INFO][2018-05-24 20:29:45,212][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527164980000 ms.0 from job set of time 1527164980000 ms
[INFO][2018-05-24 20:29:45,213][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 5 from persistence list
[INFO][2018-05-24 20:29:45,213][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 5.212 s for time 1527164980000 ms (execution: 0.032 s)
[INFO][2018-05-24 20:29:45,213][org.apache.spark.storage.BlockManager]Removing RDD 5
[INFO][2018-05-24 20:29:45,213][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 4 from persistence list
[INFO][2018-05-24 20:29:45,214][org.apache.spark.storage.BlockManager]Removing RDD 4
[INFO][2018-05-24 20:29:45,214][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:29:45,214][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527164960000 ms
[INFO][2018-05-24 20:29:55,074][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527164990000 ms
[INFO][2018-05-24 20:29:55,074][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527164990000 ms.0 from job set of time 1527164990000 ms
[INFO][2018-05-24 20:29:55,081][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:65
[INFO][2018-05-24 20:29:55,082][org.apache.spark.scheduler.DAGScheduler]Got job 4 (foreachPartition at ReceiveKafkaData.scala:65) with 1 output partitions
[INFO][2018-05-24 20:29:55,082][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 4 (foreachPartition at ReceiveKafkaData.scala:65)
[INFO][2018-05-24 20:29:55,082][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:29:55,082][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:29:55,083][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 4 (MapPartitionsRDD[9] at map at ReceiveKafkaData.scala:63), which has no missing parents
[INFO][2018-05-24 20:29:55,085][org.apache.spark.storage.memory.MemoryStore]Block broadcast_4 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 20:29:55,087][org.apache.spark.storage.memory.MemoryStore]Block broadcast_4_piece0 stored as bytes in memory (estimated size 1878.0 B, free 912.3 MB)
[INFO][2018-05-24 20:29:55,088][org.apache.spark.storage.BlockManagerInfo]Added broadcast_4_piece0 in memory on 192.168.0.102:49839 (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:29:55,089][org.apache.spark.SparkContext]Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:29:55,090][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[9] at map at ReceiveKafkaData.scala:63) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:29:55,090][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 4.0 with 1 tasks
[INFO][2018-05-24 20:29:55,091][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 4.0 (TID 4, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:29:55,092][org.apache.spark.executor.Executor]Running task 0.0 in stage 4.0 (TID 4)
[INFO][2018-05-24 20:29:55,095][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12174 is the same as ending offset skipping seven 0
[INFO][2018-05-24 20:29:55,095][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:29:55,096][org.apache.spark.executor.Executor]Finished task 0.0 in stage 4.0 (TID 4). 708 bytes result sent to driver
[INFO][2018-05-24 20:29:55,097][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 4.0 (TID 4) in 5 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:29:55,097][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 4.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:29:55,097][org.apache.spark.scheduler.DAGScheduler]ResultStage 4 (foreachPartition at ReceiveKafkaData.scala:65) finished in 0.006 s
[INFO][2018-05-24 20:29:55,097][org.apache.spark.scheduler.DAGScheduler]Job 4 finished: foreachPartition at ReceiveKafkaData.scala:65, took 0.016353 s
[INFO][2018-05-24 20:29:55,098][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527164990000 ms.0 from job set of time 1527164990000 ms
[INFO][2018-05-24 20:29:55,098][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 7 from persistence list
[INFO][2018-05-24 20:29:55,098][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 5.098 s for time 1527164990000 ms (execution: 0.024 s)
[INFO][2018-05-24 20:29:55,098][org.apache.spark.storage.BlockManager]Removing RDD 7
[INFO][2018-05-24 20:29:55,098][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 6 from persistence list
[INFO][2018-05-24 20:29:55,099][org.apache.spark.storage.BlockManager]Removing RDD 6
[INFO][2018-05-24 20:29:55,099][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:29:55,100][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527164970000 ms
[INFO][2018-05-24 20:30:05,098][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527165000000 ms
[INFO][2018-05-24 20:30:05,106][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:65
[INFO][2018-05-24 20:30:05,099][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527165000000 ms.0 from job set of time 1527165000000 ms
[INFO][2018-05-24 20:30:05,108][org.apache.spark.scheduler.DAGScheduler]Got job 5 (foreachPartition at ReceiveKafkaData.scala:65) with 1 output partitions
[INFO][2018-05-24 20:30:05,108][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 5 (foreachPartition at ReceiveKafkaData.scala:65)
[INFO][2018-05-24 20:30:05,108][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:30:05,108][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:30:05,108][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 5 (MapPartitionsRDD[11] at map at ReceiveKafkaData.scala:63), which has no missing parents
[INFO][2018-05-24 20:30:05,110][org.apache.spark.storage.memory.MemoryStore]Block broadcast_5 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 20:30:05,112][org.apache.spark.storage.memory.MemoryStore]Block broadcast_5_piece0 stored as bytes in memory (estimated size 1878.0 B, free 912.3 MB)
[INFO][2018-05-24 20:30:05,112][org.apache.spark.storage.BlockManagerInfo]Added broadcast_5_piece0 in memory on 192.168.0.102:49839 (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:30:05,113][org.apache.spark.SparkContext]Created broadcast 5 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:30:05,114][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[11] at map at ReceiveKafkaData.scala:63) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:30:05,114][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 5.0 with 1 tasks
[INFO][2018-05-24 20:30:05,114][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 5.0 (TID 5, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:30:05,115][org.apache.spark.executor.Executor]Running task 0.0 in stage 5.0 (TID 5)
[INFO][2018-05-24 20:30:05,117][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12174 is the same as ending offset skipping seven 0
[INFO][2018-05-24 20:30:05,118][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:30:05,119][org.apache.spark.executor.Executor]Finished task 0.0 in stage 5.0 (TID 5). 708 bytes result sent to driver
[INFO][2018-05-24 20:30:05,120][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 5.0 (TID 5) in 6 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:30:05,120][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 5.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:30:05,120][org.apache.spark.scheduler.DAGScheduler]ResultStage 5 (foreachPartition at ReceiveKafkaData.scala:65) finished in 0.006 s
[INFO][2018-05-24 20:30:05,121][org.apache.spark.scheduler.DAGScheduler]Job 5 finished: foreachPartition at ReceiveKafkaData.scala:65, took 0.013930 s
[INFO][2018-05-24 20:30:05,122][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527165000000 ms.0 from job set of time 1527165000000 ms
[INFO][2018-05-24 20:30:05,122][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 9 from persistence list
[INFO][2018-05-24 20:30:05,122][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 5.122 s for time 1527165000000 ms (execution: 0.023 s)
[INFO][2018-05-24 20:30:05,122][org.apache.spark.storage.BlockManager]Removing RDD 9
[INFO][2018-05-24 20:30:05,122][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 8 from persistence list
[INFO][2018-05-24 20:30:05,123][org.apache.spark.storage.BlockManager]Removing RDD 8
[INFO][2018-05-24 20:30:05,123][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:30:05,123][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527164980000 ms
[INFO][2018-05-24 20:30:10,055][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527165010000 ms
[INFO][2018-05-24 20:30:10,056][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527165010000 ms.0 from job set of time 1527165010000 ms
[INFO][2018-05-24 20:30:10,062][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:65
[INFO][2018-05-24 20:30:10,063][org.apache.spark.scheduler.DAGScheduler]Got job 6 (foreachPartition at ReceiveKafkaData.scala:65) with 1 output partitions
[INFO][2018-05-24 20:30:10,063][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 6 (foreachPartition at ReceiveKafkaData.scala:65)
[INFO][2018-05-24 20:30:10,063][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:30:10,063][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:30:10,064][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 6 (MapPartitionsRDD[13] at map at ReceiveKafkaData.scala:63), which has no missing parents
[INFO][2018-05-24 20:30:10,066][org.apache.spark.storage.memory.MemoryStore]Block broadcast_6 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 20:30:10,067][org.apache.spark.storage.memory.MemoryStore]Block broadcast_6_piece0 stored as bytes in memory (estimated size 1878.0 B, free 912.3 MB)
[INFO][2018-05-24 20:30:10,068][org.apache.spark.storage.BlockManagerInfo]Added broadcast_6_piece0 in memory on 192.168.0.102:49839 (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:30:10,068][org.apache.spark.SparkContext]Created broadcast 6 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:30:10,069][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[13] at map at ReceiveKafkaData.scala:63) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:30:10,069][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 6.0 with 1 tasks
[INFO][2018-05-24 20:30:10,070][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 6.0 (TID 6, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:30:10,070][org.apache.spark.executor.Executor]Running task 0.0 in stage 6.0 (TID 6)
[INFO][2018-05-24 20:30:10,078][org.apache.spark.streaming.kafka.KafkaRDD]Computing topic seven, partition 0 offsets 12174 -> 12175
[INFO][2018-05-24 20:30:10,078][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 20:30:10,078][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 20:30:10,078][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 20:30:15,437][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_1_piece0 on 192.168.0.102:49839 in memory (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:30:15,441][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_4_piece0 on 192.168.0.102:49839 in memory (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:30:15,443][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_5_piece0 on 192.168.0.102:49839 in memory (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:30:15,444][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_2_piece0 on 192.168.0.102:49839 in memory (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:30:15,445][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_3_piece0 on 192.168.0.102:49839 in memory (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:30:15,619][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:30:15,620][org.apache.spark.executor.Executor]Finished task 0.0 in stage 6.0 (TID 6). 751 bytes result sent to driver
[INFO][2018-05-24 20:30:15,621][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 6.0 (TID 6) in 5552 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:30:15,621][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 6.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:30:15,622][org.apache.spark.scheduler.DAGScheduler]ResultStage 6 (foreachPartition at ReceiveKafkaData.scala:65) finished in 5.553 s
[INFO][2018-05-24 20:30:15,622][org.apache.spark.scheduler.DAGScheduler]Job 6 finished: foreachPartition at ReceiveKafkaData.scala:65, took 5.559862 s
[INFO][2018-05-24 20:30:15,623][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527165010000 ms.0 from job set of time 1527165010000 ms
[INFO][2018-05-24 20:30:15,624][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 5.623 s for time 1527165010000 ms (execution: 5.567 s)
[INFO][2018-05-24 20:30:15,624][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 11 from persistence list
[INFO][2018-05-24 20:30:15,624][org.apache.spark.storage.BlockManager]Removing RDD 11
[INFO][2018-05-24 20:30:15,624][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 10 from persistence list
[INFO][2018-05-24 20:30:15,625][org.apache.spark.storage.BlockManager]Removing RDD 10
[INFO][2018-05-24 20:30:15,625][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:30:15,626][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527164990000 ms
[INFO][2018-05-24 20:30:20,063][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527165020000 ms
[INFO][2018-05-24 20:30:20,064][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527165020000 ms.0 from job set of time 1527165020000 ms
[INFO][2018-05-24 20:30:20,074][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:65
[INFO][2018-05-24 20:30:20,075][org.apache.spark.scheduler.DAGScheduler]Got job 7 (foreachPartition at ReceiveKafkaData.scala:65) with 1 output partitions
[INFO][2018-05-24 20:30:20,076][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 7 (foreachPartition at ReceiveKafkaData.scala:65)
[INFO][2018-05-24 20:30:20,076][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:30:20,076][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:30:20,076][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 7 (MapPartitionsRDD[15] at map at ReceiveKafkaData.scala:63), which has no missing parents
[INFO][2018-05-24 20:30:20,079][org.apache.spark.storage.memory.MemoryStore]Block broadcast_7 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 20:30:20,083][org.apache.spark.storage.memory.MemoryStore]Block broadcast_7_piece0 stored as bytes in memory (estimated size 1878.0 B, free 912.3 MB)
[INFO][2018-05-24 20:30:20,084][org.apache.spark.storage.BlockManagerInfo]Added broadcast_7_piece0 in memory on 192.168.0.102:49839 (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:30:20,085][org.apache.spark.SparkContext]Created broadcast 7 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:30:20,085][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[15] at map at ReceiveKafkaData.scala:63) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:30:20,086][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 7.0 with 1 tasks
[INFO][2018-05-24 20:30:20,087][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 7.0 (TID 7, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:30:20,087][org.apache.spark.executor.Executor]Running task 0.0 in stage 7.0 (TID 7)
[INFO][2018-05-24 20:30:20,091][org.apache.spark.streaming.kafka.KafkaRDD]Computing topic seven, partition 0 offsets 12175 -> 12179
[INFO][2018-05-24 20:30:20,091][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 20:30:20,092][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 20:30:20,092][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 20:30:20,225][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:30:20,227][org.apache.spark.executor.Executor]Finished task 0.0 in stage 7.0 (TID 7). 708 bytes result sent to driver
[INFO][2018-05-24 20:30:20,228][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 7.0 (TID 7) in 142 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:30:20,228][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 7.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:30:20,229][org.apache.spark.scheduler.DAGScheduler]ResultStage 7 (foreachPartition at ReceiveKafkaData.scala:65) finished in 0.143 s
[INFO][2018-05-24 20:30:20,229][org.apache.spark.scheduler.DAGScheduler]Job 7 finished: foreachPartition at ReceiveKafkaData.scala:65, took 0.154707 s
[INFO][2018-05-24 20:30:20,230][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527165020000 ms.0 from job set of time 1527165020000 ms
[INFO][2018-05-24 20:30:20,230][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 13 from persistence list
[INFO][2018-05-24 20:30:20,230][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.230 s for time 1527165020000 ms (execution: 0.166 s)
[INFO][2018-05-24 20:30:20,231][org.apache.spark.storage.BlockManager]Removing RDD 13
[INFO][2018-05-24 20:30:20,231][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 12 from persistence list
[INFO][2018-05-24 20:30:20,231][org.apache.spark.storage.BlockManager]Removing RDD 12
[INFO][2018-05-24 20:30:20,231][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:30:20,231][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527165000000 ms
[INFO][2018-05-24 20:30:35,026][org.apache.spark.util.ShutdownHookManager]Shutdown hook called
[INFO][2018-05-24 20:30:35,029][org.apache.spark.util.ShutdownHookManager]Deleting directory /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/spark-c88eeae2-3280-4283-aef7-9436a78dfcb8
[INFO][2018-05-24 20:30:35,067][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527165030000 ms
[INFO][2018-05-24 20:30:35,067][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527165030000 ms.0 from job set of time 1527165030000 ms
[INFO][2018-05-24 20:30:35,075][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:65
[INFO][2018-05-24 20:30:35,075][org.apache.spark.scheduler.DAGScheduler]Got job 8 (foreachPartition at ReceiveKafkaData.scala:65) with 1 output partitions
[INFO][2018-05-24 20:30:35,076][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 8 (foreachPartition at ReceiveKafkaData.scala:65)
[INFO][2018-05-24 20:30:35,076][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:30:35,076][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:30:35,076][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 8 (MapPartitionsRDD[17] at map at ReceiveKafkaData.scala:63), which has no missing parents
[INFO][2018-05-24 20:30:35,078][org.apache.spark.storage.memory.MemoryStore]Block broadcast_8 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 20:30:35,083][org.apache.spark.storage.memory.MemoryStore]Block broadcast_8_piece0 stored as bytes in memory (estimated size 1878.0 B, free 912.3 MB)
[INFO][2018-05-24 20:30:35,084][org.apache.spark.storage.BlockManagerInfo]Added broadcast_8_piece0 in memory on 192.168.0.102:49839 (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:30:35,084][org.apache.spark.SparkContext]Created broadcast 8 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:30:35,085][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[17] at map at ReceiveKafkaData.scala:63) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:30:35,085][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 8.0 with 1 tasks
[INFO][2018-05-24 20:30:35,086][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 8.0 (TID 8, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:30:35,087][org.apache.spark.executor.Executor]Running task 0.0 in stage 8.0 (TID 8)
[INFO][2018-05-24 20:30:35,089][org.apache.spark.streaming.kafka.KafkaRDD]Computing topic seven, partition 0 offsets 12179 -> 12184
[INFO][2018-05-24 20:30:35,089][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 20:30:35,090][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 20:30:35,090][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 20:30:40,192][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:30:40,194][org.apache.spark.executor.Executor]Finished task 0.0 in stage 8.0 (TID 8). 708 bytes result sent to driver
[INFO][2018-05-24 20:30:40,195][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 8.0 (TID 8) in 5108 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:30:40,195][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 8.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:30:40,197][org.apache.spark.scheduler.DAGScheduler]ResultStage 8 (foreachPartition at ReceiveKafkaData.scala:65) finished in 5.111 s
[INFO][2018-05-24 20:30:40,199][org.apache.spark.scheduler.DAGScheduler]Job 8 finished: foreachPartition at ReceiveKafkaData.scala:65, took 5.123408 s
[INFO][2018-05-24 20:30:40,200][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527165030000 ms.0 from job set of time 1527165030000 ms
[INFO][2018-05-24 20:30:40,201][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 10.200 s for time 1527165030000 ms (execution: 5.133 s)
[INFO][2018-05-24 20:30:45,060][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527165040000 ms
[INFO][2018-05-24 20:30:45,061][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 15 from persistence list
[INFO][2018-05-24 20:30:45,061][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527165040000 ms.0 from job set of time 1527165040000 ms
[INFO][2018-05-24 20:30:45,062][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 14 from persistence list
[INFO][2018-05-24 20:30:45,062][org.apache.spark.storage.BlockManager]Removing RDD 15
[INFO][2018-05-24 20:30:45,063][org.apache.spark.storage.BlockManager]Removing RDD 14
[INFO][2018-05-24 20:30:45,063][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:30:45,064][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527165010000 ms
[INFO][2018-05-24 20:30:45,070][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:65
[INFO][2018-05-24 20:30:45,071][org.apache.spark.scheduler.DAGScheduler]Got job 9 (foreachPartition at ReceiveKafkaData.scala:65) with 1 output partitions
[INFO][2018-05-24 20:30:45,071][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 9 (foreachPartition at ReceiveKafkaData.scala:65)
[INFO][2018-05-24 20:30:45,071][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:30:45,071][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:30:45,072][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 9 (MapPartitionsRDD[19] at map at ReceiveKafkaData.scala:63), which has no missing parents
[INFO][2018-05-24 20:30:45,074][org.apache.spark.storage.memory.MemoryStore]Block broadcast_9 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 20:30:45,081][org.apache.spark.storage.memory.MemoryStore]Block broadcast_9_piece0 stored as bytes in memory (estimated size 1878.0 B, free 912.3 MB)
[INFO][2018-05-24 20:30:45,082][org.apache.spark.storage.BlockManagerInfo]Added broadcast_9_piece0 in memory on 192.168.0.102:49839 (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:30:45,083][org.apache.spark.SparkContext]Created broadcast 9 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:30:45,084][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[19] at map at ReceiveKafkaData.scala:63) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:30:45,084][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 9.0 with 1 tasks
[INFO][2018-05-24 20:30:45,087][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 9.0 (TID 9, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:30:45,088][org.apache.spark.executor.Executor]Running task 0.0 in stage 9.0 (TID 9)
[INFO][2018-05-24 20:30:45,090][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12184 is the same as ending offset skipping seven 0
[INFO][2018-05-24 20:30:45,091][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:30:45,092][org.apache.spark.executor.Executor]Finished task 0.0 in stage 9.0 (TID 9). 708 bytes result sent to driver
[INFO][2018-05-24 20:30:45,092][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 9.0 (TID 9) in 5 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:30:45,093][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 9.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:30:45,093][org.apache.spark.scheduler.DAGScheduler]ResultStage 9 (foreachPartition at ReceiveKafkaData.scala:65) finished in 0.008 s
[INFO][2018-05-24 20:30:45,094][org.apache.spark.scheduler.DAGScheduler]Job 9 finished: foreachPartition at ReceiveKafkaData.scala:65, took 0.023504 s
[INFO][2018-05-24 20:30:45,094][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527165040000 ms.0 from job set of time 1527165040000 ms
[INFO][2018-05-24 20:30:45,095][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 5.094 s for time 1527165040000 ms (execution: 0.033 s)
[INFO][2018-05-24 20:30:45,095][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 17 from persistence list
[INFO][2018-05-24 20:30:45,095][org.apache.spark.storage.BlockManager]Removing RDD 17
[INFO][2018-05-24 20:30:45,095][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 16 from persistence list
[INFO][2018-05-24 20:30:45,096][org.apache.spark.storage.BlockManager]Removing RDD 16
[INFO][2018-05-24 20:30:45,096][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:30:45,096][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527165020000 ms
[INFO][2018-05-24 20:30:55,067][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527165050000 ms
[INFO][2018-05-24 20:30:55,068][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527165050000 ms.0 from job set of time 1527165050000 ms
[INFO][2018-05-24 20:30:55,074][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:65
[INFO][2018-05-24 20:30:55,075][org.apache.spark.scheduler.DAGScheduler]Got job 10 (foreachPartition at ReceiveKafkaData.scala:65) with 1 output partitions
[INFO][2018-05-24 20:30:55,075][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 10 (foreachPartition at ReceiveKafkaData.scala:65)
[INFO][2018-05-24 20:30:55,075][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:30:55,075][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:30:55,076][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 10 (MapPartitionsRDD[21] at map at ReceiveKafkaData.scala:63), which has no missing parents
[INFO][2018-05-24 20:30:55,078][org.apache.spark.storage.memory.MemoryStore]Block broadcast_10 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 20:30:55,083][org.apache.spark.storage.memory.MemoryStore]Block broadcast_10_piece0 stored as bytes in memory (estimated size 1878.0 B, free 912.3 MB)
[INFO][2018-05-24 20:30:55,084][org.apache.spark.storage.BlockManagerInfo]Added broadcast_10_piece0 in memory on 192.168.0.102:49839 (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:30:55,084][org.apache.spark.SparkContext]Created broadcast 10 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:30:55,085][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[21] at map at ReceiveKafkaData.scala:63) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:30:55,085][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 10.0 with 1 tasks
[INFO][2018-05-24 20:30:55,086][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 10.0 (TID 10, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:30:55,086][org.apache.spark.executor.Executor]Running task 0.0 in stage 10.0 (TID 10)
[INFO][2018-05-24 20:30:55,088][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12184 is the same as ending offset skipping seven 0
[INFO][2018-05-24 20:30:55,088][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:30:55,089][org.apache.spark.executor.Executor]Finished task 0.0 in stage 10.0 (TID 10). 708 bytes result sent to driver
[INFO][2018-05-24 20:30:55,090][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 10.0 (TID 10) in 5 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:30:55,090][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 10.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:30:55,091][org.apache.spark.scheduler.DAGScheduler]ResultStage 10 (foreachPartition at ReceiveKafkaData.scala:65) finished in 0.006 s
[INFO][2018-05-24 20:30:55,091][org.apache.spark.scheduler.DAGScheduler]Job 10 finished: foreachPartition at ReceiveKafkaData.scala:65, took 0.016832 s
[INFO][2018-05-24 20:30:55,092][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527165050000 ms.0 from job set of time 1527165050000 ms
[INFO][2018-05-24 20:30:55,093][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 19 from persistence list
[INFO][2018-05-24 20:30:55,093][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 5.092 s for time 1527165050000 ms (execution: 0.024 s)
[INFO][2018-05-24 20:30:55,093][org.apache.spark.storage.BlockManager]Removing RDD 19
[INFO][2018-05-24 20:30:55,093][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 18 from persistence list
[INFO][2018-05-24 20:30:55,094][org.apache.spark.storage.BlockManager]Removing RDD 18
[INFO][2018-05-24 20:30:55,095][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:30:55,095][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527165030000 ms
[INFO][2018-05-24 20:31:00,137][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527165060000 ms
[INFO][2018-05-24 20:31:00,138][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527165060000 ms.0 from job set of time 1527165060000 ms
[INFO][2018-05-24 20:31:00,144][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:65
[INFO][2018-05-24 20:31:00,145][org.apache.spark.scheduler.DAGScheduler]Got job 11 (foreachPartition at ReceiveKafkaData.scala:65) with 1 output partitions
[INFO][2018-05-24 20:31:00,145][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 11 (foreachPartition at ReceiveKafkaData.scala:65)
[INFO][2018-05-24 20:31:00,145][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:31:00,145][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:31:00,146][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 11 (MapPartitionsRDD[23] at map at ReceiveKafkaData.scala:63), which has no missing parents
[INFO][2018-05-24 20:31:00,147][org.apache.spark.storage.memory.MemoryStore]Block broadcast_11 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 20:31:00,151][org.apache.spark.storage.memory.MemoryStore]Block broadcast_11_piece0 stored as bytes in memory (estimated size 1878.0 B, free 912.3 MB)
[INFO][2018-05-24 20:31:00,152][org.apache.spark.storage.BlockManagerInfo]Added broadcast_11_piece0 in memory on 192.168.0.102:49839 (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:31:00,152][org.apache.spark.SparkContext]Created broadcast 11 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:31:00,153][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[23] at map at ReceiveKafkaData.scala:63) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:31:00,153][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 11.0 with 1 tasks
[INFO][2018-05-24 20:31:00,154][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 11.0 (TID 11, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:31:00,154][org.apache.spark.executor.Executor]Running task 0.0 in stage 11.0 (TID 11)
[INFO][2018-05-24 20:31:00,156][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12184 is the same as ending offset skipping seven 0
[INFO][2018-05-24 20:31:00,156][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:31:00,157][org.apache.spark.executor.Executor]Finished task 0.0 in stage 11.0 (TID 11). 665 bytes result sent to driver
[INFO][2018-05-24 20:31:00,157][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 11.0 (TID 11) in 3 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:31:00,158][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 11.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:31:00,158][org.apache.spark.scheduler.DAGScheduler]ResultStage 11 (foreachPartition at ReceiveKafkaData.scala:65) finished in 0.004 s
[INFO][2018-05-24 20:31:00,158][org.apache.spark.scheduler.DAGScheduler]Job 11 finished: foreachPartition at ReceiveKafkaData.scala:65, took 0.013833 s
[INFO][2018-05-24 20:31:00,159][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527165060000 ms.0 from job set of time 1527165060000 ms
[INFO][2018-05-24 20:31:00,159][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 21 from persistence list
[INFO][2018-05-24 20:31:00,159][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.159 s for time 1527165060000 ms (execution: 0.021 s)
[INFO][2018-05-24 20:31:00,160][org.apache.spark.storage.BlockManager]Removing RDD 21
[INFO][2018-05-24 20:31:00,160][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 20 from persistence list
[INFO][2018-05-24 20:31:00,160][org.apache.spark.storage.BlockManager]Removing RDD 20
[INFO][2018-05-24 20:31:00,161][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:31:00,161][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527165040000 ms
[INFO][2018-05-24 20:31:15,068][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527165070000 ms
[INFO][2018-05-24 20:31:15,069][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527165070000 ms.0 from job set of time 1527165070000 ms
[INFO][2018-05-24 20:31:15,076][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:65
[INFO][2018-05-24 20:31:15,077][org.apache.spark.scheduler.DAGScheduler]Got job 12 (foreachPartition at ReceiveKafkaData.scala:65) with 1 output partitions
[INFO][2018-05-24 20:31:15,077][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 12 (foreachPartition at ReceiveKafkaData.scala:65)
[INFO][2018-05-24 20:31:15,077][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:31:15,077][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:31:15,077][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 12 (MapPartitionsRDD[25] at map at ReceiveKafkaData.scala:63), which has no missing parents
[INFO][2018-05-24 20:31:15,079][org.apache.spark.storage.memory.MemoryStore]Block broadcast_12 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 20:31:15,084][org.apache.spark.storage.memory.MemoryStore]Block broadcast_12_piece0 stored as bytes in memory (estimated size 1878.0 B, free 912.3 MB)
[INFO][2018-05-24 20:31:15,085][org.apache.spark.storage.BlockManagerInfo]Added broadcast_12_piece0 in memory on 192.168.0.102:49839 (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:31:15,086][org.apache.spark.SparkContext]Created broadcast 12 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:31:15,086][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[25] at map at ReceiveKafkaData.scala:63) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:31:15,086][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 12.0 with 1 tasks
[INFO][2018-05-24 20:31:15,087][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 12.0 (TID 12, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:31:15,088][org.apache.spark.executor.Executor]Running task 0.0 in stage 12.0 (TID 12)
[INFO][2018-05-24 20:31:15,090][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12184 is the same as ending offset skipping seven 0
[INFO][2018-05-24 20:31:15,090][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:31:15,091][org.apache.spark.executor.Executor]Finished task 0.0 in stage 12.0 (TID 12). 708 bytes result sent to driver
[INFO][2018-05-24 20:31:15,091][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 12.0 (TID 12) in 4 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:31:15,091][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 12.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:31:15,092][org.apache.spark.scheduler.DAGScheduler]ResultStage 12 (foreachPartition at ReceiveKafkaData.scala:65) finished in 0.005 s
[INFO][2018-05-24 20:31:15,093][org.apache.spark.scheduler.DAGScheduler]Job 12 finished: foreachPartition at ReceiveKafkaData.scala:65, took 0.016272 s
[INFO][2018-05-24 20:31:15,093][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527165070000 ms.0 from job set of time 1527165070000 ms
[INFO][2018-05-24 20:31:15,094][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 23 from persistence list
[INFO][2018-05-24 20:31:15,094][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 5.093 s for time 1527165070000 ms (execution: 0.024 s)
[INFO][2018-05-24 20:31:15,095][org.apache.spark.storage.BlockManager]Removing RDD 23
[INFO][2018-05-24 20:31:15,095][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 22 from persistence list
[INFO][2018-05-24 20:31:15,096][org.apache.spark.storage.BlockManager]Removing RDD 22
[INFO][2018-05-24 20:31:15,096][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:31:15,097][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527165050000 ms
[INFO][2018-05-24 20:31:20,054][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527165080000 ms
[INFO][2018-05-24 20:31:20,055][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527165080000 ms.0 from job set of time 1527165080000 ms
[INFO][2018-05-24 20:31:20,067][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:65
[INFO][2018-05-24 20:31:20,068][org.apache.spark.scheduler.DAGScheduler]Got job 13 (foreachPartition at ReceiveKafkaData.scala:65) with 1 output partitions
[INFO][2018-05-24 20:31:20,068][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 13 (foreachPartition at ReceiveKafkaData.scala:65)
[INFO][2018-05-24 20:31:20,068][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:31:20,068][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:31:20,068][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 13 (MapPartitionsRDD[27] at map at ReceiveKafkaData.scala:63), which has no missing parents
[INFO][2018-05-24 20:31:20,070][org.apache.spark.storage.memory.MemoryStore]Block broadcast_13 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 20:31:20,073][org.apache.spark.storage.memory.MemoryStore]Block broadcast_13_piece0 stored as bytes in memory (estimated size 1878.0 B, free 912.3 MB)
[INFO][2018-05-24 20:31:20,074][org.apache.spark.storage.BlockManagerInfo]Added broadcast_13_piece0 in memory on 192.168.0.102:49839 (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:31:20,074][org.apache.spark.SparkContext]Created broadcast 13 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:31:20,075][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[27] at map at ReceiveKafkaData.scala:63) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:31:20,075][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 13.0 with 1 tasks
[INFO][2018-05-24 20:31:20,075][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 13.0 (TID 13, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:31:20,076][org.apache.spark.executor.Executor]Running task 0.0 in stage 13.0 (TID 13)
[INFO][2018-05-24 20:31:20,077][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12184 is the same as ending offset skipping seven 0
[INFO][2018-05-24 20:31:20,077][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:31:20,078][org.apache.spark.executor.Executor]Finished task 0.0 in stage 13.0 (TID 13). 665 bytes result sent to driver
[INFO][2018-05-24 20:31:20,079][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 13.0 (TID 13) in 4 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:31:20,079][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 13.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:31:20,080][org.apache.spark.scheduler.DAGScheduler]ResultStage 13 (foreachPartition at ReceiveKafkaData.scala:65) finished in 0.005 s
[INFO][2018-05-24 20:31:20,080][org.apache.spark.scheduler.DAGScheduler]Job 13 finished: foreachPartition at ReceiveKafkaData.scala:65, took 0.012553 s
[INFO][2018-05-24 20:31:20,081][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527165080000 ms.0 from job set of time 1527165080000 ms
[INFO][2018-05-24 20:31:20,081][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.080 s for time 1527165080000 ms (execution: 0.025 s)
[INFO][2018-05-24 20:31:20,081][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 25 from persistence list
[INFO][2018-05-24 20:31:20,082][org.apache.spark.storage.BlockManager]Removing RDD 25
[INFO][2018-05-24 20:31:20,082][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 24 from persistence list
[INFO][2018-05-24 20:31:20,083][org.apache.spark.storage.BlockManager]Removing RDD 24
[INFO][2018-05-24 20:31:20,083][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:31:20,083][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527165060000 ms
[INFO][2018-05-24 20:31:34,658][org.apache.spark.streaming.StreamingContext]Invoking stop(stopGracefully=false) from shutdown hook
[INFO][2018-05-24 20:31:34,663][org.apache.spark.streaming.scheduler.ReceiverTracker]ReceiverTracker stopped
[INFO][2018-05-24 20:31:34,664][org.apache.spark.streaming.scheduler.JobGenerator]Stopping JobGenerator immediately
[INFO][2018-05-24 20:31:34,664][org.apache.spark.streaming.util.RecurringTimer]Stopped timer for JobGenerator after time 1527165090000
[INFO][2018-05-24 20:31:35,066][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527165090000 ms
[INFO][2018-05-24 20:31:35,066][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527165090000 ms.0 from job set of time 1527165090000 ms
[INFO][2018-05-24 20:31:35,069][org.apache.spark.streaming.scheduler.JobGenerator]Stopped JobGenerator
[INFO][2018-05-24 20:31:35,071][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:65
[INFO][2018-05-24 20:31:35,072][org.apache.spark.scheduler.DAGScheduler]Got job 14 (foreachPartition at ReceiveKafkaData.scala:65) with 1 output partitions
[INFO][2018-05-24 20:31:35,072][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 14 (foreachPartition at ReceiveKafkaData.scala:65)
[INFO][2018-05-24 20:31:35,072][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:31:35,072][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:31:35,072][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 14 (MapPartitionsRDD[29] at map at ReceiveKafkaData.scala:63), which has no missing parents
[INFO][2018-05-24 20:31:35,074][org.apache.spark.storage.memory.MemoryStore]Block broadcast_14 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 20:31:35,075][org.apache.spark.storage.memory.MemoryStore]Block broadcast_14_piece0 stored as bytes in memory (estimated size 1878.0 B, free 912.3 MB)
[INFO][2018-05-24 20:31:35,075][org.apache.spark.storage.BlockManagerInfo]Added broadcast_14_piece0 in memory on 192.168.0.102:49839 (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:31:35,076][org.apache.spark.SparkContext]Created broadcast 14 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:31:35,076][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[29] at map at ReceiveKafkaData.scala:63) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:31:35,076][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 14.0 with 1 tasks
[INFO][2018-05-24 20:31:35,077][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 14.0 (TID 14, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:31:35,077][org.apache.spark.executor.Executor]Running task 0.0 in stage 14.0 (TID 14)
[INFO][2018-05-24 20:31:35,079][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12184 is the same as ending offset skipping seven 0
[INFO][2018-05-24 20:31:35,079][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:31:35,079][org.apache.spark.executor.Executor]Finished task 0.0 in stage 14.0 (TID 14). 665 bytes result sent to driver
[INFO][2018-05-24 20:31:35,080][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 14.0 (TID 14) in 3 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:31:35,080][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 14.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:31:35,080][org.apache.spark.scheduler.DAGScheduler]ResultStage 14 (foreachPartition at ReceiveKafkaData.scala:65) finished in 0.003 s
[INFO][2018-05-24 20:31:35,081][org.apache.spark.scheduler.DAGScheduler]Job 14 finished: foreachPartition at ReceiveKafkaData.scala:65, took 0.009407 s
[INFO][2018-05-24 20:31:35,081][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527165090000 ms.0 from job set of time 1527165090000 ms
[INFO][2018-05-24 20:31:35,081][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 5.081 s for time 1527165090000 ms (execution: 0.015 s)
[INFO][2018-05-24 20:31:35,082][org.apache.spark.streaming.scheduler.JobScheduler]Stopped JobScheduler
[INFO][2018-05-24 20:31:35,088][org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@6972c30a{/streaming,null,UNAVAILABLE,@Spark}
[INFO][2018-05-24 20:31:35,089][org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@5109e8cf{/streaming/batch,null,UNAVAILABLE,@Spark}
[INFO][2018-05-24 20:31:35,090][org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@6c65860d{/static/streaming,null,UNAVAILABLE,@Spark}
[INFO][2018-05-24 20:31:35,091][org.apache.spark.streaming.StreamingContext]StreamingContext stopped successfully
[INFO][2018-05-24 20:31:35,091][org.apache.spark.SparkContext]Invoking stop() from shutdown hook
[INFO][2018-05-24 20:31:35,097][org.spark_project.jetty.server.AbstractConnector]Stopped Spark@6d366c9b{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-05-24 20:31:35,100][org.apache.spark.ui.SparkUI]Stopped Spark web UI at http://192.168.0.102:4040
[INFO][2018-05-24 20:31:35,109][org.apache.spark.MapOutputTrackerMasterEndpoint]MapOutputTrackerMasterEndpoint stopped!
[INFO][2018-05-24 20:31:35,143][org.apache.spark.storage.memory.MemoryStore]MemoryStore cleared
[INFO][2018-05-24 20:31:35,143][org.apache.spark.storage.BlockManager]BlockManager stopped
[INFO][2018-05-24 20:31:35,144][org.apache.spark.storage.BlockManagerMaster]BlockManagerMaster stopped
[INFO][2018-05-24 20:31:35,146][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint]OutputCommitCoordinator stopped!
[INFO][2018-05-24 20:31:35,148][org.apache.spark.SparkContext]Successfully stopped SparkContext
[INFO][2018-05-24 20:31:35,148][org.apache.spark.util.ShutdownHookManager]Shutdown hook called
[INFO][2018-05-24 20:31:35,149][org.apache.spark.util.ShutdownHookManager]Deleting directory /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/spark-33cf4f2e-c784-426c-9bb3-23cc9b7b32a6
[INFO][2018-05-24 21:07:35,344][org.apache.spark.SparkContext]Running Spark version 2.2.0
[INFO][2018-05-24 21:07:36,300][org.apache.spark.SparkContext]Submitted application: ReceiveKafkaData$
[INFO][2018-05-24 21:07:36,341][org.apache.spark.SecurityManager]Changing view acls to: seven
[INFO][2018-05-24 21:07:36,342][org.apache.spark.SecurityManager]Changing modify acls to: seven
[INFO][2018-05-24 21:07:36,343][org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO][2018-05-24 21:07:36,343][org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO][2018-05-24 21:07:36,344][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(seven); groups with view permissions: Set(); users  with modify permissions: Set(seven); groups with modify permissions: Set()
[INFO][2018-05-24 21:07:36,627][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 50745.
[INFO][2018-05-24 21:07:36,656][org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO][2018-05-24 21:07:36,676][org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO][2018-05-24 21:07:36,679][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2018-05-24 21:07:36,680][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
[INFO][2018-05-24 21:07:36,689][org.apache.spark.storage.DiskBlockManager]Created local directory at /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/blockmgr-7fa97866-6035-4d19-9e1b-29b7ca7349b7
[INFO][2018-05-24 21:07:36,708][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 912.3 MB
[INFO][2018-05-24 21:07:36,802][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO][2018-05-24 21:07:36,893][org.spark_project.jetty.util.log]Logging initialized @2758ms
[INFO][2018-05-24 21:07:36,945][org.spark_project.jetty.server.Server]jetty-9.3.z-SNAPSHOT
[INFO][2018-05-24 21:07:36,959][org.spark_project.jetty.server.Server]Started @2825ms
[INFO][2018-05-24 21:07:36,977][org.spark_project.jetty.server.AbstractConnector]Started ServerConnector@6d366c9b{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-05-24 21:07:36,978][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4040.
[INFO][2018-05-24 21:07:36,998][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2c1b9e4b{/jobs,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:07:36,998][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3d08f3f5{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:07:36,999][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1a1da881{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:07:37,000][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@165b8a71{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:07:37,001][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2f058b8a{/stages,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:07:37,001][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3f2ef586{/stages/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:07:37,001][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@76c7beb3{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:07:37,003][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7b139eab{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:07:37,003][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@611df6e3{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:07:37,004][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6273c5a4{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:07:37,005][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@53e211ee{/storage,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:07:37,005][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3d8bbcdc{/storage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:07:37,006][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@117e0fe5{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:07:37,007][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@78aea4b9{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:07:37,007][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4b85880b{/environment,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:07:37,008][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4215838f{/environment/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:07:37,009][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2289aca5{/executors,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:07:37,010][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@184497d1{/executors/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:07:37,011][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6ffab045{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:07:37,011][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3e2943ab{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:07:37,019][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4a9f80d3{/static,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:07:37,020][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4204541c{/,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:07:37,021][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4602c2a9{/api,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:07:37,022][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@69c79f09{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:07:37,023][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5fcacc0{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:07:37,025][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://192.168.0.102:4040
[INFO][2018-05-24 21:07:37,109][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
[INFO][2018-05-24 21:07:37,148][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 50746.
[INFO][2018-05-24 21:07:37,161][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 192.168.0.102:50746
[INFO][2018-05-24 21:07:37,164][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2018-05-24 21:07:37,166][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 192.168.0.102, 50746, None)
[INFO][2018-05-24 21:07:37,170][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 192.168.0.102:50746 with 912.3 MB RAM, BlockManagerId(driver, 192.168.0.102, 50746, None)
[INFO][2018-05-24 21:07:37,175][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 192.168.0.102, 50746, None)
[INFO][2018-05-24 21:07:37,176][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 192.168.0.102, 50746, None)
[INFO][2018-05-24 21:07:37,465][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@64a1923a{/metrics/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:07:37,600][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 21:07:37,603][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 21:07:37,603][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 21:07:48,354][org.apache.spark.streaming.kafka.DirectKafkaInputDStream]Slide time = 10000 ms
[INFO][2018-05-24 21:07:48,355][org.apache.spark.streaming.kafka.DirectKafkaInputDStream]Storage level = Serialized 1x Replicated
[INFO][2018-05-24 21:07:48,356][org.apache.spark.streaming.kafka.DirectKafkaInputDStream]Checkpoint interval = null
[INFO][2018-05-24 21:07:48,356][org.apache.spark.streaming.kafka.DirectKafkaInputDStream]Remember interval = 10000 ms
[INFO][2018-05-24 21:07:48,357][org.apache.spark.streaming.kafka.DirectKafkaInputDStream]Initialized and validated org.apache.spark.streaming.kafka.DirectKafkaInputDStream@e38788f
[INFO][2018-05-24 21:07:48,357][org.apache.spark.streaming.dstream.MappedDStream]Slide time = 10000 ms
[INFO][2018-05-24 21:07:48,357][org.apache.spark.streaming.dstream.MappedDStream]Storage level = Serialized 1x Replicated
[INFO][2018-05-24 21:07:48,357][org.apache.spark.streaming.dstream.MappedDStream]Checkpoint interval = null
[INFO][2018-05-24 21:07:48,358][org.apache.spark.streaming.dstream.MappedDStream]Remember interval = 10000 ms
[INFO][2018-05-24 21:07:48,358][org.apache.spark.streaming.dstream.MappedDStream]Initialized and validated org.apache.spark.streaming.dstream.MappedDStream@13adac79
[INFO][2018-05-24 21:07:48,358][org.apache.spark.streaming.dstream.ForEachDStream]Slide time = 10000 ms
[INFO][2018-05-24 21:07:48,358][org.apache.spark.streaming.dstream.ForEachDStream]Storage level = Serialized 1x Replicated
[INFO][2018-05-24 21:07:48,358][org.apache.spark.streaming.dstream.ForEachDStream]Checkpoint interval = null
[INFO][2018-05-24 21:07:48,358][org.apache.spark.streaming.dstream.ForEachDStream]Remember interval = 10000 ms
[INFO][2018-05-24 21:07:48,358][org.apache.spark.streaming.dstream.ForEachDStream]Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@6b574b9b
[INFO][2018-05-24 21:07:48,404][org.apache.spark.streaming.util.RecurringTimer]Started timer for JobGenerator at time 1527167270000
[INFO][2018-05-24 21:07:48,404][org.apache.spark.streaming.scheduler.JobGenerator]Started JobGenerator at 1527167270000 ms
[INFO][2018-05-24 21:07:48,405][org.apache.spark.streaming.scheduler.JobScheduler]Started JobScheduler
[INFO][2018-05-24 21:07:48,411][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1ab6718{/streaming,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:07:48,412][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@460510aa{/streaming/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:07:48,413][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@78b41097{/streaming/batch,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:07:48,414][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@327c7bea{/streaming/batch/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:07:48,415][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7cf283e1{/static/streaming,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:07:48,415][org.apache.spark.streaming.StreamingContext]StreamingContext started
[INFO][2018-05-24 21:07:50,047][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 21:07:50,047][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 21:07:50,047][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 21:07:51,671][org.apache.spark.SparkContext]Running Spark version 2.2.0
[INFO][2018-05-24 21:07:52,499][org.apache.spark.SparkContext]Submitted application: SimulationKafkaSendOutData$
[INFO][2018-05-24 21:07:52,534][org.apache.spark.SecurityManager]Changing view acls to: seven
[INFO][2018-05-24 21:07:52,535][org.apache.spark.SecurityManager]Changing modify acls to: seven
[INFO][2018-05-24 21:07:52,536][org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO][2018-05-24 21:07:52,537][org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO][2018-05-24 21:07:52,538][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(seven); groups with view permissions: Set(); users  with modify permissions: Set(seven); groups with modify permissions: Set()
[INFO][2018-05-24 21:07:53,461][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 50762.
[INFO][2018-05-24 21:07:53,510][org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO][2018-05-24 21:07:53,528][org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO][2018-05-24 21:07:53,530][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2018-05-24 21:07:53,531][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
[INFO][2018-05-24 21:07:53,542][org.apache.spark.storage.DiskBlockManager]Created local directory at /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/blockmgr-1eeefccd-f09e-45c9-8f33-7d3cd383921e
[INFO][2018-05-24 21:07:53,561][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 912.3 MB
[INFO][2018-05-24 21:07:53,639][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO][2018-05-24 21:07:53,730][org.spark_project.jetty.util.log]Logging initialized @3155ms
[INFO][2018-05-24 21:07:53,797][org.spark_project.jetty.server.Server]jetty-9.3.z-SNAPSHOT
[INFO][2018-05-24 21:07:53,810][org.spark_project.jetty.server.Server]Started @3236ms
[WARN][2018-05-24 21:07:53,823][org.apache.spark.util.Utils]Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[INFO][2018-05-24 21:07:53,829][org.spark_project.jetty.server.AbstractConnector]Started ServerConnector@16751330{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}
[INFO][2018-05-24 21:07:53,829][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4041.
[INFO][2018-05-24 21:07:53,850][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7e985ce9{/jobs,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:07:53,851][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@757d6814{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:07:53,851][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3c0fae6c{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:07:53,852][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@fd0e5b6{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:07:53,853][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@36b0fcd5{/stages,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:07:53,854][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@475835b1{/stages/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:07:53,854][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5241cf67{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:07:53,855][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7e809b79{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:07:53,856][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@625e134e{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:07:53,857][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@89c10b7{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:07:53,858][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4fe89c24{/storage,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:07:53,859][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3d08f3f5{/storage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:07:53,859][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1a1da881{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:07:53,860][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7fd4acee{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:07:53,861][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6175619b{/environment,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:07:53,863][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@756cf158{/environment/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:07:53,864][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@751d3241{/executors,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:07:53,864][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@64337702{/executors/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:07:53,865][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@30ea8c23{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:07:53,866][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4e76dac{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:07:53,871][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5f2f577{/static,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:07:53,874][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@26fb628{/,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:07:53,878][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@70dd7e15{/api,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:07:53,879][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2c9399a4{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:07:53,880][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@9635fa{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:07:53,886][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://192.168.0.102:4041
[INFO][2018-05-24 21:07:53,980][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
[INFO][2018-05-24 21:07:54,013][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 50763.
[INFO][2018-05-24 21:07:54,013][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 192.168.0.102:50763
[INFO][2018-05-24 21:07:54,016][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2018-05-24 21:07:54,018][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 192.168.0.102, 50763, None)
[INFO][2018-05-24 21:07:54,024][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 192.168.0.102:50763 with 912.3 MB RAM, BlockManagerId(driver, 192.168.0.102, 50763, None)
[INFO][2018-05-24 21:07:54,030][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 192.168.0.102, 50763, None)
[INFO][2018-05-24 21:07:54,032][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 192.168.0.102, 50763, None)
[INFO][2018-05-24 21:07:54,311][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@79c3f01f{/metrics/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:07:54,860][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0 stored as values in memory (estimated size 228.1 KB, free 912.1 MB)
[INFO][2018-05-24 21:07:54,929][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0_piece0 stored as bytes in memory (estimated size 22.1 KB, free 912.1 MB)
[INFO][2018-05-24 21:07:54,931][org.apache.spark.storage.BlockManagerInfo]Added broadcast_0_piece0 in memory on 192.168.0.102:50763 (size: 22.1 KB, free: 912.3 MB)
[INFO][2018-05-24 21:07:54,937][org.apache.spark.SparkContext]Created broadcast 0 from textFile at SimulationKafkaSendOutData.scala:25
[INFO][2018-05-24 21:07:55,160][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527167270000 ms
[INFO][2018-05-24 21:07:55,163][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527167270000 ms.0 from job set of time 1527167270000 ms
[INFO][2018-05-24 21:07:55,196][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:07:55,211][org.apache.spark.scheduler.DAGScheduler]Got job 0 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:07:55,212][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 0 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:07:55,212][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:07:55,213][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:07:55,219][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 0 (MapPartitionsRDD[1] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:07:55,333][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 21:07:55,355][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0_piece0 stored as bytes in memory (estimated size 1867.0 B, free 912.3 MB)
[INFO][2018-05-24 21:07:55,357][org.apache.spark.storage.BlockManagerInfo]Added broadcast_0_piece0 in memory on 192.168.0.102:50746 (size: 1867.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:07:55,359][org.apache.spark.SparkContext]Created broadcast 0 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:07:55,373][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:07:55,374][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 0.0 with 1 tasks
[INFO][2018-05-24 21:07:55,407][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:07:55,418][org.apache.spark.executor.Executor]Running task 0.0 in stage 0.0 (TID 0)
[INFO][2018-05-24 21:07:55,449][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12184 is the same as ending offset skipping seven 0
[INFO][2018-05-24 21:07:55,589][org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper]Process identifier=hconnection-0x54bcd26 connecting to ZooKeeper ensemble=vm-xaj-bigdata-da-d01:2181,vm-xaj-bigdata-da-d02:2181,vm-xaj-bigdata-da-d03:2181
[INFO][2018-05-24 21:07:55,595][org.apache.zookeeper.ZooKeeper]Client environment:zookeeper.version=3.4.5-cdh5.7.0--1, built on 04/06/2016 14:24 GMT
[INFO][2018-05-24 21:07:55,596][org.apache.zookeeper.ZooKeeper]Client environment:host.name=192.168.0.102
[INFO][2018-05-24 21:07:55,596][org.apache.zookeeper.ZooKeeper]Client environment:java.version=1.8.0_161
[INFO][2018-05-24 21:07:55,596][org.apache.zookeeper.ZooKeeper]Client environment:java.vendor=Oracle Corporation
[INFO][2018-05-24 21:07:55,596][org.apache.zookeeper.ZooKeeper]Client environment:java.home=/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre
[INFO][2018-05-24 21:07:55,596][org.apache.zookeeper.ZooKeeper]Client environment:java.class.path=/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/charsets.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/deploy.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/cldrdata.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/dnsns.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/jaccess.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/jfxrt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/localedata.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/nashorn.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/sunec.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/sunjce_provider.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/sunpkcs11.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/zipfs.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/javaws.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/jce.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/jfr.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/jfxswt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/jsse.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/management-agent.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/plugin.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/resources.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/rt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/ant-javafx.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/dt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/javafx-mx.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/jconsole.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/packager.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/sa-jdi.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/tools.jar:/Users/seven/project/scala/dataMining/target/classes:/Users/seven/software/maven/repository/org/scala-lang/scala-library/2.11.12/scala-library-2.11.12.jar:/Users/seven/software/maven/repository/org/scala-lang/scala-compiler/2.11.12/scala-compiler-2.11.12.jar:/Users/seven/software/maven/repository/org/scala-lang/modules/scala-xml_2.11/1.0.5/scala-xml_2.11-1.0.5.jar:/Users/seven/software/maven/repository/org/scala-lang/modules/scala-parser-combinators_2.11/1.0.4/scala-parser-combinators_2.11-1.0.4.jar:/Users/seven/software/maven/repository/org/scala-lang/scala-reflect/2.11.12/scala-reflect-2.11.12.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-client/2.6.0-cdh5.7.0/hadoop-client-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-common/2.6.0-cdh5.7.0/hadoop-common-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/xmlenc/xmlenc/0.52/xmlenc-0.52.jar:/Users/seven/software/maven/repository/commons-net/commons-net/3.1/commons-net-3.1.jar:/Users/seven/software/maven/repository/commons-configuration/commons-configuration/1.6/commons-configuration-1.6.jar:/Users/seven/software/maven/repository/commons-digester/commons-digester/1.8/commons-digester-1.8.jar:/Users/seven/software/maven/repository/commons-beanutils/commons-beanutils/1.7.0/commons-beanutils-1.7.0.jar:/Users/seven/software/maven/repository/org/slf4j/slf4j-api/1.7.5/slf4j-api-1.7.5.jar:/Users/seven/software/maven/repository/org/slf4j/slf4j-log4j12/1.7.5/slf4j-log4j12-1.7.5.jar:/Users/seven/software/maven/repository/org/apache/avro/avro/1.7.6-cdh5.7.0/avro-1.7.6-cdh5.7.0.jar:/Users/seven/software/maven/repository/com/google/code/gson/gson/2.2.4/gson-2.2.4.jar:/Users/seven/software/maven/repository/org/apache/curator/curator-client/2.7.1/curator-client-2.7.1.jar:/Users/seven/software/maven/repository/org/apache/curator/curator-recipes/2.7.1/curator-recipes-2.7.1.jar:/Users/seven/software/maven/repository/com/google/code/findbugs/jsr305/3.0.0/jsr305-3.0.0.jar:/Users/seven/software/maven/repository/org/apache/htrace/htrace-core4/4.0.1-incubating/htrace-core4-4.0.1-incubating.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-compress/1.4.1/commons-compress-1.4.1.jar:/Users/seven/software/maven/repository/org/tukaani/xz/1.0/xz-1.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-hdfs/2.6.0-cdh5.7.0/hadoop-hdfs-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/io/netty/netty/3.6.2.Final/netty-3.6.2.Final.jar:/Users/seven/software/maven/repository/xerces/xercesImpl/2.9.1/xercesImpl-2.9.1.jar:/Users/seven/software/maven/repository/xml-apis/xml-apis/1.3.04/xml-apis-1.3.04.jar:/Users/seven/software/maven/repository/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-app/2.6.0-cdh5.7.0/hadoop-mapreduce-client-app-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-common/2.6.0-cdh5.7.0/hadoop-mapreduce-client-common-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-yarn-client/2.6.0-cdh5.7.0/hadoop-yarn-client-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-yarn-server-common/2.6.0-cdh5.7.0/hadoop-yarn-server-common-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-shuffle/2.6.0-cdh5.7.0/hadoop-mapreduce-client-shuffle-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-yarn-api/2.6.0-cdh5.7.0/hadoop-yarn-api-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-core/2.6.0-cdh5.7.0/hadoop-mapreduce-client-core-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-yarn-common/2.6.0-cdh5.7.0/hadoop-yarn-common-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/javax/xml/bind/jaxb-api/2.2.2/jaxb-api-2.2.2.jar:/Users/seven/software/maven/repository/javax/xml/stream/stax-api/1.0-2/stax-api-1.0-2.jar:/Users/seven/software/maven/repository/javax/activation/activation/1.1/activation-1.1.jar:/Users/seven/software/maven/repository/com/sun/jersey/jersey-client/1.9/jersey-client-1.9.jar:/Users/seven/software/maven/repository/org/codehaus/jackson/jackson-xc/1.8.8/jackson-xc-1.8.8.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.6.0-cdh5.7.0/hadoop-mapreduce-client-jobclient-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-aws/2.6.0-cdh5.7.0/hadoop-aws-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/com/amazonaws/aws-java-sdk-s3/1.10.6/aws-java-sdk-s3-1.10.6.jar:/Users/seven/software/maven/repository/com/amazonaws/aws-java-sdk-kms/1.10.6/aws-java-sdk-kms-1.10.6.jar:/Users/seven/software/maven/repository/com/amazonaws/aws-java-sdk-core/1.10.6/aws-java-sdk-core-1.10.6.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-annotations/2.6.0-cdh5.7.0/hadoop-annotations-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-client/1.2.0-cdh5.7.0/hbase-client-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-annotations/1.2.0-cdh5.7.0/hbase-annotations-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-common/1.2.0-cdh5.7.0/hbase-common-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-protocol/1.2.0-cdh5.7.0/hbase-protocol-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/commons-codec/commons-codec/1.9/commons-codec-1.9.jar:/Users/seven/software/maven/repository/commons-io/commons-io/2.4/commons-io-2.4.jar:/Users/seven/software/maven/repository/commons-lang/commons-lang/2.6/commons-lang-2.6.jar:/Users/seven/software/maven/repository/commons-logging/commons-logging/1.2/commons-logging-1.2.jar:/Users/seven/software/maven/repository/com/google/guava/guava/12.0.1/guava-12.0.1.jar:/Users/seven/software/maven/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/Users/seven/software/maven/repository/io/netty/netty-all/4.0.23.Final/netty-all-4.0.23.Final.jar:/Users/seven/software/maven/repository/org/apache/zookeeper/zookeeper/3.4.5-cdh5.7.0/zookeeper-3.4.5-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/htrace/htrace-core/3.2.0-incubating/htrace-core-3.2.0-incubating.jar:/Users/seven/software/maven/repository/org/codehaus/jackson/jackson-mapper-asl/1.8.8/jackson-mapper-asl-1.8.8.jar:/Users/seven/software/maven/repository/org/jruby/jcodings/jcodings/1.0.8/jcodings-1.0.8.jar:/Users/seven/software/maven/repository/org/jruby/joni/joni/2.1.2/joni-2.1.2.jar:/Users/seven/software/maven/repository/com/yammer/metrics/metrics-core/2.2.0/metrics-core-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-auth/2.6.0-cdh5.7.0/hadoop-auth-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/httpcomponents/httpclient/4.2.5/httpclient-4.2.5.jar:/Users/seven/software/maven/repository/org/apache/httpcomponents/httpcore/4.2.4/httpcore-4.2.4.jar:/Users/seven/software/maven/repository/org/apache/directory/server/apacheds-kerberos-codec/2.0.0-M15/apacheds-kerberos-codec-2.0.0-M15.jar:/Users/seven/software/maven/repository/org/apache/directory/server/apacheds-i18n/2.0.0-M15/apacheds-i18n-2.0.0-M15.jar:/Users/seven/software/maven/repository/org/apache/directory/api/api-asn1-api/1.0.0-M20/api-asn1-api-1.0.0-M20.jar:/Users/seven/software/maven/repository/org/apache/directory/api/api-util/1.0.0-M20/api-util-1.0.0-M20.jar:/Users/seven/software/maven/repository/org/apache/curator/curator-framework/2.7.1/curator-framework-2.7.1.jar:/Users/seven/software/maven/repository/com/github/stephenc/findbugs/findbugs-annotations/1.3.9-1/findbugs-annotations-1.3.9-1.jar:/Users/seven/software/maven/repository/junit/junit/4.12/junit-4.12.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-server/1.2.0-cdh5.7.0/hbase-server-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-procedure/1.2.0-cdh5.7.0/hbase-procedure-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-common/1.2.0-cdh5.7.0/hbase-common-1.2.0-cdh5.7.0-tests.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-prefix-tree/1.2.0-cdh5.7.0/hbase-prefix-tree-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar:/Users/seven/software/maven/repository/commons-collections/commons-collections/3.2.2/commons-collections-3.2.2.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-hadoop-compat/1.2.0-cdh5.7.0/hbase-hadoop-compat-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-hadoop2-compat/1.2.0-cdh5.7.0/hbase-hadoop2-compat-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/com/sun/jersey/jersey-core/1.9/jersey-core-1.9.jar:/Users/seven/software/maven/repository/com/sun/jersey/jersey-server/1.9/jersey-server-1.9.jar:/Users/seven/software/maven/repository/asm/asm/3.1/asm-3.1.jar:/Users/seven/software/maven/repository/commons-cli/commons-cli/1.2/commons-cli-1.2.jar:/Users/seven/software/maven/repository/com/github/stephenc/high-scale-lib/high-scale-lib/1.1.1/high-scale-lib-1.1.1.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-math/2.1/commons-math-2.1.jar:/Users/seven/software/maven/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jetty/6.1.26.cloudera.4/jetty-6.1.26.cloudera.4.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jetty-util/6.1.26.cloudera.4/jetty-util-6.1.26.cloudera.4.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jetty-sslengine/6.1.26.cloudera.4/jetty-sslengine-6.1.26.cloudera.4.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jsp-2.1/6.1.14/jsp-2.1-6.1.14.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jsp-api-2.1/6.1.14/jsp-api-2.1-6.1.14.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/servlet-api-2.5/6.1.14/servlet-api-2.5-6.1.14.jar:/Users/seven/software/maven/repository/org/codehaus/jackson/jackson-core-asl/1.8.8/jackson-core-asl-1.8.8.jar:/Users/seven/software/maven/repository/org/codehaus/jackson/jackson-jaxrs/1.8.8/jackson-jaxrs-1.8.8.jar:/Users/seven/software/maven/repository/tomcat/jasper-compiler/5.5.23/jasper-compiler-5.5.23.jar:/Users/seven/software/maven/repository/tomcat/jasper-runtime/5.5.23/jasper-runtime-5.5.23.jar:/Users/seven/software/maven/repository/commons-el/commons-el/1.0/commons-el-1.0.jar:/Users/seven/software/maven/repository/org/jamon/jamon-runtime/2.4.1/jamon-runtime-2.4.1.jar:/Users/seven/software/maven/repository/com/lmax/disruptor/3.3.0/disruptor-3.3.0.jar:/Users/seven/software/maven/repository/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar:/Users/seven/software/maven/repository/org/owasp/esapi/esapi/2.1.0/esapi-2.1.0.jar:/Users/seven/software/maven/repository/commons-beanutils/commons-beanutils-core/1.7.0/commons-beanutils-core-1.7.0.jar:/Users/seven/software/maven/repository/commons-fileupload/commons-fileupload/1.2/commons-fileupload-1.2.jar:/Users/seven/software/maven/repository/xom/xom/1.2.5/xom-1.2.5.jar:/Users/seven/software/maven/repository/xalan/xalan/2.7.0/xalan-2.7.0.jar:/Users/seven/software/maven/repository/org/beanshell/bsh-core/2.0b4/bsh-core-2.0b4.jar:/Users/seven/software/maven/repository/org/owasp/antisamy/antisamy/1.4.3/antisamy-1.4.3.jar:/Users/seven/software/maven/repository/org/apache/xmlgraphics/batik-css/1.7/batik-css-1.7.jar:/Users/seven/software/maven/repository/org/apache/xmlgraphics/batik-ext/1.7/batik-ext-1.7.jar:/Users/seven/software/maven/repository/org/apache/xmlgraphics/batik-util/1.7/batik-util-1.7.jar:/Users/seven/software/maven/repository/xml-apis/xml-apis-ext/1.3.04/xml-apis-ext-1.3.04.jar:/Users/seven/software/maven/repository/net/sourceforge/nekohtml/nekohtml/1.9.12/nekohtml-1.9.12.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-core/2.6.0-mr1-cdh5.7.0/hadoop-core-2.6.0-mr1-cdh5.7.0.jar:/Users/seven/software/maven/repository/javax/servlet/jsp/jsp-api/2.1/jsp-api-2.1.jar:/Users/seven/software/maven/repository/net/java/dev/jets3t/jets3t/0.6.1/jets3t-0.6.1.jar:/Users/seven/software/maven/repository/javax/servlet/servlet-api/2.5/servlet-api-2.5.jar:/Users/seven/software/maven/repository/hsqldb/hsqldb/1.8.0.10/hsqldb-1.8.0.10.jar:/Users/seven/software/maven/repository/org/eclipse/jdt/core/3.1.1/core-3.1.1.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-hdfs/2.6.0-cdh5.7.0/hadoop-hdfs-2.6.0-cdh5.7.0-tests.jar:/Users/seven/software/maven/repository/commons-daemon/commons-daemon/1.0.13/commons-daemon-1.0.13.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-lang3/3.5/commons-lang3-3.5.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-pool2/2.4.2/commons-pool2-2.4.2.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-mllib_2.11/2.2.0/spark-mllib_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-core_2.11/2.2.0/spark-core_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/avro/avro-mapred/1.7.7/avro-mapred-1.7.7-hadoop2.jar:/Users/seven/software/maven/repository/org/apache/avro/avro-ipc/1.7.7/avro-ipc-1.7.7-tests.jar:/Users/seven/software/maven/repository/com/twitter/chill_2.11/0.8.0/chill_2.11-0.8.0.jar:/Users/seven/software/maven/repository/com/esotericsoftware/kryo-shaded/3.0.3/kryo-shaded-3.0.3.jar:/Users/seven/software/maven/repository/com/esotericsoftware/minlog/1.3.0/minlog-1.3.0.jar:/Users/seven/software/maven/repository/org/objenesis/objenesis/2.1/objenesis-2.1.jar:/Users/seven/software/maven/repository/com/twitter/chill-java/0.8.0/chill-java-0.8.0.jar:/Users/seven/software/maven/repository/org/apache/xbean/xbean-asm5-shaded/4.4/xbean-asm5-shaded-4.4.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-launcher_2.11/2.2.0/spark-launcher_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-network-common_2.11/2.2.0/spark-network-common_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-network-shuffle_2.11/2.2.0/spark-network-shuffle_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-unsafe_2.11/2.2.0/spark-unsafe_2.11-2.2.0.jar:/Users/seven/software/maven/repository/javax/servlet/javax.servlet-api/3.1.0/javax.servlet-api-3.1.0.jar:/Users/seven/software/maven/repository/org/slf4j/jul-to-slf4j/1.7.16/jul-to-slf4j-1.7.16.jar:/Users/seven/software/maven/repository/org/slf4j/jcl-over-slf4j/1.7.16/jcl-over-slf4j-1.7.16.jar:/Users/seven/software/maven/repository/com/ning/compress-lzf/1.0.3/compress-lzf-1.0.3.jar:/Users/seven/software/maven/repository/org/xerial/snappy/snappy-java/1.1.2.6/snappy-java-1.1.2.6.jar:/Users/seven/software/maven/repository/org/roaringbitmap/RoaringBitmap/0.5.11/RoaringBitmap-0.5.11.jar:/Users/seven/software/maven/repository/org/json4s/json4s-jackson_2.11/3.2.11/json4s-jackson_2.11-3.2.11.jar:/Users/seven/software/maven/repository/org/json4s/json4s-core_2.11/3.2.11/json4s-core_2.11-3.2.11.jar:/Users/seven/software/maven/repository/org/json4s/json4s-ast_2.11/3.2.11/json4s-ast_2.11-3.2.11.jar:/Users/seven/software/maven/repository/org/scala-lang/scalap/2.11.0/scalap-2.11.0.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/core/jersey-client/2.22.2/jersey-client-2.22.2.jar:/Users/seven/software/maven/repository/javax/ws/rs/javax.ws.rs-api/2.0.1/javax.ws.rs-api-2.0.1.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/hk2-api/2.4.0-b34/hk2-api-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/hk2-utils/2.4.0-b34/hk2-utils-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/external/aopalliance-repackaged/2.4.0-b34/aopalliance-repackaged-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/external/javax.inject/2.4.0-b34/javax.inject-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/hk2-locator/2.4.0-b34/hk2-locator-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/javassist/javassist/3.18.1-GA/javassist-3.18.1-GA.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/core/jersey-common/2.22.2/jersey-common-2.22.2.jar:/Users/seven/software/maven/repository/javax/annotation/javax.annotation-api/1.2/javax.annotation-api-1.2.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/bundles/repackaged/jersey-guava/2.22.2/jersey-guava-2.22.2.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/osgi-resource-locator/1.0.1/osgi-resource-locator-1.0.1.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/core/jersey-server/2.22.2/jersey-server-2.22.2.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/media/jersey-media-jaxb/2.22.2/jersey-media-jaxb-2.22.2.jar:/Users/seven/software/maven/repository/javax/validation/validation-api/1.1.0.Final/validation-api-1.1.0.Final.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/containers/jersey-container-servlet/2.22.2/jersey-container-servlet-2.22.2.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/containers/jersey-container-servlet-core/2.22.2/jersey-container-servlet-core-2.22.2.jar:/Users/seven/software/maven/repository/com/clearspring/analytics/stream/2.7.0/stream-2.7.0.jar:/Users/seven/software/maven/repository/io/dropwizard/metrics/metrics-core/3.1.2/metrics-core-3.1.2.jar:/Users/seven/software/maven/repository/io/dropwizard/metrics/metrics-jvm/3.1.2/metrics-jvm-3.1.2.jar:/Users/seven/software/maven/repository/io/dropwizard/metrics/metrics-json/3.1.2/metrics-json-3.1.2.jar:/Users/seven/software/maven/repository/io/dropwizard/metrics/metrics-graphite/3.1.2/metrics-graphite-3.1.2.jar:/Users/seven/software/maven/repository/org/apache/ivy/ivy/2.4.0/ivy-2.4.0.jar:/Users/seven/software/maven/repository/oro/oro/2.0.8/oro-2.0.8.jar:/Users/seven/software/maven/repository/net/razorvine/pyrolite/4.13/pyrolite-4.13.jar:/Users/seven/software/maven/repository/net/sf/py4j/py4j/0.10.4/py4j-0.10.4.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-crypto/1.0.0/commons-crypto-1.0.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-streaming_2.11/2.2.0/spark-streaming_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-sql_2.11/2.2.0/spark-sql_2.11-2.2.0.jar:/Users/seven/software/maven/repository/com/univocity/univocity-parsers/2.2.1/univocity-parsers-2.2.1.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-sketch_2.11/2.2.0/spark-sketch_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-catalyst_2.11/2.2.0/spark-catalyst_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/codehaus/janino/janino/3.0.0/janino-3.0.0.jar:/Users/seven/software/maven/repository/org/codehaus/janino/commons-compiler/3.0.0/commons-compiler-3.0.0.jar:/Users/seven/software/maven/repository/org/antlr/antlr4-runtime/4.5.3/antlr4-runtime-4.5.3.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-column/1.8.2/parquet-column-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-common/1.8.2/parquet-common-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-encoding/1.8.2/parquet-encoding-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-hadoop/1.8.2/parquet-hadoop-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-format/2.3.1/parquet-format-2.3.1.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-jackson/1.8.2/parquet-jackson-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-graphx_2.11/2.2.0/spark-graphx_2.11-2.2.0.jar:/Users/seven/software/maven/repository/com/github/fommil/netlib/core/1.1.2/core-1.1.2.jar:/Users/seven/software/maven/repository/net/sourceforge/f2j/arpack_combined_all/0.1/arpack_combined_all-0.1.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-mllib-local_2.11/2.2.0/spark-mllib-local_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/scalanlp/breeze_2.11/0.13.1/breeze_2.11-0.13.1.jar:/Users/seven/software/maven/repository/org/scalanlp/breeze-macros_2.11/0.13.1/breeze-macros_2.11-0.13.1.jar:/Users/seven/software/maven/repository/net/sf/opencsv/opencsv/2.3/opencsv-2.3.jar:/Users/seven/software/maven/repository/com/github/rwl/jtransforms/2.4.0/jtransforms-2.4.0.jar:/Users/seven/software/maven/repository/org/spire-math/spire_2.11/0.13.0/spire_2.11-0.13.0.jar:/Users/seven/software/maven/repository/org/spire-math/spire-macros_2.11/0.13.0/spire-macros_2.11-0.13.0.jar:/Users/seven/software/maven/repository/org/typelevel/machinist_2.11/0.6.1/machinist_2.11-0.6.1.jar:/Users/seven/software/maven/repository/com/chuusai/shapeless_2.11/2.3.2/shapeless_2.11-2.3.2.jar:/Users/seven/software/maven/repository/org/typelevel/macro-compat_2.11/1.1.1/macro-compat_2.11-1.1.1.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-math3/3.4.1/commons-math3-3.4.1.jar:/Users/seven/software/maven/repository/org/jpmml/pmml-model/1.2.15/pmml-model-1.2.15.jar:/Users/seven/software/maven/repository/org/jpmml/pmml-schema/1.2.15/pmml-schema-1.2.15.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-tags_2.11/2.2.0/spark-tags_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-streaming-kafka-0-8_2.11/2.2.0/spark-streaming-kafka-0-8_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/kafka/kafka_2.11/0.8.2.1/kafka_2.11-0.8.2.1.jar:/Users/seven/software/maven/repository/com/101tec/zkclient/0.3/zkclient-0.3.jar:/Users/seven/software/maven/repository/org/apache/kafka/kafka-clients/0.8.2.1/kafka-clients-0.8.2.1.jar:/Users/seven/software/maven/repository/mysql/mysql-connector-java/5.1.31/mysql-connector-java-5.1.31.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/module/jackson-module-scala_2.11/2.6.5/jackson-module-scala_2.11-2.6.5.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/core/jackson-core/2.6.5/jackson-core-2.6.5.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/core/jackson-annotations/2.6.5/jackson-annotations-2.6.5.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/core/jackson-databind/2.6.5/jackson-databind-2.6.5.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/module/jackson-module-paranamer/2.6.5/jackson-module-paranamer-2.6.5.jar:/Users/seven/software/maven/repository/com/thoughtworks/paranamer/paranamer/2.6/paranamer-2.6.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-streaming-flume_2.11/2.2.0/spark-streaming-flume_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/flume/flume-ng-core/1.6.0/flume-ng-core-1.6.0.jar:/Users/seven/software/maven/repository/org/apache/flume/flume-ng-configuration/1.6.0/flume-ng-configuration-1.6.0.jar:/Users/seven/software/maven/repository/org/apache/avro/avro-ipc/1.7.4/avro-ipc-1.7.4.jar:/Users/seven/software/maven/repository/org/apache/velocity/velocity/1.7/velocity-1.7.jar:/Users/seven/software/maven/repository/joda-time/joda-time/2.1/joda-time-2.1.jar:/Users/seven/software/maven/repository/org/apache/mina/mina-core/2.0.4/mina-core-2.0.4.jar:/Users/seven/software/maven/repository/org/apache/flume/flume-ng-sdk/1.6.0/flume-ng-sdk-1.6.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-streaming-flume-sink_2.11/2.2.0/spark-streaming-flume-sink_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/flume/flume-ng-clients/flume-ng-log4jappender/1.6.0/flume-ng-log4jappender-1.6.0.jar:/Users/seven/software/maven/repository/net/jpountz/lz4/lz4/1.3.0/lz4-1.3.0.jar:/Applications/IntelliJ IDEA.app/Contents/lib/idea_rt.jar
[INFO][2018-05-24 21:07:55,597][org.apache.zookeeper.ZooKeeper]Client environment:java.library.path=/Users/seven/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:.
[INFO][2018-05-24 21:07:55,597][org.apache.zookeeper.ZooKeeper]Client environment:java.io.tmpdir=/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/
[INFO][2018-05-24 21:07:55,597][org.apache.zookeeper.ZooKeeper]Client environment:java.compiler=<NA>
[INFO][2018-05-24 21:07:55,597][org.apache.zookeeper.ZooKeeper]Client environment:os.name=Mac OS X
[INFO][2018-05-24 21:07:55,597][org.apache.zookeeper.ZooKeeper]Client environment:os.arch=x86_64
[INFO][2018-05-24 21:07:55,597][org.apache.zookeeper.ZooKeeper]Client environment:os.version=10.13.4
[INFO][2018-05-24 21:07:55,597][org.apache.zookeeper.ZooKeeper]Client environment:user.name=seven
[INFO][2018-05-24 21:07:55,597][org.apache.zookeeper.ZooKeeper]Client environment:user.home=/Users/seven
[INFO][2018-05-24 21:07:55,597][org.apache.zookeeper.ZooKeeper]Client environment:user.dir=/Users/seven/project/scala/dataMining
[INFO][2018-05-24 21:07:55,598][org.apache.zookeeper.ZooKeeper]Initiating client connection, connectString=vm-xaj-bigdata-da-d01:2181,vm-xaj-bigdata-da-d02:2181,vm-xaj-bigdata-da-d03:2181 sessionTimeout=60000 watcher=hconnection-0x54bcd260x0, quorum=vm-xaj-bigdata-da-d01:2181,vm-xaj-bigdata-da-d02:2181,vm-xaj-bigdata-da-d03:2181, baseZNode=/hbase
[INFO][2018-05-24 21:08:00,323][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527167280000 ms
[WARN][2018-05-24 21:08:00,380][org.apache.hadoop.util.NativeCodeLoader]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO][2018-05-24 21:08:00,532][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 1
[INFO][2018-05-24 21:08:00,633][org.apache.zookeeper.ClientCnxn]Opening socket connection to server vm-xaj-bigdata-da-d03/10.213.4.27:2181. Will not attempt to authenticate using SASL (unknown error)
[INFO][2018-05-24 21:08:00,649][org.apache.zookeeper.ClientCnxn]Socket connection established, initiating session, client: /10.0.8.39:50777, server: vm-xaj-bigdata-da-d03/10.213.4.27:2181
[INFO][2018-05-24 21:08:00,667][org.apache.spark.SparkContext]Starting job: collect at SimulationKafkaSendOutData.scala:25
[INFO][2018-05-24 21:08:00,676][org.apache.zookeeper.ClientCnxn]Session establishment complete on server vm-xaj-bigdata-da-d03/10.213.4.27:2181, sessionid = 0x362b50d75ec5f7f, negotiated timeout = 60000
[INFO][2018-05-24 21:08:00,687][org.apache.spark.scheduler.DAGScheduler]Got job 0 (collect at SimulationKafkaSendOutData.scala:25) with 2 output partitions
[INFO][2018-05-24 21:08:00,689][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 0 (collect at SimulationKafkaSendOutData.scala:25)
[INFO][2018-05-24 21:08:00,690][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:08:00,691][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:08:00,703][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 0 (hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/* MapPartitionsRDD[1] at textFile at SimulationKafkaSendOutData.scala:25), which has no missing parents
[INFO][2018-05-24 21:08:00,745][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1 stored as values in memory (estimated size 3.3 KB, free 912.1 MB)
[INFO][2018-05-24 21:08:00,748][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1_piece0 stored as bytes in memory (estimated size 1969.0 B, free 912.1 MB)
[INFO][2018-05-24 21:08:00,748][org.apache.spark.storage.BlockManagerInfo]Added broadcast_1_piece0 in memory on 192.168.0.102:50763 (size: 1969.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:08:00,752][org.apache.spark.SparkContext]Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:08:00,794][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ResultStage 0 (hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/* MapPartitionsRDD[1] at textFile at SimulationKafkaSendOutData.scala:25) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2018-05-24 21:08:00,795][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 0.0 with 2 tasks
[INFO][2018-05-24 21:08:00,847][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 4883 bytes)
[INFO][2018-05-24 21:08:00,849][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 4883 bytes)
[INFO][2018-05-24 21:08:00,858][org.apache.spark.executor.Executor]Running task 0.0 in stage 0.0 (TID 0)
[INFO][2018-05-24 21:08:00,858][org.apache.spark.executor.Executor]Running task 1.0 in stage 0.0 (TID 1)
[INFO][2018-05-24 21:08:00,931][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/part-00000:0+10717435
[INFO][2018-05-24 21:08:00,932][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/part-00000:10717435+10717436
[WARN][2018-05-24 21:08:01,331][org.apache.hadoop.util.NativeCodeLoader]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO][2018-05-24 21:08:01,432][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:08:01,451][org.apache.spark.executor.Executor]Finished task 0.0 in stage 0.0 (TID 0). 751 bytes result sent to driver
[INFO][2018-05-24 21:08:01,458][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 0.0 (TID 0) in 6058 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:08:01,461][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:08:01,465][org.apache.spark.scheduler.DAGScheduler]ResultStage 0 (foreachPartition at ReceiveKafkaData.scala:66) finished in 6.073 s
[INFO][2018-05-24 21:08:01,472][org.apache.spark.scheduler.DAGScheduler]Job 0 finished: foreachPartition at ReceiveKafkaData.scala:66, took 6.275625 s
[INFO][2018-05-24 21:08:01,477][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527167270000 ms.0 from job set of time 1527167270000 ms
[INFO][2018-05-24 21:08:01,478][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 11.476 s for time 1527167270000 ms (execution: 6.314 s)
[INFO][2018-05-24 21:08:01,478][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527167280000 ms.0 from job set of time 1527167280000 ms
[INFO][2018-05-24 21:08:01,487][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 21:08:01,487][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:08:01,488][org.apache.spark.scheduler.DAGScheduler]Got job 1 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:08:01,488][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 1 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:08:01,488][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:08:01,488][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:08:01,489][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 1 (MapPartitionsRDD[3] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:08:01,491][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 
[INFO][2018-05-24 21:08:01,494][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 21:08:01,499][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1_piece0 stored as bytes in memory (estimated size 1868.0 B, free 912.3 MB)
[INFO][2018-05-24 21:08:01,500][org.apache.spark.storage.BlockManagerInfo]Added broadcast_1_piece0 in memory on 192.168.0.102:50746 (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:08:01,501][org.apache.spark.SparkContext]Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:08:01,502][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:08:01,502][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 1.0 with 1 tasks
[INFO][2018-05-24 21:08:01,503][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:08:01,504][org.apache.spark.executor.Executor]Running task 0.0 in stage 1.0 (TID 1)
[INFO][2018-05-24 21:08:01,544][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12184 is the same as ending offset skipping seven 0
[INFO][2018-05-24 21:08:01,544][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:08:01,546][org.apache.spark.executor.Executor]Finished task 0.0 in stage 1.0 (TID 1). 708 bytes result sent to driver
[INFO][2018-05-24 21:08:01,547][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 1.0 (TID 1) in 44 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:08:01,547][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:08:01,548][org.apache.spark.scheduler.DAGScheduler]ResultStage 1 (foreachPartition at ReceiveKafkaData.scala:66) finished in 0.045 s
[INFO][2018-05-24 21:08:01,548][org.apache.spark.scheduler.DAGScheduler]Job 1 finished: foreachPartition at ReceiveKafkaData.scala:66, took 0.061364 s
[INFO][2018-05-24 21:08:01,550][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527167280000 ms.0 from job set of time 1527167280000 ms
[INFO][2018-05-24 21:08:01,550][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 1.550 s for time 1527167280000 ms (execution: 0.072 s)
[INFO][2018-05-24 21:08:01,551][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 1 from persistence list
[INFO][2018-05-24 21:08:01,556][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 0 from persistence list
[INFO][2018-05-24 21:08:01,557][org.apache.spark.storage.BlockManager]Removing RDD 1
[INFO][2018-05-24 21:08:01,558][org.apache.spark.storage.BlockManager]Removing RDD 0
[INFO][2018-05-24 21:08:01,559][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 21:08:01,560][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 
[INFO][2018-05-24 21:08:03,515][org.apache.spark.storage.memory.MemoryStore]Block taskresult_0 stored as bytes in memory (estimated size 10.4 MB, free 901.7 MB)
[INFO][2018-05-24 21:08:03,518][org.apache.spark.storage.BlockManagerInfo]Added taskresult_0 in memory on 192.168.0.102:50763 (size: 10.4 MB, free: 901.9 MB)
[INFO][2018-05-24 21:08:03,519][org.apache.spark.executor.Executor]Finished task 0.0 in stage 0.0 (TID 0). 10856306 bytes result sent via BlockManager)
[INFO][2018-05-24 21:08:03,552][org.apache.spark.network.client.TransportClientFactory]Successfully created connection to /192.168.0.102:50763 after 19 ms (0 ms spent in bootstraps)
[INFO][2018-05-24 21:08:03,732][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 0.0 (TID 0) in 2899 ms on localhost (executor driver) (1/2)
[INFO][2018-05-24 21:08:03,735][org.apache.spark.storage.BlockManagerInfo]Removed taskresult_0 on 192.168.0.102:50763 in memory (size: 10.4 MB, free: 912.3 MB)
[INFO][2018-05-24 21:08:10,092][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527167290000 ms
[INFO][2018-05-24 21:08:10,094][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527167290000 ms.0 from job set of time 1527167290000 ms
[INFO][2018-05-24 21:08:10,105][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:08:10,107][org.apache.spark.scheduler.DAGScheduler]Got job 2 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:08:10,107][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 2 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:08:10,107][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:08:10,107][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:08:10,109][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 2 (MapPartitionsRDD[5] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:08:10,114][org.apache.spark.storage.memory.MemoryStore]Block broadcast_2 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 21:08:10,118][org.apache.spark.storage.memory.MemoryStore]Block broadcast_2_piece0 stored as bytes in memory (estimated size 1868.0 B, free 912.3 MB)
[INFO][2018-05-24 21:08:10,119][org.apache.spark.storage.BlockManagerInfo]Added broadcast_2_piece0 in memory on 192.168.0.102:50746 (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:08:10,123][org.apache.spark.SparkContext]Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:08:10,125][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[5] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:08:10,125][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 2.0 with 1 tasks
[INFO][2018-05-24 21:08:10,128][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:08:10,129][org.apache.spark.executor.Executor]Running task 0.0 in stage 2.0 (TID 2)
[INFO][2018-05-24 21:08:10,134][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12184 is the same as ending offset skipping seven 0
[INFO][2018-05-24 21:08:10,136][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:08:10,138][org.apache.spark.executor.Executor]Finished task 0.0 in stage 2.0 (TID 2). 708 bytes result sent to driver
[INFO][2018-05-24 21:08:10,139][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 2.0 (TID 2) in 12 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:08:10,139][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:08:10,140][org.apache.spark.scheduler.DAGScheduler]ResultStage 2 (foreachPartition at ReceiveKafkaData.scala:66) finished in 0.014 s
[INFO][2018-05-24 21:08:10,141][org.apache.spark.scheduler.DAGScheduler]Job 2 finished: foreachPartition at ReceiveKafkaData.scala:66, took 0.035053 s
[INFO][2018-05-24 21:08:10,141][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527167290000 ms.0 from job set of time 1527167290000 ms
[INFO][2018-05-24 21:08:10,141][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 3 from persistence list
[INFO][2018-05-24 21:08:10,141][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.141 s for time 1527167290000 ms (execution: 0.047 s)
[INFO][2018-05-24 21:08:10,142][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 2 from persistence list
[INFO][2018-05-24 21:08:10,142][org.apache.spark.storage.BlockManager]Removing RDD 3
[INFO][2018-05-24 21:08:10,143][org.apache.spark.storage.BlockManager]Removing RDD 2
[INFO][2018-05-24 21:08:10,144][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 21:08:10,144][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527167270000 ms
[INFO][2018-05-24 21:08:30,107][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527167300000 ms
[INFO][2018-05-24 21:08:30,108][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527167300000 ms.0 from job set of time 1527167300000 ms
[INFO][2018-05-24 21:08:30,116][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:08:30,117][org.apache.spark.scheduler.DAGScheduler]Got job 3 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:08:30,117][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 3 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:08:30,117][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:08:30,117][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:08:30,118][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 3 (MapPartitionsRDD[7] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:08:30,120][org.apache.spark.storage.memory.MemoryStore]Block broadcast_3 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 21:08:30,125][org.apache.spark.storage.memory.MemoryStore]Block broadcast_3_piece0 stored as bytes in memory (estimated size 1868.0 B, free 912.3 MB)
[INFO][2018-05-24 21:08:30,126][org.apache.spark.storage.BlockManagerInfo]Added broadcast_3_piece0 in memory on 192.168.0.102:50746 (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:08:30,127][org.apache.spark.SparkContext]Created broadcast 3 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:08:30,128][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[7] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:08:30,128][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 3.0 with 1 tasks
[INFO][2018-05-24 21:08:30,129][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 3.0 (TID 3, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:08:30,130][org.apache.spark.executor.Executor]Running task 0.0 in stage 3.0 (TID 3)
[INFO][2018-05-24 21:08:30,133][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12184 is the same as ending offset skipping seven 0
[INFO][2018-05-24 21:08:30,133][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:08:30,134][org.apache.spark.executor.Executor]Finished task 0.0 in stage 3.0 (TID 3). 708 bytes result sent to driver
[INFO][2018-05-24 21:08:30,135][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 3.0 (TID 3) in 6 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:08:30,135][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 3.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:08:30,136][org.apache.spark.scheduler.DAGScheduler]ResultStage 3 (foreachPartition at ReceiveKafkaData.scala:66) finished in 0.007 s
[INFO][2018-05-24 21:08:30,136][org.apache.spark.scheduler.DAGScheduler]Job 3 finished: foreachPartition at ReceiveKafkaData.scala:66, took 0.020197 s
[INFO][2018-05-24 21:08:30,137][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527167300000 ms.0 from job set of time 1527167300000 ms
[INFO][2018-05-24 21:08:30,137][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 10.137 s for time 1527167300000 ms (execution: 0.029 s)
[INFO][2018-05-24 21:08:35,189][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527167310000 ms
[INFO][2018-05-24 21:08:35,189][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 5 from persistence list
[INFO][2018-05-24 21:08:35,191][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527167310000 ms.0 from job set of time 1527167310000 ms
[INFO][2018-05-24 21:08:35,191][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 4 from persistence list
[INFO][2018-05-24 21:08:35,192][org.apache.spark.storage.BlockManager]Removing RDD 5
[INFO][2018-05-24 21:08:35,195][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 21:08:35,196][org.apache.spark.storage.BlockManager]Removing RDD 4
[INFO][2018-05-24 21:08:35,196][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527167280000 ms
[INFO][2018-05-24 21:08:35,199][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:08:35,200][org.apache.spark.scheduler.DAGScheduler]Got job 4 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:08:35,200][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 4 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:08:35,200][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:08:35,200][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:08:35,201][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 4 (MapPartitionsRDD[9] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:08:35,203][org.apache.spark.storage.memory.MemoryStore]Block broadcast_4 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 21:08:35,207][org.apache.spark.storage.memory.MemoryStore]Block broadcast_4_piece0 stored as bytes in memory (estimated size 1868.0 B, free 912.3 MB)
[INFO][2018-05-24 21:08:35,208][org.apache.spark.storage.BlockManagerInfo]Added broadcast_4_piece0 in memory on 192.168.0.102:50746 (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:08:35,209][org.apache.spark.SparkContext]Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:08:35,211][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[9] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:08:35,211][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 4.0 with 1 tasks
[INFO][2018-05-24 21:08:35,212][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 4.0 (TID 4, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:08:35,212][org.apache.spark.executor.Executor]Running task 0.0 in stage 4.0 (TID 4)
[INFO][2018-05-24 21:08:35,216][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12184 is the same as ending offset skipping seven 0
[INFO][2018-05-24 21:08:35,216][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:08:35,217][org.apache.spark.executor.Executor]Finished task 0.0 in stage 4.0 (TID 4). 708 bytes result sent to driver
[INFO][2018-05-24 21:08:35,218][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 4.0 (TID 4) in 6 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:08:35,218][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 4.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:08:35,219][org.apache.spark.scheduler.DAGScheduler]ResultStage 4 (foreachPartition at ReceiveKafkaData.scala:66) finished in 0.008 s
[INFO][2018-05-24 21:08:35,219][org.apache.spark.scheduler.DAGScheduler]Job 4 finished: foreachPartition at ReceiveKafkaData.scala:66, took 0.020352 s
[INFO][2018-05-24 21:08:35,220][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527167310000 ms.0 from job set of time 1527167310000 ms
[INFO][2018-05-24 21:08:35,220][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 7 from persistence list
[INFO][2018-05-24 21:08:35,220][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 5.219 s for time 1527167310000 ms (execution: 0.028 s)
[INFO][2018-05-24 21:08:35,220][org.apache.spark.storage.BlockManager]Removing RDD 7
[INFO][2018-05-24 21:08:35,220][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 6 from persistence list
[INFO][2018-05-24 21:08:35,220][org.apache.spark.storage.BlockManager]Removing RDD 6
[INFO][2018-05-24 21:08:35,221][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 21:08:35,221][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527167290000 ms
[INFO][2018-05-24 21:08:46,093][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527167320000 ms
[INFO][2018-05-24 21:08:46,094][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527167320000 ms.0 from job set of time 1527167320000 ms
[INFO][2018-05-24 21:08:46,101][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:08:46,102][org.apache.spark.scheduler.DAGScheduler]Got job 5 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:08:46,102][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 5 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:08:46,102][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:08:46,102][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:08:46,102][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 5 (MapPartitionsRDD[11] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:08:46,105][org.apache.spark.storage.memory.MemoryStore]Block broadcast_5 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 21:08:46,109][org.apache.spark.storage.memory.MemoryStore]Block broadcast_5_piece0 stored as bytes in memory (estimated size 1868.0 B, free 912.3 MB)
[INFO][2018-05-24 21:08:46,110][org.apache.spark.storage.BlockManagerInfo]Added broadcast_5_piece0 in memory on 192.168.0.102:50746 (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:08:46,110][org.apache.spark.SparkContext]Created broadcast 5 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:08:46,111][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[11] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:08:46,112][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 5.0 with 1 tasks
[INFO][2018-05-24 21:08:46,113][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 5.0 (TID 5, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:08:46,113][org.apache.spark.executor.Executor]Running task 0.0 in stage 5.0 (TID 5)
[INFO][2018-05-24 21:08:46,116][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12184 is the same as ending offset skipping seven 0
[INFO][2018-05-24 21:08:46,116][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:08:46,117][org.apache.spark.executor.Executor]Finished task 0.0 in stage 5.0 (TID 5). 708 bytes result sent to driver
[INFO][2018-05-24 21:08:46,119][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 5.0 (TID 5) in 7 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:08:46,119][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 5.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:08:46,120][org.apache.spark.scheduler.DAGScheduler]ResultStage 5 (foreachPartition at ReceiveKafkaData.scala:66) finished in 0.007 s
[INFO][2018-05-24 21:08:46,120][org.apache.spark.scheduler.DAGScheduler]Job 5 finished: foreachPartition at ReceiveKafkaData.scala:66, took 0.019206 s
[INFO][2018-05-24 21:08:46,121][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527167320000 ms.0 from job set of time 1527167320000 ms
[INFO][2018-05-24 21:08:46,121][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 9 from persistence list
[INFO][2018-05-24 21:08:46,121][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 6.121 s for time 1527167320000 ms (execution: 0.027 s)
[INFO][2018-05-24 21:08:46,122][org.apache.spark.storage.BlockManager]Removing RDD 9
[INFO][2018-05-24 21:08:46,122][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 8 from persistence list
[INFO][2018-05-24 21:08:46,124][org.apache.spark.storage.BlockManager]Removing RDD 8
[INFO][2018-05-24 21:08:46,124][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 21:08:46,125][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527167300000 ms
[INFO][2018-05-24 21:08:55,079][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527167330000 ms
[INFO][2018-05-24 21:08:55,081][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527167330000 ms.0 from job set of time 1527167330000 ms
[INFO][2018-05-24 21:08:55,089][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:08:55,089][org.apache.spark.scheduler.DAGScheduler]Got job 6 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:08:55,089][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 6 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:08:55,089][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:08:55,090][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:08:55,090][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 6 (MapPartitionsRDD[13] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:08:55,092][org.apache.spark.storage.memory.MemoryStore]Block broadcast_6 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 21:08:55,136][org.apache.spark.storage.memory.MemoryStore]Block broadcast_6_piece0 stored as bytes in memory (estimated size 1868.0 B, free 912.3 MB)
[INFO][2018-05-24 21:08:55,139][org.apache.spark.storage.BlockManagerInfo]Added broadcast_6_piece0 in memory on 192.168.0.102:50746 (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:08:55,140][org.apache.spark.SparkContext]Created broadcast 6 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:08:55,142][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[13] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:08:55,142][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 6.0 with 1 tasks
[INFO][2018-05-24 21:08:55,144][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 6.0 (TID 6, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:08:55,146][org.apache.spark.executor.Executor]Running task 0.0 in stage 6.0 (TID 6)
[INFO][2018-05-24 21:08:55,149][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12184 is the same as ending offset skipping seven 0
[INFO][2018-05-24 21:08:55,149][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:08:55,150][org.apache.spark.executor.Executor]Finished task 0.0 in stage 6.0 (TID 6). 708 bytes result sent to driver
[INFO][2018-05-24 21:08:55,150][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 6.0 (TID 6) in 6 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:08:55,151][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 6.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:08:55,151][org.apache.spark.scheduler.DAGScheduler]ResultStage 6 (foreachPartition at ReceiveKafkaData.scala:66) finished in 0.008 s
[INFO][2018-05-24 21:08:55,152][org.apache.spark.scheduler.DAGScheduler]Job 6 finished: foreachPartition at ReceiveKafkaData.scala:66, took 0.062843 s
[INFO][2018-05-24 21:08:55,152][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527167330000 ms.0 from job set of time 1527167330000 ms
[INFO][2018-05-24 21:08:55,152][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 11 from persistence list
[INFO][2018-05-24 21:08:55,152][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 5.152 s for time 1527167330000 ms (execution: 0.071 s)
[INFO][2018-05-24 21:08:55,153][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_5_piece0 on 192.168.0.102:50746 in memory (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:08:55,153][org.apache.spark.storage.BlockManager]Removing RDD 11
[INFO][2018-05-24 21:08:55,154][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 10 from persistence list
[INFO][2018-05-24 21:08:55,154][org.apache.spark.storage.BlockManager]Removing RDD 10
[INFO][2018-05-24 21:08:55,154][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 21:08:55,154][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527167310000 ms
[INFO][2018-05-24 21:08:55,155][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_0_piece0 on 192.168.0.102:50746 in memory (size: 1867.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:08:55,157][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_3_piece0 on 192.168.0.102:50746 in memory (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:08:55,158][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_1_piece0 on 192.168.0.102:50746 in memory (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:08:55,159][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_2_piece0 on 192.168.0.102:50746 in memory (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:08:55,161][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_4_piece0 on 192.168.0.102:50746 in memory (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:09:00,103][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527167340000 ms
[INFO][2018-05-24 21:09:00,103][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527167340000 ms.0 from job set of time 1527167340000 ms
[INFO][2018-05-24 21:09:00,109][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:09:00,110][org.apache.spark.scheduler.DAGScheduler]Got job 7 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:09:00,110][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 7 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:09:00,110][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:09:00,110][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:09:00,111][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 7 (MapPartitionsRDD[15] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:09:00,113][org.apache.spark.storage.memory.MemoryStore]Block broadcast_7 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 21:09:00,115][org.apache.spark.storage.memory.MemoryStore]Block broadcast_7_piece0 stored as bytes in memory (estimated size 1868.0 B, free 912.3 MB)
[INFO][2018-05-24 21:09:00,115][org.apache.spark.storage.BlockManagerInfo]Added broadcast_7_piece0 in memory on 192.168.0.102:50746 (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:09:00,116][org.apache.spark.SparkContext]Created broadcast 7 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:09:00,117][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[15] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:09:00,117][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 7.0 with 1 tasks
[INFO][2018-05-24 21:09:00,118][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 7.0 (TID 7, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:09:00,118][org.apache.spark.executor.Executor]Running task 0.0 in stage 7.0 (TID 7)
[INFO][2018-05-24 21:09:00,121][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12184 is the same as ending offset skipping seven 0
[INFO][2018-05-24 21:09:00,121][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:09:00,122][org.apache.spark.executor.Executor]Finished task 0.0 in stage 7.0 (TID 7). 708 bytes result sent to driver
[INFO][2018-05-24 21:09:00,123][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 7.0 (TID 7) in 5 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:09:00,123][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 7.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:09:00,124][org.apache.spark.scheduler.DAGScheduler]ResultStage 7 (foreachPartition at ReceiveKafkaData.scala:66) finished in 0.007 s
[INFO][2018-05-24 21:09:00,124][org.apache.spark.scheduler.DAGScheduler]Job 7 finished: foreachPartition at ReceiveKafkaData.scala:66, took 0.014799 s
[INFO][2018-05-24 21:09:00,126][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527167340000 ms.0 from job set of time 1527167340000 ms
[INFO][2018-05-24 21:09:00,126][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 13 from persistence list
[INFO][2018-05-24 21:09:00,126][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.126 s for time 1527167340000 ms (execution: 0.023 s)
[INFO][2018-05-24 21:09:00,127][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 12 from persistence list
[INFO][2018-05-24 21:09:00,127][org.apache.spark.storage.BlockManager]Removing RDD 13
[INFO][2018-05-24 21:09:00,128][org.apache.spark.storage.BlockManager]Removing RDD 12
[INFO][2018-05-24 21:09:00,128][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 21:09:00,128][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527167320000 ms
[INFO][2018-05-24 21:09:20,099][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527167350000 ms
[INFO][2018-05-24 21:09:20,099][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527167350000 ms.0 from job set of time 1527167350000 ms
[INFO][2018-05-24 21:09:20,105][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:09:20,106][org.apache.spark.scheduler.DAGScheduler]Got job 8 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:09:20,106][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 8 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:09:20,106][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:09:20,107][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:09:20,107][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 8 (MapPartitionsRDD[17] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:09:20,109][org.apache.spark.storage.memory.MemoryStore]Block broadcast_8 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 21:09:20,110][org.apache.spark.storage.memory.MemoryStore]Block broadcast_8_piece0 stored as bytes in memory (estimated size 1868.0 B, free 912.3 MB)
[INFO][2018-05-24 21:09:20,111][org.apache.spark.storage.BlockManagerInfo]Added broadcast_8_piece0 in memory on 192.168.0.102:50746 (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:09:20,112][org.apache.spark.SparkContext]Created broadcast 8 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:09:20,113][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[17] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:09:20,113][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 8.0 with 1 tasks
[INFO][2018-05-24 21:09:20,114][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 8.0 (TID 8, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:09:20,115][org.apache.spark.executor.Executor]Running task 0.0 in stage 8.0 (TID 8)
[INFO][2018-05-24 21:09:20,117][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12184 is the same as ending offset skipping seven 0
[INFO][2018-05-24 21:09:20,117][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:09:20,118][org.apache.spark.executor.Executor]Finished task 0.0 in stage 8.0 (TID 8). 708 bytes result sent to driver
[INFO][2018-05-24 21:09:20,119][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 8.0 (TID 8) in 5 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:09:20,119][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 8.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:09:20,120][org.apache.spark.scheduler.DAGScheduler]ResultStage 8 (foreachPartition at ReceiveKafkaData.scala:66) finished in 0.006 s
[INFO][2018-05-24 21:09:20,122][org.apache.spark.scheduler.DAGScheduler]Job 8 finished: foreachPartition at ReceiveKafkaData.scala:66, took 0.016624 s
[INFO][2018-05-24 21:09:20,123][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527167350000 ms.0 from job set of time 1527167350000 ms
[INFO][2018-05-24 21:09:20,123][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 10.123 s for time 1527167350000 ms (execution: 0.024 s)
[INFO][2018-05-24 21:09:20,176][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527167360000 ms
[INFO][2018-05-24 21:09:20,176][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 15 from persistence list
[INFO][2018-05-24 21:09:20,176][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527167360000 ms.0 from job set of time 1527167360000 ms
[INFO][2018-05-24 21:09:20,177][org.apache.spark.storage.BlockManager]Removing RDD 15
[INFO][2018-05-24 21:09:20,177][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 14 from persistence list
[INFO][2018-05-24 21:09:20,179][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 21:09:20,179][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527167330000 ms
[INFO][2018-05-24 21:09:20,179][org.apache.spark.storage.BlockManager]Removing RDD 14
[INFO][2018-05-24 21:09:20,183][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:09:20,184][org.apache.spark.scheduler.DAGScheduler]Got job 9 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:09:20,184][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 9 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:09:20,184][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:09:20,184][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:09:20,185][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 9 (MapPartitionsRDD[19] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:09:20,187][org.apache.spark.storage.memory.MemoryStore]Block broadcast_9 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 21:09:20,188][org.apache.spark.storage.memory.MemoryStore]Block broadcast_9_piece0 stored as bytes in memory (estimated size 1868.0 B, free 912.3 MB)
[INFO][2018-05-24 21:09:20,189][org.apache.spark.storage.BlockManagerInfo]Added broadcast_9_piece0 in memory on 192.168.0.102:50746 (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:09:20,190][org.apache.spark.SparkContext]Created broadcast 9 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:09:20,190][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[19] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:09:20,190][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 9.0 with 1 tasks
[INFO][2018-05-24 21:09:20,191][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 9.0 (TID 9, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:09:20,192][org.apache.spark.executor.Executor]Running task 0.0 in stage 9.0 (TID 9)
[INFO][2018-05-24 21:09:20,193][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12184 is the same as ending offset skipping seven 0
[INFO][2018-05-24 21:09:20,194][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:09:20,195][org.apache.spark.executor.Executor]Finished task 0.0 in stage 9.0 (TID 9). 708 bytes result sent to driver
[INFO][2018-05-24 21:09:20,195][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 9.0 (TID 9) in 4 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:09:20,195][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 9.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:09:20,196][org.apache.spark.scheduler.DAGScheduler]ResultStage 9 (foreachPartition at ReceiveKafkaData.scala:66) finished in 0.005 s
[INFO][2018-05-24 21:09:20,196][org.apache.spark.scheduler.DAGScheduler]Job 9 finished: foreachPartition at ReceiveKafkaData.scala:66, took 0.012819 s
[INFO][2018-05-24 21:09:20,197][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527167360000 ms.0 from job set of time 1527167360000 ms
[INFO][2018-05-24 21:09:20,197][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 17 from persistence list
[INFO][2018-05-24 21:09:20,197][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.197 s for time 1527167360000 ms (execution: 0.021 s)
[INFO][2018-05-24 21:09:20,197][org.apache.spark.storage.BlockManager]Removing RDD 17
[INFO][2018-05-24 21:09:20,197][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 16 from persistence list
[INFO][2018-05-24 21:09:20,198][org.apache.spark.storage.BlockManager]Removing RDD 16
[INFO][2018-05-24 21:09:20,198][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 21:09:20,198][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527167340000 ms
[INFO][2018-05-24 21:09:35,068][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527167370000 ms
[INFO][2018-05-24 21:09:35,069][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527167370000 ms.0 from job set of time 1527167370000 ms
[INFO][2018-05-24 21:09:35,075][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:09:35,075][org.apache.spark.scheduler.DAGScheduler]Got job 10 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:09:35,076][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 10 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:09:35,076][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:09:35,076][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:09:35,076][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 10 (MapPartitionsRDD[21] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:09:35,078][org.apache.spark.storage.memory.MemoryStore]Block broadcast_10 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 21:09:35,079][org.apache.spark.storage.memory.MemoryStore]Block broadcast_10_piece0 stored as bytes in memory (estimated size 1868.0 B, free 912.3 MB)
[INFO][2018-05-24 21:09:35,080][org.apache.spark.storage.BlockManagerInfo]Added broadcast_10_piece0 in memory on 192.168.0.102:50746 (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:09:35,080][org.apache.spark.SparkContext]Created broadcast 10 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:09:35,081][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[21] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:09:35,081][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 10.0 with 1 tasks
[INFO][2018-05-24 21:09:35,081][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 10.0 (TID 10, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:09:35,082][org.apache.spark.executor.Executor]Running task 0.0 in stage 10.0 (TID 10)
[INFO][2018-05-24 21:09:35,083][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12184 is the same as ending offset skipping seven 0
[INFO][2018-05-24 21:09:35,083][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:09:35,084][org.apache.spark.executor.Executor]Finished task 0.0 in stage 10.0 (TID 10). 708 bytes result sent to driver
[INFO][2018-05-24 21:09:35,085][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 10.0 (TID 10) in 4 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:09:35,085][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 10.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:09:35,085][org.apache.spark.scheduler.DAGScheduler]ResultStage 10 (foreachPartition at ReceiveKafkaData.scala:66) finished in 0.004 s
[INFO][2018-05-24 21:09:35,085][org.apache.spark.scheduler.DAGScheduler]Job 10 finished: foreachPartition at ReceiveKafkaData.scala:66, took 0.010696 s
[INFO][2018-05-24 21:09:35,086][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527167370000 ms.0 from job set of time 1527167370000 ms
[INFO][2018-05-24 21:09:35,086][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 5.086 s for time 1527167370000 ms (execution: 0.017 s)
[INFO][2018-05-24 21:09:35,086][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 19 from persistence list
[INFO][2018-05-24 21:09:35,087][org.apache.spark.storage.BlockManager]Removing RDD 19
[INFO][2018-05-24 21:09:35,087][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 18 from persistence list
[INFO][2018-05-24 21:09:35,087][org.apache.spark.storage.BlockManager]Removing RDD 18
[INFO][2018-05-24 21:09:35,087][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 21:09:35,087][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527167350000 ms
[INFO][2018-05-24 21:09:37,216][org.apache.spark.storage.memory.MemoryStore]Block taskresult_1 stored as bytes in memory (estimated size 10.4 MB, free 901.7 MB)
[INFO][2018-05-24 21:09:37,217][org.apache.spark.storage.BlockManagerInfo]Added taskresult_1 in memory on 192.168.0.102:50763 (size: 10.4 MB, free: 901.9 MB)
[INFO][2018-05-24 21:09:37,217][org.apache.spark.executor.Executor]Finished task 1.0 in stage 0.0 (TID 1). 10855899 bytes result sent via BlockManager)
[INFO][2018-05-24 21:09:37,288][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 0.0 (TID 1) in 96438 ms on localhost (executor driver) (2/2)
[INFO][2018-05-24 21:09:37,288][org.apache.spark.storage.BlockManagerInfo]Removed taskresult_1 on 192.168.0.102:50763 in memory (size: 10.4 MB, free: 912.3 MB)
[INFO][2018-05-24 21:09:37,289][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:09:37,290][org.apache.spark.scheduler.DAGScheduler]ResultStage 0 (collect at SimulationKafkaSendOutData.scala:25) finished in 96.469 s
[INFO][2018-05-24 21:09:37,295][org.apache.spark.scheduler.DAGScheduler]Job 0 finished: collect at SimulationKafkaSendOutData.scala:25, took 96.626874 s
[INFO][2018-05-24 21:09:37,438][org.spark_project.jetty.server.AbstractConnector]Stopped Spark@16751330{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}
[INFO][2018-05-24 21:09:37,440][org.apache.spark.ui.SparkUI]Stopped Spark web UI at http://192.168.0.102:4041
[INFO][2018-05-24 21:09:37,448][org.apache.spark.MapOutputTrackerMasterEndpoint]MapOutputTrackerMasterEndpoint stopped!
[INFO][2018-05-24 21:09:37,481][org.apache.spark.storage.memory.MemoryStore]MemoryStore cleared
[INFO][2018-05-24 21:09:37,481][org.apache.spark.storage.BlockManager]BlockManager stopped
[INFO][2018-05-24 21:09:37,482][org.apache.spark.storage.BlockManagerMaster]BlockManagerMaster stopped
[INFO][2018-05-24 21:09:37,485][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint]OutputCommitCoordinator stopped!
[INFO][2018-05-24 21:09:37,486][org.apache.spark.SparkContext]Successfully stopped SparkContext
[INFO][2018-05-24 21:09:37,496][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	compression.type = none
	metric.reporters = []
	metadata.max.age.ms = 300000
	metadata.fetch.timeout.ms = 60000
	acks = all
	batch.size = 16384
	reconnect.backoff.ms = 10
	bootstrap.servers = [vm-xaj-bigdata-da-d01:9092, vm-xaj-bigdata-da-d02:9092, vm-xaj-bigdata-da-d03:9092, vm-xaj-bigdata-da-d04:9092, vm-xaj-bigdata-da-d05:9092, vm-xaj-bigdata-da-d06:9092, vm-xaj-bigdata-da-d07:9092]
	receive.buffer.bytes = 32768
	retry.backoff.ms = 100
	buffer.memory = 33554432
	timeout.ms = 30000
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	retries = 0
	max.request.size = 1048576
	block.on.buffer.full = true
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
	metrics.sample.window.ms = 30000
	send.buffer.bytes = 131072
	max.in.flight.requests.per.connection = 5
	metrics.num.samples = 2
	linger.ms = 1
	client.id = 

[INFO][2018-05-24 21:09:40,053][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527167380000 ms
[INFO][2018-05-24 21:09:40,054][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527167380000 ms.0 from job set of time 1527167380000 ms
[INFO][2018-05-24 21:09:40,065][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:09:40,066][org.apache.spark.scheduler.DAGScheduler]Got job 11 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:09:40,066][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 11 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:09:40,066][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:09:40,066][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:09:40,067][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 11 (MapPartitionsRDD[23] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:09:40,069][org.apache.spark.storage.memory.MemoryStore]Block broadcast_11 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 21:09:40,070][org.apache.spark.storage.memory.MemoryStore]Block broadcast_11_piece0 stored as bytes in memory (estimated size 1868.0 B, free 912.3 MB)
[INFO][2018-05-24 21:09:40,071][org.apache.spark.storage.BlockManagerInfo]Added broadcast_11_piece0 in memory on 192.168.0.102:50746 (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:09:40,071][org.apache.spark.SparkContext]Created broadcast 11 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:09:40,072][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[23] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:09:40,072][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 11.0 with 1 tasks
[INFO][2018-05-24 21:09:40,073][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 11.0 (TID 11, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:09:40,073][org.apache.spark.executor.Executor]Running task 0.0 in stage 11.0 (TID 11)
[INFO][2018-05-24 21:09:40,075][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12184 is the same as ending offset skipping seven 0
[INFO][2018-05-24 21:09:40,075][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:09:40,076][org.apache.spark.executor.Executor]Finished task 0.0 in stage 11.0 (TID 11). 708 bytes result sent to driver
[INFO][2018-05-24 21:09:40,077][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 11.0 (TID 11) in 5 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:09:40,077][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 11.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:09:40,077][org.apache.spark.scheduler.DAGScheduler]ResultStage 11 (foreachPartition at ReceiveKafkaData.scala:66) finished in 0.005 s
[INFO][2018-05-24 21:09:40,078][org.apache.spark.scheduler.DAGScheduler]Job 11 finished: foreachPartition at ReceiveKafkaData.scala:66, took 0.012115 s
[INFO][2018-05-24 21:09:40,078][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527167380000 ms.0 from job set of time 1527167380000 ms
[INFO][2018-05-24 21:09:40,078][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 21 from persistence list
[INFO][2018-05-24 21:09:40,078][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.078 s for time 1527167380000 ms (execution: 0.025 s)
[INFO][2018-05-24 21:09:40,079][org.apache.spark.storage.BlockManager]Removing RDD 21
[INFO][2018-05-24 21:09:40,079][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 20 from persistence list
[INFO][2018-05-24 21:09:40,080][org.apache.spark.storage.BlockManager]Removing RDD 20
[INFO][2018-05-24 21:09:40,080][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 21:09:40,080][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527167360000 ms
[INFO][2018-05-24 21:09:50,070][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527167390000 ms
[INFO][2018-05-24 21:09:50,070][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527167390000 ms.0 from job set of time 1527167390000 ms
[INFO][2018-05-24 21:09:50,077][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:09:50,078][org.apache.spark.scheduler.DAGScheduler]Got job 12 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:09:50,078][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 12 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:09:50,078][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:09:50,078][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:09:50,078][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 12 (MapPartitionsRDD[25] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:09:50,080][org.apache.spark.storage.memory.MemoryStore]Block broadcast_12 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 21:09:50,082][org.apache.spark.storage.memory.MemoryStore]Block broadcast_12_piece0 stored as bytes in memory (estimated size 1868.0 B, free 912.3 MB)
[INFO][2018-05-24 21:09:50,082][org.apache.spark.storage.BlockManagerInfo]Added broadcast_12_piece0 in memory on 192.168.0.102:50746 (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:09:50,083][org.apache.spark.SparkContext]Created broadcast 12 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:09:50,084][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[25] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:09:50,084][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 12.0 with 1 tasks
[INFO][2018-05-24 21:09:50,084][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 12.0 (TID 12, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:09:50,085][org.apache.spark.executor.Executor]Running task 0.0 in stage 12.0 (TID 12)
[INFO][2018-05-24 21:09:50,086][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12184 is the same as ending offset skipping seven 0
[INFO][2018-05-24 21:09:50,086][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:09:50,087][org.apache.spark.executor.Executor]Finished task 0.0 in stage 12.0 (TID 12). 665 bytes result sent to driver
[INFO][2018-05-24 21:09:50,087][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 12.0 (TID 12) in 3 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:09:50,088][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 12.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:09:50,088][org.apache.spark.scheduler.DAGScheduler]ResultStage 12 (foreachPartition at ReceiveKafkaData.scala:66) finished in 0.004 s
[INFO][2018-05-24 21:09:50,088][org.apache.spark.scheduler.DAGScheduler]Job 12 finished: foreachPartition at ReceiveKafkaData.scala:66, took 0.011433 s
[INFO][2018-05-24 21:09:50,089][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527167390000 ms.0 from job set of time 1527167390000 ms
[INFO][2018-05-24 21:09:50,089][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.089 s for time 1527167390000 ms (execution: 0.019 s)
[INFO][2018-05-24 21:09:50,089][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 23 from persistence list
[INFO][2018-05-24 21:09:50,089][org.apache.spark.storage.BlockManager]Removing RDD 23
[INFO][2018-05-24 21:09:50,090][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 22 from persistence list
[INFO][2018-05-24 21:09:50,090][org.apache.spark.storage.BlockManager]Removing RDD 22
[INFO][2018-05-24 21:09:50,090][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 21:09:50,090][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527167370000 ms
[INFO][2018-05-24 21:10:05,065][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527167400000 ms
[INFO][2018-05-24 21:10:05,066][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527167400000 ms.0 from job set of time 1527167400000 ms
[INFO][2018-05-24 21:10:05,071][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:10:05,072][org.apache.spark.scheduler.DAGScheduler]Got job 13 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:10:05,072][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 13 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:10:05,072][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:10:05,072][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:10:05,073][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 13 (MapPartitionsRDD[27] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:10:05,074][org.apache.spark.storage.memory.MemoryStore]Block broadcast_13 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 21:10:05,077][org.apache.spark.storage.memory.MemoryStore]Block broadcast_13_piece0 stored as bytes in memory (estimated size 1868.0 B, free 912.3 MB)
[INFO][2018-05-24 21:10:05,078][org.apache.spark.storage.BlockManagerInfo]Added broadcast_13_piece0 in memory on 192.168.0.102:50746 (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:10:05,078][org.apache.spark.SparkContext]Created broadcast 13 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:10:05,079][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[27] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:10:05,079][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 13.0 with 1 tasks
[INFO][2018-05-24 21:10:05,080][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 13.0 (TID 13, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:10:05,080][org.apache.spark.executor.Executor]Running task 0.0 in stage 13.0 (TID 13)
[INFO][2018-05-24 21:10:05,082][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12184 is the same as ending offset skipping seven 0
[INFO][2018-05-24 21:10:05,082][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:10:05,083][org.apache.spark.executor.Executor]Finished task 0.0 in stage 13.0 (TID 13). 708 bytes result sent to driver
[INFO][2018-05-24 21:10:05,083][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 13.0 (TID 13) in 3 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:10:05,083][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 13.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:10:05,083][org.apache.spark.scheduler.DAGScheduler]ResultStage 13 (foreachPartition at ReceiveKafkaData.scala:66) finished in 0.004 s
[INFO][2018-05-24 21:10:05,084][org.apache.spark.scheduler.DAGScheduler]Job 13 finished: foreachPartition at ReceiveKafkaData.scala:66, took 0.012404 s
[INFO][2018-05-24 21:10:05,084][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527167400000 ms.0 from job set of time 1527167400000 ms
[INFO][2018-05-24 21:10:05,084][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 5.084 s for time 1527167400000 ms (execution: 0.018 s)
[INFO][2018-05-24 21:10:05,084][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 25 from persistence list
[INFO][2018-05-24 21:10:05,084][org.apache.spark.storage.BlockManager]Removing RDD 25
[INFO][2018-05-24 21:10:05,085][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 24 from persistence list
[INFO][2018-05-24 21:10:05,085][org.apache.spark.storage.BlockManager]Removing RDD 24
[INFO][2018-05-24 21:10:05,085][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 21:10:05,085][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527167380000 ms
[INFO][2018-05-24 21:10:15,071][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527167410000 ms
[INFO][2018-05-24 21:10:15,071][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527167410000 ms.0 from job set of time 1527167410000 ms
[INFO][2018-05-24 21:10:15,077][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:10:15,077][org.apache.spark.scheduler.DAGScheduler]Got job 14 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:10:15,078][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 14 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:10:15,078][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:10:15,078][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:10:15,078][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 14 (MapPartitionsRDD[29] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:10:15,080][org.apache.spark.storage.memory.MemoryStore]Block broadcast_14 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 21:10:15,081][org.apache.spark.storage.memory.MemoryStore]Block broadcast_14_piece0 stored as bytes in memory (estimated size 1868.0 B, free 912.3 MB)
[INFO][2018-05-24 21:10:15,082][org.apache.spark.storage.BlockManagerInfo]Added broadcast_14_piece0 in memory on 192.168.0.102:50746 (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:10:15,082][org.apache.spark.SparkContext]Created broadcast 14 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:10:15,083][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[29] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:10:15,083][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 14.0 with 1 tasks
[INFO][2018-05-24 21:10:15,084][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 14.0 (TID 14, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:10:15,084][org.apache.spark.executor.Executor]Running task 0.0 in stage 14.0 (TID 14)
[INFO][2018-05-24 21:10:15,088][org.apache.spark.streaming.kafka.KafkaRDD]Computing topic seven, partition 0 offsets 12184 -> 12185
[INFO][2018-05-24 21:10:15,089][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 21:10:15,089][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 21:10:15,089][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 21:10:20,522][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527167420000 ms
[INFO][2018-05-24 21:10:20,550][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:10:20,551][org.apache.spark.executor.Executor]Finished task 0.0 in stage 14.0 (TID 14). 665 bytes result sent to driver
[INFO][2018-05-24 21:10:20,551][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 14.0 (TID 14) in 5468 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:10:20,551][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 14.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:10:20,552][org.apache.spark.scheduler.DAGScheduler]ResultStage 14 (foreachPartition at ReceiveKafkaData.scala:66) finished in 5.469 s
[INFO][2018-05-24 21:10:20,552][org.apache.spark.scheduler.DAGScheduler]Job 14 finished: foreachPartition at ReceiveKafkaData.scala:66, took 5.475123 s
[INFO][2018-05-24 21:10:20,552][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527167410000 ms.0 from job set of time 1527167410000 ms
[INFO][2018-05-24 21:10:20,553][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 10.552 s for time 1527167410000 ms (execution: 5.481 s)
[INFO][2018-05-24 21:10:20,553][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527167420000 ms.0 from job set of time 1527167420000 ms
[INFO][2018-05-24 21:10:20,553][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 27 from persistence list
[INFO][2018-05-24 21:10:20,556][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 26 from persistence list
[INFO][2018-05-24 21:10:20,557][org.apache.spark.storage.BlockManager]Removing RDD 27
[INFO][2018-05-24 21:10:20,558][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 21:10:20,558][org.apache.spark.storage.BlockManager]Removing RDD 26
[INFO][2018-05-24 21:10:20,558][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527167390000 ms
[INFO][2018-05-24 21:10:20,559][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:10:20,560][org.apache.spark.scheduler.DAGScheduler]Got job 15 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:10:20,560][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 15 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:10:20,560][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:10:20,560][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:10:20,560][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 15 (MapPartitionsRDD[31] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:10:20,561][org.apache.spark.storage.memory.MemoryStore]Block broadcast_15 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 21:10:20,562][org.apache.spark.storage.memory.MemoryStore]Block broadcast_15_piece0 stored as bytes in memory (estimated size 1868.0 B, free 912.3 MB)
[INFO][2018-05-24 21:10:20,563][org.apache.spark.storage.BlockManagerInfo]Added broadcast_15_piece0 in memory on 192.168.0.102:50746 (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:10:20,563][org.apache.spark.SparkContext]Created broadcast 15 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:10:20,564][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 15 (MapPartitionsRDD[31] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:10:20,564][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 15.0 with 1 tasks
[INFO][2018-05-24 21:10:20,565][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 15.0 (TID 15, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:10:20,565][org.apache.spark.executor.Executor]Running task 0.0 in stage 15.0 (TID 15)
[INFO][2018-05-24 21:10:20,567][org.apache.spark.streaming.kafka.KafkaRDD]Computing topic seven, partition 0 offsets 12185 -> 12188
[INFO][2018-05-24 21:10:20,567][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 21:10:20,567][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 21:10:20,567][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 21:10:20,632][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:10:20,635][org.apache.spark.executor.Executor]Finished task 0.0 in stage 15.0 (TID 15). 708 bytes result sent to driver
[INFO][2018-05-24 21:10:20,638][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 15.0 (TID 15) in 73 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:10:20,638][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 15.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:10:20,638][org.apache.spark.scheduler.DAGScheduler]ResultStage 15 (foreachPartition at ReceiveKafkaData.scala:66) finished in 0.074 s
[INFO][2018-05-24 21:10:20,639][org.apache.spark.scheduler.DAGScheduler]Job 15 finished: foreachPartition at ReceiveKafkaData.scala:66, took 0.079641 s
[INFO][2018-05-24 21:10:20,639][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527167420000 ms.0 from job set of time 1527167420000 ms
[INFO][2018-05-24 21:10:20,640][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.639 s for time 1527167420000 ms (execution: 0.086 s)
[INFO][2018-05-24 21:10:20,640][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 29 from persistence list
[INFO][2018-05-24 21:10:20,640][org.apache.spark.storage.BlockManager]Removing RDD 29
[INFO][2018-05-24 21:10:20,641][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 28 from persistence list
[INFO][2018-05-24 21:10:20,641][org.apache.spark.storage.BlockManager]Removing RDD 28
[INFO][2018-05-24 21:10:20,642][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 21:10:20,642][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527167400000 ms
[INFO][2018-05-24 21:10:35,060][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527167430000 ms
[INFO][2018-05-24 21:10:35,061][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527167430000 ms.0 from job set of time 1527167430000 ms
[INFO][2018-05-24 21:10:35,067][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:10:35,068][org.apache.spark.scheduler.DAGScheduler]Got job 16 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:10:35,068][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 16 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:10:35,068][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:10:35,068][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:10:35,068][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 16 (MapPartitionsRDD[33] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:10:35,070][org.apache.spark.storage.memory.MemoryStore]Block broadcast_16 stored as values in memory (estimated size 3.3 KB, free 912.2 MB)
[INFO][2018-05-24 21:10:35,076][org.apache.spark.storage.memory.MemoryStore]Block broadcast_16_piece0 stored as bytes in memory (estimated size 1868.0 B, free 912.2 MB)
[INFO][2018-05-24 21:10:35,076][org.apache.spark.storage.BlockManagerInfo]Added broadcast_16_piece0 in memory on 192.168.0.102:50746 (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:10:35,077][org.apache.spark.SparkContext]Created broadcast 16 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:10:35,078][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 16 (MapPartitionsRDD[33] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:10:35,078][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 16.0 with 1 tasks
[INFO][2018-05-24 21:10:35,078][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 16.0 (TID 16, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:10:35,079][org.apache.spark.executor.Executor]Running task 0.0 in stage 16.0 (TID 16)
[INFO][2018-05-24 21:10:35,080][org.apache.spark.streaming.kafka.KafkaRDD]Computing topic seven, partition 0 offsets 12188 -> 12193
[INFO][2018-05-24 21:10:35,080][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 21:10:35,080][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 21:10:35,080][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 21:10:37,859][org.apache.spark.util.ShutdownHookManager]Shutdown hook called
[INFO][2018-05-24 21:10:37,861][org.apache.spark.util.ShutdownHookManager]Deleting directory /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/spark-a2240a3a-0a8f-4655-87ee-cae952ae8136
[INFO][2018-05-24 21:10:39,302][org.apache.spark.streaming.StreamingContext]Invoking stop(stopGracefully=false) from shutdown hook
[INFO][2018-05-24 21:10:39,304][org.apache.spark.streaming.scheduler.ReceiverTracker]ReceiverTracker stopped
[INFO][2018-05-24 21:10:39,304][org.apache.spark.streaming.scheduler.JobGenerator]Stopping JobGenerator immediately
[INFO][2018-05-24 21:10:39,305][org.apache.spark.streaming.util.RecurringTimer]Stopped timer for JobGenerator after time 1527167430000
[INFO][2018-05-24 21:10:39,309][org.apache.spark.streaming.scheduler.JobGenerator]Stopped JobGenerator
[INFO][2018-05-24 21:10:40,153][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:10:40,154][org.apache.spark.executor.Executor]Finished task 0.0 in stage 16.0 (TID 16). 708 bytes result sent to driver
[INFO][2018-05-24 21:10:40,155][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 16.0 (TID 16) in 5077 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:10:40,156][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 16.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:10:40,156][org.apache.spark.scheduler.DAGScheduler]ResultStage 16 (foreachPartition at ReceiveKafkaData.scala:66) finished in 5.078 s
[INFO][2018-05-24 21:10:40,157][org.apache.spark.scheduler.DAGScheduler]Job 16 finished: foreachPartition at ReceiveKafkaData.scala:66, took 5.089574 s
[INFO][2018-05-24 21:10:40,157][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527167430000 ms.0 from job set of time 1527167430000 ms
[INFO][2018-05-24 21:10:40,158][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 10.157 s for time 1527167430000 ms (execution: 5.096 s)
[INFO][2018-05-24 21:10:40,160][org.apache.spark.streaming.scheduler.JobScheduler]Stopped JobScheduler
[INFO][2018-05-24 21:10:40,166][org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@1ab6718{/streaming,null,UNAVAILABLE,@Spark}
[INFO][2018-05-24 21:10:40,166][org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@78b41097{/streaming/batch,null,UNAVAILABLE,@Spark}
[INFO][2018-05-24 21:10:40,167][org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@7cf283e1{/static/streaming,null,UNAVAILABLE,@Spark}
[INFO][2018-05-24 21:10:40,168][org.apache.spark.streaming.StreamingContext]StreamingContext stopped successfully
[INFO][2018-05-24 21:10:40,168][org.apache.spark.SparkContext]Invoking stop() from shutdown hook
[INFO][2018-05-24 21:10:40,175][org.spark_project.jetty.server.AbstractConnector]Stopped Spark@6d366c9b{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-05-24 21:10:40,176][org.apache.spark.ui.SparkUI]Stopped Spark web UI at http://192.168.0.102:4040
[INFO][2018-05-24 21:10:40,185][org.apache.spark.MapOutputTrackerMasterEndpoint]MapOutputTrackerMasterEndpoint stopped!
[INFO][2018-05-24 21:10:40,203][org.apache.spark.storage.memory.MemoryStore]MemoryStore cleared
[INFO][2018-05-24 21:10:40,204][org.apache.spark.storage.BlockManager]BlockManager stopped
[INFO][2018-05-24 21:10:40,204][org.apache.spark.storage.BlockManagerMaster]BlockManagerMaster stopped
[INFO][2018-05-24 21:10:40,207][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint]OutputCommitCoordinator stopped!
[INFO][2018-05-24 21:10:40,208][org.apache.spark.SparkContext]Successfully stopped SparkContext
[INFO][2018-05-24 21:10:40,209][org.apache.spark.util.ShutdownHookManager]Shutdown hook called
[INFO][2018-05-24 21:10:40,210][org.apache.spark.util.ShutdownHookManager]Deleting directory /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/spark-232c486d-c3c1-442e-9d8d-bc5001c86041
[INFO][2018-05-24 21:11:04,061][org.apache.spark.SparkContext]Running Spark version 2.2.0
[INFO][2018-05-24 21:11:05,162][org.apache.spark.SparkContext]Submitted application: ReceiveKafkaData$
[INFO][2018-05-24 21:11:05,184][org.apache.spark.SecurityManager]Changing view acls to: seven
[INFO][2018-05-24 21:11:05,185][org.apache.spark.SecurityManager]Changing modify acls to: seven
[INFO][2018-05-24 21:11:05,185][org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO][2018-05-24 21:11:05,186][org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO][2018-05-24 21:11:05,186][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(seven); groups with view permissions: Set(); users  with modify permissions: Set(seven); groups with modify permissions: Set()
[INFO][2018-05-24 21:11:05,474][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 50859.
[INFO][2018-05-24 21:11:05,499][org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO][2018-05-24 21:11:05,534][org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO][2018-05-24 21:11:05,540][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2018-05-24 21:11:05,541][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
[INFO][2018-05-24 21:11:05,556][org.apache.spark.storage.DiskBlockManager]Created local directory at /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/blockmgr-e2b10f97-7286-4247-a936-d6282f209627
[INFO][2018-05-24 21:11:05,576][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 912.3 MB
[INFO][2018-05-24 21:11:05,676][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO][2018-05-24 21:11:05,797][org.spark_project.jetty.util.log]Logging initialized @2742ms
[INFO][2018-05-24 21:11:05,866][org.spark_project.jetty.server.Server]jetty-9.3.z-SNAPSHOT
[INFO][2018-05-24 21:11:05,880][org.spark_project.jetty.server.Server]Started @2827ms
[INFO][2018-05-24 21:11:05,909][org.spark_project.jetty.server.AbstractConnector]Started ServerConnector@1b85fa0b{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-05-24 21:11:05,910][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4040.
[INFO][2018-05-24 21:11:05,937][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@c7a975a{/jobs,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:11:05,939][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@55795845{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:11:05,940][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@119f1f2a{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:11:05,941][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7fd4acee{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:11:05,941][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6175619b{/stages,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:11:05,942][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@756cf158{/stages/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:11:05,943][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@751d3241{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:11:05,944][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@30ea8c23{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:11:05,945][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4e76dac{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:11:05,945][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5f2f577{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:11:05,946][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5d465e4b{/storage,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:11:05,947][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@41a90fa8{/storage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:11:05,947][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@52500920{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:11:05,948][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@18a3962d{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:11:05,949][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2a65bb85{/environment,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:11:05,953][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4f936da8{/environment/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:11:05,954][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@452ba1db{/executors,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:11:05,955][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@76a36b71{/executors/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:11:05,956][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@f9d87b{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:11:05,956][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@26fb628{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:11:05,968][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@70dd7e15{/static,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:11:05,973][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@60d8c0dc{/,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:11:05,978][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6a62689d{/api,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:11:05,979][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1849db1a{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:11:05,980][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1ca25c47{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:11:05,982][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://192.168.0.102:4040
[INFO][2018-05-24 21:11:06,084][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
[INFO][2018-05-24 21:11:06,103][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 50860.
[INFO][2018-05-24 21:11:06,104][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 192.168.0.102:50860
[INFO][2018-05-24 21:11:06,106][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2018-05-24 21:11:06,108][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 192.168.0.102, 50860, None)
[INFO][2018-05-24 21:11:06,111][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 192.168.0.102:50860 with 912.3 MB RAM, BlockManagerId(driver, 192.168.0.102, 50860, None)
[INFO][2018-05-24 21:11:06,114][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 192.168.0.102, 50860, None)
[INFO][2018-05-24 21:11:06,116][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 192.168.0.102, 50860, None)
[INFO][2018-05-24 21:11:06,332][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@17f460bb{/metrics/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:11:06,509][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 21:11:06,514][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 21:11:06,515][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 21:11:19,133][org.apache.spark.SparkContext]Running Spark version 2.2.0
[INFO][2018-05-24 21:11:19,944][org.apache.spark.SparkContext]Submitted application: SimulationKafkaSendOutData$
[INFO][2018-05-24 21:11:19,970][org.apache.spark.SecurityManager]Changing view acls to: seven
[INFO][2018-05-24 21:11:19,972][org.apache.spark.SecurityManager]Changing modify acls to: seven
[INFO][2018-05-24 21:11:19,973][org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO][2018-05-24 21:11:19,974][org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO][2018-05-24 21:11:19,975][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(seven); groups with view permissions: Set(); users  with modify permissions: Set(seven); groups with modify permissions: Set()
[INFO][2018-05-24 21:11:20,382][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 50868.
[INFO][2018-05-24 21:11:20,406][org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO][2018-05-24 21:11:20,423][org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO][2018-05-24 21:11:20,425][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2018-05-24 21:11:20,426][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
[INFO][2018-05-24 21:11:20,438][org.apache.spark.storage.DiskBlockManager]Created local directory at /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/blockmgr-f312f2c6-cef6-4af9-a3a3-7ad8c6ebfeaa
[INFO][2018-05-24 21:11:20,463][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 912.3 MB
[INFO][2018-05-24 21:11:20,561][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO][2018-05-24 21:11:20,715][org.spark_project.jetty.util.log]Logging initialized @2499ms
[INFO][2018-05-24 21:11:20,790][org.spark_project.jetty.server.Server]jetty-9.3.z-SNAPSHOT
[INFO][2018-05-24 21:11:20,802][org.spark_project.jetty.server.Server]Started @2588ms
[WARN][2018-05-24 21:11:20,816][org.apache.spark.util.Utils]Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[INFO][2018-05-24 21:11:20,821][org.spark_project.jetty.server.AbstractConnector]Started ServerConnector@5427d3ac{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}
[INFO][2018-05-24 21:11:20,821][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4041.
[INFO][2018-05-24 21:11:20,843][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@42b64ab8{/jobs,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:11:20,844][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2c1b9e4b{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:11:20,844][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@649725e3{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:11:20,845][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@52b56a3e{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:11:20,846][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4eed46ee{/stages,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:11:20,846][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4fad94a7{/stages/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:11:20,847][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6326d182{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:11:20,848][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@226642a5{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:11:20,849][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5cc126dc{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:11:20,849][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@72bd06ca{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:11:20,850][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5dbe30be{/storage,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:11:20,851][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@55795845{/storage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:11:20,851][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@119f1f2a{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:11:20,852][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5b970f7{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:11:20,853][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@165b8a71{/environment,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:11:20,855][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2f058b8a{/environment/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:11:20,855][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3f2ef586{/executors,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:11:20,856][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@76c7beb3{/executors/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:11:20,856][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2cf92cc7{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:11:20,857][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7b139eab{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:11:20,863][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@611df6e3{/static,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:11:20,864][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6ffab045{/,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:11:20,865][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3e2943ab{/api,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:11:20,866][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6c67e137{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:11:20,866][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@191ae03f{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:11:20,869][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://192.168.0.102:4041
[INFO][2018-05-24 21:11:21,014][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
[INFO][2018-05-24 21:11:21,047][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 50869.
[INFO][2018-05-24 21:11:21,048][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 192.168.0.102:50869
[INFO][2018-05-24 21:11:21,050][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2018-05-24 21:11:21,052][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 192.168.0.102, 50869, None)
[INFO][2018-05-24 21:11:21,056][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 192.168.0.102:50869 with 912.3 MB RAM, BlockManagerId(driver, 192.168.0.102, 50869, None)
[INFO][2018-05-24 21:11:21,061][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 192.168.0.102, 50869, None)
[INFO][2018-05-24 21:11:21,062][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 192.168.0.102, 50869, None)
[INFO][2018-05-24 21:11:21,255][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3e1162e7{/metrics/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:11:21,823][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0 stored as values in memory (estimated size 228.1 KB, free 912.1 MB)
[INFO][2018-05-24 21:11:21,906][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0_piece0 stored as bytes in memory (estimated size 22.1 KB, free 912.1 MB)
[INFO][2018-05-24 21:11:21,909][org.apache.spark.storage.BlockManagerInfo]Added broadcast_0_piece0 in memory on 192.168.0.102:50869 (size: 22.1 KB, free: 912.3 MB)
[INFO][2018-05-24 21:11:21,921][org.apache.spark.SparkContext]Created broadcast 0 from textFile at SimulationKafkaSendOutData.scala:25
[INFO][2018-05-24 21:11:22,143][org.apache.spark.streaming.kafka.DirectKafkaInputDStream]Slide time = 10000 ms
[INFO][2018-05-24 21:11:22,143][org.apache.spark.streaming.kafka.DirectKafkaInputDStream]Storage level = Serialized 1x Replicated
[INFO][2018-05-24 21:11:22,144][org.apache.spark.streaming.kafka.DirectKafkaInputDStream]Checkpoint interval = null
[INFO][2018-05-24 21:11:22,145][org.apache.spark.streaming.kafka.DirectKafkaInputDStream]Remember interval = 10000 ms
[INFO][2018-05-24 21:11:22,145][org.apache.spark.streaming.kafka.DirectKafkaInputDStream]Initialized and validated org.apache.spark.streaming.kafka.DirectKafkaInputDStream@459095da
[INFO][2018-05-24 21:11:22,146][org.apache.spark.streaming.dstream.MappedDStream]Slide time = 10000 ms
[INFO][2018-05-24 21:11:22,146][org.apache.spark.streaming.dstream.MappedDStream]Storage level = Serialized 1x Replicated
[INFO][2018-05-24 21:11:22,146][org.apache.spark.streaming.dstream.MappedDStream]Checkpoint interval = null
[INFO][2018-05-24 21:11:22,146][org.apache.spark.streaming.dstream.MappedDStream]Remember interval = 10000 ms
[INFO][2018-05-24 21:11:22,146][org.apache.spark.streaming.dstream.MappedDStream]Initialized and validated org.apache.spark.streaming.dstream.MappedDStream@41341f7f
[INFO][2018-05-24 21:11:22,146][org.apache.spark.streaming.dstream.ForEachDStream]Slide time = 10000 ms
[INFO][2018-05-24 21:11:22,146][org.apache.spark.streaming.dstream.ForEachDStream]Storage level = Serialized 1x Replicated
[INFO][2018-05-24 21:11:22,147][org.apache.spark.streaming.dstream.ForEachDStream]Checkpoint interval = null
[INFO][2018-05-24 21:11:22,147][org.apache.spark.streaming.dstream.ForEachDStream]Remember interval = 10000 ms
[INFO][2018-05-24 21:11:22,147][org.apache.spark.streaming.dstream.ForEachDStream]Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@3f1bae71
[INFO][2018-05-24 21:11:22,198][org.apache.spark.streaming.util.RecurringTimer]Started timer for JobGenerator at time 1527167490000
[INFO][2018-05-24 21:11:22,198][org.apache.spark.streaming.scheduler.JobGenerator]Started JobGenerator at 1527167490000 ms
[INFO][2018-05-24 21:11:22,199][org.apache.spark.streaming.scheduler.JobScheduler]Started JobScheduler
[INFO][2018-05-24 21:11:22,203][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@57a48985{/streaming,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:11:22,203][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3910fe11{/streaming/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:11:22,206][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3f672204{/streaming/batch,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:11:22,207][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2c2db130{/streaming/batch/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:11:22,208][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2d000e80{/static/streaming,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:11:22,208][org.apache.spark.streaming.StreamingContext]StreamingContext started
[WARN][2018-05-24 21:11:27,351][org.apache.hadoop.util.NativeCodeLoader]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO][2018-05-24 21:11:27,497][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 1
[INFO][2018-05-24 21:11:27,574][org.apache.spark.SparkContext]Starting job: collect at SimulationKafkaSendOutData.scala:25
[INFO][2018-05-24 21:11:27,596][org.apache.spark.scheduler.DAGScheduler]Got job 0 (collect at SimulationKafkaSendOutData.scala:25) with 2 output partitions
[INFO][2018-05-24 21:11:27,598][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 0 (collect at SimulationKafkaSendOutData.scala:25)
[INFO][2018-05-24 21:11:27,598][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:11:27,601][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:11:27,618][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 0 (hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/* MapPartitionsRDD[1] at textFile at SimulationKafkaSendOutData.scala:25), which has no missing parents
[INFO][2018-05-24 21:11:27,640][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1 stored as values in memory (estimated size 3.3 KB, free 912.1 MB)
[INFO][2018-05-24 21:11:27,642][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1_piece0 stored as bytes in memory (estimated size 1969.0 B, free 912.1 MB)
[INFO][2018-05-24 21:11:27,643][org.apache.spark.storage.BlockManagerInfo]Added broadcast_1_piece0 in memory on 192.168.0.102:50869 (size: 1969.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:11:27,643][org.apache.spark.SparkContext]Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:11:27,656][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ResultStage 0 (hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/* MapPartitionsRDD[1] at textFile at SimulationKafkaSendOutData.scala:25) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2018-05-24 21:11:27,657][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 0.0 with 2 tasks
[INFO][2018-05-24 21:11:27,691][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 4883 bytes)
[INFO][2018-05-24 21:11:27,693][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 4883 bytes)
[INFO][2018-05-24 21:11:27,700][org.apache.spark.executor.Executor]Running task 1.0 in stage 0.0 (TID 1)
[INFO][2018-05-24 21:11:27,700][org.apache.spark.executor.Executor]Running task 0.0 in stage 0.0 (TID 0)
[INFO][2018-05-24 21:11:27,750][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/part-00000:0+10717435
[INFO][2018-05-24 21:11:27,750][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/part-00000:10717435+10717436
[INFO][2018-05-24 21:11:30,053][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 21:11:30,053][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 21:11:30,053][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 21:11:35,162][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527167490000 ms
[INFO][2018-05-24 21:11:35,166][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527167490000 ms.0 from job set of time 1527167490000 ms
[INFO][2018-05-24 21:11:35,206][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:11:35,223][org.apache.spark.scheduler.DAGScheduler]Got job 0 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:11:35,224][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 0 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:11:35,225][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:11:35,226][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:11:35,236][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 0 (MapPartitionsRDD[1] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:11:35,372][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 21:11:35,397][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0_piece0 stored as bytes in memory (estimated size 1867.0 B, free 912.3 MB)
[INFO][2018-05-24 21:11:35,398][org.apache.spark.storage.BlockManagerInfo]Added broadcast_0_piece0 in memory on 192.168.0.102:50860 (size: 1867.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:11:35,400][org.apache.spark.SparkContext]Created broadcast 0 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:11:35,414][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:11:35,415][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 0.0 with 1 tasks
[INFO][2018-05-24 21:11:35,450][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:11:35,459][org.apache.spark.executor.Executor]Running task 0.0 in stage 0.0 (TID 0)
[INFO][2018-05-24 21:11:35,490][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12194 is the same as ending offset skipping seven 0
[INFO][2018-05-24 21:11:35,694][org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper]Process identifier=hconnection-0x53fa245a connecting to ZooKeeper ensemble=vm-xaj-bigdata-da-d01:2181,vm-xaj-bigdata-da-d02:2181,vm-xaj-bigdata-da-d03:2181
[INFO][2018-05-24 21:11:35,700][org.apache.zookeeper.ZooKeeper]Client environment:zookeeper.version=3.4.5-cdh5.7.0--1, built on 04/06/2016 14:24 GMT
[INFO][2018-05-24 21:11:35,700][org.apache.zookeeper.ZooKeeper]Client environment:host.name=192.168.0.102
[INFO][2018-05-24 21:11:35,700][org.apache.zookeeper.ZooKeeper]Client environment:java.version=1.8.0_161
[INFO][2018-05-24 21:11:35,700][org.apache.zookeeper.ZooKeeper]Client environment:java.vendor=Oracle Corporation
[INFO][2018-05-24 21:11:35,700][org.apache.zookeeper.ZooKeeper]Client environment:java.home=/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre
[INFO][2018-05-24 21:11:35,700][org.apache.zookeeper.ZooKeeper]Client environment:java.class.path=/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/charsets.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/deploy.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/cldrdata.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/dnsns.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/jaccess.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/jfxrt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/localedata.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/nashorn.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/sunec.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/sunjce_provider.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/sunpkcs11.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/zipfs.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/javaws.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/jce.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/jfr.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/jfxswt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/jsse.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/management-agent.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/plugin.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/resources.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/rt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/ant-javafx.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/dt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/javafx-mx.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/jconsole.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/packager.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/sa-jdi.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/tools.jar:/Users/seven/project/scala/dataMining/target/classes:/Users/seven/software/maven/repository/org/scala-lang/scala-library/2.11.12/scala-library-2.11.12.jar:/Users/seven/software/maven/repository/org/scala-lang/scala-compiler/2.11.12/scala-compiler-2.11.12.jar:/Users/seven/software/maven/repository/org/scala-lang/modules/scala-xml_2.11/1.0.5/scala-xml_2.11-1.0.5.jar:/Users/seven/software/maven/repository/org/scala-lang/modules/scala-parser-combinators_2.11/1.0.4/scala-parser-combinators_2.11-1.0.4.jar:/Users/seven/software/maven/repository/org/scala-lang/scala-reflect/2.11.12/scala-reflect-2.11.12.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-client/2.6.0-cdh5.7.0/hadoop-client-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-common/2.6.0-cdh5.7.0/hadoop-common-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/xmlenc/xmlenc/0.52/xmlenc-0.52.jar:/Users/seven/software/maven/repository/commons-net/commons-net/3.1/commons-net-3.1.jar:/Users/seven/software/maven/repository/commons-configuration/commons-configuration/1.6/commons-configuration-1.6.jar:/Users/seven/software/maven/repository/commons-digester/commons-digester/1.8/commons-digester-1.8.jar:/Users/seven/software/maven/repository/commons-beanutils/commons-beanutils/1.7.0/commons-beanutils-1.7.0.jar:/Users/seven/software/maven/repository/org/slf4j/slf4j-api/1.7.5/slf4j-api-1.7.5.jar:/Users/seven/software/maven/repository/org/slf4j/slf4j-log4j12/1.7.5/slf4j-log4j12-1.7.5.jar:/Users/seven/software/maven/repository/org/apache/avro/avro/1.7.6-cdh5.7.0/avro-1.7.6-cdh5.7.0.jar:/Users/seven/software/maven/repository/com/google/code/gson/gson/2.2.4/gson-2.2.4.jar:/Users/seven/software/maven/repository/org/apache/curator/curator-client/2.7.1/curator-client-2.7.1.jar:/Users/seven/software/maven/repository/org/apache/curator/curator-recipes/2.7.1/curator-recipes-2.7.1.jar:/Users/seven/software/maven/repository/com/google/code/findbugs/jsr305/3.0.0/jsr305-3.0.0.jar:/Users/seven/software/maven/repository/org/apache/htrace/htrace-core4/4.0.1-incubating/htrace-core4-4.0.1-incubating.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-compress/1.4.1/commons-compress-1.4.1.jar:/Users/seven/software/maven/repository/org/tukaani/xz/1.0/xz-1.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-hdfs/2.6.0-cdh5.7.0/hadoop-hdfs-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/io/netty/netty/3.6.2.Final/netty-3.6.2.Final.jar:/Users/seven/software/maven/repository/xerces/xercesImpl/2.9.1/xercesImpl-2.9.1.jar:/Users/seven/software/maven/repository/xml-apis/xml-apis/1.3.04/xml-apis-1.3.04.jar:/Users/seven/software/maven/repository/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-app/2.6.0-cdh5.7.0/hadoop-mapreduce-client-app-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-common/2.6.0-cdh5.7.0/hadoop-mapreduce-client-common-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-yarn-client/2.6.0-cdh5.7.0/hadoop-yarn-client-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-yarn-server-common/2.6.0-cdh5.7.0/hadoop-yarn-server-common-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-shuffle/2.6.0-cdh5.7.0/hadoop-mapreduce-client-shuffle-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-yarn-api/2.6.0-cdh5.7.0/hadoop-yarn-api-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-core/2.6.0-cdh5.7.0/hadoop-mapreduce-client-core-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-yarn-common/2.6.0-cdh5.7.0/hadoop-yarn-common-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/javax/xml/bind/jaxb-api/2.2.2/jaxb-api-2.2.2.jar:/Users/seven/software/maven/repository/javax/xml/stream/stax-api/1.0-2/stax-api-1.0-2.jar:/Users/seven/software/maven/repository/javax/activation/activation/1.1/activation-1.1.jar:/Users/seven/software/maven/repository/com/sun/jersey/jersey-client/1.9/jersey-client-1.9.jar:/Users/seven/software/maven/repository/org/codehaus/jackson/jackson-xc/1.8.8/jackson-xc-1.8.8.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.6.0-cdh5.7.0/hadoop-mapreduce-client-jobclient-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-aws/2.6.0-cdh5.7.0/hadoop-aws-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/com/amazonaws/aws-java-sdk-s3/1.10.6/aws-java-sdk-s3-1.10.6.jar:/Users/seven/software/maven/repository/com/amazonaws/aws-java-sdk-kms/1.10.6/aws-java-sdk-kms-1.10.6.jar:/Users/seven/software/maven/repository/com/amazonaws/aws-java-sdk-core/1.10.6/aws-java-sdk-core-1.10.6.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-annotations/2.6.0-cdh5.7.0/hadoop-annotations-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-client/1.2.0-cdh5.7.0/hbase-client-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-annotations/1.2.0-cdh5.7.0/hbase-annotations-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-common/1.2.0-cdh5.7.0/hbase-common-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-protocol/1.2.0-cdh5.7.0/hbase-protocol-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/commons-codec/commons-codec/1.9/commons-codec-1.9.jar:/Users/seven/software/maven/repository/commons-io/commons-io/2.4/commons-io-2.4.jar:/Users/seven/software/maven/repository/commons-lang/commons-lang/2.6/commons-lang-2.6.jar:/Users/seven/software/maven/repository/commons-logging/commons-logging/1.2/commons-logging-1.2.jar:/Users/seven/software/maven/repository/com/google/guava/guava/12.0.1/guava-12.0.1.jar:/Users/seven/software/maven/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/Users/seven/software/maven/repository/io/netty/netty-all/4.0.23.Final/netty-all-4.0.23.Final.jar:/Users/seven/software/maven/repository/org/apache/zookeeper/zookeeper/3.4.5-cdh5.7.0/zookeeper-3.4.5-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/htrace/htrace-core/3.2.0-incubating/htrace-core-3.2.0-incubating.jar:/Users/seven/software/maven/repository/org/codehaus/jackson/jackson-mapper-asl/1.8.8/jackson-mapper-asl-1.8.8.jar:/Users/seven/software/maven/repository/org/jruby/jcodings/jcodings/1.0.8/jcodings-1.0.8.jar:/Users/seven/software/maven/repository/org/jruby/joni/joni/2.1.2/joni-2.1.2.jar:/Users/seven/software/maven/repository/com/yammer/metrics/metrics-core/2.2.0/metrics-core-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-auth/2.6.0-cdh5.7.0/hadoop-auth-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/httpcomponents/httpclient/4.2.5/httpclient-4.2.5.jar:/Users/seven/software/maven/repository/org/apache/httpcomponents/httpcore/4.2.4/httpcore-4.2.4.jar:/Users/seven/software/maven/repository/org/apache/directory/server/apacheds-kerberos-codec/2.0.0-M15/apacheds-kerberos-codec-2.0.0-M15.jar:/Users/seven/software/maven/repository/org/apache/directory/server/apacheds-i18n/2.0.0-M15/apacheds-i18n-2.0.0-M15.jar:/Users/seven/software/maven/repository/org/apache/directory/api/api-asn1-api/1.0.0-M20/api-asn1-api-1.0.0-M20.jar:/Users/seven/software/maven/repository/org/apache/directory/api/api-util/1.0.0-M20/api-util-1.0.0-M20.jar:/Users/seven/software/maven/repository/org/apache/curator/curator-framework/2.7.1/curator-framework-2.7.1.jar:/Users/seven/software/maven/repository/com/github/stephenc/findbugs/findbugs-annotations/1.3.9-1/findbugs-annotations-1.3.9-1.jar:/Users/seven/software/maven/repository/junit/junit/4.12/junit-4.12.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-server/1.2.0-cdh5.7.0/hbase-server-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-procedure/1.2.0-cdh5.7.0/hbase-procedure-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-common/1.2.0-cdh5.7.0/hbase-common-1.2.0-cdh5.7.0-tests.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-prefix-tree/1.2.0-cdh5.7.0/hbase-prefix-tree-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar:/Users/seven/software/maven/repository/commons-collections/commons-collections/3.2.2/commons-collections-3.2.2.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-hadoop-compat/1.2.0-cdh5.7.0/hbase-hadoop-compat-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-hadoop2-compat/1.2.0-cdh5.7.0/hbase-hadoop2-compat-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/com/sun/jersey/jersey-core/1.9/jersey-core-1.9.jar:/Users/seven/software/maven/repository/com/sun/jersey/jersey-server/1.9/jersey-server-1.9.jar:/Users/seven/software/maven/repository/asm/asm/3.1/asm-3.1.jar:/Users/seven/software/maven/repository/commons-cli/commons-cli/1.2/commons-cli-1.2.jar:/Users/seven/software/maven/repository/com/github/stephenc/high-scale-lib/high-scale-lib/1.1.1/high-scale-lib-1.1.1.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-math/2.1/commons-math-2.1.jar:/Users/seven/software/maven/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jetty/6.1.26.cloudera.4/jetty-6.1.26.cloudera.4.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jetty-util/6.1.26.cloudera.4/jetty-util-6.1.26.cloudera.4.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jetty-sslengine/6.1.26.cloudera.4/jetty-sslengine-6.1.26.cloudera.4.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jsp-2.1/6.1.14/jsp-2.1-6.1.14.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jsp-api-2.1/6.1.14/jsp-api-2.1-6.1.14.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/servlet-api-2.5/6.1.14/servlet-api-2.5-6.1.14.jar:/Users/seven/software/maven/repository/org/codehaus/jackson/jackson-core-asl/1.8.8/jackson-core-asl-1.8.8.jar:/Users/seven/software/maven/repository/org/codehaus/jackson/jackson-jaxrs/1.8.8/jackson-jaxrs-1.8.8.jar:/Users/seven/software/maven/repository/tomcat/jasper-compiler/5.5.23/jasper-compiler-5.5.23.jar:/Users/seven/software/maven/repository/tomcat/jasper-runtime/5.5.23/jasper-runtime-5.5.23.jar:/Users/seven/software/maven/repository/commons-el/commons-el/1.0/commons-el-1.0.jar:/Users/seven/software/maven/repository/org/jamon/jamon-runtime/2.4.1/jamon-runtime-2.4.1.jar:/Users/seven/software/maven/repository/com/lmax/disruptor/3.3.0/disruptor-3.3.0.jar:/Users/seven/software/maven/repository/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar:/Users/seven/software/maven/repository/org/owasp/esapi/esapi/2.1.0/esapi-2.1.0.jar:/Users/seven/software/maven/repository/commons-beanutils/commons-beanutils-core/1.7.0/commons-beanutils-core-1.7.0.jar:/Users/seven/software/maven/repository/commons-fileupload/commons-fileupload/1.2/commons-fileupload-1.2.jar:/Users/seven/software/maven/repository/xom/xom/1.2.5/xom-1.2.5.jar:/Users/seven/software/maven/repository/xalan/xalan/2.7.0/xalan-2.7.0.jar:/Users/seven/software/maven/repository/org/beanshell/bsh-core/2.0b4/bsh-core-2.0b4.jar:/Users/seven/software/maven/repository/org/owasp/antisamy/antisamy/1.4.3/antisamy-1.4.3.jar:/Users/seven/software/maven/repository/org/apache/xmlgraphics/batik-css/1.7/batik-css-1.7.jar:/Users/seven/software/maven/repository/org/apache/xmlgraphics/batik-ext/1.7/batik-ext-1.7.jar:/Users/seven/software/maven/repository/org/apache/xmlgraphics/batik-util/1.7/batik-util-1.7.jar:/Users/seven/software/maven/repository/xml-apis/xml-apis-ext/1.3.04/xml-apis-ext-1.3.04.jar:/Users/seven/software/maven/repository/net/sourceforge/nekohtml/nekohtml/1.9.12/nekohtml-1.9.12.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-core/2.6.0-mr1-cdh5.7.0/hadoop-core-2.6.0-mr1-cdh5.7.0.jar:/Users/seven/software/maven/repository/javax/servlet/jsp/jsp-api/2.1/jsp-api-2.1.jar:/Users/seven/software/maven/repository/net/java/dev/jets3t/jets3t/0.6.1/jets3t-0.6.1.jar:/Users/seven/software/maven/repository/javax/servlet/servlet-api/2.5/servlet-api-2.5.jar:/Users/seven/software/maven/repository/hsqldb/hsqldb/1.8.0.10/hsqldb-1.8.0.10.jar:/Users/seven/software/maven/repository/org/eclipse/jdt/core/3.1.1/core-3.1.1.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-hdfs/2.6.0-cdh5.7.0/hadoop-hdfs-2.6.0-cdh5.7.0-tests.jar:/Users/seven/software/maven/repository/commons-daemon/commons-daemon/1.0.13/commons-daemon-1.0.13.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-lang3/3.5/commons-lang3-3.5.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-pool2/2.4.2/commons-pool2-2.4.2.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-mllib_2.11/2.2.0/spark-mllib_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-core_2.11/2.2.0/spark-core_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/avro/avro-mapred/1.7.7/avro-mapred-1.7.7-hadoop2.jar:/Users/seven/software/maven/repository/org/apache/avro/avro-ipc/1.7.7/avro-ipc-1.7.7-tests.jar:/Users/seven/software/maven/repository/com/twitter/chill_2.11/0.8.0/chill_2.11-0.8.0.jar:/Users/seven/software/maven/repository/com/esotericsoftware/kryo-shaded/3.0.3/kryo-shaded-3.0.3.jar:/Users/seven/software/maven/repository/com/esotericsoftware/minlog/1.3.0/minlog-1.3.0.jar:/Users/seven/software/maven/repository/org/objenesis/objenesis/2.1/objenesis-2.1.jar:/Users/seven/software/maven/repository/com/twitter/chill-java/0.8.0/chill-java-0.8.0.jar:/Users/seven/software/maven/repository/org/apache/xbean/xbean-asm5-shaded/4.4/xbean-asm5-shaded-4.4.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-launcher_2.11/2.2.0/spark-launcher_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-network-common_2.11/2.2.0/spark-network-common_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-network-shuffle_2.11/2.2.0/spark-network-shuffle_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-unsafe_2.11/2.2.0/spark-unsafe_2.11-2.2.0.jar:/Users/seven/software/maven/repository/javax/servlet/javax.servlet-api/3.1.0/javax.servlet-api-3.1.0.jar:/Users/seven/software/maven/repository/org/slf4j/jul-to-slf4j/1.7.16/jul-to-slf4j-1.7.16.jar:/Users/seven/software/maven/repository/org/slf4j/jcl-over-slf4j/1.7.16/jcl-over-slf4j-1.7.16.jar:/Users/seven/software/maven/repository/com/ning/compress-lzf/1.0.3/compress-lzf-1.0.3.jar:/Users/seven/software/maven/repository/org/xerial/snappy/snappy-java/1.1.2.6/snappy-java-1.1.2.6.jar:/Users/seven/software/maven/repository/org/roaringbitmap/RoaringBitmap/0.5.11/RoaringBitmap-0.5.11.jar:/Users/seven/software/maven/repository/org/json4s/json4s-jackson_2.11/3.2.11/json4s-jackson_2.11-3.2.11.jar:/Users/seven/software/maven/repository/org/json4s/json4s-core_2.11/3.2.11/json4s-core_2.11-3.2.11.jar:/Users/seven/software/maven/repository/org/json4s/json4s-ast_2.11/3.2.11/json4s-ast_2.11-3.2.11.jar:/Users/seven/software/maven/repository/org/scala-lang/scalap/2.11.0/scalap-2.11.0.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/core/jersey-client/2.22.2/jersey-client-2.22.2.jar:/Users/seven/software/maven/repository/javax/ws/rs/javax.ws.rs-api/2.0.1/javax.ws.rs-api-2.0.1.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/hk2-api/2.4.0-b34/hk2-api-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/hk2-utils/2.4.0-b34/hk2-utils-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/external/aopalliance-repackaged/2.4.0-b34/aopalliance-repackaged-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/external/javax.inject/2.4.0-b34/javax.inject-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/hk2-locator/2.4.0-b34/hk2-locator-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/javassist/javassist/3.18.1-GA/javassist-3.18.1-GA.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/core/jersey-common/2.22.2/jersey-common-2.22.2.jar:/Users/seven/software/maven/repository/javax/annotation/javax.annotation-api/1.2/javax.annotation-api-1.2.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/bundles/repackaged/jersey-guava/2.22.2/jersey-guava-2.22.2.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/osgi-resource-locator/1.0.1/osgi-resource-locator-1.0.1.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/core/jersey-server/2.22.2/jersey-server-2.22.2.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/media/jersey-media-jaxb/2.22.2/jersey-media-jaxb-2.22.2.jar:/Users/seven/software/maven/repository/javax/validation/validation-api/1.1.0.Final/validation-api-1.1.0.Final.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/containers/jersey-container-servlet/2.22.2/jersey-container-servlet-2.22.2.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/containers/jersey-container-servlet-core/2.22.2/jersey-container-servlet-core-2.22.2.jar:/Users/seven/software/maven/repository/com/clearspring/analytics/stream/2.7.0/stream-2.7.0.jar:/Users/seven/software/maven/repository/io/dropwizard/metrics/metrics-core/3.1.2/metrics-core-3.1.2.jar:/Users/seven/software/maven/repository/io/dropwizard/metrics/metrics-jvm/3.1.2/metrics-jvm-3.1.2.jar:/Users/seven/software/maven/repository/io/dropwizard/metrics/metrics-json/3.1.2/metrics-json-3.1.2.jar:/Users/seven/software/maven/repository/io/dropwizard/metrics/metrics-graphite/3.1.2/metrics-graphite-3.1.2.jar:/Users/seven/software/maven/repository/org/apache/ivy/ivy/2.4.0/ivy-2.4.0.jar:/Users/seven/software/maven/repository/oro/oro/2.0.8/oro-2.0.8.jar:/Users/seven/software/maven/repository/net/razorvine/pyrolite/4.13/pyrolite-4.13.jar:/Users/seven/software/maven/repository/net/sf/py4j/py4j/0.10.4/py4j-0.10.4.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-crypto/1.0.0/commons-crypto-1.0.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-streaming_2.11/2.2.0/spark-streaming_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-sql_2.11/2.2.0/spark-sql_2.11-2.2.0.jar:/Users/seven/software/maven/repository/com/univocity/univocity-parsers/2.2.1/univocity-parsers-2.2.1.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-sketch_2.11/2.2.0/spark-sketch_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-catalyst_2.11/2.2.0/spark-catalyst_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/codehaus/janino/janino/3.0.0/janino-3.0.0.jar:/Users/seven/software/maven/repository/org/codehaus/janino/commons-compiler/3.0.0/commons-compiler-3.0.0.jar:/Users/seven/software/maven/repository/org/antlr/antlr4-runtime/4.5.3/antlr4-runtime-4.5.3.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-column/1.8.2/parquet-column-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-common/1.8.2/parquet-common-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-encoding/1.8.2/parquet-encoding-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-hadoop/1.8.2/parquet-hadoop-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-format/2.3.1/parquet-format-2.3.1.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-jackson/1.8.2/parquet-jackson-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-graphx_2.11/2.2.0/spark-graphx_2.11-2.2.0.jar:/Users/seven/software/maven/repository/com/github/fommil/netlib/core/1.1.2/core-1.1.2.jar:/Users/seven/software/maven/repository/net/sourceforge/f2j/arpack_combined_all/0.1/arpack_combined_all-0.1.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-mllib-local_2.11/2.2.0/spark-mllib-local_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/scalanlp/breeze_2.11/0.13.1/breeze_2.11-0.13.1.jar:/Users/seven/software/maven/repository/org/scalanlp/breeze-macros_2.11/0.13.1/breeze-macros_2.11-0.13.1.jar:/Users/seven/software/maven/repository/net/sf/opencsv/opencsv/2.3/opencsv-2.3.jar:/Users/seven/software/maven/repository/com/github/rwl/jtransforms/2.4.0/jtransforms-2.4.0.jar:/Users/seven/software/maven/repository/org/spire-math/spire_2.11/0.13.0/spire_2.11-0.13.0.jar:/Users/seven/software/maven/repository/org/spire-math/spire-macros_2.11/0.13.0/spire-macros_2.11-0.13.0.jar:/Users/seven/software/maven/repository/org/typelevel/machinist_2.11/0.6.1/machinist_2.11-0.6.1.jar:/Users/seven/software/maven/repository/com/chuusai/shapeless_2.11/2.3.2/shapeless_2.11-2.3.2.jar:/Users/seven/software/maven/repository/org/typelevel/macro-compat_2.11/1.1.1/macro-compat_2.11-1.1.1.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-math3/3.4.1/commons-math3-3.4.1.jar:/Users/seven/software/maven/repository/org/jpmml/pmml-model/1.2.15/pmml-model-1.2.15.jar:/Users/seven/software/maven/repository/org/jpmml/pmml-schema/1.2.15/pmml-schema-1.2.15.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-tags_2.11/2.2.0/spark-tags_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-streaming-kafka-0-8_2.11/2.2.0/spark-streaming-kafka-0-8_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/kafka/kafka_2.11/0.8.2.1/kafka_2.11-0.8.2.1.jar:/Users/seven/software/maven/repository/com/101tec/zkclient/0.3/zkclient-0.3.jar:/Users/seven/software/maven/repository/org/apache/kafka/kafka-clients/0.8.2.1/kafka-clients-0.8.2.1.jar:/Users/seven/software/maven/repository/mysql/mysql-connector-java/5.1.31/mysql-connector-java-5.1.31.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/module/jackson-module-scala_2.11/2.6.5/jackson-module-scala_2.11-2.6.5.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/core/jackson-core/2.6.5/jackson-core-2.6.5.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/core/jackson-annotations/2.6.5/jackson-annotations-2.6.5.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/core/jackson-databind/2.6.5/jackson-databind-2.6.5.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/module/jackson-module-paranamer/2.6.5/jackson-module-paranamer-2.6.5.jar:/Users/seven/software/maven/repository/com/thoughtworks/paranamer/paranamer/2.6/paranamer-2.6.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-streaming-flume_2.11/2.2.0/spark-streaming-flume_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/flume/flume-ng-core/1.6.0/flume-ng-core-1.6.0.jar:/Users/seven/software/maven/repository/org/apache/flume/flume-ng-configuration/1.6.0/flume-ng-configuration-1.6.0.jar:/Users/seven/software/maven/repository/org/apache/avro/avro-ipc/1.7.4/avro-ipc-1.7.4.jar:/Users/seven/software/maven/repository/org/apache/velocity/velocity/1.7/velocity-1.7.jar:/Users/seven/software/maven/repository/joda-time/joda-time/2.1/joda-time-2.1.jar:/Users/seven/software/maven/repository/org/apache/mina/mina-core/2.0.4/mina-core-2.0.4.jar:/Users/seven/software/maven/repository/org/apache/flume/flume-ng-sdk/1.6.0/flume-ng-sdk-1.6.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-streaming-flume-sink_2.11/2.2.0/spark-streaming-flume-sink_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/flume/flume-ng-clients/flume-ng-log4jappender/1.6.0/flume-ng-log4jappender-1.6.0.jar:/Users/seven/software/maven/repository/net/jpountz/lz4/lz4/1.3.0/lz4-1.3.0.jar:/Applications/IntelliJ IDEA.app/Contents/lib/idea_rt.jar
[INFO][2018-05-24 21:11:35,701][org.apache.zookeeper.ZooKeeper]Client environment:java.library.path=/Users/seven/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:.
[INFO][2018-05-24 21:11:35,701][org.apache.zookeeper.ZooKeeper]Client environment:java.io.tmpdir=/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/
[INFO][2018-05-24 21:11:35,701][org.apache.zookeeper.ZooKeeper]Client environment:java.compiler=<NA>
[INFO][2018-05-24 21:11:35,701][org.apache.zookeeper.ZooKeeper]Client environment:os.name=Mac OS X
[INFO][2018-05-24 21:11:35,701][org.apache.zookeeper.ZooKeeper]Client environment:os.arch=x86_64
[INFO][2018-05-24 21:11:35,701][org.apache.zookeeper.ZooKeeper]Client environment:os.version=10.13.4
[INFO][2018-05-24 21:11:35,701][org.apache.zookeeper.ZooKeeper]Client environment:user.name=seven
[INFO][2018-05-24 21:11:35,701][org.apache.zookeeper.ZooKeeper]Client environment:user.home=/Users/seven
[INFO][2018-05-24 21:11:35,701][org.apache.zookeeper.ZooKeeper]Client environment:user.dir=/Users/seven/project/scala/dataMining
[INFO][2018-05-24 21:11:35,703][org.apache.zookeeper.ZooKeeper]Initiating client connection, connectString=vm-xaj-bigdata-da-d01:2181,vm-xaj-bigdata-da-d02:2181,vm-xaj-bigdata-da-d03:2181 sessionTimeout=60000 watcher=hconnection-0x53fa245a0x0, quorum=vm-xaj-bigdata-da-d01:2181,vm-xaj-bigdata-da-d02:2181,vm-xaj-bigdata-da-d03:2181, baseZNode=/hbase
[INFO][2018-05-24 21:11:40,000][org.apache.spark.storage.memory.MemoryStore]Block taskresult_0 stored as bytes in memory (estimated size 10.4 MB, free 901.7 MB)
[INFO][2018-05-24 21:11:40,002][org.apache.spark.storage.BlockManagerInfo]Added taskresult_0 in memory on 192.168.0.102:50869 (size: 10.4 MB, free: 901.9 MB)
[INFO][2018-05-24 21:11:40,003][org.apache.spark.executor.Executor]Finished task 0.0 in stage 0.0 (TID 0). 10856349 bytes result sent via BlockManager)
[INFO][2018-05-24 21:11:40,038][org.apache.spark.network.client.TransportClientFactory]Successfully created connection to /192.168.0.102:50869 after 19 ms (0 ms spent in bootstraps)
[INFO][2018-05-24 21:11:40,237][org.apache.spark.storage.memory.MemoryStore]Block taskresult_1 stored as bytes in memory (estimated size 10.4 MB, free 891.3 MB)
[INFO][2018-05-24 21:11:40,238][org.apache.spark.storage.BlockManagerInfo]Added taskresult_1 in memory on 192.168.0.102:50869 (size: 10.4 MB, free: 891.6 MB)
[INFO][2018-05-24 21:11:40,241][org.apache.spark.executor.Executor]Finished task 1.0 in stage 0.0 (TID 1). 10855899 bytes result sent via BlockManager)
[INFO][2018-05-24 21:11:40,428][org.apache.spark.storage.BlockManagerInfo]Removed taskresult_0 on 192.168.0.102:50869 in memory (size: 10.4 MB, free: 901.9 MB)
[INFO][2018-05-24 21:11:40,436][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 0.0 (TID 0) in 12747 ms on localhost (executor driver) (1/2)
[INFO][2018-05-24 21:11:40,451][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 0.0 (TID 1) in 12758 ms on localhost (executor driver) (2/2)
[INFO][2018-05-24 21:11:40,452][org.apache.spark.storage.BlockManagerInfo]Removed taskresult_1 on 192.168.0.102:50869 in memory (size: 10.4 MB, free: 912.3 MB)
[INFO][2018-05-24 21:11:40,454][org.apache.spark.scheduler.DAGScheduler]ResultStage 0 (collect at SimulationKafkaSendOutData.scala:25) finished in 12.783 s
[INFO][2018-05-24 21:11:40,457][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:11:40,461][org.apache.spark.scheduler.DAGScheduler]Job 0 finished: collect at SimulationKafkaSendOutData.scala:25, took 12.887064 s
[INFO][2018-05-24 21:11:40,613][org.spark_project.jetty.server.AbstractConnector]Stopped Spark@5427d3ac{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}
[INFO][2018-05-24 21:11:40,617][org.apache.spark.ui.SparkUI]Stopped Spark web UI at http://192.168.0.102:4041
[INFO][2018-05-24 21:11:40,631][org.apache.spark.MapOutputTrackerMasterEndpoint]MapOutputTrackerMasterEndpoint stopped!
[INFO][2018-05-24 21:11:40,647][org.apache.spark.storage.memory.MemoryStore]MemoryStore cleared
[INFO][2018-05-24 21:11:40,648][org.apache.spark.storage.BlockManager]BlockManager stopped
[INFO][2018-05-24 21:11:40,648][org.apache.spark.storage.BlockManagerMaster]BlockManagerMaster stopped
[INFO][2018-05-24 21:11:40,650][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint]OutputCommitCoordinator stopped!
[INFO][2018-05-24 21:11:40,651][org.apache.spark.SparkContext]Successfully stopped SparkContext
[INFO][2018-05-24 21:11:40,661][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	compression.type = none
	metric.reporters = []
	metadata.max.age.ms = 300000
	metadata.fetch.timeout.ms = 60000
	acks = all
	batch.size = 16384
	reconnect.backoff.ms = 10
	bootstrap.servers = [vm-xaj-bigdata-da-d01:9092, vm-xaj-bigdata-da-d02:9092, vm-xaj-bigdata-da-d03:9092, vm-xaj-bigdata-da-d04:9092, vm-xaj-bigdata-da-d05:9092, vm-xaj-bigdata-da-d06:9092, vm-xaj-bigdata-da-d07:9092]
	receive.buffer.bytes = 32768
	retry.backoff.ms = 100
	buffer.memory = 33554432
	timeout.ms = 30000
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	retries = 0
	max.request.size = 1048576
	block.on.buffer.full = true
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
	metrics.sample.window.ms = 30000
	send.buffer.bytes = 131072
	max.in.flight.requests.per.connection = 5
	metrics.num.samples = 2
	linger.ms = 1
	client.id = 

[INFO][2018-05-24 21:11:40,725][org.apache.zookeeper.ClientCnxn]Opening socket connection to server vm-xaj-bigdata-da-d03/10.213.4.27:2181. Will not attempt to authenticate using SASL (unknown error)
[INFO][2018-05-24 21:11:40,740][org.apache.zookeeper.ClientCnxn]Socket connection established, initiating session, client: /10.0.8.39:50882, server: vm-xaj-bigdata-da-d03/10.213.4.27:2181
[INFO][2018-05-24 21:11:40,773][org.apache.zookeeper.ClientCnxn]Session establishment complete on server vm-xaj-bigdata-da-d03/10.213.4.27:2181, sessionid = 0x362b50d75ec5f83, negotiated timeout = 60000
[INFO][2018-05-24 21:11:40,775][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527167500000 ms
[WARN][2018-05-24 21:11:41,279][org.apache.hadoop.util.NativeCodeLoader]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO][2018-05-24 21:11:41,363][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:11:41,377][org.apache.spark.executor.Executor]Finished task 0.0 in stage 0.0 (TID 0). 708 bytes result sent to driver
[INFO][2018-05-24 21:11:41,383][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 0.0 (TID 0) in 5942 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:11:41,385][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:11:41,389][org.apache.spark.scheduler.DAGScheduler]ResultStage 0 (foreachPartition at ReceiveKafkaData.scala:66) finished in 5.959 s
[INFO][2018-05-24 21:11:41,396][org.apache.spark.scheduler.DAGScheduler]Job 0 finished: foreachPartition at ReceiveKafkaData.scala:66, took 6.189089 s
[INFO][2018-05-24 21:11:41,400][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527167490000 ms.0 from job set of time 1527167490000 ms
[INFO][2018-05-24 21:11:41,402][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 11.400 s for time 1527167490000 ms (execution: 6.236 s)
[INFO][2018-05-24 21:11:41,402][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527167500000 ms.0 from job set of time 1527167500000 ms
[INFO][2018-05-24 21:11:41,411][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:11:41,414][org.apache.spark.scheduler.DAGScheduler]Got job 1 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:11:41,414][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 1 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:11:41,414][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:11:41,414][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 21:11:41,414][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:11:41,415][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 1 (MapPartitionsRDD[3] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:11:41,417][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 21:11:41,421][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 
[INFO][2018-05-24 21:11:41,423][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1_piece0 stored as bytes in memory (estimated size 1868.0 B, free 912.3 MB)
[INFO][2018-05-24 21:11:41,424][org.apache.spark.storage.BlockManagerInfo]Added broadcast_1_piece0 in memory on 192.168.0.102:50860 (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:11:41,424][org.apache.spark.SparkContext]Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:11:41,426][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:11:41,427][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 1.0 with 1 tasks
[INFO][2018-05-24 21:11:41,428][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:11:41,429][org.apache.spark.executor.Executor]Running task 0.0 in stage 1.0 (TID 1)
[INFO][2018-05-24 21:11:41,464][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12194 is the same as ending offset skipping seven 0
[INFO][2018-05-24 21:11:41,464][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:11:41,466][org.apache.spark.executor.Executor]Finished task 0.0 in stage 1.0 (TID 1). 708 bytes result sent to driver
[INFO][2018-05-24 21:11:41,467][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 1.0 (TID 1) in 39 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:11:41,467][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:11:41,468][org.apache.spark.scheduler.DAGScheduler]ResultStage 1 (foreachPartition at ReceiveKafkaData.scala:66) finished in 0.040 s
[INFO][2018-05-24 21:11:41,468][org.apache.spark.scheduler.DAGScheduler]Job 1 finished: foreachPartition at ReceiveKafkaData.scala:66, took 0.057110 s
[INFO][2018-05-24 21:11:41,469][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527167500000 ms.0 from job set of time 1527167500000 ms
[INFO][2018-05-24 21:11:41,470][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 1.469 s for time 1527167500000 ms (execution: 0.067 s)
[INFO][2018-05-24 21:11:41,470][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 1 from persistence list
[INFO][2018-05-24 21:11:41,476][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 0 from persistence list
[INFO][2018-05-24 21:11:41,477][org.apache.spark.storage.BlockManager]Removing RDD 1
[INFO][2018-05-24 21:11:41,477][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 21:11:41,478][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 
[INFO][2018-05-24 21:11:41,479][org.apache.spark.storage.BlockManager]Removing RDD 0
[INFO][2018-05-24 21:11:50,052][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527167510000 ms
[INFO][2018-05-24 21:11:50,053][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527167510000 ms.0 from job set of time 1527167510000 ms
[INFO][2018-05-24 21:11:50,063][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:11:50,064][org.apache.spark.scheduler.DAGScheduler]Got job 2 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:11:50,064][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 2 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:11:50,064][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:11:50,065][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:11:50,066][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 2 (MapPartitionsRDD[5] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:11:50,069][org.apache.spark.storage.memory.MemoryStore]Block broadcast_2 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 21:11:50,081][org.apache.spark.storage.memory.MemoryStore]Block broadcast_2_piece0 stored as bytes in memory (estimated size 1868.0 B, free 912.3 MB)
[INFO][2018-05-24 21:11:50,081][org.apache.spark.storage.BlockManagerInfo]Added broadcast_2_piece0 in memory on 192.168.0.102:50860 (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:11:50,086][org.apache.spark.SparkContext]Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:11:50,087][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[5] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:11:50,087][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 2.0 with 1 tasks
[INFO][2018-05-24 21:11:50,089][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:11:50,090][org.apache.spark.executor.Executor]Running task 0.0 in stage 2.0 (TID 2)
[INFO][2018-05-24 21:11:50,099][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12194 is the same as ending offset skipping seven 0
[INFO][2018-05-24 21:11:50,100][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:11:50,101][org.apache.spark.executor.Executor]Finished task 0.0 in stage 2.0 (TID 2). 708 bytes result sent to driver
[INFO][2018-05-24 21:11:50,103][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 2.0 (TID 2) in 15 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:11:50,103][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:11:50,104][org.apache.spark.scheduler.DAGScheduler]ResultStage 2 (foreachPartition at ReceiveKafkaData.scala:66) finished in 0.015 s
[INFO][2018-05-24 21:11:50,104][org.apache.spark.scheduler.DAGScheduler]Job 2 finished: foreachPartition at ReceiveKafkaData.scala:66, took 0.041326 s
[INFO][2018-05-24 21:11:50,105][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527167510000 ms.0 from job set of time 1527167510000 ms
[INFO][2018-05-24 21:11:50,106][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.105 s for time 1527167510000 ms (execution: 0.052 s)
[INFO][2018-05-24 21:11:50,106][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 3 from persistence list
[INFO][2018-05-24 21:11:50,106][org.apache.spark.storage.BlockManager]Removing RDD 3
[INFO][2018-05-24 21:11:50,107][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 2 from persistence list
[INFO][2018-05-24 21:11:50,109][org.apache.spark.storage.BlockManager]Removing RDD 2
[INFO][2018-05-24 21:11:50,110][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 21:11:50,110][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527167490000 ms
[INFO][2018-05-24 21:12:10,114][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527167520000 ms
[INFO][2018-05-24 21:12:10,116][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527167520000 ms.0 from job set of time 1527167520000 ms
[INFO][2018-05-24 21:12:10,130][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:12:10,131][org.apache.spark.scheduler.DAGScheduler]Got job 3 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:12:10,131][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 3 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:12:10,131][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:12:10,131][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:12:10,132][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 3 (MapPartitionsRDD[7] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:12:10,134][org.apache.spark.storage.memory.MemoryStore]Block broadcast_3 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 21:12:10,138][org.apache.spark.storage.memory.MemoryStore]Block broadcast_3_piece0 stored as bytes in memory (estimated size 1868.0 B, free 912.3 MB)
[INFO][2018-05-24 21:12:10,139][org.apache.spark.storage.BlockManagerInfo]Added broadcast_3_piece0 in memory on 192.168.0.102:50860 (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:12:10,139][org.apache.spark.SparkContext]Created broadcast 3 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:12:10,140][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[7] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:12:10,140][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 3.0 with 1 tasks
[INFO][2018-05-24 21:12:10,141][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 3.0 (TID 3, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:12:10,142][org.apache.spark.executor.Executor]Running task 0.0 in stage 3.0 (TID 3)
[INFO][2018-05-24 21:12:10,145][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12194 is the same as ending offset skipping seven 0
[INFO][2018-05-24 21:12:10,145][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:12:10,146][org.apache.spark.executor.Executor]Finished task 0.0 in stage 3.0 (TID 3). 708 bytes result sent to driver
[INFO][2018-05-24 21:12:10,148][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 3.0 (TID 3) in 7 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:12:10,148][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 3.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:12:10,148][org.apache.spark.scheduler.DAGScheduler]ResultStage 3 (foreachPartition at ReceiveKafkaData.scala:66) finished in 0.007 s
[INFO][2018-05-24 21:12:10,149][org.apache.spark.scheduler.DAGScheduler]Job 3 finished: foreachPartition at ReceiveKafkaData.scala:66, took 0.018131 s
[INFO][2018-05-24 21:12:10,149][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527167520000 ms.0 from job set of time 1527167520000 ms
[INFO][2018-05-24 21:12:10,150][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 10.149 s for time 1527167520000 ms (execution: 0.033 s)
[INFO][2018-05-24 21:12:10,245][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527167530000 ms
[INFO][2018-05-24 21:12:10,245][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 5 from persistence list
[INFO][2018-05-24 21:12:10,245][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527167530000 ms.0 from job set of time 1527167530000 ms
[INFO][2018-05-24 21:12:10,245][org.apache.spark.storage.BlockManager]Removing RDD 5
[INFO][2018-05-24 21:12:10,246][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 4 from persistence list
[INFO][2018-05-24 21:12:10,246][org.apache.spark.storage.BlockManager]Removing RDD 4
[INFO][2018-05-24 21:12:10,247][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 21:12:10,247][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527167500000 ms
[INFO][2018-05-24 21:12:10,253][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:12:10,254][org.apache.spark.scheduler.DAGScheduler]Got job 4 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:12:10,255][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 4 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:12:10,255][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:12:10,255][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:12:10,255][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 4 (MapPartitionsRDD[9] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:12:10,257][org.apache.spark.storage.memory.MemoryStore]Block broadcast_4 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 21:12:10,273][org.apache.spark.storage.memory.MemoryStore]Block broadcast_4_piece0 stored as bytes in memory (estimated size 1868.0 B, free 912.3 MB)
[INFO][2018-05-24 21:12:10,276][org.apache.spark.storage.BlockManagerInfo]Added broadcast_4_piece0 in memory on 192.168.0.102:50860 (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:12:10,278][org.apache.spark.SparkContext]Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:12:10,279][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[9] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:12:10,280][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 4.0 with 1 tasks
[INFO][2018-05-24 21:12:10,280][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 4.0 (TID 4, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:12:10,281][org.apache.spark.executor.Executor]Running task 0.0 in stage 4.0 (TID 4)
[INFO][2018-05-24 21:12:10,284][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12194 is the same as ending offset skipping seven 0
[INFO][2018-05-24 21:12:10,284][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:12:10,286][org.apache.spark.executor.Executor]Finished task 0.0 in stage 4.0 (TID 4). 708 bytes result sent to driver
[INFO][2018-05-24 21:12:10,287][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 4.0 (TID 4) in 6 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:12:10,287][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 4.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:12:10,287][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_1_piece0 on 192.168.0.102:50860 in memory (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:12:10,287][org.apache.spark.scheduler.DAGScheduler]ResultStage 4 (foreachPartition at ReceiveKafkaData.scala:66) finished in 0.007 s
[INFO][2018-05-24 21:12:10,288][org.apache.spark.scheduler.DAGScheduler]Job 4 finished: foreachPartition at ReceiveKafkaData.scala:66, took 0.034278 s
[INFO][2018-05-24 21:12:10,288][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527167530000 ms.0 from job set of time 1527167530000 ms
[INFO][2018-05-24 21:12:10,289][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 7 from persistence list
[INFO][2018-05-24 21:12:10,289][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.288 s for time 1527167530000 ms (execution: 0.043 s)
[INFO][2018-05-24 21:12:10,290][org.apache.spark.storage.BlockManager]Removing RDD 7
[INFO][2018-05-24 21:12:10,290][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 6 from persistence list
[INFO][2018-05-24 21:12:10,291][org.apache.spark.storage.BlockManager]Removing RDD 6
[INFO][2018-05-24 21:12:10,291][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 21:12:10,291][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_0_piece0 on 192.168.0.102:50860 in memory (size: 1867.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:12:10,291][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527167510000 ms
[INFO][2018-05-24 21:12:10,293][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_2_piece0 on 192.168.0.102:50860 in memory (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:12:10,294][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_3_piece0 on 192.168.0.102:50860 in memory (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:12:25,119][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527167540000 ms
[INFO][2018-05-24 21:12:25,120][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527167540000 ms.0 from job set of time 1527167540000 ms
[INFO][2018-05-24 21:12:25,131][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:12:25,132][org.apache.spark.scheduler.DAGScheduler]Got job 5 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:12:25,132][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 5 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:12:25,132][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:12:25,133][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:12:25,133][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 5 (MapPartitionsRDD[11] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:12:25,137][org.apache.spark.storage.memory.MemoryStore]Block broadcast_5 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 21:12:25,143][org.apache.spark.storage.memory.MemoryStore]Block broadcast_5_piece0 stored as bytes in memory (estimated size 1868.0 B, free 912.3 MB)
[INFO][2018-05-24 21:12:25,147][org.apache.spark.storage.BlockManagerInfo]Added broadcast_5_piece0 in memory on 192.168.0.102:50860 (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:12:25,147][org.apache.spark.SparkContext]Created broadcast 5 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:12:25,148][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[11] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:12:25,148][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 5.0 with 1 tasks
[INFO][2018-05-24 21:12:25,151][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 5.0 (TID 5, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:12:25,151][org.apache.spark.executor.Executor]Running task 0.0 in stage 5.0 (TID 5)
[INFO][2018-05-24 21:12:25,160][org.apache.spark.streaming.kafka.KafkaRDD]Computing topic seven, partition 0 offsets 12194 -> 12200
[INFO][2018-05-24 21:12:25,160][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 21:12:25,160][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 21:12:25,161][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 21:12:30,106][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527167550000 ms
[INFO][2018-05-24 21:12:30,752][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:12:30,753][org.apache.spark.executor.Executor]Finished task 0.0 in stage 5.0 (TID 5). 708 bytes result sent to driver
[INFO][2018-05-24 21:12:30,754][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 5.0 (TID 5) in 5604 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:12:30,754][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 5.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:12:30,754][org.apache.spark.scheduler.DAGScheduler]ResultStage 5 (foreachPartition at ReceiveKafkaData.scala:66) finished in 5.604 s
[INFO][2018-05-24 21:12:30,755][org.apache.spark.scheduler.DAGScheduler]Job 5 finished: foreachPartition at ReceiveKafkaData.scala:66, took 5.623008 s
[INFO][2018-05-24 21:12:30,755][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527167540000 ms.0 from job set of time 1527167540000 ms
[INFO][2018-05-24 21:12:30,755][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 9 from persistence list
[INFO][2018-05-24 21:12:30,755][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 10.755 s for time 1527167540000 ms (execution: 5.635 s)
[INFO][2018-05-24 21:12:30,756][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527167550000 ms.0 from job set of time 1527167550000 ms
[INFO][2018-05-24 21:12:30,756][org.apache.spark.storage.BlockManager]Removing RDD 9
[INFO][2018-05-24 21:12:30,756][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 8 from persistence list
[INFO][2018-05-24 21:12:30,757][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 21:12:30,757][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527167520000 ms
[INFO][2018-05-24 21:12:30,757][org.apache.spark.storage.BlockManager]Removing RDD 8
[INFO][2018-05-24 21:12:30,762][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:12:30,762][org.apache.spark.scheduler.DAGScheduler]Got job 6 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:12:30,762][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 6 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:12:30,763][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:12:30,763][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:12:30,763][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 6 (MapPartitionsRDD[13] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:12:30,765][org.apache.spark.storage.memory.MemoryStore]Block broadcast_6 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 21:12:30,767][org.apache.spark.storage.memory.MemoryStore]Block broadcast_6_piece0 stored as bytes in memory (estimated size 1868.0 B, free 912.3 MB)
[INFO][2018-05-24 21:12:30,768][org.apache.spark.storage.BlockManagerInfo]Added broadcast_6_piece0 in memory on 192.168.0.102:50860 (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:12:30,769][org.apache.spark.SparkContext]Created broadcast 6 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:12:30,769][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[13] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:12:30,769][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 6.0 with 1 tasks
[INFO][2018-05-24 21:12:30,770][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 6.0 (TID 6, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:12:30,770][org.apache.spark.executor.Executor]Running task 0.0 in stage 6.0 (TID 6)
[INFO][2018-05-24 21:12:30,773][org.apache.spark.streaming.kafka.KafkaRDD]Computing topic seven, partition 0 offsets 12200 -> 12202
[INFO][2018-05-24 21:12:30,773][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 21:12:30,773][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 21:12:30,773][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 21:12:30,905][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:12:30,906][org.apache.spark.executor.Executor]Finished task 0.0 in stage 6.0 (TID 6). 708 bytes result sent to driver
[INFO][2018-05-24 21:12:30,907][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 6.0 (TID 6) in 137 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:12:30,907][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 6.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:12:30,908][org.apache.spark.scheduler.DAGScheduler]ResultStage 6 (foreachPartition at ReceiveKafkaData.scala:66) finished in 0.138 s
[INFO][2018-05-24 21:12:30,908][org.apache.spark.scheduler.DAGScheduler]Job 6 finished: foreachPartition at ReceiveKafkaData.scala:66, took 0.146409 s
[INFO][2018-05-24 21:12:30,909][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527167550000 ms.0 from job set of time 1527167550000 ms
[INFO][2018-05-24 21:12:30,909][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 11 from persistence list
[INFO][2018-05-24 21:12:30,909][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.909 s for time 1527167550000 ms (execution: 0.153 s)
[INFO][2018-05-24 21:12:30,910][org.apache.spark.storage.BlockManager]Removing RDD 11
[INFO][2018-05-24 21:12:30,910][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 10 from persistence list
[INFO][2018-05-24 21:12:30,911][org.apache.spark.storage.BlockManager]Removing RDD 10
[INFO][2018-05-24 21:12:30,911][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 21:12:30,911][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527167530000 ms
[INFO][2018-05-24 21:12:36,134][org.apache.spark.util.ShutdownHookManager]Shutdown hook called
[INFO][2018-05-24 21:12:36,134][org.apache.spark.util.ShutdownHookManager]Deleting directory /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/spark-ad6ac75d-3ce7-4fde-97ce-25be816868bf
[INFO][2018-05-24 21:12:50,188][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527167560000 ms
[INFO][2018-05-24 21:12:50,189][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527167560000 ms.0 from job set of time 1527167560000 ms
[INFO][2018-05-24 21:12:50,195][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:12:50,197][org.apache.spark.scheduler.DAGScheduler]Got job 7 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:12:50,198][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 7 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:12:50,198][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:12:50,199][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:12:50,200][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 7 (MapPartitionsRDD[15] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:12:50,204][org.apache.spark.storage.memory.MemoryStore]Block broadcast_7 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 21:12:50,206][org.apache.spark.storage.memory.MemoryStore]Block broadcast_7_piece0 stored as bytes in memory (estimated size 1868.0 B, free 912.3 MB)
[INFO][2018-05-24 21:12:50,206][org.apache.spark.storage.BlockManagerInfo]Added broadcast_7_piece0 in memory on 192.168.0.102:50860 (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:12:50,207][org.apache.spark.SparkContext]Created broadcast 7 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:12:50,208][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[15] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:12:50,208][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 7.0 with 1 tasks
[INFO][2018-05-24 21:12:50,209][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 7.0 (TID 7, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:12:50,209][org.apache.spark.executor.Executor]Running task 0.0 in stage 7.0 (TID 7)
[INFO][2018-05-24 21:12:50,212][org.apache.spark.streaming.kafka.KafkaRDD]Computing topic seven, partition 0 offsets 12202 -> 12204
[INFO][2018-05-24 21:12:50,213][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 21:12:50,213][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 21:12:50,213][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 21:12:50,359][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:12:50,360][org.apache.spark.executor.Executor]Finished task 0.0 in stage 7.0 (TID 7). 708 bytes result sent to driver
[INFO][2018-05-24 21:12:50,361][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 7.0 (TID 7) in 152 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:12:50,361][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 7.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:12:50,363][org.apache.spark.scheduler.DAGScheduler]ResultStage 7 (foreachPartition at ReceiveKafkaData.scala:66) finished in 0.154 s
[INFO][2018-05-24 21:12:50,363][org.apache.spark.scheduler.DAGScheduler]Job 7 finished: foreachPartition at ReceiveKafkaData.scala:66, took 0.167807 s
[INFO][2018-05-24 21:12:50,364][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527167560000 ms.0 from job set of time 1527167560000 ms
[INFO][2018-05-24 21:12:50,365][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 10.364 s for time 1527167560000 ms (execution: 0.176 s)
[INFO][2018-05-24 21:12:55,365][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527167570000 ms
[INFO][2018-05-24 21:12:55,365][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 13 from persistence list
[INFO][2018-05-24 21:12:55,366][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527167570000 ms.0 from job set of time 1527167570000 ms
[INFO][2018-05-24 21:12:55,366][org.apache.spark.storage.BlockManager]Removing RDD 13
[INFO][2018-05-24 21:12:55,366][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 12 from persistence list
[INFO][2018-05-24 21:12:55,367][org.apache.spark.storage.BlockManager]Removing RDD 12
[INFO][2018-05-24 21:12:55,367][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 21:12:55,367][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527167540000 ms
[INFO][2018-05-24 21:12:55,371][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:12:55,372][org.apache.spark.scheduler.DAGScheduler]Got job 8 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:12:55,372][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 8 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:12:55,372][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:12:55,373][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:12:55,373][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 8 (MapPartitionsRDD[17] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:12:55,374][org.apache.spark.storage.memory.MemoryStore]Block broadcast_8 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 21:12:55,376][org.apache.spark.storage.memory.MemoryStore]Block broadcast_8_piece0 stored as bytes in memory (estimated size 1868.0 B, free 912.3 MB)
[INFO][2018-05-24 21:12:55,376][org.apache.spark.storage.BlockManagerInfo]Added broadcast_8_piece0 in memory on 192.168.0.102:50860 (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:12:55,377][org.apache.spark.SparkContext]Created broadcast 8 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:12:55,378][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[17] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:12:55,378][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 8.0 with 1 tasks
[INFO][2018-05-24 21:12:55,379][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 8.0 (TID 8, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:12:55,379][org.apache.spark.executor.Executor]Running task 0.0 in stage 8.0 (TID 8)
[INFO][2018-05-24 21:12:55,381][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12204 is the same as ending offset skipping seven 0
[INFO][2018-05-24 21:12:55,382][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:12:55,383][org.apache.spark.executor.Executor]Finished task 0.0 in stage 8.0 (TID 8). 751 bytes result sent to driver
[INFO][2018-05-24 21:12:55,384][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 8.0 (TID 8) in 6 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:12:55,384][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 8.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:12:55,385][org.apache.spark.scheduler.DAGScheduler]ResultStage 8 (foreachPartition at ReceiveKafkaData.scala:66) finished in 0.006 s
[INFO][2018-05-24 21:12:55,385][org.apache.spark.scheduler.DAGScheduler]Job 8 finished: foreachPartition at ReceiveKafkaData.scala:66, took 0.013658 s
[INFO][2018-05-24 21:12:55,386][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527167570000 ms.0 from job set of time 1527167570000 ms
[INFO][2018-05-24 21:12:55,387][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 5.386 s for time 1527167570000 ms (execution: 0.020 s)
[INFO][2018-05-24 21:12:55,387][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 15 from persistence list
[INFO][2018-05-24 21:12:55,387][org.apache.spark.storage.BlockManager]Removing RDD 15
[INFO][2018-05-24 21:12:55,388][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 14 from persistence list
[INFO][2018-05-24 21:12:55,389][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 21:12:55,389][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527167550000 ms
[INFO][2018-05-24 21:12:55,389][org.apache.spark.storage.BlockManager]Removing RDD 14
[INFO][2018-05-24 21:13:05,027][org.apache.spark.streaming.StreamingContext]Invoking stop(stopGracefully=false) from shutdown hook
[INFO][2018-05-24 21:13:05,028][org.apache.spark.streaming.scheduler.ReceiverTracker]ReceiverTracker stopped
[INFO][2018-05-24 21:13:05,031][org.apache.spark.streaming.scheduler.JobGenerator]Stopping JobGenerator immediately
[INFO][2018-05-24 21:13:05,032][org.apache.spark.streaming.util.RecurringTimer]Stopped timer for JobGenerator after time 1527167580000
[INFO][2018-05-24 21:13:05,061][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527167580000 ms
[INFO][2018-05-24 21:13:05,062][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527167580000 ms.0 from job set of time 1527167580000 ms
[INFO][2018-05-24 21:13:05,063][org.apache.spark.streaming.scheduler.JobGenerator]Stopped JobGenerator
[INFO][2018-05-24 21:13:05,067][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:13:05,068][org.apache.spark.scheduler.DAGScheduler]Got job 9 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:13:05,068][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 9 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:13:05,068][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:13:05,069][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:13:05,069][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 9 (MapPartitionsRDD[19] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:13:05,071][org.apache.spark.storage.memory.MemoryStore]Block broadcast_9 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 21:13:05,073][org.apache.spark.storage.memory.MemoryStore]Block broadcast_9_piece0 stored as bytes in memory (estimated size 1868.0 B, free 912.3 MB)
[INFO][2018-05-24 21:13:05,074][org.apache.spark.storage.BlockManagerInfo]Added broadcast_9_piece0 in memory on 192.168.0.102:50860 (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:13:05,075][org.apache.spark.SparkContext]Created broadcast 9 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:13:05,076][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[19] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:13:05,076][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 9.0 with 1 tasks
[INFO][2018-05-24 21:13:05,077][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 9.0 (TID 9, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:13:05,077][org.apache.spark.executor.Executor]Running task 0.0 in stage 9.0 (TID 9)
[INFO][2018-05-24 21:13:05,079][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12204 is the same as ending offset skipping seven 0
[INFO][2018-05-24 21:13:05,079][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:13:05,080][org.apache.spark.executor.Executor]Finished task 0.0 in stage 9.0 (TID 9). 708 bytes result sent to driver
[INFO][2018-05-24 21:13:05,081][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 9.0 (TID 9) in 5 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:13:05,082][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 9.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:13:05,082][org.apache.spark.scheduler.DAGScheduler]ResultStage 9 (foreachPartition at ReceiveKafkaData.scala:66) finished in 0.006 s
[INFO][2018-05-24 21:13:05,083][org.apache.spark.scheduler.DAGScheduler]Job 9 finished: foreachPartition at ReceiveKafkaData.scala:66, took 0.015242 s
[INFO][2018-05-24 21:13:05,083][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527167580000 ms.0 from job set of time 1527167580000 ms
[INFO][2018-05-24 21:13:05,084][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 5.083 s for time 1527167580000 ms (execution: 0.022 s)
[INFO][2018-05-24 21:13:05,085][org.apache.spark.streaming.scheduler.JobScheduler]Stopped JobScheduler
[INFO][2018-05-24 21:13:05,092][org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@57a48985{/streaming,null,UNAVAILABLE,@Spark}
[INFO][2018-05-24 21:13:05,093][org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@3f672204{/streaming/batch,null,UNAVAILABLE,@Spark}
[INFO][2018-05-24 21:13:05,094][org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@2d000e80{/static/streaming,null,UNAVAILABLE,@Spark}
[INFO][2018-05-24 21:13:05,095][org.apache.spark.streaming.StreamingContext]StreamingContext stopped successfully
[INFO][2018-05-24 21:13:05,095][org.apache.spark.SparkContext]Invoking stop() from shutdown hook
[INFO][2018-05-24 21:13:05,101][org.spark_project.jetty.server.AbstractConnector]Stopped Spark@1b85fa0b{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-05-24 21:13:05,104][org.apache.spark.ui.SparkUI]Stopped Spark web UI at http://192.168.0.102:4040
[ERROR][2018-05-24 21:13:05,108][org.apache.spark.scheduler.LiveListenerBus]SparkListenerBus has already stopped! Dropping event SparkListenerExecutorMetricsUpdate(driver,WrappedArray())
[INFO][2018-05-24 21:13:05,113][org.apache.spark.MapOutputTrackerMasterEndpoint]MapOutputTrackerMasterEndpoint stopped!
[INFO][2018-05-24 21:13:05,130][org.apache.spark.storage.memory.MemoryStore]MemoryStore cleared
[INFO][2018-05-24 21:13:05,130][org.apache.spark.storage.BlockManager]BlockManager stopped
[INFO][2018-05-24 21:13:05,131][org.apache.spark.storage.BlockManagerMaster]BlockManagerMaster stopped
[INFO][2018-05-24 21:13:05,133][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint]OutputCommitCoordinator stopped!
[INFO][2018-05-24 21:13:05,134][org.apache.spark.SparkContext]Successfully stopped SparkContext
[INFO][2018-05-24 21:13:05,134][org.apache.spark.util.ShutdownHookManager]Shutdown hook called
[INFO][2018-05-24 21:13:05,135][org.apache.spark.util.ShutdownHookManager]Deleting directory /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/spark-bd15aa67-b8af-4cac-a0a3-d589736f3d1d
[INFO][2018-05-24 21:18:46,633][org.apache.spark.SparkContext]Running Spark version 2.2.0
[INFO][2018-05-24 21:18:47,594][org.apache.spark.SparkContext]Submitted application: ReceiveKafkaData$
[INFO][2018-05-24 21:18:47,616][org.apache.spark.SecurityManager]Changing view acls to: seven
[INFO][2018-05-24 21:18:47,619][org.apache.spark.SecurityManager]Changing modify acls to: seven
[INFO][2018-05-24 21:18:47,621][org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO][2018-05-24 21:18:47,622][org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO][2018-05-24 21:18:47,623][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(seven); groups with view permissions: Set(); users  with modify permissions: Set(seven); groups with modify permissions: Set()
[INFO][2018-05-24 21:18:47,976][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 51152.
[INFO][2018-05-24 21:18:47,997][org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO][2018-05-24 21:18:48,015][org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO][2018-05-24 21:18:48,017][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2018-05-24 21:18:48,017][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
[INFO][2018-05-24 21:18:48,027][org.apache.spark.storage.DiskBlockManager]Created local directory at /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/blockmgr-9d1c075e-9db1-43d2-bdb8-bae54f78524c
[INFO][2018-05-24 21:18:48,042][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 912.3 MB
[INFO][2018-05-24 21:18:48,114][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO][2018-05-24 21:18:48,212][org.spark_project.jetty.util.log]Logging initialized @2595ms
[INFO][2018-05-24 21:18:48,287][org.spark_project.jetty.server.Server]jetty-9.3.z-SNAPSHOT
[INFO][2018-05-24 21:18:48,306][org.spark_project.jetty.server.Server]Started @2692ms
[INFO][2018-05-24 21:18:48,330][org.spark_project.jetty.server.AbstractConnector]Started ServerConnector@25177d29{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-05-24 21:18:48,330][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4040.
[INFO][2018-05-24 21:18:48,370][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@c7a975a{/jobs,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:18:48,371][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@55795845{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:18:48,372][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@119f1f2a{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:18:48,373][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7fd4acee{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:18:48,373][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6175619b{/stages,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:18:48,374][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@756cf158{/stages/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:18:48,375][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@751d3241{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:18:48,376][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@30ea8c23{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:18:48,378][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4e76dac{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:18:48,379][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5f2f577{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:18:48,380][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5d465e4b{/storage,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:18:48,381][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@41a90fa8{/storage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:18:48,382][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@52500920{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:18:48,383][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@18a3962d{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:18:48,384][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2a65bb85{/environment,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:18:48,386][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4f936da8{/environment/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:18:48,386][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@452ba1db{/executors,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:18:48,387][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@76a36b71{/executors/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:18:48,388][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@f9d87b{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:18:48,389][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@26fb628{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:18:48,403][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@70dd7e15{/static,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:18:48,404][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@60d8c0dc{/,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:18:48,406][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6a62689d{/api,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:18:48,407][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1849db1a{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:18:48,408][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1ca25c47{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:18:48,410][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://192.168.0.102:4040
[INFO][2018-05-24 21:18:48,492][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
[INFO][2018-05-24 21:18:48,516][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 51153.
[INFO][2018-05-24 21:18:48,517][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 192.168.0.102:51153
[INFO][2018-05-24 21:18:48,519][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2018-05-24 21:18:48,521][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 192.168.0.102, 51153, None)
[INFO][2018-05-24 21:18:48,526][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 192.168.0.102:51153 with 912.3 MB RAM, BlockManagerId(driver, 192.168.0.102, 51153, None)
[INFO][2018-05-24 21:18:48,549][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 192.168.0.102, 51153, None)
[INFO][2018-05-24 21:18:48,551][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 192.168.0.102, 51153, None)
[INFO][2018-05-24 21:18:48,824][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@17f460bb{/metrics/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:18:48,948][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 21:18:48,952][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 21:18:48,952][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 21:18:59,634][org.apache.spark.streaming.kafka.DirectKafkaInputDStream]Slide time = 5000 ms
[INFO][2018-05-24 21:18:59,635][org.apache.spark.streaming.kafka.DirectKafkaInputDStream]Storage level = Serialized 1x Replicated
[INFO][2018-05-24 21:18:59,636][org.apache.spark.streaming.kafka.DirectKafkaInputDStream]Checkpoint interval = null
[INFO][2018-05-24 21:18:59,636][org.apache.spark.streaming.kafka.DirectKafkaInputDStream]Remember interval = 5000 ms
[INFO][2018-05-24 21:18:59,639][org.apache.spark.streaming.kafka.DirectKafkaInputDStream]Initialized and validated org.apache.spark.streaming.kafka.DirectKafkaInputDStream@2c50c720
[INFO][2018-05-24 21:18:59,640][org.apache.spark.streaming.dstream.MappedDStream]Slide time = 5000 ms
[INFO][2018-05-24 21:18:59,640][org.apache.spark.streaming.dstream.MappedDStream]Storage level = Serialized 1x Replicated
[INFO][2018-05-24 21:18:59,641][org.apache.spark.streaming.dstream.MappedDStream]Checkpoint interval = null
[INFO][2018-05-24 21:18:59,641][org.apache.spark.streaming.dstream.MappedDStream]Remember interval = 5000 ms
[INFO][2018-05-24 21:18:59,641][org.apache.spark.streaming.dstream.MappedDStream]Initialized and validated org.apache.spark.streaming.dstream.MappedDStream@1ee99424
[INFO][2018-05-24 21:18:59,641][org.apache.spark.streaming.dstream.ForEachDStream]Slide time = 5000 ms
[INFO][2018-05-24 21:18:59,641][org.apache.spark.streaming.dstream.ForEachDStream]Storage level = Serialized 1x Replicated
[INFO][2018-05-24 21:18:59,641][org.apache.spark.streaming.dstream.ForEachDStream]Checkpoint interval = null
[INFO][2018-05-24 21:18:59,642][org.apache.spark.streaming.dstream.ForEachDStream]Remember interval = 5000 ms
[INFO][2018-05-24 21:18:59,642][org.apache.spark.streaming.dstream.ForEachDStream]Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@141f35a8
[INFO][2018-05-24 21:18:59,724][org.apache.spark.streaming.util.RecurringTimer]Started timer for JobGenerator at time 1527167940000
[INFO][2018-05-24 21:18:59,725][org.apache.spark.streaming.scheduler.JobGenerator]Started JobGenerator at 1527167940000 ms
[INFO][2018-05-24 21:18:59,726][org.apache.spark.streaming.scheduler.JobScheduler]Started JobScheduler
[INFO][2018-05-24 21:18:59,730][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@57a48985{/streaming,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:18:59,731][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3910fe11{/streaming/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:18:59,733][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3f672204{/streaming/batch,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:18:59,734][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2c2db130{/streaming/batch/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:18:59,735][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2d000e80{/static/streaming,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:18:59,736][org.apache.spark.streaming.StreamingContext]StreamingContext started
[INFO][2018-05-24 21:19:00,086][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 21:19:00,089][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 21:19:00,091][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 21:19:05,205][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527167940000 ms
[INFO][2018-05-24 21:19:05,210][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527167940000 ms.0 from job set of time 1527167940000 ms
[INFO][2018-05-24 21:19:05,261][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:19:05,282][org.apache.spark.scheduler.DAGScheduler]Got job 0 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:19:05,284][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 0 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:19:05,285][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:19:05,287][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:19:05,288][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527167945000 ms
[INFO][2018-05-24 21:19:05,300][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 0 (MapPartitionsRDD[1] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:19:05,510][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 21:19:05,575][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0_piece0 stored as bytes in memory (estimated size 1867.0 B, free 912.3 MB)
[INFO][2018-05-24 21:19:05,578][org.apache.spark.storage.BlockManagerInfo]Added broadcast_0_piece0 in memory on 192.168.0.102:51153 (size: 1867.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:19:05,585][org.apache.spark.SparkContext]Created broadcast 0 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:19:05,613][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:19:05,614][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 0.0 with 1 tasks
[INFO][2018-05-24 21:19:05,652][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:19:05,663][org.apache.spark.executor.Executor]Running task 0.0 in stage 0.0 (TID 0)
[INFO][2018-05-24 21:19:05,708][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12204 is the same as ending offset skipping seven 0
[INFO][2018-05-24 21:19:05,927][org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper]Process identifier=hconnection-0x4841d9cd connecting to ZooKeeper ensemble=vm-xaj-bigdata-da-d01:2181,vm-xaj-bigdata-da-d02:2181,vm-xaj-bigdata-da-d03:2181
[INFO][2018-05-24 21:19:06,055][org.apache.zookeeper.ZooKeeper]Client environment:zookeeper.version=3.4.5-cdh5.7.0--1, built on 04/06/2016 14:24 GMT
[INFO][2018-05-24 21:19:06,055][org.apache.zookeeper.ZooKeeper]Client environment:host.name=192.168.0.102
[INFO][2018-05-24 21:19:06,055][org.apache.zookeeper.ZooKeeper]Client environment:java.version=1.8.0_161
[INFO][2018-05-24 21:19:06,055][org.apache.zookeeper.ZooKeeper]Client environment:java.vendor=Oracle Corporation
[INFO][2018-05-24 21:19:06,055][org.apache.zookeeper.ZooKeeper]Client environment:java.home=/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre
[INFO][2018-05-24 21:19:06,055][org.apache.zookeeper.ZooKeeper]Client environment:java.class.path=/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/charsets.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/deploy.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/cldrdata.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/dnsns.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/jaccess.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/jfxrt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/localedata.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/nashorn.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/sunec.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/sunjce_provider.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/sunpkcs11.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/zipfs.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/javaws.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/jce.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/jfr.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/jfxswt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/jsse.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/management-agent.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/plugin.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/resources.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/rt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/ant-javafx.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/dt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/javafx-mx.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/jconsole.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/packager.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/sa-jdi.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/tools.jar:/Users/seven/project/scala/dataMining/target/classes:/Users/seven/software/maven/repository/org/scala-lang/scala-library/2.11.12/scala-library-2.11.12.jar:/Users/seven/software/maven/repository/org/scala-lang/scala-compiler/2.11.12/scala-compiler-2.11.12.jar:/Users/seven/software/maven/repository/org/scala-lang/modules/scala-xml_2.11/1.0.5/scala-xml_2.11-1.0.5.jar:/Users/seven/software/maven/repository/org/scala-lang/modules/scala-parser-combinators_2.11/1.0.4/scala-parser-combinators_2.11-1.0.4.jar:/Users/seven/software/maven/repository/org/scala-lang/scala-reflect/2.11.12/scala-reflect-2.11.12.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-client/2.6.0-cdh5.7.0/hadoop-client-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-common/2.6.0-cdh5.7.0/hadoop-common-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/xmlenc/xmlenc/0.52/xmlenc-0.52.jar:/Users/seven/software/maven/repository/commons-net/commons-net/3.1/commons-net-3.1.jar:/Users/seven/software/maven/repository/commons-configuration/commons-configuration/1.6/commons-configuration-1.6.jar:/Users/seven/software/maven/repository/commons-digester/commons-digester/1.8/commons-digester-1.8.jar:/Users/seven/software/maven/repository/commons-beanutils/commons-beanutils/1.7.0/commons-beanutils-1.7.0.jar:/Users/seven/software/maven/repository/org/slf4j/slf4j-api/1.7.5/slf4j-api-1.7.5.jar:/Users/seven/software/maven/repository/org/slf4j/slf4j-log4j12/1.7.5/slf4j-log4j12-1.7.5.jar:/Users/seven/software/maven/repository/org/apache/avro/avro/1.7.6-cdh5.7.0/avro-1.7.6-cdh5.7.0.jar:/Users/seven/software/maven/repository/com/google/code/gson/gson/2.2.4/gson-2.2.4.jar:/Users/seven/software/maven/repository/org/apache/curator/curator-client/2.7.1/curator-client-2.7.1.jar:/Users/seven/software/maven/repository/org/apache/curator/curator-recipes/2.7.1/curator-recipes-2.7.1.jar:/Users/seven/software/maven/repository/com/google/code/findbugs/jsr305/3.0.0/jsr305-3.0.0.jar:/Users/seven/software/maven/repository/org/apache/htrace/htrace-core4/4.0.1-incubating/htrace-core4-4.0.1-incubating.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-compress/1.4.1/commons-compress-1.4.1.jar:/Users/seven/software/maven/repository/org/tukaani/xz/1.0/xz-1.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-hdfs/2.6.0-cdh5.7.0/hadoop-hdfs-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/io/netty/netty/3.6.2.Final/netty-3.6.2.Final.jar:/Users/seven/software/maven/repository/xerces/xercesImpl/2.9.1/xercesImpl-2.9.1.jar:/Users/seven/software/maven/repository/xml-apis/xml-apis/1.3.04/xml-apis-1.3.04.jar:/Users/seven/software/maven/repository/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-app/2.6.0-cdh5.7.0/hadoop-mapreduce-client-app-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-common/2.6.0-cdh5.7.0/hadoop-mapreduce-client-common-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-yarn-client/2.6.0-cdh5.7.0/hadoop-yarn-client-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-yarn-server-common/2.6.0-cdh5.7.0/hadoop-yarn-server-common-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-shuffle/2.6.0-cdh5.7.0/hadoop-mapreduce-client-shuffle-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-yarn-api/2.6.0-cdh5.7.0/hadoop-yarn-api-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-core/2.6.0-cdh5.7.0/hadoop-mapreduce-client-core-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-yarn-common/2.6.0-cdh5.7.0/hadoop-yarn-common-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/javax/xml/bind/jaxb-api/2.2.2/jaxb-api-2.2.2.jar:/Users/seven/software/maven/repository/javax/xml/stream/stax-api/1.0-2/stax-api-1.0-2.jar:/Users/seven/software/maven/repository/javax/activation/activation/1.1/activation-1.1.jar:/Users/seven/software/maven/repository/com/sun/jersey/jersey-client/1.9/jersey-client-1.9.jar:/Users/seven/software/maven/repository/org/codehaus/jackson/jackson-xc/1.8.8/jackson-xc-1.8.8.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.6.0-cdh5.7.0/hadoop-mapreduce-client-jobclient-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-aws/2.6.0-cdh5.7.0/hadoop-aws-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/com/amazonaws/aws-java-sdk-s3/1.10.6/aws-java-sdk-s3-1.10.6.jar:/Users/seven/software/maven/repository/com/amazonaws/aws-java-sdk-kms/1.10.6/aws-java-sdk-kms-1.10.6.jar:/Users/seven/software/maven/repository/com/amazonaws/aws-java-sdk-core/1.10.6/aws-java-sdk-core-1.10.6.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-annotations/2.6.0-cdh5.7.0/hadoop-annotations-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-client/1.2.0-cdh5.7.0/hbase-client-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-annotations/1.2.0-cdh5.7.0/hbase-annotations-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-common/1.2.0-cdh5.7.0/hbase-common-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-protocol/1.2.0-cdh5.7.0/hbase-protocol-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/commons-codec/commons-codec/1.9/commons-codec-1.9.jar:/Users/seven/software/maven/repository/commons-io/commons-io/2.4/commons-io-2.4.jar:/Users/seven/software/maven/repository/commons-lang/commons-lang/2.6/commons-lang-2.6.jar:/Users/seven/software/maven/repository/commons-logging/commons-logging/1.2/commons-logging-1.2.jar:/Users/seven/software/maven/repository/com/google/guava/guava/12.0.1/guava-12.0.1.jar:/Users/seven/software/maven/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/Users/seven/software/maven/repository/io/netty/netty-all/4.0.23.Final/netty-all-4.0.23.Final.jar:/Users/seven/software/maven/repository/org/apache/zookeeper/zookeeper/3.4.5-cdh5.7.0/zookeeper-3.4.5-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/htrace/htrace-core/3.2.0-incubating/htrace-core-3.2.0-incubating.jar:/Users/seven/software/maven/repository/org/codehaus/jackson/jackson-mapper-asl/1.8.8/jackson-mapper-asl-1.8.8.jar:/Users/seven/software/maven/repository/org/jruby/jcodings/jcodings/1.0.8/jcodings-1.0.8.jar:/Users/seven/software/maven/repository/org/jruby/joni/joni/2.1.2/joni-2.1.2.jar:/Users/seven/software/maven/repository/com/yammer/metrics/metrics-core/2.2.0/metrics-core-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-auth/2.6.0-cdh5.7.0/hadoop-auth-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/httpcomponents/httpclient/4.2.5/httpclient-4.2.5.jar:/Users/seven/software/maven/repository/org/apache/httpcomponents/httpcore/4.2.4/httpcore-4.2.4.jar:/Users/seven/software/maven/repository/org/apache/directory/server/apacheds-kerberos-codec/2.0.0-M15/apacheds-kerberos-codec-2.0.0-M15.jar:/Users/seven/software/maven/repository/org/apache/directory/server/apacheds-i18n/2.0.0-M15/apacheds-i18n-2.0.0-M15.jar:/Users/seven/software/maven/repository/org/apache/directory/api/api-asn1-api/1.0.0-M20/api-asn1-api-1.0.0-M20.jar:/Users/seven/software/maven/repository/org/apache/directory/api/api-util/1.0.0-M20/api-util-1.0.0-M20.jar:/Users/seven/software/maven/repository/org/apache/curator/curator-framework/2.7.1/curator-framework-2.7.1.jar:/Users/seven/software/maven/repository/com/github/stephenc/findbugs/findbugs-annotations/1.3.9-1/findbugs-annotations-1.3.9-1.jar:/Users/seven/software/maven/repository/junit/junit/4.12/junit-4.12.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-server/1.2.0-cdh5.7.0/hbase-server-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-procedure/1.2.0-cdh5.7.0/hbase-procedure-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-common/1.2.0-cdh5.7.0/hbase-common-1.2.0-cdh5.7.0-tests.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-prefix-tree/1.2.0-cdh5.7.0/hbase-prefix-tree-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar:/Users/seven/software/maven/repository/commons-collections/commons-collections/3.2.2/commons-collections-3.2.2.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-hadoop-compat/1.2.0-cdh5.7.0/hbase-hadoop-compat-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-hadoop2-compat/1.2.0-cdh5.7.0/hbase-hadoop2-compat-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/com/sun/jersey/jersey-core/1.9/jersey-core-1.9.jar:/Users/seven/software/maven/repository/com/sun/jersey/jersey-server/1.9/jersey-server-1.9.jar:/Users/seven/software/maven/repository/asm/asm/3.1/asm-3.1.jar:/Users/seven/software/maven/repository/commons-cli/commons-cli/1.2/commons-cli-1.2.jar:/Users/seven/software/maven/repository/com/github/stephenc/high-scale-lib/high-scale-lib/1.1.1/high-scale-lib-1.1.1.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-math/2.1/commons-math-2.1.jar:/Users/seven/software/maven/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jetty/6.1.26.cloudera.4/jetty-6.1.26.cloudera.4.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jetty-util/6.1.26.cloudera.4/jetty-util-6.1.26.cloudera.4.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jetty-sslengine/6.1.26.cloudera.4/jetty-sslengine-6.1.26.cloudera.4.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jsp-2.1/6.1.14/jsp-2.1-6.1.14.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jsp-api-2.1/6.1.14/jsp-api-2.1-6.1.14.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/servlet-api-2.5/6.1.14/servlet-api-2.5-6.1.14.jar:/Users/seven/software/maven/repository/org/codehaus/jackson/jackson-core-asl/1.8.8/jackson-core-asl-1.8.8.jar:/Users/seven/software/maven/repository/org/codehaus/jackson/jackson-jaxrs/1.8.8/jackson-jaxrs-1.8.8.jar:/Users/seven/software/maven/repository/tomcat/jasper-compiler/5.5.23/jasper-compiler-5.5.23.jar:/Users/seven/software/maven/repository/tomcat/jasper-runtime/5.5.23/jasper-runtime-5.5.23.jar:/Users/seven/software/maven/repository/commons-el/commons-el/1.0/commons-el-1.0.jar:/Users/seven/software/maven/repository/org/jamon/jamon-runtime/2.4.1/jamon-runtime-2.4.1.jar:/Users/seven/software/maven/repository/com/lmax/disruptor/3.3.0/disruptor-3.3.0.jar:/Users/seven/software/maven/repository/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar:/Users/seven/software/maven/repository/org/owasp/esapi/esapi/2.1.0/esapi-2.1.0.jar:/Users/seven/software/maven/repository/commons-beanutils/commons-beanutils-core/1.7.0/commons-beanutils-core-1.7.0.jar:/Users/seven/software/maven/repository/commons-fileupload/commons-fileupload/1.2/commons-fileupload-1.2.jar:/Users/seven/software/maven/repository/xom/xom/1.2.5/xom-1.2.5.jar:/Users/seven/software/maven/repository/xalan/xalan/2.7.0/xalan-2.7.0.jar:/Users/seven/software/maven/repository/org/beanshell/bsh-core/2.0b4/bsh-core-2.0b4.jar:/Users/seven/software/maven/repository/org/owasp/antisamy/antisamy/1.4.3/antisamy-1.4.3.jar:/Users/seven/software/maven/repository/org/apache/xmlgraphics/batik-css/1.7/batik-css-1.7.jar:/Users/seven/software/maven/repository/org/apache/xmlgraphics/batik-ext/1.7/batik-ext-1.7.jar:/Users/seven/software/maven/repository/org/apache/xmlgraphics/batik-util/1.7/batik-util-1.7.jar:/Users/seven/software/maven/repository/xml-apis/xml-apis-ext/1.3.04/xml-apis-ext-1.3.04.jar:/Users/seven/software/maven/repository/net/sourceforge/nekohtml/nekohtml/1.9.12/nekohtml-1.9.12.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-core/2.6.0-mr1-cdh5.7.0/hadoop-core-2.6.0-mr1-cdh5.7.0.jar:/Users/seven/software/maven/repository/javax/servlet/jsp/jsp-api/2.1/jsp-api-2.1.jar:/Users/seven/software/maven/repository/net/java/dev/jets3t/jets3t/0.6.1/jets3t-0.6.1.jar:/Users/seven/software/maven/repository/javax/servlet/servlet-api/2.5/servlet-api-2.5.jar:/Users/seven/software/maven/repository/hsqldb/hsqldb/1.8.0.10/hsqldb-1.8.0.10.jar:/Users/seven/software/maven/repository/org/eclipse/jdt/core/3.1.1/core-3.1.1.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-hdfs/2.6.0-cdh5.7.0/hadoop-hdfs-2.6.0-cdh5.7.0-tests.jar:/Users/seven/software/maven/repository/commons-daemon/commons-daemon/1.0.13/commons-daemon-1.0.13.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-lang3/3.5/commons-lang3-3.5.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-pool2/2.4.2/commons-pool2-2.4.2.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-mllib_2.11/2.2.0/spark-mllib_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-core_2.11/2.2.0/spark-core_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/avro/avro-mapred/1.7.7/avro-mapred-1.7.7-hadoop2.jar:/Users/seven/software/maven/repository/org/apache/avro/avro-ipc/1.7.7/avro-ipc-1.7.7-tests.jar:/Users/seven/software/maven/repository/com/twitter/chill_2.11/0.8.0/chill_2.11-0.8.0.jar:/Users/seven/software/maven/repository/com/esotericsoftware/kryo-shaded/3.0.3/kryo-shaded-3.0.3.jar:/Users/seven/software/maven/repository/com/esotericsoftware/minlog/1.3.0/minlog-1.3.0.jar:/Users/seven/software/maven/repository/org/objenesis/objenesis/2.1/objenesis-2.1.jar:/Users/seven/software/maven/repository/com/twitter/chill-java/0.8.0/chill-java-0.8.0.jar:/Users/seven/software/maven/repository/org/apache/xbean/xbean-asm5-shaded/4.4/xbean-asm5-shaded-4.4.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-launcher_2.11/2.2.0/spark-launcher_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-network-common_2.11/2.2.0/spark-network-common_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-network-shuffle_2.11/2.2.0/spark-network-shuffle_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-unsafe_2.11/2.2.0/spark-unsafe_2.11-2.2.0.jar:/Users/seven/software/maven/repository/javax/servlet/javax.servlet-api/3.1.0/javax.servlet-api-3.1.0.jar:/Users/seven/software/maven/repository/org/slf4j/jul-to-slf4j/1.7.16/jul-to-slf4j-1.7.16.jar:/Users/seven/software/maven/repository/org/slf4j/jcl-over-slf4j/1.7.16/jcl-over-slf4j-1.7.16.jar:/Users/seven/software/maven/repository/com/ning/compress-lzf/1.0.3/compress-lzf-1.0.3.jar:/Users/seven/software/maven/repository/org/xerial/snappy/snappy-java/1.1.2.6/snappy-java-1.1.2.6.jar:/Users/seven/software/maven/repository/org/roaringbitmap/RoaringBitmap/0.5.11/RoaringBitmap-0.5.11.jar:/Users/seven/software/maven/repository/org/json4s/json4s-jackson_2.11/3.2.11/json4s-jackson_2.11-3.2.11.jar:/Users/seven/software/maven/repository/org/json4s/json4s-core_2.11/3.2.11/json4s-core_2.11-3.2.11.jar:/Users/seven/software/maven/repository/org/json4s/json4s-ast_2.11/3.2.11/json4s-ast_2.11-3.2.11.jar:/Users/seven/software/maven/repository/org/scala-lang/scalap/2.11.0/scalap-2.11.0.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/core/jersey-client/2.22.2/jersey-client-2.22.2.jar:/Users/seven/software/maven/repository/javax/ws/rs/javax.ws.rs-api/2.0.1/javax.ws.rs-api-2.0.1.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/hk2-api/2.4.0-b34/hk2-api-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/hk2-utils/2.4.0-b34/hk2-utils-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/external/aopalliance-repackaged/2.4.0-b34/aopalliance-repackaged-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/external/javax.inject/2.4.0-b34/javax.inject-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/hk2-locator/2.4.0-b34/hk2-locator-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/javassist/javassist/3.18.1-GA/javassist-3.18.1-GA.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/core/jersey-common/2.22.2/jersey-common-2.22.2.jar:/Users/seven/software/maven/repository/javax/annotation/javax.annotation-api/1.2/javax.annotation-api-1.2.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/bundles/repackaged/jersey-guava/2.22.2/jersey-guava-2.22.2.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/osgi-resource-locator/1.0.1/osgi-resource-locator-1.0.1.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/core/jersey-server/2.22.2/jersey-server-2.22.2.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/media/jersey-media-jaxb/2.22.2/jersey-media-jaxb-2.22.2.jar:/Users/seven/software/maven/repository/javax/validation/validation-api/1.1.0.Final/validation-api-1.1.0.Final.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/containers/jersey-container-servlet/2.22.2/jersey-container-servlet-2.22.2.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/containers/jersey-container-servlet-core/2.22.2/jersey-container-servlet-core-2.22.2.jar:/Users/seven/software/maven/repository/com/clearspring/analytics/stream/2.7.0/stream-2.7.0.jar:/Users/seven/software/maven/repository/io/dropwizard/metrics/metrics-core/3.1.2/metrics-core-3.1.2.jar:/Users/seven/software/maven/repository/io/dropwizard/metrics/metrics-jvm/3.1.2/metrics-jvm-3.1.2.jar:/Users/seven/software/maven/repository/io/dropwizard/metrics/metrics-json/3.1.2/metrics-json-3.1.2.jar:/Users/seven/software/maven/repository/io/dropwizard/metrics/metrics-graphite/3.1.2/metrics-graphite-3.1.2.jar:/Users/seven/software/maven/repository/org/apache/ivy/ivy/2.4.0/ivy-2.4.0.jar:/Users/seven/software/maven/repository/oro/oro/2.0.8/oro-2.0.8.jar:/Users/seven/software/maven/repository/net/razorvine/pyrolite/4.13/pyrolite-4.13.jar:/Users/seven/software/maven/repository/net/sf/py4j/py4j/0.10.4/py4j-0.10.4.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-crypto/1.0.0/commons-crypto-1.0.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-streaming_2.11/2.2.0/spark-streaming_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-sql_2.11/2.2.0/spark-sql_2.11-2.2.0.jar:/Users/seven/software/maven/repository/com/univocity/univocity-parsers/2.2.1/univocity-parsers-2.2.1.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-sketch_2.11/2.2.0/spark-sketch_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-catalyst_2.11/2.2.0/spark-catalyst_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/codehaus/janino/janino/3.0.0/janino-3.0.0.jar:/Users/seven/software/maven/repository/org/codehaus/janino/commons-compiler/3.0.0/commons-compiler-3.0.0.jar:/Users/seven/software/maven/repository/org/antlr/antlr4-runtime/4.5.3/antlr4-runtime-4.5.3.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-column/1.8.2/parquet-column-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-common/1.8.2/parquet-common-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-encoding/1.8.2/parquet-encoding-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-hadoop/1.8.2/parquet-hadoop-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-format/2.3.1/parquet-format-2.3.1.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-jackson/1.8.2/parquet-jackson-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-graphx_2.11/2.2.0/spark-graphx_2.11-2.2.0.jar:/Users/seven/software/maven/repository/com/github/fommil/netlib/core/1.1.2/core-1.1.2.jar:/Users/seven/software/maven/repository/net/sourceforge/f2j/arpack_combined_all/0.1/arpack_combined_all-0.1.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-mllib-local_2.11/2.2.0/spark-mllib-local_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/scalanlp/breeze_2.11/0.13.1/breeze_2.11-0.13.1.jar:/Users/seven/software/maven/repository/org/scalanlp/breeze-macros_2.11/0.13.1/breeze-macros_2.11-0.13.1.jar:/Users/seven/software/maven/repository/net/sf/opencsv/opencsv/2.3/opencsv-2.3.jar:/Users/seven/software/maven/repository/com/github/rwl/jtransforms/2.4.0/jtransforms-2.4.0.jar:/Users/seven/software/maven/repository/org/spire-math/spire_2.11/0.13.0/spire_2.11-0.13.0.jar:/Users/seven/software/maven/repository/org/spire-math/spire-macros_2.11/0.13.0/spire-macros_2.11-0.13.0.jar:/Users/seven/software/maven/repository/org/typelevel/machinist_2.11/0.6.1/machinist_2.11-0.6.1.jar:/Users/seven/software/maven/repository/com/chuusai/shapeless_2.11/2.3.2/shapeless_2.11-2.3.2.jar:/Users/seven/software/maven/repository/org/typelevel/macro-compat_2.11/1.1.1/macro-compat_2.11-1.1.1.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-math3/3.4.1/commons-math3-3.4.1.jar:/Users/seven/software/maven/repository/org/jpmml/pmml-model/1.2.15/pmml-model-1.2.15.jar:/Users/seven/software/maven/repository/org/jpmml/pmml-schema/1.2.15/pmml-schema-1.2.15.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-tags_2.11/2.2.0/spark-tags_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-streaming-kafka-0-8_2.11/2.2.0/spark-streaming-kafka-0-8_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/kafka/kafka_2.11/0.8.2.1/kafka_2.11-0.8.2.1.jar:/Users/seven/software/maven/repository/com/101tec/zkclient/0.3/zkclient-0.3.jar:/Users/seven/software/maven/repository/org/apache/kafka/kafka-clients/0.8.2.1/kafka-clients-0.8.2.1.jar:/Users/seven/software/maven/repository/mysql/mysql-connector-java/5.1.31/mysql-connector-java-5.1.31.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/module/jackson-module-scala_2.11/2.6.5/jackson-module-scala_2.11-2.6.5.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/core/jackson-core/2.6.5/jackson-core-2.6.5.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/core/jackson-annotations/2.6.5/jackson-annotations-2.6.5.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/core/jackson-databind/2.6.5/jackson-databind-2.6.5.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/module/jackson-module-paranamer/2.6.5/jackson-module-paranamer-2.6.5.jar:/Users/seven/software/maven/repository/com/thoughtworks/paranamer/paranamer/2.6/paranamer-2.6.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-streaming-flume_2.11/2.2.0/spark-streaming-flume_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/flume/flume-ng-core/1.6.0/flume-ng-core-1.6.0.jar:/Users/seven/software/maven/repository/org/apache/flume/flume-ng-configuration/1.6.0/flume-ng-configuration-1.6.0.jar:/Users/seven/software/maven/repository/org/apache/avro/avro-ipc/1.7.4/avro-ipc-1.7.4.jar:/Users/seven/software/maven/repository/org/apache/velocity/velocity/1.7/velocity-1.7.jar:/Users/seven/software/maven/repository/joda-time/joda-time/2.1/joda-time-2.1.jar:/Users/seven/software/maven/repository/org/apache/mina/mina-core/2.0.4/mina-core-2.0.4.jar:/Users/seven/software/maven/repository/org/apache/flume/flume-ng-sdk/1.6.0/flume-ng-sdk-1.6.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-streaming-flume-sink_2.11/2.2.0/spark-streaming-flume-sink_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/flume/flume-ng-clients/flume-ng-log4jappender/1.6.0/flume-ng-log4jappender-1.6.0.jar:/Users/seven/software/maven/repository/net/jpountz/lz4/lz4/1.3.0/lz4-1.3.0.jar:/Applications/IntelliJ IDEA.app/Contents/lib/idea_rt.jar
[INFO][2018-05-24 21:19:06,056][org.apache.zookeeper.ZooKeeper]Client environment:java.library.path=/Users/seven/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:.
[INFO][2018-05-24 21:19:06,056][org.apache.zookeeper.ZooKeeper]Client environment:java.io.tmpdir=/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/
[INFO][2018-05-24 21:19:06,057][org.apache.zookeeper.ZooKeeper]Client environment:java.compiler=<NA>
[INFO][2018-05-24 21:19:06,057][org.apache.zookeeper.ZooKeeper]Client environment:os.name=Mac OS X
[INFO][2018-05-24 21:19:06,057][org.apache.zookeeper.ZooKeeper]Client environment:os.arch=x86_64
[INFO][2018-05-24 21:19:06,057][org.apache.zookeeper.ZooKeeper]Client environment:os.version=10.13.4
[INFO][2018-05-24 21:19:06,057][org.apache.zookeeper.ZooKeeper]Client environment:user.name=seven
[INFO][2018-05-24 21:19:06,057][org.apache.zookeeper.ZooKeeper]Client environment:user.home=/Users/seven
[INFO][2018-05-24 21:19:06,057][org.apache.zookeeper.ZooKeeper]Client environment:user.dir=/Users/seven/project/scala/dataMining
[INFO][2018-05-24 21:19:06,058][org.apache.zookeeper.ZooKeeper]Initiating client connection, connectString=vm-xaj-bigdata-da-d01:2181,vm-xaj-bigdata-da-d02:2181,vm-xaj-bigdata-da-d03:2181 sessionTimeout=60000 watcher=hconnection-0x4841d9cd0x0, quorum=vm-xaj-bigdata-da-d01:2181,vm-xaj-bigdata-da-d02:2181,vm-xaj-bigdata-da-d03:2181, baseZNode=/hbase
[INFO][2018-05-24 21:19:08,532][org.apache.spark.SparkContext]Running Spark version 2.2.0
[INFO][2018-05-24 21:19:09,304][org.apache.spark.SparkContext]Submitted application: SimulationKafkaSendOutData$
[INFO][2018-05-24 21:19:09,325][org.apache.spark.SecurityManager]Changing view acls to: seven
[INFO][2018-05-24 21:19:09,326][org.apache.spark.SecurityManager]Changing modify acls to: seven
[INFO][2018-05-24 21:19:09,327][org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO][2018-05-24 21:19:09,328][org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO][2018-05-24 21:19:09,329][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(seven); groups with view permissions: Set(); users  with modify permissions: Set(seven); groups with modify permissions: Set()
[INFO][2018-05-24 21:19:09,720][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 51165.
[INFO][2018-05-24 21:19:09,746][org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO][2018-05-24 21:19:09,763][org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO][2018-05-24 21:19:09,766][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2018-05-24 21:19:09,766][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
[INFO][2018-05-24 21:19:09,777][org.apache.spark.storage.DiskBlockManager]Created local directory at /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/blockmgr-d11bb7ff-9868-4ec8-945f-54850ad1c4d3
[INFO][2018-05-24 21:19:09,798][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 912.3 MB
[INFO][2018-05-24 21:19:09,900][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO][2018-05-24 21:19:09,995][org.spark_project.jetty.util.log]Logging initialized @2794ms
[INFO][2018-05-24 21:19:10,054][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527167950000 ms
[INFO][2018-05-24 21:19:10,057][org.spark_project.jetty.server.Server]jetty-9.3.z-SNAPSHOT
[INFO][2018-05-24 21:19:10,070][org.spark_project.jetty.server.Server]Started @2870ms
[WARN][2018-05-24 21:19:10,084][org.apache.spark.util.Utils]Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[INFO][2018-05-24 21:19:10,088][org.spark_project.jetty.server.AbstractConnector]Started ServerConnector@1643d68f{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}
[INFO][2018-05-24 21:19:10,089][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4041.
[INFO][2018-05-24 21:19:10,109][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2a39fe6a{/jobs,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:19:10,110][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@649725e3{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:19:10,110][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4c168660{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:19:10,111][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4eed46ee{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:19:10,111][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4fad94a7{/stages,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:19:10,112][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6326d182{/stages/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:19:10,113][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@716a7124{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:19:10,114][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5cc126dc{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:19:10,115][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@72bd06ca{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:19:10,115][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5dbe30be{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:19:10,116][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@55795845{/storage,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:19:10,116][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@119f1f2a{/storage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:19:10,117][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5b970f7{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:19:10,118][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@165b8a71{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:19:10,118][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2f058b8a{/environment,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:19:10,119][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3f2ef586{/environment/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:19:10,120][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@76c7beb3{/executors,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:19:10,120][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2cf92cc7{/executors/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:19:10,121][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7b139eab{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:19:10,122][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@611df6e3{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:19:10,134][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6273c5a4{/static,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:19:10,135][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3e2943ab{/,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:19:10,136][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4a9f80d3{/api,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:19:10,137][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@191ae03f{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:19:10,138][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@53ab0286{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:19:10,140][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://192.168.0.102:4041
[INFO][2018-05-24 21:19:10,232][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
[INFO][2018-05-24 21:19:10,257][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 51168.
[INFO][2018-05-24 21:19:10,259][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 192.168.0.102:51168
[INFO][2018-05-24 21:19:10,261][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2018-05-24 21:19:10,263][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 192.168.0.102, 51168, None)
[INFO][2018-05-24 21:19:10,266][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 192.168.0.102:51168 with 912.3 MB RAM, BlockManagerId(driver, 192.168.0.102, 51168, None)
[INFO][2018-05-24 21:19:10,269][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 192.168.0.102, 51168, None)
[INFO][2018-05-24 21:19:10,269][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 192.168.0.102, 51168, None)
[INFO][2018-05-24 21:19:10,451][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6c2f1700{/metrics/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:19:10,959][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0 stored as values in memory (estimated size 228.1 KB, free 912.1 MB)
[INFO][2018-05-24 21:19:11,026][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0_piece0 stored as bytes in memory (estimated size 22.1 KB, free 912.1 MB)
[INFO][2018-05-24 21:19:11,029][org.apache.spark.storage.BlockManagerInfo]Added broadcast_0_piece0 in memory on 192.168.0.102:51168 (size: 22.1 KB, free: 912.3 MB)
[INFO][2018-05-24 21:19:11,033][org.apache.spark.SparkContext]Created broadcast 0 from textFile at SimulationKafkaSendOutData.scala:25
[INFO][2018-05-24 21:19:16,186][org.apache.zookeeper.ClientCnxn]Opening socket connection to server vm-xaj-bigdata-da-d03/10.213.4.27:2181. Will not attempt to authenticate using SASL (unknown error)
[INFO][2018-05-24 21:19:16,205][org.apache.zookeeper.ClientCnxn]Socket connection established, initiating session, client: /10.0.8.39:51169, server: vm-xaj-bigdata-da-d03/10.213.4.27:2181
[INFO][2018-05-24 21:19:16,225][org.apache.zookeeper.ClientCnxn]Session establishment complete on server vm-xaj-bigdata-da-d03/10.213.4.27:2181, sessionid = 0x362b50d75ec5f8f, negotiated timeout = 60000
[WARN][2018-05-24 21:19:16,630][org.apache.hadoop.util.NativeCodeLoader]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO][2018-05-24 21:19:16,799][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 1
[WARN][2018-05-24 21:19:16,804][org.apache.hadoop.util.NativeCodeLoader]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO][2018-05-24 21:19:16,935][org.apache.spark.SparkContext]Starting job: collect at SimulationKafkaSendOutData.scala:25
[INFO][2018-05-24 21:19:16,946][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:19:16,948][org.apache.spark.scheduler.DAGScheduler]Got job 0 (collect at SimulationKafkaSendOutData.scala:25) with 2 output partitions
[INFO][2018-05-24 21:19:16,948][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 0 (collect at SimulationKafkaSendOutData.scala:25)
[INFO][2018-05-24 21:19:16,949][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:19:16,950][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:19:16,958][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 0 (hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/* MapPartitionsRDD[1] at textFile at SimulationKafkaSendOutData.scala:25), which has no missing parents
[INFO][2018-05-24 21:19:16,966][org.apache.spark.executor.Executor]Finished task 0.0 in stage 0.0 (TID 0). 794 bytes result sent to driver
[INFO][2018-05-24 21:19:16,977][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 0.0 (TID 0) in 11333 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:19:16,979][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:19:16,981][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1 stored as values in memory (estimated size 3.3 KB, free 912.1 MB)
[INFO][2018-05-24 21:19:16,984][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1_piece0 stored as bytes in memory (estimated size 1969.0 B, free 912.1 MB)
[INFO][2018-05-24 21:19:16,985][org.apache.spark.storage.BlockManagerInfo]Added broadcast_1_piece0 in memory on 192.168.0.102:51168 (size: 1969.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:19:16,986][org.apache.spark.SparkContext]Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:19:16,987][org.apache.spark.scheduler.DAGScheduler]ResultStage 0 (foreachPartition at ReceiveKafkaData.scala:66) finished in 11.354 s
[INFO][2018-05-24 21:19:16,995][org.apache.spark.scheduler.DAGScheduler]Job 0 finished: foreachPartition at ReceiveKafkaData.scala:66, took 11.733577 s
[INFO][2018-05-24 21:19:17,001][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527167940000 ms.0 from job set of time 1527167940000 ms
[INFO][2018-05-24 21:19:17,002][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 17.000 s for time 1527167940000 ms (execution: 11.791 s)
[INFO][2018-05-24 21:19:17,003][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527167945000 ms.0 from job set of time 1527167945000 ms
[INFO][2018-05-24 21:19:17,012][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:19:17,013][org.apache.spark.scheduler.DAGScheduler]Got job 1 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:19:17,013][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 1 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:19:17,013][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:19:17,013][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:19:17,013][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ResultStage 0 (hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/* MapPartitionsRDD[1] at textFile at SimulationKafkaSendOutData.scala:25) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2018-05-24 21:19:17,013][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 1 (MapPartitionsRDD[3] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:19:17,015][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 0.0 with 2 tasks
[INFO][2018-05-24 21:19:17,017][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 21:19:17,018][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1_piece0 stored as bytes in memory (estimated size 1868.0 B, free 912.3 MB)
[INFO][2018-05-24 21:19:17,019][org.apache.spark.storage.BlockManagerInfo]Added broadcast_1_piece0 in memory on 192.168.0.102:51153 (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:19:17,020][org.apache.spark.SparkContext]Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:19:17,021][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:19:17,021][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 1.0 with 1 tasks
[INFO][2018-05-24 21:19:17,023][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:19:17,032][org.apache.spark.executor.Executor]Running task 0.0 in stage 1.0 (TID 1)
[INFO][2018-05-24 21:19:17,073][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 4883 bytes)
[INFO][2018-05-24 21:19:17,078][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 4883 bytes)
[INFO][2018-05-24 21:19:17,087][org.apache.spark.executor.Executor]Running task 1.0 in stage 0.0 (TID 1)
[INFO][2018-05-24 21:19:17,087][org.apache.spark.executor.Executor]Running task 0.0 in stage 0.0 (TID 0)
[INFO][2018-05-24 21:19:17,154][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12204 is the same as ending offset skipping seven 0
[INFO][2018-05-24 21:19:17,154][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:19:17,177][org.apache.spark.executor.Executor]Finished task 0.0 in stage 1.0 (TID 1). 708 bytes result sent to driver
[INFO][2018-05-24 21:19:17,187][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 1.0 (TID 1) in 165 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:19:17,188][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:19:17,190][org.apache.spark.scheduler.DAGScheduler]ResultStage 1 (foreachPartition at ReceiveKafkaData.scala:66) finished in 0.167 s
[INFO][2018-05-24 21:19:17,191][org.apache.spark.scheduler.DAGScheduler]Job 1 finished: foreachPartition at ReceiveKafkaData.scala:66, took 0.178596 s
[INFO][2018-05-24 21:19:17,192][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527167945000 ms.0 from job set of time 1527167945000 ms
[INFO][2018-05-24 21:19:17,192][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 12.192 s for time 1527167945000 ms (execution: 0.190 s)
[INFO][2018-05-24 21:19:17,194][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527167950000 ms.0 from job set of time 1527167950000 ms
[INFO][2018-05-24 21:19:17,202][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:19:17,203][org.apache.spark.scheduler.DAGScheduler]Got job 2 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:19:17,203][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 2 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:19:17,203][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:19:17,203][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:19:17,203][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 2 (MapPartitionsRDD[5] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:19:17,206][org.apache.spark.storage.memory.MemoryStore]Block broadcast_2 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 21:19:17,208][org.apache.spark.storage.memory.MemoryStore]Block broadcast_2_piece0 stored as bytes in memory (estimated size 1868.0 B, free 912.3 MB)
[INFO][2018-05-24 21:19:17,209][org.apache.spark.storage.BlockManagerInfo]Added broadcast_2_piece0 in memory on 192.168.0.102:51153 (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:19:17,210][org.apache.spark.SparkContext]Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:19:17,211][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[5] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:19:17,211][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 2.0 with 1 tasks
[INFO][2018-05-24 21:19:17,212][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:19:17,213][org.apache.spark.executor.Executor]Running task 0.0 in stage 2.0 (TID 2)
[INFO][2018-05-24 21:19:17,217][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12204 is the same as ending offset skipping seven 0
[INFO][2018-05-24 21:19:17,218][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:19:17,220][org.apache.spark.executor.Executor]Finished task 0.0 in stage 2.0 (TID 2). 708 bytes result sent to driver
[INFO][2018-05-24 21:19:17,221][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 2.0 (TID 2) in 9 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:19:17,222][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:19:17,222][org.apache.spark.scheduler.DAGScheduler]ResultStage 2 (foreachPartition at ReceiveKafkaData.scala:66) finished in 0.010 s
[INFO][2018-05-24 21:19:17,223][org.apache.spark.scheduler.DAGScheduler]Job 2 finished: foreachPartition at ReceiveKafkaData.scala:66, took 0.021201 s
[INFO][2018-05-24 21:19:17,225][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527167950000 ms.0 from job set of time 1527167950000 ms
[INFO][2018-05-24 21:19:17,225][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 7.224 s for time 1527167950000 ms (execution: 0.030 s)
[INFO][2018-05-24 21:19:17,230][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/part-00000:0+10717435
[INFO][2018-05-24 21:19:17,230][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/part-00000:10717435+10717436
[INFO][2018-05-24 21:19:20,580][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527167955000 ms
[INFO][2018-05-24 21:19:20,583][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527167955000 ms.0 from job set of time 1527167955000 ms
[INFO][2018-05-24 21:19:20,596][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 21:19:20,597][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:19:20,600][org.apache.spark.scheduler.DAGScheduler]Got job 3 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:19:20,600][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 3 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:19:20,600][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:19:20,600][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:19:20,601][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 
[INFO][2018-05-24 21:19:20,605][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 1 from persistence list
[INFO][2018-05-24 21:19:20,606][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 3 (MapPartitionsRDD[7] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:19:20,609][org.apache.spark.storage.memory.MemoryStore]Block broadcast_3 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 21:19:20,611][org.apache.spark.storage.memory.MemoryStore]Block broadcast_3_piece0 stored as bytes in memory (estimated size 1868.0 B, free 912.3 MB)
[INFO][2018-05-24 21:19:20,615][org.apache.spark.storage.BlockManagerInfo]Added broadcast_3_piece0 in memory on 192.168.0.102:51153 (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:19:20,616][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 0 from persistence list
[INFO][2018-05-24 21:19:20,618][org.apache.spark.SparkContext]Created broadcast 3 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:19:20,619][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[7] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:19:20,619][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 3.0 with 1 tasks
[INFO][2018-05-24 21:19:20,619][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 21:19:20,620][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 
[INFO][2018-05-24 21:19:20,620][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 3 from persistence list
[INFO][2018-05-24 21:19:20,621][org.apache.spark.storage.BlockManager]Removing RDD 1
[INFO][2018-05-24 21:19:20,622][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 3.0 (TID 3, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:19:20,624][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 2 from persistence list
[INFO][2018-05-24 21:19:20,624][org.apache.spark.executor.Executor]Running task 0.0 in stage 3.0 (TID 3)
[INFO][2018-05-24 21:19:20,626][org.apache.spark.storage.BlockManager]Removing RDD 0
[INFO][2018-05-24 21:19:20,626][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 21:19:20,627][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527167940000 ms
[INFO][2018-05-24 21:19:20,628][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12204 is the same as ending offset skipping seven 0
[INFO][2018-05-24 21:19:20,629][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:19:20,630][org.apache.spark.executor.Executor]Finished task 0.0 in stage 3.0 (TID 3). 708 bytes result sent to driver
[INFO][2018-05-24 21:19:20,631][org.apache.spark.storage.BlockManager]Removing RDD 3
[INFO][2018-05-24 21:19:20,632][org.apache.spark.storage.BlockManager]Removing RDD 2
[INFO][2018-05-24 21:19:20,634][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 3.0 (TID 3) in 14 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:19:20,634][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 3.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:19:20,635][org.apache.spark.scheduler.DAGScheduler]ResultStage 3 (foreachPartition at ReceiveKafkaData.scala:66) finished in 0.016 s
[INFO][2018-05-24 21:19:20,636][org.apache.spark.scheduler.DAGScheduler]Job 3 finished: foreachPartition at ReceiveKafkaData.scala:66, took 0.037998 s
[INFO][2018-05-24 21:19:20,636][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527167955000 ms.0 from job set of time 1527167955000 ms
[INFO][2018-05-24 21:19:20,636][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 5.636 s for time 1527167955000 ms (execution: 0.053 s)
[INFO][2018-05-24 21:19:20,704][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527167960000 ms
[INFO][2018-05-24 21:19:20,705][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 5 from persistence list
[INFO][2018-05-24 21:19:20,707][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527167960000 ms.0 from job set of time 1527167960000 ms
[INFO][2018-05-24 21:19:20,708][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 4 from persistence list
[INFO][2018-05-24 21:19:20,708][org.apache.spark.storage.BlockManager]Removing RDD 5
[INFO][2018-05-24 21:19:20,709][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 21:19:20,710][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527167945000 ms
[INFO][2018-05-24 21:19:20,710][org.apache.spark.storage.BlockManager]Removing RDD 4
[INFO][2018-05-24 21:19:20,715][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:19:20,717][org.apache.spark.scheduler.DAGScheduler]Got job 4 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:19:20,717][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 4 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:19:20,717][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:19:20,718][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:19:20,718][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 4 (MapPartitionsRDD[9] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:19:20,721][org.apache.spark.storage.memory.MemoryStore]Block broadcast_4 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 21:19:20,723][org.apache.spark.storage.memory.MemoryStore]Block broadcast_4_piece0 stored as bytes in memory (estimated size 1868.0 B, free 912.3 MB)
[INFO][2018-05-24 21:19:20,724][org.apache.spark.storage.BlockManagerInfo]Added broadcast_4_piece0 in memory on 192.168.0.102:51153 (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:19:20,724][org.apache.spark.SparkContext]Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:19:20,725][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[9] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:19:20,725][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 4.0 with 1 tasks
[INFO][2018-05-24 21:19:20,726][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 4.0 (TID 4, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:19:20,727][org.apache.spark.executor.Executor]Running task 0.0 in stage 4.0 (TID 4)
[INFO][2018-05-24 21:19:20,731][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12204 is the same as ending offset skipping seven 0
[INFO][2018-05-24 21:19:20,732][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:19:20,733][org.apache.spark.executor.Executor]Finished task 0.0 in stage 4.0 (TID 4). 708 bytes result sent to driver
[INFO][2018-05-24 21:19:20,735][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 4.0 (TID 4) in 8 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:19:20,735][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 4.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:19:20,737][org.apache.spark.scheduler.DAGScheduler]ResultStage 4 (foreachPartition at ReceiveKafkaData.scala:66) finished in 0.010 s
[INFO][2018-05-24 21:19:20,740][org.apache.spark.scheduler.DAGScheduler]Job 4 finished: foreachPartition at ReceiveKafkaData.scala:66, took 0.023860 s
[INFO][2018-05-24 21:19:20,740][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527167960000 ms.0 from job set of time 1527167960000 ms
[INFO][2018-05-24 21:19:20,741][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.740 s for time 1527167960000 ms (execution: 0.033 s)
[INFO][2018-05-24 21:19:20,741][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 7 from persistence list
[INFO][2018-05-24 21:19:20,742][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 6 from persistence list
[INFO][2018-05-24 21:19:20,742][org.apache.spark.storage.BlockManager]Removing RDD 7
[INFO][2018-05-24 21:19:20,745][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 21:19:20,745][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527167950000 ms
[INFO][2018-05-24 21:19:20,745][org.apache.spark.storage.BlockManager]Removing RDD 6
[INFO][2018-05-24 21:19:20,841][org.apache.spark.SparkContext]Invoking stop() from shutdown hook
[INFO][2018-05-24 21:19:20,854][org.spark_project.jetty.server.AbstractConnector]Stopped Spark@1643d68f{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}
[INFO][2018-05-24 21:19:20,856][org.apache.spark.ui.SparkUI]Stopped Spark web UI at http://192.168.0.102:4041
[INFO][2018-05-24 21:19:20,876][org.apache.spark.scheduler.DAGScheduler]ResultStage 0 (collect at SimulationKafkaSendOutData.scala:25) failed in 3.835 s due to Stage cancelled because SparkContext was shut down
[INFO][2018-05-24 21:19:20,873][org.apache.spark.scheduler.DAGScheduler]Job 0 failed: collect at SimulationKafkaSendOutData.scala:25, took 3.936438 s
[ERROR][2018-05-24 21:19:20,884][org.apache.spark.scheduler.LiveListenerBus]SparkListenerBus has already stopped! Dropping event SparkListenerStageCompleted(org.apache.spark.scheduler.StageInfo@3939ee88)
[ERROR][2018-05-24 21:19:20,896][org.apache.spark.scheduler.LiveListenerBus]SparkListenerBus has already stopped! Dropping event SparkListenerJobEnd(0,1527167960891,JobFailed(org.apache.spark.SparkException: Job 0 cancelled because SparkContext was shut down))
[INFO][2018-05-24 21:19:20,906][org.apache.spark.MapOutputTrackerMasterEndpoint]MapOutputTrackerMasterEndpoint stopped!
[INFO][2018-05-24 21:19:20,937][org.apache.spark.storage.memory.MemoryStore]MemoryStore cleared
[INFO][2018-05-24 21:19:20,941][org.apache.spark.storage.BlockManager]BlockManager stopped
[INFO][2018-05-24 21:19:20,957][org.apache.spark.storage.BlockManagerMaster]BlockManagerMaster stopped
[INFO][2018-05-24 21:19:20,960][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint]OutputCommitCoordinator stopped!
[INFO][2018-05-24 21:19:20,964][org.apache.spark.SparkContext]Successfully stopped SparkContext
[INFO][2018-05-24 21:19:20,965][org.apache.spark.util.ShutdownHookManager]Shutdown hook called
[INFO][2018-05-24 21:19:20,966][org.apache.spark.util.ShutdownHookManager]Deleting directory /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/spark-7f76c4bf-49f3-4825-8441-80f7f14a489c
[ERROR][2018-05-24 21:19:21,038][org.apache.spark.TaskContextImpl]Error in TaskCompletionListener
java.lang.IllegalStateException: Block broadcast_0 not found
	at org.apache.spark.storage.BlockInfoManager$$anonfun$2.apply(BlockInfoManager.scala:293)
	at org.apache.spark.storage.BlockInfoManager$$anonfun$2.apply(BlockInfoManager.scala:293)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.storage.BlockInfoManager.unlock(BlockInfoManager.scala:292)
	at org.apache.spark.storage.BlockManager.releaseLock(BlockManager.scala:720)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$releaseLock$1.apply(TorrentBroadcast.scala:250)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$releaseLock$1.apply(TorrentBroadcast.scala:250)
	at org.apache.spark.TaskContext$$anon$1.onTaskCompletion(TaskContext.scala:128)
	at org.apache.spark.TaskContextImpl$$anonfun$markTaskCompleted$1.apply(TaskContextImpl.scala:117)
	at org.apache.spark.TaskContextImpl$$anonfun$markTaskCompleted$1.apply(TaskContextImpl.scala:117)
	at org.apache.spark.TaskContextImpl$$anonfun$invokeListeners$1.apply(TaskContextImpl.scala:130)
	at org.apache.spark.TaskContextImpl$$anonfun$invokeListeners$1.apply(TaskContextImpl.scala:128)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:128)
	at org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:116)
	at org.apache.spark.scheduler.Task.run(Task.scala:118)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[ERROR][2018-05-24 21:19:21,043][org.apache.spark.TaskContextImpl]Error in TaskCompletionListener
java.lang.IllegalStateException: Block broadcast_1 not found
	at org.apache.spark.storage.BlockInfoManager$$anonfun$2.apply(BlockInfoManager.scala:293)
	at org.apache.spark.storage.BlockInfoManager$$anonfun$2.apply(BlockInfoManager.scala:293)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.storage.BlockInfoManager.unlock(BlockInfoManager.scala:292)
	at org.apache.spark.storage.BlockManager.releaseLock(BlockManager.scala:720)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$releaseLock$1.apply(TorrentBroadcast.scala:250)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$releaseLock$1.apply(TorrentBroadcast.scala:250)
	at org.apache.spark.TaskContext$$anon$1.onTaskCompletion(TaskContext.scala:128)
	at org.apache.spark.TaskContextImpl$$anonfun$markTaskCompleted$1.apply(TaskContextImpl.scala:117)
	at org.apache.spark.TaskContextImpl$$anonfun$markTaskCompleted$1.apply(TaskContextImpl.scala:117)
	at org.apache.spark.TaskContextImpl$$anonfun$invokeListeners$1.apply(TaskContextImpl.scala:130)
	at org.apache.spark.TaskContextImpl$$anonfun$invokeListeners$1.apply(TaskContextImpl.scala:128)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:128)
	at org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:116)
	at org.apache.spark.scheduler.Task.run(Task.scala:118)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[ERROR][2018-05-24 21:19:21,045][org.apache.spark.util.Utils]Uncaught exception in thread Executor task launch worker for task 0
java.lang.NullPointerException
	at org.apache.spark.scheduler.Task$$anonfun$run$1.apply$mcV$sp(Task.scala:129)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1317)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[ERROR][2018-05-24 21:19:21,071][org.apache.spark.TaskContextImpl]Error in TaskCompletionListener
java.lang.IllegalStateException: Block broadcast_0 not found
	at org.apache.spark.storage.BlockInfoManager$$anonfun$2.apply(BlockInfoManager.scala:293)
	at org.apache.spark.storage.BlockInfoManager$$anonfun$2.apply(BlockInfoManager.scala:293)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.storage.BlockInfoManager.unlock(BlockInfoManager.scala:292)
	at org.apache.spark.storage.BlockManager.releaseLock(BlockManager.scala:720)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$releaseLock$1.apply(TorrentBroadcast.scala:250)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$releaseLock$1.apply(TorrentBroadcast.scala:250)
	at org.apache.spark.TaskContext$$anon$1.onTaskCompletion(TaskContext.scala:128)
	at org.apache.spark.TaskContextImpl$$anonfun$markTaskCompleted$1.apply(TaskContextImpl.scala:117)
	at org.apache.spark.TaskContextImpl$$anonfun$markTaskCompleted$1.apply(TaskContextImpl.scala:117)
	at org.apache.spark.TaskContextImpl$$anonfun$invokeListeners$1.apply(TaskContextImpl.scala:130)
	at org.apache.spark.TaskContextImpl$$anonfun$invokeListeners$1.apply(TaskContextImpl.scala:128)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:128)
	at org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:116)
	at org.apache.spark.scheduler.Task.run(Task.scala:118)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[ERROR][2018-05-24 21:19:21,072][org.apache.spark.TaskContextImpl]Error in TaskCompletionListener
java.lang.IllegalStateException: Block broadcast_1 not found
	at org.apache.spark.storage.BlockInfoManager$$anonfun$2.apply(BlockInfoManager.scala:293)
	at org.apache.spark.storage.BlockInfoManager$$anonfun$2.apply(BlockInfoManager.scala:293)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.storage.BlockInfoManager.unlock(BlockInfoManager.scala:292)
	at org.apache.spark.storage.BlockManager.releaseLock(BlockManager.scala:720)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$releaseLock$1.apply(TorrentBroadcast.scala:250)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$releaseLock$1.apply(TorrentBroadcast.scala:250)
	at org.apache.spark.TaskContext$$anon$1.onTaskCompletion(TaskContext.scala:128)
	at org.apache.spark.TaskContextImpl$$anonfun$markTaskCompleted$1.apply(TaskContextImpl.scala:117)
	at org.apache.spark.TaskContextImpl$$anonfun$markTaskCompleted$1.apply(TaskContextImpl.scala:117)
	at org.apache.spark.TaskContextImpl$$anonfun$invokeListeners$1.apply(TaskContextImpl.scala:130)
	at org.apache.spark.TaskContextImpl$$anonfun$invokeListeners$1.apply(TaskContextImpl.scala:128)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:128)
	at org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:116)
	at org.apache.spark.scheduler.Task.run(Task.scala:118)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[ERROR][2018-05-24 21:19:21,072][org.apache.spark.util.Utils]Uncaught exception in thread Executor task launch worker for task 1
java.lang.NullPointerException
	at org.apache.spark.scheduler.Task$$anonfun$run$1.apply$mcV$sp(Task.scala:129)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1317)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO][2018-05-24 21:19:33,306][org.apache.spark.SparkContext]Running Spark version 2.2.0
[INFO][2018-05-24 21:19:34,034][org.apache.spark.SparkContext]Submitted application: SimulationKafkaSendOutData$
[INFO][2018-05-24 21:19:34,052][org.apache.spark.SecurityManager]Changing view acls to: seven
[INFO][2018-05-24 21:19:34,053][org.apache.spark.SecurityManager]Changing modify acls to: seven
[INFO][2018-05-24 21:19:34,053][org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO][2018-05-24 21:19:34,054][org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO][2018-05-24 21:19:34,055][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(seven); groups with view permissions: Set(); users  with modify permissions: Set(seven); groups with modify permissions: Set()
[INFO][2018-05-24 21:19:34,358][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 51187.
[INFO][2018-05-24 21:19:34,381][org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO][2018-05-24 21:19:34,397][org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO][2018-05-24 21:19:34,399][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2018-05-24 21:19:34,399][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
[INFO][2018-05-24 21:19:34,408][org.apache.spark.storage.DiskBlockManager]Created local directory at /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/blockmgr-0ba97365-b2ec-4620-a57f-dbe73d75b4ca
[INFO][2018-05-24 21:19:34,430][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 912.3 MB
[INFO][2018-05-24 21:19:34,520][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO][2018-05-24 21:19:34,643][org.spark_project.jetty.util.log]Logging initialized @2573ms
[INFO][2018-05-24 21:19:34,705][org.spark_project.jetty.server.Server]jetty-9.3.z-SNAPSHOT
[INFO][2018-05-24 21:19:34,716][org.spark_project.jetty.server.Server]Started @2647ms
[WARN][2018-05-24 21:19:34,734][org.apache.spark.util.Utils]Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[INFO][2018-05-24 21:19:34,739][org.spark_project.jetty.server.AbstractConnector]Started ServerConnector@1643d68f{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}
[INFO][2018-05-24 21:19:34,739][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4041.
[INFO][2018-05-24 21:19:34,761][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2a39fe6a{/jobs,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:19:34,762][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@649725e3{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:19:34,762][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4c168660{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:19:34,764][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4eed46ee{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:19:34,765][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4fad94a7{/stages,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:19:34,765][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6326d182{/stages/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:19:34,766][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@716a7124{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:19:34,767][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5cc126dc{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:19:34,768][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@72bd06ca{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:19:34,768][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5dbe30be{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:19:34,768][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@55795845{/storage,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:19:34,769][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@119f1f2a{/storage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:19:34,769][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5b970f7{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:19:34,770][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@165b8a71{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:19:34,771][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2f058b8a{/environment,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:19:34,771][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3f2ef586{/environment/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:19:34,772][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@76c7beb3{/executors,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:19:34,773][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2cf92cc7{/executors/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:19:34,774][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7b139eab{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:19:34,774][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@611df6e3{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:19:34,781][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6273c5a4{/static,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:19:34,782][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3e2943ab{/,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:19:34,784][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4a9f80d3{/api,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:19:34,784][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@191ae03f{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:19:34,785][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@53ab0286{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:19:34,792][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://192.168.0.102:4041
[INFO][2018-05-24 21:19:34,902][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
[INFO][2018-05-24 21:19:34,947][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 51188.
[INFO][2018-05-24 21:19:34,947][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 192.168.0.102:51188
[INFO][2018-05-24 21:19:34,949][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2018-05-24 21:19:34,950][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 192.168.0.102, 51188, None)
[INFO][2018-05-24 21:19:34,954][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 192.168.0.102:51188 with 912.3 MB RAM, BlockManagerId(driver, 192.168.0.102, 51188, None)
[INFO][2018-05-24 21:19:34,968][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 192.168.0.102, 51188, None)
[INFO][2018-05-24 21:19:34,970][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 192.168.0.102, 51188, None)
[INFO][2018-05-24 21:19:35,083][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527167965000 ms
[INFO][2018-05-24 21:19:35,083][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527167965000 ms.0 from job set of time 1527167965000 ms
[INFO][2018-05-24 21:19:35,091][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:19:35,092][org.apache.spark.scheduler.DAGScheduler]Got job 5 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:19:35,092][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 5 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:19:35,092][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:19:35,092][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:19:35,092][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 5 (MapPartitionsRDD[11] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:19:35,095][org.apache.spark.storage.memory.MemoryStore]Block broadcast_5 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 21:19:35,096][org.apache.spark.storage.memory.MemoryStore]Block broadcast_5_piece0 stored as bytes in memory (estimated size 1868.0 B, free 912.3 MB)
[INFO][2018-05-24 21:19:35,097][org.apache.spark.storage.BlockManagerInfo]Added broadcast_5_piece0 in memory on 192.168.0.102:51153 (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:19:35,098][org.apache.spark.SparkContext]Created broadcast 5 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:19:35,098][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[11] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:19:35,098][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 5.0 with 1 tasks
[INFO][2018-05-24 21:19:35,099][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 5.0 (TID 5, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:19:35,100][org.apache.spark.executor.Executor]Running task 0.0 in stage 5.0 (TID 5)
[INFO][2018-05-24 21:19:35,102][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12204 is the same as ending offset skipping seven 0
[INFO][2018-05-24 21:19:35,103][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:19:35,104][org.apache.spark.executor.Executor]Finished task 0.0 in stage 5.0 (TID 5). 708 bytes result sent to driver
[INFO][2018-05-24 21:19:35,104][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 5.0 (TID 5) in 5 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:19:35,105][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 5.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:19:35,105][org.apache.spark.scheduler.DAGScheduler]ResultStage 5 (foreachPartition at ReceiveKafkaData.scala:66) finished in 0.006 s
[INFO][2018-05-24 21:19:35,105][org.apache.spark.scheduler.DAGScheduler]Job 5 finished: foreachPartition at ReceiveKafkaData.scala:66, took 0.014538 s
[INFO][2018-05-24 21:19:35,106][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527167965000 ms.0 from job set of time 1527167965000 ms
[INFO][2018-05-24 21:19:35,106][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 10.106 s for time 1527167965000 ms (execution: 0.023 s)
[INFO][2018-05-24 21:19:35,133][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527167970000 ms
[INFO][2018-05-24 21:19:35,134][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527167970000 ms.0 from job set of time 1527167970000 ms
[INFO][2018-05-24 21:19:35,141][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:19:35,141][org.apache.spark.scheduler.DAGScheduler]Got job 6 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:19:35,141][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 6 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:19:35,142][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:19:35,142][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:19:35,142][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 6 (MapPartitionsRDD[13] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:19:35,144][org.apache.spark.storage.memory.MemoryStore]Block broadcast_6 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 21:19:35,147][org.apache.spark.storage.memory.MemoryStore]Block broadcast_6_piece0 stored as bytes in memory (estimated size 1868.0 B, free 912.3 MB)
[INFO][2018-05-24 21:19:35,149][org.apache.spark.storage.BlockManagerInfo]Added broadcast_6_piece0 in memory on 192.168.0.102:51153 (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:19:35,149][org.apache.spark.SparkContext]Created broadcast 6 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:19:35,150][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[13] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:19:35,150][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 6.0 with 1 tasks
[INFO][2018-05-24 21:19:35,151][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 6.0 (TID 6, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:19:35,151][org.apache.spark.executor.Executor]Running task 0.0 in stage 6.0 (TID 6)
[INFO][2018-05-24 21:19:35,154][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12204 is the same as ending offset skipping seven 0
[INFO][2018-05-24 21:19:35,154][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:19:35,155][org.apache.spark.executor.Executor]Finished task 0.0 in stage 6.0 (TID 6). 708 bytes result sent to driver
[INFO][2018-05-24 21:19:35,156][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 6.0 (TID 6) in 5 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:19:35,156][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 6.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:19:35,157][org.apache.spark.scheduler.DAGScheduler]ResultStage 6 (foreachPartition at ReceiveKafkaData.scala:66) finished in 0.006 s
[INFO][2018-05-24 21:19:35,157][org.apache.spark.scheduler.DAGScheduler]Job 6 finished: foreachPartition at ReceiveKafkaData.scala:66, took 0.016298 s
[INFO][2018-05-24 21:19:35,158][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527167970000 ms.0 from job set of time 1527167970000 ms
[INFO][2018-05-24 21:19:35,158][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 5.158 s for time 1527167970000 ms (execution: 0.024 s)
[INFO][2018-05-24 21:19:35,283][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6c2f1700{/metrics/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:19:35,895][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0 stored as values in memory (estimated size 228.1 KB, free 912.1 MB)
[INFO][2018-05-24 21:19:36,014][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0_piece0 stored as bytes in memory (estimated size 22.1 KB, free 912.1 MB)
[INFO][2018-05-24 21:19:36,017][org.apache.spark.storage.BlockManagerInfo]Added broadcast_0_piece0 in memory on 192.168.0.102:51188 (size: 22.1 KB, free: 912.3 MB)
[INFO][2018-05-24 21:19:36,023][org.apache.spark.SparkContext]Created broadcast 0 from textFile at SimulationKafkaSendOutData.scala:25
[INFO][2018-05-24 21:19:40,214][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527167975000 ms
[INFO][2018-05-24 21:19:40,218][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527167975000 ms.0 from job set of time 1527167975000 ms
[INFO][2018-05-24 21:19:40,221][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 9 from persistence list
[INFO][2018-05-24 21:19:40,231][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:19:40,232][org.apache.spark.storage.BlockManager]Removing RDD 9
[INFO][2018-05-24 21:19:40,235][org.apache.spark.scheduler.DAGScheduler]Got job 7 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:19:40,236][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 7 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:19:40,236][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:19:40,236][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:19:40,237][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 7 (MapPartitionsRDD[15] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:19:40,239][org.apache.spark.storage.memory.MemoryStore]Block broadcast_7 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 21:19:40,241][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 8 from persistence list
[INFO][2018-05-24 21:19:40,241][org.apache.spark.storage.memory.MemoryStore]Block broadcast_7_piece0 stored as bytes in memory (estimated size 1868.0 B, free 912.3 MB)
[INFO][2018-05-24 21:19:40,242][org.apache.spark.storage.BlockManager]Removing RDD 8
[INFO][2018-05-24 21:19:40,243][org.apache.spark.storage.BlockManagerInfo]Added broadcast_7_piece0 in memory on 192.168.0.102:51153 (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:19:40,245][org.apache.spark.SparkContext]Created broadcast 7 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:19:40,245][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[15] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:19:40,245][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 7.0 with 1 tasks
[INFO][2018-05-24 21:19:40,247][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 7.0 (TID 7, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:19:40,251][org.apache.spark.executor.Executor]Running task 0.0 in stage 7.0 (TID 7)
[INFO][2018-05-24 21:19:40,254][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12204 is the same as ending offset skipping seven 0
[INFO][2018-05-24 21:19:40,254][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:19:40,257][org.apache.spark.executor.Executor]Finished task 0.0 in stage 7.0 (TID 7). 708 bytes result sent to driver
[INFO][2018-05-24 21:19:40,258][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 7.0 (TID 7) in 11 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:19:40,258][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 7.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:19:40,258][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 21:19:40,258][org.apache.spark.scheduler.DAGScheduler]ResultStage 7 (foreachPartition at ReceiveKafkaData.scala:66) finished in 0.012 s
[INFO][2018-05-24 21:19:40,259][org.apache.spark.scheduler.DAGScheduler]Job 7 finished: foreachPartition at ReceiveKafkaData.scala:66, took 0.027624 s
[INFO][2018-05-24 21:19:40,260][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527167975000 ms.0 from job set of time 1527167975000 ms
[INFO][2018-05-24 21:19:40,260][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 5.260 s for time 1527167975000 ms (execution: 0.043 s)
[INFO][2018-05-24 21:19:40,259][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527167955000 ms
[INFO][2018-05-24 21:19:40,260][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 11 from persistence list
[INFO][2018-05-24 21:19:40,260][org.apache.spark.storage.BlockManager]Removing RDD 11
[INFO][2018-05-24 21:19:40,261][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 10 from persistence list
[INFO][2018-05-24 21:19:40,261][org.apache.spark.storage.BlockManager]Removing RDD 10
[INFO][2018-05-24 21:19:40,261][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 21:19:40,261][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527167960000 ms
[WARN][2018-05-24 21:19:41,622][org.apache.hadoop.util.NativeCodeLoader]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO][2018-05-24 21:19:41,767][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 1
[INFO][2018-05-24 21:19:41,863][org.apache.spark.SparkContext]Starting job: collect at SimulationKafkaSendOutData.scala:25
[INFO][2018-05-24 21:19:41,873][org.apache.spark.scheduler.DAGScheduler]Got job 0 (collect at SimulationKafkaSendOutData.scala:25) with 2 output partitions
[INFO][2018-05-24 21:19:41,874][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 0 (collect at SimulationKafkaSendOutData.scala:25)
[INFO][2018-05-24 21:19:41,874][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:19:41,876][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:19:41,883][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 0 (hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/* MapPartitionsRDD[1] at textFile at SimulationKafkaSendOutData.scala:25), which has no missing parents
[INFO][2018-05-24 21:19:41,898][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1 stored as values in memory (estimated size 3.3 KB, free 912.1 MB)
[INFO][2018-05-24 21:19:41,900][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1_piece0 stored as bytes in memory (estimated size 1969.0 B, free 912.1 MB)
[INFO][2018-05-24 21:19:41,900][org.apache.spark.storage.BlockManagerInfo]Added broadcast_1_piece0 in memory on 192.168.0.102:51188 (size: 1969.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:19:41,901][org.apache.spark.SparkContext]Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:19:41,916][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ResultStage 0 (hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/* MapPartitionsRDD[1] at textFile at SimulationKafkaSendOutData.scala:25) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2018-05-24 21:19:41,916][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 0.0 with 2 tasks
[INFO][2018-05-24 21:19:41,970][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 4883 bytes)
[INFO][2018-05-24 21:19:41,973][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 4883 bytes)
[INFO][2018-05-24 21:19:41,982][org.apache.spark.executor.Executor]Running task 1.0 in stage 0.0 (TID 1)
[INFO][2018-05-24 21:19:41,982][org.apache.spark.executor.Executor]Running task 0.0 in stage 0.0 (TID 0)
[INFO][2018-05-24 21:19:42,097][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/part-00000:0+10717435
[INFO][2018-05-24 21:19:42,097][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/part-00000:10717435+10717436
[INFO][2018-05-24 21:19:45,325][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527167980000 ms
[INFO][2018-05-24 21:19:45,325][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 13 from persistence list
[INFO][2018-05-24 21:19:45,325][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527167980000 ms.0 from job set of time 1527167980000 ms
[INFO][2018-05-24 21:19:45,325][org.apache.spark.storage.BlockManager]Removing RDD 13
[INFO][2018-05-24 21:19:45,326][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 12 from persistence list
[INFO][2018-05-24 21:19:45,327][org.apache.spark.storage.BlockManager]Removing RDD 12
[INFO][2018-05-24 21:19:45,327][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 21:19:45,327][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527167965000 ms
[INFO][2018-05-24 21:19:45,335][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:19:45,336][org.apache.spark.scheduler.DAGScheduler]Got job 8 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:19:45,336][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 8 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:19:45,336][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:19:45,336][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:19:45,337][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 8 (MapPartitionsRDD[17] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:19:45,341][org.apache.spark.storage.memory.MemoryStore]Block broadcast_8 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 21:19:45,344][org.apache.spark.storage.memory.MemoryStore]Block broadcast_8_piece0 stored as bytes in memory (estimated size 1868.0 B, free 912.3 MB)
[INFO][2018-05-24 21:19:45,346][org.apache.spark.storage.BlockManagerInfo]Added broadcast_8_piece0 in memory on 192.168.0.102:51153 (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:19:45,347][org.apache.spark.SparkContext]Created broadcast 8 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:19:45,348][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[17] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:19:45,349][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 8.0 with 1 tasks
[INFO][2018-05-24 21:19:45,350][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 8.0 (TID 8, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:19:45,351][org.apache.spark.executor.Executor]Running task 0.0 in stage 8.0 (TID 8)
[INFO][2018-05-24 21:19:45,356][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12204 is the same as ending offset skipping seven 0
[INFO][2018-05-24 21:19:45,356][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:19:45,357][org.apache.spark.executor.Executor]Finished task 0.0 in stage 8.0 (TID 8). 751 bytes result sent to driver
[INFO][2018-05-24 21:19:45,358][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 8.0 (TID 8) in 8 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:19:45,358][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 8.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:19:45,364][org.apache.spark.scheduler.DAGScheduler]ResultStage 8 (foreachPartition at ReceiveKafkaData.scala:66) finished in 0.015 s
[INFO][2018-05-24 21:19:45,365][org.apache.spark.scheduler.DAGScheduler]Job 8 finished: foreachPartition at ReceiveKafkaData.scala:66, took 0.029541 s
[INFO][2018-05-24 21:19:45,365][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527167980000 ms.0 from job set of time 1527167980000 ms
[INFO][2018-05-24 21:19:45,366][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 5.365 s for time 1527167980000 ms (execution: 0.040 s)
[INFO][2018-05-24 21:19:45,383][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527167985000 ms
[INFO][2018-05-24 21:19:45,383][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 15 from persistence list
[INFO][2018-05-24 21:19:45,384][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527167985000 ms.0 from job set of time 1527167985000 ms
[INFO][2018-05-24 21:19:45,384][org.apache.spark.storage.BlockManager]Removing RDD 15
[INFO][2018-05-24 21:19:45,386][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 14 from persistence list
[INFO][2018-05-24 21:19:45,386][org.apache.spark.storage.BlockManager]Removing RDD 14
[INFO][2018-05-24 21:19:45,387][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 21:19:45,388][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527167970000 ms
[INFO][2018-05-24 21:19:45,396][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:19:45,398][org.apache.spark.scheduler.DAGScheduler]Got job 9 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:19:45,398][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 9 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:19:45,398][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:19:45,398][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:19:45,399][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 9 (MapPartitionsRDD[19] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:19:45,401][org.apache.spark.storage.memory.MemoryStore]Block broadcast_9 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 21:19:45,407][org.apache.spark.storage.memory.MemoryStore]Block broadcast_9_piece0 stored as bytes in memory (estimated size 1868.0 B, free 912.3 MB)
[INFO][2018-05-24 21:19:45,408][org.apache.spark.storage.BlockManagerInfo]Added broadcast_9_piece0 in memory on 192.168.0.102:51153 (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:19:45,409][org.apache.spark.SparkContext]Created broadcast 9 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:19:45,410][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[19] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:19:45,411][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 9.0 with 1 tasks
[INFO][2018-05-24 21:19:45,412][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 9.0 (TID 9, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:19:45,413][org.apache.spark.executor.Executor]Running task 0.0 in stage 9.0 (TID 9)
[INFO][2018-05-24 21:19:45,417][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12204 is the same as ending offset skipping seven 0
[INFO][2018-05-24 21:19:45,418][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:19:45,419][org.apache.spark.executor.Executor]Finished task 0.0 in stage 9.0 (TID 9). 708 bytes result sent to driver
[INFO][2018-05-24 21:19:45,420][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 9.0 (TID 9) in 8 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:19:45,420][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 9.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:19:45,423][org.apache.spark.scheduler.DAGScheduler]ResultStage 9 (foreachPartition at ReceiveKafkaData.scala:66) finished in 0.011 s
[INFO][2018-05-24 21:19:45,424][org.apache.spark.scheduler.DAGScheduler]Job 9 finished: foreachPartition at ReceiveKafkaData.scala:66, took 0.027195 s
[INFO][2018-05-24 21:19:45,424][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527167985000 ms.0 from job set of time 1527167985000 ms
[INFO][2018-05-24 21:19:45,425][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.424 s for time 1527167985000 ms (execution: 0.041 s)
[INFO][2018-05-24 21:19:45,425][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 17 from persistence list
[INFO][2018-05-24 21:19:45,425][org.apache.spark.storage.BlockManager]Removing RDD 17
[INFO][2018-05-24 21:19:45,426][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 16 from persistence list
[INFO][2018-05-24 21:19:45,426][org.apache.spark.storage.BlockManager]Removing RDD 16
[INFO][2018-05-24 21:19:45,426][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 21:19:45,426][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527167975000 ms
[INFO][2018-05-24 21:19:50,067][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527167990000 ms
[INFO][2018-05-24 21:19:50,067][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527167990000 ms.0 from job set of time 1527167990000 ms
[INFO][2018-05-24 21:19:50,076][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:19:50,076][org.apache.spark.scheduler.DAGScheduler]Got job 10 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:19:50,077][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 10 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:19:50,077][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:19:50,077][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:19:50,077][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 10 (MapPartitionsRDD[21] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:19:50,080][org.apache.spark.storage.memory.MemoryStore]Block broadcast_10 stored as values in memory (estimated size 3.3 KB, free 912.2 MB)
[INFO][2018-05-24 21:19:50,085][org.apache.spark.storage.memory.MemoryStore]Block broadcast_10_piece0 stored as bytes in memory (estimated size 1868.0 B, free 912.2 MB)
[INFO][2018-05-24 21:19:50,086][org.apache.spark.storage.BlockManagerInfo]Added broadcast_10_piece0 in memory on 192.168.0.102:51153 (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:19:50,086][org.apache.spark.SparkContext]Created broadcast 10 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:19:50,087][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[21] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:19:50,087][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 10.0 with 1 tasks
[INFO][2018-05-24 21:19:50,088][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 10.0 (TID 10, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:19:50,089][org.apache.spark.executor.Executor]Running task 0.0 in stage 10.0 (TID 10)
[INFO][2018-05-24 21:19:50,091][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12204 is the same as ending offset skipping seven 0
[INFO][2018-05-24 21:19:50,091][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:19:50,093][org.apache.spark.executor.Executor]Finished task 0.0 in stage 10.0 (TID 10). 708 bytes result sent to driver
[INFO][2018-05-24 21:19:50,099][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 10.0 (TID 10) in 11 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:19:50,099][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 10.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:19:50,100][org.apache.spark.scheduler.DAGScheduler]ResultStage 10 (foreachPartition at ReceiveKafkaData.scala:66) finished in 0.013 s
[INFO][2018-05-24 21:19:50,101][org.apache.spark.scheduler.DAGScheduler]Job 10 finished: foreachPartition at ReceiveKafkaData.scala:66, took 0.024666 s
[INFO][2018-05-24 21:19:50,104][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527167990000 ms.0 from job set of time 1527167990000 ms
[INFO][2018-05-24 21:19:50,105][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 19 from persistence list
[INFO][2018-05-24 21:19:50,105][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.104 s for time 1527167990000 ms (execution: 0.037 s)
[INFO][2018-05-24 21:19:50,105][org.apache.spark.storage.BlockManager]Removing RDD 19
[INFO][2018-05-24 21:19:50,105][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 18 from persistence list
[INFO][2018-05-24 21:19:50,108][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 21:19:50,109][org.apache.spark.storage.BlockManager]Removing RDD 18
[INFO][2018-05-24 21:19:50,109][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527167980000 ms
[INFO][2018-05-24 21:19:51,582][org.apache.spark.storage.memory.MemoryStore]Block taskresult_1 stored as bytes in memory (estimated size 10.4 MB, free 901.7 MB)
[INFO][2018-05-24 21:19:51,583][org.apache.spark.storage.BlockManagerInfo]Added taskresult_1 in memory on 192.168.0.102:51188 (size: 10.4 MB, free: 901.9 MB)
[INFO][2018-05-24 21:19:51,583][org.apache.spark.executor.Executor]Finished task 1.0 in stage 0.0 (TID 1). 10855899 bytes result sent via BlockManager)
[INFO][2018-05-24 21:19:51,624][org.apache.spark.network.client.TransportClientFactory]Successfully created connection to /192.168.0.102:51188 after 25 ms (0 ms spent in bootstraps)
[INFO][2018-05-24 21:19:51,951][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 0.0 (TID 1) in 9976 ms on localhost (executor driver) (1/2)
[INFO][2018-05-24 21:19:51,954][org.apache.spark.storage.BlockManagerInfo]Removed taskresult_1 on 192.168.0.102:51188 in memory (size: 10.4 MB, free: 912.3 MB)
[INFO][2018-05-24 21:19:53,606][org.apache.spark.storage.memory.MemoryStore]Block taskresult_0 stored as bytes in memory (estimated size 10.4 MB, free 901.7 MB)
[INFO][2018-05-24 21:19:53,607][org.apache.spark.storage.BlockManagerInfo]Added taskresult_0 in memory on 192.168.0.102:51188 (size: 10.4 MB, free: 901.9 MB)
[INFO][2018-05-24 21:19:53,607][org.apache.spark.executor.Executor]Finished task 0.0 in stage 0.0 (TID 0). 10856349 bytes result sent via BlockManager)
[INFO][2018-05-24 21:19:53,684][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 0.0 (TID 0) in 11738 ms on localhost (executor driver) (2/2)
[INFO][2018-05-24 21:19:53,685][org.apache.spark.storage.BlockManagerInfo]Removed taskresult_0 on 192.168.0.102:51188 in memory (size: 10.4 MB, free: 912.3 MB)
[INFO][2018-05-24 21:19:53,687][org.apache.spark.scheduler.DAGScheduler]ResultStage 0 (collect at SimulationKafkaSendOutData.scala:25) finished in 11.752 s
[INFO][2018-05-24 21:19:53,687][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:19:53,691][org.apache.spark.scheduler.DAGScheduler]Job 0 finished: collect at SimulationKafkaSendOutData.scala:25, took 11.828028 s
[INFO][2018-05-24 21:19:53,799][org.spark_project.jetty.server.AbstractConnector]Stopped Spark@1643d68f{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}
[INFO][2018-05-24 21:19:53,801][org.apache.spark.ui.SparkUI]Stopped Spark web UI at http://192.168.0.102:4041
[INFO][2018-05-24 21:19:53,809][org.apache.spark.MapOutputTrackerMasterEndpoint]MapOutputTrackerMasterEndpoint stopped!
[INFO][2018-05-24 21:19:53,821][org.apache.spark.storage.memory.MemoryStore]MemoryStore cleared
[INFO][2018-05-24 21:19:53,822][org.apache.spark.storage.BlockManager]BlockManager stopped
[INFO][2018-05-24 21:19:53,822][org.apache.spark.storage.BlockManagerMaster]BlockManagerMaster stopped
[INFO][2018-05-24 21:19:53,824][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint]OutputCommitCoordinator stopped!
[INFO][2018-05-24 21:19:53,828][org.apache.spark.SparkContext]Successfully stopped SparkContext
[INFO][2018-05-24 21:19:53,842][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	compression.type = none
	metric.reporters = []
	metadata.max.age.ms = 300000
	metadata.fetch.timeout.ms = 60000
	acks = all
	batch.size = 16384
	reconnect.backoff.ms = 10
	bootstrap.servers = [vm-xaj-bigdata-da-d01:9092, vm-xaj-bigdata-da-d02:9092, vm-xaj-bigdata-da-d03:9092, vm-xaj-bigdata-da-d04:9092, vm-xaj-bigdata-da-d05:9092, vm-xaj-bigdata-da-d06:9092, vm-xaj-bigdata-da-d07:9092]
	receive.buffer.bytes = 32768
	retry.backoff.ms = 100
	buffer.memory = 33554432
	timeout.ms = 30000
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	retries = 0
	max.request.size = 1048576
	block.on.buffer.full = true
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
	metrics.sample.window.ms = 30000
	send.buffer.bytes = 131072
	max.in.flight.requests.per.connection = 5
	metrics.num.samples = 2
	linger.ms = 1
	client.id = 

[INFO][2018-05-24 21:20:00,082][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527167995000 ms
[INFO][2018-05-24 21:20:00,082][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527167995000 ms.0 from job set of time 1527167995000 ms
[INFO][2018-05-24 21:20:00,101][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:20:00,102][org.apache.spark.scheduler.DAGScheduler]Got job 11 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:20:00,102][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 11 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:20:00,102][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:20:00,102][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:20:00,102][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 11 (MapPartitionsRDD[23] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:20:00,104][org.apache.spark.storage.memory.MemoryStore]Block broadcast_11 stored as values in memory (estimated size 3.3 KB, free 912.2 MB)
[INFO][2018-05-24 21:20:00,117][org.apache.spark.storage.memory.MemoryStore]Block broadcast_11_piece0 stored as bytes in memory (estimated size 1868.0 B, free 912.2 MB)
[INFO][2018-05-24 21:20:00,118][org.apache.spark.storage.BlockManagerInfo]Added broadcast_11_piece0 in memory on 192.168.0.102:51153 (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:20:00,118][org.apache.spark.SparkContext]Created broadcast 11 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:20:00,119][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[23] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:20:00,119][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 11.0 with 1 tasks
[INFO][2018-05-24 21:20:00,120][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 11.0 (TID 11, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:20:00,120][org.apache.spark.executor.Executor]Running task 0.0 in stage 11.0 (TID 11)
[INFO][2018-05-24 21:20:00,122][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12204 is the same as ending offset skipping seven 0
[INFO][2018-05-24 21:20:00,122][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:20:00,123][org.apache.spark.executor.Executor]Finished task 0.0 in stage 11.0 (TID 11). 708 bytes result sent to driver
[INFO][2018-05-24 21:20:00,124][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 11.0 (TID 11) in 5 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:20:00,124][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 11.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:20:00,125][org.apache.spark.scheduler.DAGScheduler]ResultStage 11 (foreachPartition at ReceiveKafkaData.scala:66) finished in 0.006 s
[INFO][2018-05-24 21:20:00,126][org.apache.spark.scheduler.DAGScheduler]Job 11 finished: foreachPartition at ReceiveKafkaData.scala:66, took 0.024518 s
[INFO][2018-05-24 21:20:00,126][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527167995000 ms.0 from job set of time 1527167995000 ms
[INFO][2018-05-24 21:20:00,127][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 5.126 s for time 1527167995000 ms (execution: 0.044 s)
[INFO][2018-05-24 21:20:00,130][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_8_piece0 on 192.168.0.102:51153 in memory (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:20:00,132][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_5_piece0 on 192.168.0.102:51153 in memory (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:20:00,136][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_4_piece0 on 192.168.0.102:51153 in memory (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:20:00,140][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_10_piece0 on 192.168.0.102:51153 in memory (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:20:00,143][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_2_piece0 on 192.168.0.102:51153 in memory (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:20:00,146][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_1_piece0 on 192.168.0.102:51153 in memory (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:20:00,152][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_3_piece0 on 192.168.0.102:51153 in memory (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:20:00,157][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_6_piece0 on 192.168.0.102:51153 in memory (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:20:00,159][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_9_piece0 on 192.168.0.102:51153 in memory (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:20:00,163][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_7_piece0 on 192.168.0.102:51153 in memory (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:20:10,159][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527168000000 ms
[INFO][2018-05-24 21:20:10,161][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 21 from persistence list
[INFO][2018-05-24 21:20:10,162][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527168000000 ms.0 from job set of time 1527168000000 ms
[INFO][2018-05-24 21:20:10,163][org.apache.spark.storage.BlockManager]Removing RDD 21
[INFO][2018-05-24 21:20:10,163][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 20 from persistence list
[INFO][2018-05-24 21:20:10,166][org.apache.spark.storage.BlockManager]Removing RDD 20
[INFO][2018-05-24 21:20:10,173][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 21:20:10,173][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527167985000 ms
[INFO][2018-05-24 21:20:10,182][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:20:10,183][org.apache.spark.scheduler.DAGScheduler]Got job 12 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:20:10,184][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 12 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:20:10,184][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:20:10,184][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:20:10,184][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 12 (MapPartitionsRDD[25] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:20:10,192][org.apache.spark.storage.memory.MemoryStore]Block broadcast_12 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 21:20:10,194][org.apache.spark.storage.memory.MemoryStore]Block broadcast_12_piece0 stored as bytes in memory (estimated size 1868.0 B, free 912.3 MB)
[INFO][2018-05-24 21:20:10,195][org.apache.spark.storage.BlockManagerInfo]Added broadcast_12_piece0 in memory on 192.168.0.102:51153 (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:20:10,195][org.apache.spark.SparkContext]Created broadcast 12 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:20:10,199][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[25] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:20:10,199][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 12.0 with 1 tasks
[INFO][2018-05-24 21:20:10,200][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 12.0 (TID 12, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:20:10,201][org.apache.spark.executor.Executor]Running task 0.0 in stage 12.0 (TID 12)
[INFO][2018-05-24 21:20:10,206][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12204 is the same as ending offset skipping seven 0
[INFO][2018-05-24 21:20:10,206][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:20:10,207][org.apache.spark.executor.Executor]Finished task 0.0 in stage 12.0 (TID 12). 708 bytes result sent to driver
[INFO][2018-05-24 21:20:10,208][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 12.0 (TID 12) in 8 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:20:10,208][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 12.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:20:10,209][org.apache.spark.scheduler.DAGScheduler]ResultStage 12 (foreachPartition at ReceiveKafkaData.scala:66) finished in 0.009 s
[INFO][2018-05-24 21:20:10,209][org.apache.spark.scheduler.DAGScheduler]Job 12 finished: foreachPartition at ReceiveKafkaData.scala:66, took 0.026520 s
[INFO][2018-05-24 21:20:10,210][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527168000000 ms.0 from job set of time 1527168000000 ms
[INFO][2018-05-24 21:20:10,210][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 10.210 s for time 1527168000000 ms (execution: 0.049 s)
[INFO][2018-05-24 21:20:10,241][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527168005000 ms
[INFO][2018-05-24 21:20:10,241][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527168005000 ms.0 from job set of time 1527168005000 ms
[INFO][2018-05-24 21:20:10,247][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:20:10,248][org.apache.spark.scheduler.DAGScheduler]Got job 13 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:20:10,249][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 13 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:20:10,249][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:20:10,249][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:20:10,249][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 13 (MapPartitionsRDD[27] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:20:10,251][org.apache.spark.storage.memory.MemoryStore]Block broadcast_13 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 21:20:10,252][org.apache.spark.storage.memory.MemoryStore]Block broadcast_13_piece0 stored as bytes in memory (estimated size 1868.0 B, free 912.3 MB)
[INFO][2018-05-24 21:20:10,253][org.apache.spark.storage.BlockManagerInfo]Added broadcast_13_piece0 in memory on 192.168.0.102:51153 (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:20:10,253][org.apache.spark.SparkContext]Created broadcast 13 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:20:10,254][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[27] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:20:10,254][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 13.0 with 1 tasks
[INFO][2018-05-24 21:20:10,255][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 13.0 (TID 13, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:20:10,255][org.apache.spark.executor.Executor]Running task 0.0 in stage 13.0 (TID 13)
[INFO][2018-05-24 21:20:10,256][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12204 is the same as ending offset skipping seven 0
[INFO][2018-05-24 21:20:10,257][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:20:10,258][org.apache.spark.executor.Executor]Finished task 0.0 in stage 13.0 (TID 13). 665 bytes result sent to driver
[INFO][2018-05-24 21:20:10,258][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 13.0 (TID 13) in 3 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:20:10,258][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 13.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:20:10,259][org.apache.spark.scheduler.DAGScheduler]ResultStage 13 (foreachPartition at ReceiveKafkaData.scala:66) finished in 0.005 s
[INFO][2018-05-24 21:20:10,259][org.apache.spark.scheduler.DAGScheduler]Job 13 finished: foreachPartition at ReceiveKafkaData.scala:66, took 0.011896 s
[INFO][2018-05-24 21:20:10,260][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527168005000 ms.0 from job set of time 1527168005000 ms
[INFO][2018-05-24 21:20:10,260][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 5.260 s for time 1527168005000 ms (execution: 0.019 s)
[INFO][2018-05-24 21:20:10,296][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527168010000 ms
[INFO][2018-05-24 21:20:10,297][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 23 from persistence list
[INFO][2018-05-24 21:20:10,297][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527168010000 ms.0 from job set of time 1527168010000 ms
[INFO][2018-05-24 21:20:10,297][org.apache.spark.storage.BlockManager]Removing RDD 23
[INFO][2018-05-24 21:20:10,297][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 22 from persistence list
[INFO][2018-05-24 21:20:10,298][org.apache.spark.storage.BlockManager]Removing RDD 22
[INFO][2018-05-24 21:20:10,298][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 21:20:10,298][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527167990000 ms
[INFO][2018-05-24 21:20:10,298][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 25 from persistence list
[INFO][2018-05-24 21:20:10,299][org.apache.spark.storage.BlockManager]Removing RDD 25
[INFO][2018-05-24 21:20:10,299][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 24 from persistence list
[INFO][2018-05-24 21:20:10,300][org.apache.spark.storage.BlockManager]Removing RDD 24
[INFO][2018-05-24 21:20:10,300][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 21:20:10,300][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527167995000 ms
[INFO][2018-05-24 21:20:10,302][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:20:10,303][org.apache.spark.scheduler.DAGScheduler]Got job 14 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:20:10,303][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 14 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:20:10,303][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:20:10,303][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:20:10,303][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 14 (MapPartitionsRDD[29] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:20:10,305][org.apache.spark.storage.memory.MemoryStore]Block broadcast_14 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 21:20:10,307][org.apache.spark.storage.memory.MemoryStore]Block broadcast_14_piece0 stored as bytes in memory (estimated size 1868.0 B, free 912.3 MB)
[INFO][2018-05-24 21:20:10,307][org.apache.spark.storage.BlockManagerInfo]Added broadcast_14_piece0 in memory on 192.168.0.102:51153 (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:20:10,308][org.apache.spark.SparkContext]Created broadcast 14 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:20:10,309][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[29] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:20:10,309][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 14.0 with 1 tasks
[INFO][2018-05-24 21:20:10,309][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 14.0 (TID 14, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:20:10,310][org.apache.spark.executor.Executor]Running task 0.0 in stage 14.0 (TID 14)
[INFO][2018-05-24 21:20:10,311][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12204 is the same as ending offset skipping seven 0
[INFO][2018-05-24 21:20:10,311][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:20:10,312][org.apache.spark.executor.Executor]Finished task 0.0 in stage 14.0 (TID 14). 708 bytes result sent to driver
[INFO][2018-05-24 21:20:10,313][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 14.0 (TID 14) in 4 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:20:10,313][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 14.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:20:10,314][org.apache.spark.scheduler.DAGScheduler]ResultStage 14 (foreachPartition at ReceiveKafkaData.scala:66) finished in 0.004 s
[INFO][2018-05-24 21:20:10,314][org.apache.spark.scheduler.DAGScheduler]Job 14 finished: foreachPartition at ReceiveKafkaData.scala:66, took 0.011465 s
[INFO][2018-05-24 21:20:10,314][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527168010000 ms.0 from job set of time 1527168010000 ms
[INFO][2018-05-24 21:20:10,314][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 27 from persistence list
[INFO][2018-05-24 21:20:10,314][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.314 s for time 1527168010000 ms (execution: 0.017 s)
[INFO][2018-05-24 21:20:10,315][org.apache.spark.storage.BlockManager]Removing RDD 27
[INFO][2018-05-24 21:20:10,315][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 26 from persistence list
[INFO][2018-05-24 21:20:10,315][org.apache.spark.storage.BlockManager]Removing RDD 26
[INFO][2018-05-24 21:20:10,315][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 21:20:10,315][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527168000000 ms
[INFO][2018-05-24 21:20:15,055][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527168015000 ms
[INFO][2018-05-24 21:20:15,056][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527168015000 ms.0 from job set of time 1527168015000 ms
[INFO][2018-05-24 21:20:15,063][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:20:15,064][org.apache.spark.scheduler.DAGScheduler]Got job 15 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:20:15,064][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 15 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:20:15,064][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:20:15,065][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:20:15,065][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 15 (MapPartitionsRDD[31] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:20:15,067][org.apache.spark.storage.memory.MemoryStore]Block broadcast_15 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 21:20:15,068][org.apache.spark.storage.memory.MemoryStore]Block broadcast_15_piece0 stored as bytes in memory (estimated size 1868.0 B, free 912.3 MB)
[INFO][2018-05-24 21:20:15,069][org.apache.spark.storage.BlockManagerInfo]Added broadcast_15_piece0 in memory on 192.168.0.102:51153 (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:20:15,069][org.apache.spark.SparkContext]Created broadcast 15 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:20:15,070][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 15 (MapPartitionsRDD[31] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:20:15,070][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 15.0 with 1 tasks
[INFO][2018-05-24 21:20:15,071][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 15.0 (TID 15, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:20:15,072][org.apache.spark.executor.Executor]Running task 0.0 in stage 15.0 (TID 15)
[INFO][2018-05-24 21:20:15,074][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12204 is the same as ending offset skipping seven 0
[INFO][2018-05-24 21:20:15,075][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:20:15,076][org.apache.spark.executor.Executor]Finished task 0.0 in stage 15.0 (TID 15). 708 bytes result sent to driver
[INFO][2018-05-24 21:20:15,077][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 15.0 (TID 15) in 6 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:20:15,077][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 15.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:20:15,077][org.apache.spark.scheduler.DAGScheduler]ResultStage 15 (foreachPartition at ReceiveKafkaData.scala:66) finished in 0.006 s
[INFO][2018-05-24 21:20:15,078][org.apache.spark.scheduler.DAGScheduler]Job 15 finished: foreachPartition at ReceiveKafkaData.scala:66, took 0.014816 s
[INFO][2018-05-24 21:20:15,078][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527168015000 ms.0 from job set of time 1527168015000 ms
[INFO][2018-05-24 21:20:15,079][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 29 from persistence list
[INFO][2018-05-24 21:20:15,079][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.078 s for time 1527168015000 ms (execution: 0.023 s)
[INFO][2018-05-24 21:20:15,079][org.apache.spark.storage.BlockManager]Removing RDD 29
[INFO][2018-05-24 21:20:15,079][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 28 from persistence list
[INFO][2018-05-24 21:20:15,080][org.apache.spark.storage.BlockManager]Removing RDD 28
[INFO][2018-05-24 21:20:15,080][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 21:20:15,080][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527168005000 ms
[INFO][2018-05-24 21:20:20,062][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527168020000 ms
[INFO][2018-05-24 21:20:20,062][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527168020000 ms.0 from job set of time 1527168020000 ms
[INFO][2018-05-24 21:20:20,068][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:20:20,068][org.apache.spark.scheduler.DAGScheduler]Got job 16 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:20:20,068][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 16 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:20:20,068][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:20:20,069][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:20:20,069][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 16 (MapPartitionsRDD[33] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:20:20,071][org.apache.spark.storage.memory.MemoryStore]Block broadcast_16 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 21:20:20,072][org.apache.spark.storage.memory.MemoryStore]Block broadcast_16_piece0 stored as bytes in memory (estimated size 1868.0 B, free 912.3 MB)
[INFO][2018-05-24 21:20:20,073][org.apache.spark.storage.BlockManagerInfo]Added broadcast_16_piece0 in memory on 192.168.0.102:51153 (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:20:20,073][org.apache.spark.SparkContext]Created broadcast 16 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:20:20,074][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 16 (MapPartitionsRDD[33] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:20:20,074][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 16.0 with 1 tasks
[INFO][2018-05-24 21:20:20,074][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 16.0 (TID 16, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:20:20,075][org.apache.spark.executor.Executor]Running task 0.0 in stage 16.0 (TID 16)
[INFO][2018-05-24 21:20:20,076][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12204 is the same as ending offset skipping seven 0
[INFO][2018-05-24 21:20:20,077][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:20:20,077][org.apache.spark.executor.Executor]Finished task 0.0 in stage 16.0 (TID 16). 665 bytes result sent to driver
[INFO][2018-05-24 21:20:20,078][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 16.0 (TID 16) in 4 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:20:20,078][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 16.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:20:20,078][org.apache.spark.scheduler.DAGScheduler]ResultStage 16 (foreachPartition at ReceiveKafkaData.scala:66) finished in 0.004 s
[INFO][2018-05-24 21:20:20,079][org.apache.spark.scheduler.DAGScheduler]Job 16 finished: foreachPartition at ReceiveKafkaData.scala:66, took 0.010684 s
[INFO][2018-05-24 21:20:20,079][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527168020000 ms.0 from job set of time 1527168020000 ms
[INFO][2018-05-24 21:20:20,079][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 31 from persistence list
[INFO][2018-05-24 21:20:20,079][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.079 s for time 1527168020000 ms (execution: 0.017 s)
[INFO][2018-05-24 21:20:20,080][org.apache.spark.storage.BlockManager]Removing RDD 31
[INFO][2018-05-24 21:20:20,080][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 30 from persistence list
[INFO][2018-05-24 21:20:20,080][org.apache.spark.storage.BlockManager]Removing RDD 30
[INFO][2018-05-24 21:20:20,080][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 21:20:20,081][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527168010000 ms
[INFO][2018-05-24 21:20:30,070][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527168025000 ms
[INFO][2018-05-24 21:20:30,070][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527168025000 ms.0 from job set of time 1527168025000 ms
[INFO][2018-05-24 21:20:30,075][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:20:30,076][org.apache.spark.scheduler.DAGScheduler]Got job 17 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:20:30,076][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 17 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:20:30,076][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:20:30,076][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:20:30,076][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 17 (MapPartitionsRDD[35] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:20:30,077][org.apache.spark.storage.memory.MemoryStore]Block broadcast_17 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 21:20:30,079][org.apache.spark.storage.memory.MemoryStore]Block broadcast_17_piece0 stored as bytes in memory (estimated size 1868.0 B, free 912.3 MB)
[INFO][2018-05-24 21:20:30,079][org.apache.spark.storage.BlockManagerInfo]Added broadcast_17_piece0 in memory on 192.168.0.102:51153 (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:20:30,079][org.apache.spark.SparkContext]Created broadcast 17 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:20:30,080][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[35] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:20:30,080][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 17.0 with 1 tasks
[INFO][2018-05-24 21:20:30,080][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 17.0 (TID 17, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:20:30,081][org.apache.spark.executor.Executor]Running task 0.0 in stage 17.0 (TID 17)
[INFO][2018-05-24 21:20:30,085][org.apache.spark.streaming.kafka.KafkaRDD]Computing topic seven, partition 0 offsets 12204 -> 12216
[INFO][2018-05-24 21:20:30,085][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 21:20:30,085][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 21:20:30,086][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 21:20:30,536][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:20:30,537][org.apache.spark.executor.Executor]Finished task 0.0 in stage 17.0 (TID 17). 665 bytes result sent to driver
[INFO][2018-05-24 21:20:30,537][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 17.0 (TID 17) in 457 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:20:30,538][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 17.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:20:30,538][org.apache.spark.scheduler.DAGScheduler]ResultStage 17 (foreachPartition at ReceiveKafkaData.scala:66) finished in 0.458 s
[INFO][2018-05-24 21:20:30,538][org.apache.spark.scheduler.DAGScheduler]Job 17 finished: foreachPartition at ReceiveKafkaData.scala:66, took 0.463037 s
[INFO][2018-05-24 21:20:30,539][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527168025000 ms.0 from job set of time 1527168025000 ms
[INFO][2018-05-24 21:20:30,539][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 5.539 s for time 1527168025000 ms (execution: 0.469 s)
[INFO][2018-05-24 21:20:35,133][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527168030000 ms
[INFO][2018-05-24 21:20:35,134][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 33 from persistence list
[INFO][2018-05-24 21:20:35,134][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527168030000 ms.0 from job set of time 1527168030000 ms
[INFO][2018-05-24 21:20:35,134][org.apache.spark.storage.BlockManager]Removing RDD 33
[INFO][2018-05-24 21:20:35,134][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 32 from persistence list
[INFO][2018-05-24 21:20:35,136][org.apache.spark.storage.BlockManager]Removing RDD 32
[INFO][2018-05-24 21:20:35,136][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 21:20:35,136][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527168015000 ms
[INFO][2018-05-24 21:20:35,140][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:20:35,141][org.apache.spark.scheduler.DAGScheduler]Got job 18 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:20:35,141][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 18 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:20:35,141][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:20:35,141][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:20:35,141][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 18 (MapPartitionsRDD[37] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:20:35,143][org.apache.spark.storage.memory.MemoryStore]Block broadcast_18 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 21:20:35,144][org.apache.spark.storage.memory.MemoryStore]Block broadcast_18_piece0 stored as bytes in memory (estimated size 1868.0 B, free 912.3 MB)
[INFO][2018-05-24 21:20:35,144][org.apache.spark.storage.BlockManagerInfo]Added broadcast_18_piece0 in memory on 192.168.0.102:51153 (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:20:35,144][org.apache.spark.SparkContext]Created broadcast 18 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:20:35,145][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 18 (MapPartitionsRDD[37] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:20:35,145][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 18.0 with 1 tasks
[INFO][2018-05-24 21:20:35,146][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 18.0 (TID 18, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:20:35,146][org.apache.spark.executor.Executor]Running task 0.0 in stage 18.0 (TID 18)
[INFO][2018-05-24 21:20:35,148][org.apache.spark.streaming.kafka.KafkaRDD]Computing topic seven, partition 0 offsets 12216 -> 12226
[INFO][2018-05-24 21:20:35,148][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 21:20:35,148][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 21:20:35,148][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 21:20:35,215][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:20:35,216][org.apache.spark.executor.Executor]Finished task 0.0 in stage 18.0 (TID 18). 708 bytes result sent to driver
[INFO][2018-05-24 21:20:35,217][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 18.0 (TID 18) in 71 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:20:35,217][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 18.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:20:35,218][org.apache.spark.scheduler.DAGScheduler]ResultStage 18 (foreachPartition at ReceiveKafkaData.scala:66) finished in 0.071 s
[INFO][2018-05-24 21:20:35,218][org.apache.spark.scheduler.DAGScheduler]Job 18 finished: foreachPartition at ReceiveKafkaData.scala:66, took 0.077650 s
[INFO][2018-05-24 21:20:35,218][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527168030000 ms.0 from job set of time 1527168030000 ms
[INFO][2018-05-24 21:20:35,219][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 5.218 s for time 1527168030000 ms (execution: 0.084 s)
[INFO][2018-05-24 21:20:45,210][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527168035000 ms
[INFO][2018-05-24 21:20:45,211][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 35 from persistence list
[INFO][2018-05-24 21:20:45,211][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527168035000 ms.0 from job set of time 1527168035000 ms
[INFO][2018-05-24 21:20:45,211][org.apache.spark.storage.BlockManager]Removing RDD 35
[INFO][2018-05-24 21:20:45,211][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 34 from persistence list
[INFO][2018-05-24 21:20:45,212][org.apache.spark.storage.BlockManager]Removing RDD 34
[INFO][2018-05-24 21:20:45,212][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 21:20:45,213][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527168020000 ms
[INFO][2018-05-24 21:20:45,217][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:20:45,218][org.apache.spark.scheduler.DAGScheduler]Got job 19 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:20:45,218][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 19 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:20:45,218][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:20:45,218][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:20:45,218][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 19 (MapPartitionsRDD[39] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:20:45,220][org.apache.spark.storage.memory.MemoryStore]Block broadcast_19 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 21:20:45,221][org.apache.spark.storage.memory.MemoryStore]Block broadcast_19_piece0 stored as bytes in memory (estimated size 1868.0 B, free 912.3 MB)
[INFO][2018-05-24 21:20:45,222][org.apache.spark.storage.BlockManagerInfo]Added broadcast_19_piece0 in memory on 192.168.0.102:51153 (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:20:45,223][org.apache.spark.SparkContext]Created broadcast 19 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:20:45,223][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 19 (MapPartitionsRDD[39] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:20:45,223][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 19.0 with 1 tasks
[INFO][2018-05-24 21:20:45,224][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 19.0 (TID 19, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:20:45,225][org.apache.spark.executor.Executor]Running task 0.0 in stage 19.0 (TID 19)
[INFO][2018-05-24 21:20:45,226][org.apache.spark.streaming.kafka.KafkaRDD]Computing topic seven, partition 0 offsets 12226 -> 12246
[INFO][2018-05-24 21:20:45,226][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 21:20:45,226][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 21:20:45,226][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 21:20:45,305][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:20:45,306][org.apache.spark.executor.Executor]Finished task 0.0 in stage 19.0 (TID 19). 708 bytes result sent to driver
[INFO][2018-05-24 21:20:45,307][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 19.0 (TID 19) in 83 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:20:45,307][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 19.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:20:45,308][org.apache.spark.scheduler.DAGScheduler]ResultStage 19 (foreachPartition at ReceiveKafkaData.scala:66) finished in 0.083 s
[INFO][2018-05-24 21:20:45,308][org.apache.spark.scheduler.DAGScheduler]Job 19 finished: foreachPartition at ReceiveKafkaData.scala:66, took 0.090911 s
[INFO][2018-05-24 21:20:45,308][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527168035000 ms.0 from job set of time 1527168035000 ms
[INFO][2018-05-24 21:20:45,309][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 10.308 s for time 1527168035000 ms (execution: 0.097 s)
[INFO][2018-05-24 21:20:50,271][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527168040000 ms
[INFO][2018-05-24 21:20:50,271][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527168040000 ms.0 from job set of time 1527168040000 ms
[INFO][2018-05-24 21:20:50,277][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:20:50,277][org.apache.spark.scheduler.DAGScheduler]Got job 20 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:20:50,277][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 20 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:20:50,278][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:20:50,278][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:20:50,278][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 20 (MapPartitionsRDD[41] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:20:50,279][org.apache.spark.storage.memory.MemoryStore]Block broadcast_20 stored as values in memory (estimated size 3.3 KB, free 912.2 MB)
[INFO][2018-05-24 21:20:50,281][org.apache.spark.storage.memory.MemoryStore]Block broadcast_20_piece0 stored as bytes in memory (estimated size 1868.0 B, free 912.2 MB)
[INFO][2018-05-24 21:20:50,281][org.apache.spark.storage.BlockManagerInfo]Added broadcast_20_piece0 in memory on 192.168.0.102:51153 (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:20:50,282][org.apache.spark.SparkContext]Created broadcast 20 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:20:50,282][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 20 (MapPartitionsRDD[41] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:20:50,282][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 20.0 with 1 tasks
[INFO][2018-05-24 21:20:50,283][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 20.0 (TID 20, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:20:50,283][org.apache.spark.executor.Executor]Running task 0.0 in stage 20.0 (TID 20)
[INFO][2018-05-24 21:20:50,284][org.apache.spark.streaming.kafka.KafkaRDD]Computing topic seven, partition 0 offsets 12246 -> 12256
[INFO][2018-05-24 21:20:50,284][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 21:20:50,284][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 21:20:50,284][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 21:20:50,354][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:20:50,355][org.apache.spark.executor.Executor]Finished task 0.0 in stage 20.0 (TID 20). 665 bytes result sent to driver
[INFO][2018-05-24 21:20:50,355][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 20.0 (TID 20) in 72 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:20:50,355][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 20.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:20:50,356][org.apache.spark.scheduler.DAGScheduler]ResultStage 20 (foreachPartition at ReceiveKafkaData.scala:66) finished in 0.074 s
[INFO][2018-05-24 21:20:50,357][org.apache.spark.scheduler.DAGScheduler]Job 20 finished: foreachPartition at ReceiveKafkaData.scala:66, took 0.079885 s
[INFO][2018-05-24 21:20:50,358][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527168040000 ms.0 from job set of time 1527168040000 ms
[INFO][2018-05-24 21:20:50,358][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 10.358 s for time 1527168040000 ms (execution: 0.087 s)
[INFO][2018-05-24 21:20:55,332][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527168045000 ms
[INFO][2018-05-24 21:20:55,332][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 37 from persistence list
[INFO][2018-05-24 21:20:55,332][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527168045000 ms.0 from job set of time 1527168045000 ms
[INFO][2018-05-24 21:20:55,332][org.apache.spark.storage.BlockManager]Removing RDD 37
[INFO][2018-05-24 21:20:55,333][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 36 from persistence list
[INFO][2018-05-24 21:20:55,333][org.apache.spark.storage.BlockManager]Removing RDD 36
[INFO][2018-05-24 21:20:55,333][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 21:20:55,334][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527168025000 ms
[INFO][2018-05-24 21:20:55,337][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:20:55,338][org.apache.spark.scheduler.DAGScheduler]Got job 21 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:20:55,338][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 21 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:20:55,338][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:20:55,338][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:20:55,338][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 21 (MapPartitionsRDD[43] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:20:55,340][org.apache.spark.storage.memory.MemoryStore]Block broadcast_21 stored as values in memory (estimated size 3.3 KB, free 912.2 MB)
[INFO][2018-05-24 21:20:55,345][org.apache.spark.storage.memory.MemoryStore]Block broadcast_21_piece0 stored as bytes in memory (estimated size 1868.0 B, free 912.2 MB)
[INFO][2018-05-24 21:20:55,346][org.apache.spark.storage.BlockManagerInfo]Added broadcast_21_piece0 in memory on 192.168.0.102:51153 (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:20:55,346][org.apache.spark.SparkContext]Created broadcast 21 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:20:55,346][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 21 (MapPartitionsRDD[43] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:20:55,347][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 21.0 with 1 tasks
[INFO][2018-05-24 21:20:55,347][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 21.0 (TID 21, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:20:55,348][org.apache.spark.executor.Executor]Running task 0.0 in stage 21.0 (TID 21)
[INFO][2018-05-24 21:20:55,349][org.apache.spark.streaming.kafka.KafkaRDD]Computing topic seven, partition 0 offsets 12256 -> 12266
[INFO][2018-05-24 21:20:55,349][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 21:20:55,350][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 21:20:55,350][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 21:20:55,384][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527168050000 ms
[INFO][2018-05-24 21:20:55,385][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 39 from persistence list
[INFO][2018-05-24 21:20:55,385][org.apache.spark.storage.BlockManager]Removing RDD 39
[INFO][2018-05-24 21:20:55,385][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 38 from persistence list
[INFO][2018-05-24 21:20:55,385][org.apache.spark.storage.BlockManager]Removing RDD 38
[INFO][2018-05-24 21:20:55,385][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 21:20:55,385][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527168030000 ms
[INFO][2018-05-24 21:20:55,415][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:20:55,416][org.apache.spark.executor.Executor]Finished task 0.0 in stage 21.0 (TID 21). 751 bytes result sent to driver
[INFO][2018-05-24 21:20:55,416][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 21.0 (TID 21) in 69 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:20:55,417][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 21.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:20:55,417][org.apache.spark.scheduler.DAGScheduler]ResultStage 21 (foreachPartition at ReceiveKafkaData.scala:66) finished in 0.070 s
[INFO][2018-05-24 21:20:55,418][org.apache.spark.scheduler.DAGScheduler]Job 21 finished: foreachPartition at ReceiveKafkaData.scala:66, took 0.080053 s
[INFO][2018-05-24 21:20:55,418][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527168045000 ms.0 from job set of time 1527168045000 ms
[INFO][2018-05-24 21:20:55,419][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 10.418 s for time 1527168045000 ms (execution: 0.086 s)
[INFO][2018-05-24 21:20:55,419][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527168050000 ms.0 from job set of time 1527168050000 ms
[INFO][2018-05-24 21:20:55,426][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:20:55,427][org.apache.spark.scheduler.DAGScheduler]Got job 22 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:20:55,427][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 22 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:20:55,427][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:20:55,427][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:20:55,428][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 22 (MapPartitionsRDD[45] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:20:55,430][org.apache.spark.storage.memory.MemoryStore]Block broadcast_22 stored as values in memory (estimated size 3.3 KB, free 912.2 MB)
[INFO][2018-05-24 21:20:55,433][org.apache.spark.storage.memory.MemoryStore]Block broadcast_22_piece0 stored as bytes in memory (estimated size 1868.0 B, free 912.2 MB)
[INFO][2018-05-24 21:20:55,433][org.apache.spark.storage.BlockManagerInfo]Added broadcast_22_piece0 in memory on 192.168.0.102:51153 (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:20:55,434][org.apache.spark.SparkContext]Created broadcast 22 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:20:55,434][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 22 (MapPartitionsRDD[45] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:20:55,434][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 22.0 with 1 tasks
[INFO][2018-05-24 21:20:55,435][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 22.0 (TID 22, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:20:55,436][org.apache.spark.executor.Executor]Running task 0.0 in stage 22.0 (TID 22)
[INFO][2018-05-24 21:20:55,436][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527168055000 ms
[INFO][2018-05-24 21:20:55,436][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 41 from persistence list
[INFO][2018-05-24 21:20:55,437][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12266 is the same as ending offset skipping seven 0
[INFO][2018-05-24 21:20:55,438][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:20:55,438][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 40 from persistence list
[INFO][2018-05-24 21:20:55,438][org.apache.spark.executor.Executor]Finished task 0.0 in stage 22.0 (TID 22). 708 bytes result sent to driver
[INFO][2018-05-24 21:20:55,438][org.apache.spark.storage.BlockManager]Removing RDD 41
[INFO][2018-05-24 21:20:55,439][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 21:20:55,439][org.apache.spark.storage.BlockManager]Removing RDD 40
[INFO][2018-05-24 21:20:55,439][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527168035000 ms
[INFO][2018-05-24 21:20:55,440][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 22.0 (TID 22) in 5 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:20:55,440][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 22.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:20:55,440][org.apache.spark.scheduler.DAGScheduler]ResultStage 22 (foreachPartition at ReceiveKafkaData.scala:66) finished in 0.005 s
[INFO][2018-05-24 21:20:55,441][org.apache.spark.scheduler.DAGScheduler]Job 22 finished: foreachPartition at ReceiveKafkaData.scala:66, took 0.014450 s
[INFO][2018-05-24 21:20:55,441][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527168050000 ms.0 from job set of time 1527168050000 ms
[INFO][2018-05-24 21:20:55,441][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 5.441 s for time 1527168050000 ms (execution: 0.022 s)
[INFO][2018-05-24 21:20:55,441][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 43 from persistence list
[INFO][2018-05-24 21:20:55,441][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527168055000 ms.0 from job set of time 1527168055000 ms
[INFO][2018-05-24 21:20:55,442][org.apache.spark.storage.BlockManager]Removing RDD 43
[INFO][2018-05-24 21:20:55,442][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 42 from persistence list
[INFO][2018-05-24 21:20:55,442][org.apache.spark.storage.BlockManager]Removing RDD 42
[INFO][2018-05-24 21:20:55,443][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 21:20:55,443][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527168040000 ms
[INFO][2018-05-24 21:20:55,446][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:20:55,446][org.apache.spark.scheduler.DAGScheduler]Got job 23 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:20:55,446][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 23 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:20:55,446][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:20:55,446][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:20:55,447][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 23 (MapPartitionsRDD[47] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:20:55,448][org.apache.spark.storage.memory.MemoryStore]Block broadcast_23 stored as values in memory (estimated size 3.3 KB, free 912.2 MB)
[INFO][2018-05-24 21:20:55,451][org.apache.spark.storage.memory.MemoryStore]Block broadcast_23_piece0 stored as bytes in memory (estimated size 1867.0 B, free 912.2 MB)
[INFO][2018-05-24 21:20:55,452][org.apache.spark.storage.BlockManagerInfo]Added broadcast_23_piece0 in memory on 192.168.0.102:51153 (size: 1867.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:20:55,452][org.apache.spark.SparkContext]Created broadcast 23 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:20:55,453][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 23 (MapPartitionsRDD[47] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:20:55,453][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 23.0 with 1 tasks
[INFO][2018-05-24 21:20:55,453][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 23.0 (TID 23, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:20:55,454][org.apache.spark.executor.Executor]Running task 0.0 in stage 23.0 (TID 23)
[INFO][2018-05-24 21:20:55,456][org.apache.spark.streaming.kafka.KafkaRDD]Computing topic seven, partition 0 offsets 12266 -> 12267
[INFO][2018-05-24 21:20:55,456][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 21:20:55,456][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 21:20:55,456][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 21:20:55,522][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:20:55,522][org.apache.spark.executor.Executor]Finished task 0.0 in stage 23.0 (TID 23). 708 bytes result sent to driver
[INFO][2018-05-24 21:20:55,523][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 23.0 (TID 23) in 70 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:20:55,523][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 23.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:20:55,524][org.apache.spark.scheduler.DAGScheduler]ResultStage 23 (foreachPartition at ReceiveKafkaData.scala:66) finished in 0.070 s
[INFO][2018-05-24 21:20:55,524][org.apache.spark.scheduler.DAGScheduler]Job 23 finished: foreachPartition at ReceiveKafkaData.scala:66, took 0.078283 s
[INFO][2018-05-24 21:20:55,525][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527168055000 ms.0 from job set of time 1527168055000 ms
[INFO][2018-05-24 21:20:55,525][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.525 s for time 1527168055000 ms (execution: 0.084 s)
[INFO][2018-05-24 21:20:55,525][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 45 from persistence list
[INFO][2018-05-24 21:20:55,526][org.apache.spark.storage.BlockManager]Removing RDD 45
[INFO][2018-05-24 21:20:55,526][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 44 from persistence list
[INFO][2018-05-24 21:20:55,526][org.apache.spark.storage.BlockManager]Removing RDD 44
[INFO][2018-05-24 21:20:55,527][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 21:20:55,527][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527168045000 ms
[INFO][2018-05-24 21:21:00,059][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527168060000 ms
[INFO][2018-05-24 21:21:00,059][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527168060000 ms.0 from job set of time 1527168060000 ms
[INFO][2018-05-24 21:21:00,064][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:21:00,065][org.apache.spark.scheduler.DAGScheduler]Got job 24 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:21:00,065][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 24 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:21:00,065][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:21:00,065][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:21:00,065][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 24 (MapPartitionsRDD[49] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:21:00,066][org.apache.spark.storage.memory.MemoryStore]Block broadcast_24 stored as values in memory (estimated size 3.3 KB, free 912.2 MB)
[INFO][2018-05-24 21:21:00,070][org.apache.spark.storage.memory.MemoryStore]Block broadcast_24_piece0 stored as bytes in memory (estimated size 1868.0 B, free 912.2 MB)
[INFO][2018-05-24 21:21:00,070][org.apache.spark.storage.BlockManagerInfo]Added broadcast_24_piece0 in memory on 192.168.0.102:51153 (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:21:00,071][org.apache.spark.SparkContext]Created broadcast 24 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:21:00,071][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 24 (MapPartitionsRDD[49] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:21:00,071][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 24.0 with 1 tasks
[INFO][2018-05-24 21:21:00,072][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 24.0 (TID 24, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:21:00,072][org.apache.spark.executor.Executor]Running task 0.0 in stage 24.0 (TID 24)
[INFO][2018-05-24 21:21:00,073][org.apache.spark.streaming.kafka.KafkaRDD]Computing topic seven, partition 0 offsets 12267 -> 12276
[INFO][2018-05-24 21:21:00,073][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 21:21:00,073][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 21:21:00,073][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 21:21:00,138][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:21:00,139][org.apache.spark.executor.Executor]Finished task 0.0 in stage 24.0 (TID 24). 665 bytes result sent to driver
[INFO][2018-05-24 21:21:00,140][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 24.0 (TID 24) in 68 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:21:00,140][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 24.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:21:00,140][org.apache.spark.scheduler.DAGScheduler]ResultStage 24 (foreachPartition at ReceiveKafkaData.scala:66) finished in 0.068 s
[INFO][2018-05-24 21:21:00,141][org.apache.spark.scheduler.DAGScheduler]Job 24 finished: foreachPartition at ReceiveKafkaData.scala:66, took 0.076437 s
[INFO][2018-05-24 21:21:00,141][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527168060000 ms.0 from job set of time 1527168060000 ms
[INFO][2018-05-24 21:21:00,141][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.141 s for time 1527168060000 ms (execution: 0.082 s)
[INFO][2018-05-24 21:21:00,141][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 47 from persistence list
[INFO][2018-05-24 21:21:00,142][org.apache.spark.storage.BlockManager]Removing RDD 47
[INFO][2018-05-24 21:21:00,142][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 46 from persistence list
[INFO][2018-05-24 21:21:00,142][org.apache.spark.storage.BlockManager]Removing RDD 46
[INFO][2018-05-24 21:21:00,143][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 21:21:00,143][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527168050000 ms
[INFO][2018-05-24 21:21:05,047][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527168065000 ms
[INFO][2018-05-24 21:21:05,047][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527168065000 ms.0 from job set of time 1527168065000 ms
[INFO][2018-05-24 21:21:05,053][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:21:05,053][org.apache.spark.scheduler.DAGScheduler]Got job 25 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:21:05,054][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 25 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:21:05,054][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:21:05,054][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:21:05,054][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 25 (MapPartitionsRDD[51] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:21:05,056][org.apache.spark.storage.memory.MemoryStore]Block broadcast_25 stored as values in memory (estimated size 3.3 KB, free 912.2 MB)
[INFO][2018-05-24 21:21:05,059][org.apache.spark.storage.memory.MemoryStore]Block broadcast_25_piece0 stored as bytes in memory (estimated size 1868.0 B, free 912.2 MB)
[INFO][2018-05-24 21:21:05,060][org.apache.spark.storage.BlockManagerInfo]Added broadcast_25_piece0 in memory on 192.168.0.102:51153 (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:21:05,060][org.apache.spark.SparkContext]Created broadcast 25 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:21:05,061][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 25 (MapPartitionsRDD[51] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:21:05,061][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 25.0 with 1 tasks
[INFO][2018-05-24 21:21:05,061][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 25.0 (TID 25, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:21:05,062][org.apache.spark.executor.Executor]Running task 0.0 in stage 25.0 (TID 25)
[INFO][2018-05-24 21:21:05,063][org.apache.spark.streaming.kafka.KafkaRDD]Computing topic seven, partition 0 offsets 12276 -> 12286
[INFO][2018-05-24 21:21:05,063][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 21:21:05,063][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 21:21:05,064][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 21:21:05,130][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:21:05,131][org.apache.spark.executor.Executor]Finished task 0.0 in stage 25.0 (TID 25). 665 bytes result sent to driver
[INFO][2018-05-24 21:21:05,132][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 25.0 (TID 25) in 71 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:21:05,132][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 25.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:21:05,133][org.apache.spark.scheduler.DAGScheduler]ResultStage 25 (foreachPartition at ReceiveKafkaData.scala:66) finished in 0.071 s
[INFO][2018-05-24 21:21:05,133][org.apache.spark.scheduler.DAGScheduler]Job 25 finished: foreachPartition at ReceiveKafkaData.scala:66, took 0.080034 s
[INFO][2018-05-24 21:21:05,134][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527168065000 ms.0 from job set of time 1527168065000 ms
[INFO][2018-05-24 21:21:05,134][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 49 from persistence list
[INFO][2018-05-24 21:21:05,134][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.134 s for time 1527168065000 ms (execution: 0.087 s)
[INFO][2018-05-24 21:21:05,134][org.apache.spark.storage.BlockManager]Removing RDD 49
[INFO][2018-05-24 21:21:05,135][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 48 from persistence list
[INFO][2018-05-24 21:21:05,135][org.apache.spark.storage.BlockManager]Removing RDD 48
[INFO][2018-05-24 21:21:05,135][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 21:21:05,135][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527168055000 ms
[INFO][2018-05-24 21:21:14,552][org.apache.spark.util.ShutdownHookManager]Shutdown hook called
[INFO][2018-05-24 21:21:14,554][org.apache.spark.util.ShutdownHookManager]Deleting directory /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/spark-f4b6eff1-3412-40c7-94e7-407d32344d93
[INFO][2018-05-24 21:21:15,064][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527168070000 ms
[INFO][2018-05-24 21:21:15,064][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527168070000 ms.0 from job set of time 1527168070000 ms
[INFO][2018-05-24 21:21:15,070][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:21:15,071][org.apache.spark.scheduler.DAGScheduler]Got job 26 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:21:15,071][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 26 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:21:15,071][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:21:15,071][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:21:15,071][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 26 (MapPartitionsRDD[53] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:21:15,072][org.apache.spark.storage.memory.MemoryStore]Block broadcast_26 stored as values in memory (estimated size 3.3 KB, free 912.2 MB)
[INFO][2018-05-24 21:21:15,076][org.apache.spark.storage.memory.MemoryStore]Block broadcast_26_piece0 stored as bytes in memory (estimated size 1868.0 B, free 912.2 MB)
[INFO][2018-05-24 21:21:15,076][org.apache.spark.storage.BlockManagerInfo]Added broadcast_26_piece0 in memory on 192.168.0.102:51153 (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:21:15,077][org.apache.spark.SparkContext]Created broadcast 26 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:21:15,077][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 26 (MapPartitionsRDD[53] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:21:15,078][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 26.0 with 1 tasks
[INFO][2018-05-24 21:21:15,078][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 26.0 (TID 26, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:21:15,079][org.apache.spark.executor.Executor]Running task 0.0 in stage 26.0 (TID 26)
[INFO][2018-05-24 21:21:15,081][org.apache.spark.streaming.kafka.KafkaRDD]Computing topic seven, partition 0 offsets 12286 -> 12304
[INFO][2018-05-24 21:21:15,081][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 21:21:15,081][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 21:21:15,081][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 21:21:15,120][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527168075000 ms
[INFO][2018-05-24 21:21:15,167][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:21:15,168][org.apache.spark.executor.Executor]Finished task 0.0 in stage 26.0 (TID 26). 708 bytes result sent to driver
[INFO][2018-05-24 21:21:15,169][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 26.0 (TID 26) in 91 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:21:15,169][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 26.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:21:15,170][org.apache.spark.scheduler.DAGScheduler]ResultStage 26 (foreachPartition at ReceiveKafkaData.scala:66) finished in 0.092 s
[INFO][2018-05-24 21:21:15,171][org.apache.spark.scheduler.DAGScheduler]Job 26 finished: foreachPartition at ReceiveKafkaData.scala:66, took 0.100446 s
[INFO][2018-05-24 21:21:15,171][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527168070000 ms.0 from job set of time 1527168070000 ms
[INFO][2018-05-24 21:21:15,173][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 5.171 s for time 1527168070000 ms (execution: 0.107 s)
[INFO][2018-05-24 21:21:15,173][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527168075000 ms.0 from job set of time 1527168075000 ms
[INFO][2018-05-24 21:21:15,173][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 51 from persistence list
[INFO][2018-05-24 21:21:15,174][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 50 from persistence list
[INFO][2018-05-24 21:21:15,175][org.apache.spark.storage.BlockManager]Removing RDD 51
[INFO][2018-05-24 21:21:15,177][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:21:15,178][org.apache.spark.storage.BlockManager]Removing RDD 50
[INFO][2018-05-24 21:21:15,178][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 21:21:15,178][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527168060000 ms
[INFO][2018-05-24 21:21:15,178][org.apache.spark.scheduler.DAGScheduler]Got job 27 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:21:15,178][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 27 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:21:15,178][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:21:15,178][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:21:15,179][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 27 (MapPartitionsRDD[55] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:21:15,180][org.apache.spark.storage.memory.MemoryStore]Block broadcast_27 stored as values in memory (estimated size 3.3 KB, free 912.2 MB)
[INFO][2018-05-24 21:21:15,184][org.apache.spark.storage.memory.MemoryStore]Block broadcast_27_piece0 stored as bytes in memory (estimated size 1868.0 B, free 912.2 MB)
[INFO][2018-05-24 21:21:15,185][org.apache.spark.storage.BlockManagerInfo]Added broadcast_27_piece0 in memory on 192.168.0.102:51153 (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:21:15,185][org.apache.spark.SparkContext]Created broadcast 27 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:21:15,186][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 27 (MapPartitionsRDD[55] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:21:15,186][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 27.0 with 1 tasks
[INFO][2018-05-24 21:21:15,187][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 27.0 (TID 27, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:21:15,188][org.apache.spark.executor.Executor]Running task 0.0 in stage 27.0 (TID 27)
[INFO][2018-05-24 21:21:15,190][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12304 is the same as ending offset skipping seven 0
[INFO][2018-05-24 21:21:15,190][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:21:15,190][org.apache.spark.executor.Executor]Finished task 0.0 in stage 27.0 (TID 27). 665 bytes result sent to driver
[INFO][2018-05-24 21:21:15,191][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 27.0 (TID 27) in 4 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:21:15,191][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 27.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:21:15,192][org.apache.spark.scheduler.DAGScheduler]ResultStage 27 (foreachPartition at ReceiveKafkaData.scala:66) finished in 0.006 s
[INFO][2018-05-24 21:21:15,193][org.apache.spark.scheduler.DAGScheduler]Job 27 finished: foreachPartition at ReceiveKafkaData.scala:66, took 0.015933 s
[INFO][2018-05-24 21:21:15,193][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527168075000 ms.0 from job set of time 1527168075000 ms
[INFO][2018-05-24 21:21:15,194][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 53 from persistence list
[INFO][2018-05-24 21:21:15,195][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.193 s for time 1527168075000 ms (execution: 0.020 s)
[INFO][2018-05-24 21:21:15,195][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 52 from persistence list
[INFO][2018-05-24 21:21:15,195][org.apache.spark.storage.BlockManager]Removing RDD 53
[INFO][2018-05-24 21:21:15,197][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 21:21:15,197][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527168065000 ms
[INFO][2018-05-24 21:21:15,197][org.apache.spark.storage.BlockManager]Removing RDD 52
[INFO][2018-05-24 21:21:25,072][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527168080000 ms
[INFO][2018-05-24 21:21:25,073][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527168080000 ms.0 from job set of time 1527168080000 ms
[INFO][2018-05-24 21:21:25,079][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:21:25,080][org.apache.spark.scheduler.DAGScheduler]Got job 28 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:21:25,080][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 28 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:21:25,080][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:21:25,080][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:21:25,080][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 28 (MapPartitionsRDD[57] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:21:25,082][org.apache.spark.storage.memory.MemoryStore]Block broadcast_28 stored as values in memory (estimated size 3.3 KB, free 912.2 MB)
[INFO][2018-05-24 21:21:25,085][org.apache.spark.storage.memory.MemoryStore]Block broadcast_28_piece0 stored as bytes in memory (estimated size 1868.0 B, free 912.2 MB)
[INFO][2018-05-24 21:21:25,086][org.apache.spark.storage.BlockManagerInfo]Added broadcast_28_piece0 in memory on 192.168.0.102:51153 (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:21:25,086][org.apache.spark.SparkContext]Created broadcast 28 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:21:25,086][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 28 (MapPartitionsRDD[57] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:21:25,086][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 28.0 with 1 tasks
[INFO][2018-05-24 21:21:25,087][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 28.0 (TID 28, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:21:25,087][org.apache.spark.executor.Executor]Running task 0.0 in stage 28.0 (TID 28)
[INFO][2018-05-24 21:21:25,088][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12304 is the same as ending offset skipping seven 0
[INFO][2018-05-24 21:21:25,088][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:21:25,089][org.apache.spark.executor.Executor]Finished task 0.0 in stage 28.0 (TID 28). 665 bytes result sent to driver
[INFO][2018-05-24 21:21:25,089][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 28.0 (TID 28) in 2 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:21:25,089][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 28.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:21:25,090][org.apache.spark.scheduler.DAGScheduler]ResultStage 28 (foreachPartition at ReceiveKafkaData.scala:66) finished in 0.003 s
[INFO][2018-05-24 21:21:25,090][org.apache.spark.scheduler.DAGScheduler]Job 28 finished: foreachPartition at ReceiveKafkaData.scala:66, took 0.011053 s
[INFO][2018-05-24 21:21:25,091][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527168080000 ms.0 from job set of time 1527168080000 ms
[INFO][2018-05-24 21:21:25,091][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 5.091 s for time 1527168080000 ms (execution: 0.018 s)
[INFO][2018-05-24 21:21:30,127][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527168085000 ms
[INFO][2018-05-24 21:21:30,127][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527168085000 ms.0 from job set of time 1527168085000 ms
[INFO][2018-05-24 21:21:30,128][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 55 from persistence list
[INFO][2018-05-24 21:21:30,128][org.apache.spark.storage.BlockManager]Removing RDD 55
[INFO][2018-05-24 21:21:30,128][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 54 from persistence list
[INFO][2018-05-24 21:21:30,129][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 21:21:30,130][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527168070000 ms
[INFO][2018-05-24 21:21:30,130][org.apache.spark.storage.BlockManager]Removing RDD 54
[INFO][2018-05-24 21:21:30,135][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:21:30,136][org.apache.spark.scheduler.DAGScheduler]Got job 29 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:21:30,136][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 29 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:21:30,136][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:21:30,136][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:21:30,136][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 29 (MapPartitionsRDD[59] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:21:30,137][org.apache.spark.storage.memory.MemoryStore]Block broadcast_29 stored as values in memory (estimated size 3.3 KB, free 912.2 MB)
[INFO][2018-05-24 21:21:30,140][org.apache.spark.storage.memory.MemoryStore]Block broadcast_29_piece0 stored as bytes in memory (estimated size 1868.0 B, free 912.2 MB)
[INFO][2018-05-24 21:21:30,140][org.apache.spark.storage.BlockManagerInfo]Added broadcast_29_piece0 in memory on 192.168.0.102:51153 (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:21:30,141][org.apache.spark.SparkContext]Created broadcast 29 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:21:30,141][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 29 (MapPartitionsRDD[59] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:21:30,141][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 29.0 with 1 tasks
[INFO][2018-05-24 21:21:30,142][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 29.0 (TID 29, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:21:30,142][org.apache.spark.executor.Executor]Running task 0.0 in stage 29.0 (TID 29)
[INFO][2018-05-24 21:21:30,143][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12304 is the same as ending offset skipping seven 0
[INFO][2018-05-24 21:21:30,144][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:21:30,144][org.apache.spark.executor.Executor]Finished task 0.0 in stage 29.0 (TID 29). 665 bytes result sent to driver
[INFO][2018-05-24 21:21:30,146][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 29.0 (TID 29) in 4 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:21:30,146][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 29.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:21:30,146][org.apache.spark.scheduler.DAGScheduler]ResultStage 29 (foreachPartition at ReceiveKafkaData.scala:66) finished in 0.005 s
[INFO][2018-05-24 21:21:30,146][org.apache.spark.scheduler.DAGScheduler]Job 29 finished: foreachPartition at ReceiveKafkaData.scala:66, took 0.011248 s
[INFO][2018-05-24 21:21:30,147][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527168085000 ms.0 from job set of time 1527168085000 ms
[INFO][2018-05-24 21:21:30,147][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 5.147 s for time 1527168085000 ms (execution: 0.020 s)
[INFO][2018-05-24 21:21:35,191][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527168090000 ms
[INFO][2018-05-24 21:21:35,191][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 57 from persistence list
[INFO][2018-05-24 21:21:35,191][org.apache.spark.storage.BlockManager]Removing RDD 57
[INFO][2018-05-24 21:21:35,191][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 56 from persistence list
[INFO][2018-05-24 21:21:35,192][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527168090000 ms.0 from job set of time 1527168090000 ms
[INFO][2018-05-24 21:21:35,192][org.apache.spark.storage.BlockManager]Removing RDD 56
[INFO][2018-05-24 21:21:35,192][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 21:21:35,192][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527168075000 ms
[INFO][2018-05-24 21:21:35,198][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:21:35,198][org.apache.spark.scheduler.DAGScheduler]Got job 30 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:21:35,198][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 30 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:21:35,198][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:21:35,199][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:21:35,199][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 30 (MapPartitionsRDD[61] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:21:35,200][org.apache.spark.storage.memory.MemoryStore]Block broadcast_30 stored as values in memory (estimated size 3.3 KB, free 912.2 MB)
[INFO][2018-05-24 21:21:35,203][org.apache.spark.storage.memory.MemoryStore]Block broadcast_30_piece0 stored as bytes in memory (estimated size 1868.0 B, free 912.2 MB)
[INFO][2018-05-24 21:21:35,204][org.apache.spark.storage.BlockManagerInfo]Added broadcast_30_piece0 in memory on 192.168.0.102:51153 (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:21:35,204][org.apache.spark.SparkContext]Created broadcast 30 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:21:35,205][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 30 (MapPartitionsRDD[61] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:21:35,205][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 30.0 with 1 tasks
[INFO][2018-05-24 21:21:35,205][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 30.0 (TID 30, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:21:35,206][org.apache.spark.executor.Executor]Running task 0.0 in stage 30.0 (TID 30)
[INFO][2018-05-24 21:21:35,207][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12304 is the same as ending offset skipping seven 0
[INFO][2018-05-24 21:21:35,208][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:21:35,208][org.apache.spark.executor.Executor]Finished task 0.0 in stage 30.0 (TID 30). 665 bytes result sent to driver
[INFO][2018-05-24 21:21:35,209][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 30.0 (TID 30) in 4 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:21:35,209][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 30.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:21:35,209][org.apache.spark.scheduler.DAGScheduler]ResultStage 30 (foreachPartition at ReceiveKafkaData.scala:66) finished in 0.004 s
[INFO][2018-05-24 21:21:35,209][org.apache.spark.scheduler.DAGScheduler]Job 30 finished: foreachPartition at ReceiveKafkaData.scala:66, took 0.011461 s
[INFO][2018-05-24 21:21:35,210][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527168090000 ms.0 from job set of time 1527168090000 ms
[INFO][2018-05-24 21:21:35,210][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 5.210 s for time 1527168090000 ms (execution: 0.019 s)
[INFO][2018-05-24 21:21:40,263][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527168095000 ms
[INFO][2018-05-24 21:21:40,264][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 59 from persistence list
[INFO][2018-05-24 21:21:40,265][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527168095000 ms.0 from job set of time 1527168095000 ms
[INFO][2018-05-24 21:21:40,269][org.apache.spark.storage.BlockManager]Removing RDD 59
[INFO][2018-05-24 21:21:40,270][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 58 from persistence list
[INFO][2018-05-24 21:21:40,271][org.apache.spark.storage.BlockManager]Removing RDD 58
[INFO][2018-05-24 21:21:40,271][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 21:21:40,271][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527168080000 ms
[INFO][2018-05-24 21:21:40,273][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:21:40,274][org.apache.spark.scheduler.DAGScheduler]Got job 31 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:21:40,274][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 31 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:21:40,274][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:21:40,274][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:21:40,274][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 31 (MapPartitionsRDD[63] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:21:40,276][org.apache.spark.storage.memory.MemoryStore]Block broadcast_31 stored as values in memory (estimated size 3.3 KB, free 912.2 MB)
[INFO][2018-05-24 21:21:40,279][org.apache.spark.storage.memory.MemoryStore]Block broadcast_31_piece0 stored as bytes in memory (estimated size 1868.0 B, free 912.2 MB)
[INFO][2018-05-24 21:21:40,279][org.apache.spark.storage.BlockManagerInfo]Added broadcast_31_piece0 in memory on 192.168.0.102:51153 (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:21:40,280][org.apache.spark.SparkContext]Created broadcast 31 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:21:40,280][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 31 (MapPartitionsRDD[63] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:21:40,280][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 31.0 with 1 tasks
[INFO][2018-05-24 21:21:40,281][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 31.0 (TID 31, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:21:40,281][org.apache.spark.executor.Executor]Running task 0.0 in stage 31.0 (TID 31)
[INFO][2018-05-24 21:21:40,282][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12304 is the same as ending offset skipping seven 0
[INFO][2018-05-24 21:21:40,282][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:21:40,283][org.apache.spark.executor.Executor]Finished task 0.0 in stage 31.0 (TID 31). 665 bytes result sent to driver
[INFO][2018-05-24 21:21:40,283][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 31.0 (TID 31) in 3 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:21:40,283][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 31.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:21:40,283][org.apache.spark.scheduler.DAGScheduler]ResultStage 31 (foreachPartition at ReceiveKafkaData.scala:66) finished in 0.003 s
[INFO][2018-05-24 21:21:40,284][org.apache.spark.scheduler.DAGScheduler]Job 31 finished: foreachPartition at ReceiveKafkaData.scala:66, took 0.010110 s
[INFO][2018-05-24 21:21:40,284][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527168095000 ms.0 from job set of time 1527168095000 ms
[INFO][2018-05-24 21:21:40,284][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 5.284 s for time 1527168095000 ms (execution: 0.019 s)
[INFO][2018-05-24 21:21:40,323][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527168100000 ms
[INFO][2018-05-24 21:21:40,324][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 61 from persistence list
[INFO][2018-05-24 21:21:40,326][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 60 from persistence list
[INFO][2018-05-24 21:21:40,327][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527168100000 ms.0 from job set of time 1527168100000 ms
[INFO][2018-05-24 21:21:40,328][org.apache.spark.storage.BlockManager]Removing RDD 61
[INFO][2018-05-24 21:21:40,329][org.apache.spark.storage.BlockManager]Removing RDD 60
[INFO][2018-05-24 21:21:40,330][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 21:21:40,330][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527168085000 ms
[INFO][2018-05-24 21:21:40,333][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:21:40,333][org.apache.spark.scheduler.DAGScheduler]Got job 32 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:21:40,333][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 32 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:21:40,333][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:21:40,333][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:21:40,334][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 32 (MapPartitionsRDD[65] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:21:40,335][org.apache.spark.storage.memory.MemoryStore]Block broadcast_32 stored as values in memory (estimated size 3.3 KB, free 912.2 MB)
[INFO][2018-05-24 21:21:40,339][org.apache.spark.storage.memory.MemoryStore]Block broadcast_32_piece0 stored as bytes in memory (estimated size 1868.0 B, free 912.2 MB)
[INFO][2018-05-24 21:21:40,339][org.apache.spark.storage.BlockManagerInfo]Added broadcast_32_piece0 in memory on 192.168.0.102:51153 (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:21:40,340][org.apache.spark.SparkContext]Created broadcast 32 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:21:40,340][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 32 (MapPartitionsRDD[65] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:21:40,340][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 32.0 with 1 tasks
[INFO][2018-05-24 21:21:40,341][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 32.0 (TID 32, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:21:40,341][org.apache.spark.executor.Executor]Running task 0.0 in stage 32.0 (TID 32)
[INFO][2018-05-24 21:21:40,345][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12304 is the same as ending offset skipping seven 0
[INFO][2018-05-24 21:21:40,345][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:21:40,346][org.apache.spark.executor.Executor]Finished task 0.0 in stage 32.0 (TID 32). 708 bytes result sent to driver
[INFO][2018-05-24 21:21:40,347][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 32.0 (TID 32) in 6 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:21:40,347][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 32.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:21:40,347][org.apache.spark.scheduler.DAGScheduler]ResultStage 32 (foreachPartition at ReceiveKafkaData.scala:66) finished in 0.007 s
[INFO][2018-05-24 21:21:40,348][org.apache.spark.scheduler.DAGScheduler]Job 32 finished: foreachPartition at ReceiveKafkaData.scala:66, took 0.014493 s
[INFO][2018-05-24 21:21:40,348][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527168100000 ms.0 from job set of time 1527168100000 ms
[INFO][2018-05-24 21:21:40,348][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.348 s for time 1527168100000 ms (execution: 0.022 s)
[INFO][2018-05-24 21:21:40,348][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 63 from persistence list
[INFO][2018-05-24 21:21:40,348][org.apache.spark.storage.BlockManager]Removing RDD 63
[INFO][2018-05-24 21:21:40,349][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 62 from persistence list
[INFO][2018-05-24 21:21:40,349][org.apache.spark.storage.BlockManager]Removing RDD 62
[INFO][2018-05-24 21:21:40,349][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 21:21:40,349][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527168090000 ms
[INFO][2018-05-24 21:21:42,864][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_29_piece0 on 192.168.0.102:51153 in memory (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:21:42,867][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_24_piece0 on 192.168.0.102:51153 in memory (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:21:42,868][org.apache.spark.streaming.StreamingContext]Invoking stop(stopGracefully=false) from shutdown hook
[INFO][2018-05-24 21:21:42,869][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_13_piece0 on 192.168.0.102:51153 in memory (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:21:42,870][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_26_piece0 on 192.168.0.102:51153 in memory (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:21:42,871][org.apache.spark.streaming.scheduler.ReceiverTracker]ReceiverTracker stopped
[INFO][2018-05-24 21:21:42,872][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_25_piece0 on 192.168.0.102:51153 in memory (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:21:42,872][org.apache.spark.streaming.scheduler.JobGenerator]Stopping JobGenerator immediately
[INFO][2018-05-24 21:21:42,873][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_14_piece0 on 192.168.0.102:51153 in memory (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:21:42,873][org.apache.spark.streaming.util.RecurringTimer]Stopped timer for JobGenerator after time 1527168100000
[INFO][2018-05-24 21:21:42,874][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_20_piece0 on 192.168.0.102:51153 in memory (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:21:42,875][org.apache.spark.streaming.scheduler.JobGenerator]Stopped JobGenerator
[INFO][2018-05-24 21:21:42,875][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_22_piece0 on 192.168.0.102:51153 in memory (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:21:42,877][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_19_piece0 on 192.168.0.102:51153 in memory (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:21:42,877][org.apache.spark.streaming.scheduler.JobScheduler]Stopped JobScheduler
[INFO][2018-05-24 21:21:42,878][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_12_piece0 on 192.168.0.102:51153 in memory (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:21:42,880][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_18_piece0 on 192.168.0.102:51153 in memory (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:21:42,881][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_27_piece0 on 192.168.0.102:51153 in memory (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:21:42,882][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_23_piece0 on 192.168.0.102:51153 in memory (size: 1867.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:21:42,883][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_21_piece0 on 192.168.0.102:51153 in memory (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:21:42,885][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_15_piece0 on 192.168.0.102:51153 in memory (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:21:42,886][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_11_piece0 on 192.168.0.102:51153 in memory (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:21:42,886][org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@57a48985{/streaming,null,UNAVAILABLE,@Spark}
[INFO][2018-05-24 21:21:42,887][org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@3f672204{/streaming/batch,null,UNAVAILABLE,@Spark}
[INFO][2018-05-24 21:21:42,887][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_30_piece0 on 192.168.0.102:51153 in memory (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:21:42,889][org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@2d000e80{/static/streaming,null,UNAVAILABLE,@Spark}
[INFO][2018-05-24 21:21:42,889][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_17_piece0 on 192.168.0.102:51153 in memory (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:21:42,890][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_32_piece0 on 192.168.0.102:51153 in memory (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:21:42,890][org.apache.spark.streaming.StreamingContext]StreamingContext stopped successfully
[INFO][2018-05-24 21:21:42,890][org.apache.spark.SparkContext]Invoking stop() from shutdown hook
[INFO][2018-05-24 21:21:42,891][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_28_piece0 on 192.168.0.102:51153 in memory (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:21:42,892][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_31_piece0 on 192.168.0.102:51153 in memory (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:21:42,893][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_16_piece0 on 192.168.0.102:51153 in memory (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:21:42,898][org.spark_project.jetty.server.AbstractConnector]Stopped Spark@25177d29{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-05-24 21:21:42,901][org.apache.spark.ui.SparkUI]Stopped Spark web UI at http://192.168.0.102:4040
[INFO][2018-05-24 21:21:42,909][org.apache.spark.MapOutputTrackerMasterEndpoint]MapOutputTrackerMasterEndpoint stopped!
[INFO][2018-05-24 21:21:42,929][org.apache.spark.storage.memory.MemoryStore]MemoryStore cleared
[INFO][2018-05-24 21:21:42,929][org.apache.spark.storage.BlockManager]BlockManager stopped
[INFO][2018-05-24 21:21:42,930][org.apache.spark.storage.BlockManagerMaster]BlockManagerMaster stopped
[INFO][2018-05-24 21:21:42,932][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint]OutputCommitCoordinator stopped!
[INFO][2018-05-24 21:21:42,934][org.apache.spark.SparkContext]Successfully stopped SparkContext
[INFO][2018-05-24 21:21:42,934][org.apache.spark.util.ShutdownHookManager]Shutdown hook called
[INFO][2018-05-24 21:21:42,935][org.apache.spark.util.ShutdownHookManager]Deleting directory /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/spark-afc1956a-fafa-47f4-9dfb-e8e2c33a0d65
[INFO][2018-05-25 11:14:02,261][org.apache.spark.SparkContext]Running Spark version 2.2.0
[INFO][2018-05-25 11:14:02,717][org.apache.spark.util.ShutdownHookManager]Shutdown hook called
[INFO][2018-05-25 11:17:07,910][org.apache.spark.SparkContext]Running Spark version 2.2.0
[INFO][2018-05-25 11:17:08,997][org.apache.spark.SparkContext]Submitted application: ReceiveKafkaData$
[INFO][2018-05-25 11:17:09,031][org.apache.spark.SecurityManager]Changing view acls to: seven
[INFO][2018-05-25 11:17:09,032][org.apache.spark.SecurityManager]Changing modify acls to: seven
[INFO][2018-05-25 11:17:09,032][org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO][2018-05-25 11:17:09,033][org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO][2018-05-25 11:17:09,034][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(seven); groups with view permissions: Set(); users  with modify permissions: Set(seven); groups with modify permissions: Set()
[INFO][2018-05-25 11:17:09,436][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 53388.
[INFO][2018-05-25 11:17:09,474][org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO][2018-05-25 11:17:09,506][org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO][2018-05-25 11:17:09,511][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2018-05-25 11:17:09,512][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
[INFO][2018-05-25 11:17:09,529][org.apache.spark.storage.DiskBlockManager]Created local directory at /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/blockmgr-bc78c737-ee9c-4c37-bfc7-da28dda14459
[INFO][2018-05-25 11:17:09,550][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 912.3 MB
[INFO][2018-05-25 11:17:09,658][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO][2018-05-25 11:17:09,796][org.spark_project.jetty.util.log]Logging initialized @2867ms
[INFO][2018-05-25 11:17:09,875][org.spark_project.jetty.server.Server]jetty-9.3.z-SNAPSHOT
[INFO][2018-05-25 11:17:09,894][org.spark_project.jetty.server.Server]Started @2966ms
[INFO][2018-05-25 11:17:09,930][org.spark_project.jetty.server.AbstractConnector]Started ServerConnector@24faea88{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-05-25 11:17:09,930][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4040.
[INFO][2018-05-25 11:17:09,964][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@649725e3{/jobs,null,AVAILABLE,@Spark}
[INFO][2018-05-25 11:17:09,965][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1a1da881{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2018-05-25 11:17:09,965][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7fd4acee{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2018-05-25 11:17:09,967][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2f058b8a{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2018-05-25 11:17:09,967][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3f2ef586{/stages,null,AVAILABLE,@Spark}
[INFO][2018-05-25 11:17:09,968][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@76c7beb3{/stages/json,null,AVAILABLE,@Spark}
[INFO][2018-05-25 11:17:09,968][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2cf92cc7{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2018-05-25 11:17:09,970][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@611df6e3{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-25 11:17:09,971][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6273c5a4{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2018-05-25 11:17:09,971][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@53e211ee{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2018-05-25 11:17:09,972][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3d8bbcdc{/storage,null,AVAILABLE,@Spark}
[INFO][2018-05-25 11:17:09,973][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@117e0fe5{/storage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-25 11:17:09,973][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@78aea4b9{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2018-05-25 11:17:09,974][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4b85880b{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2018-05-25 11:17:09,975][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4215838f{/environment,null,AVAILABLE,@Spark}
[INFO][2018-05-25 11:17:09,976][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2289aca5{/environment/json,null,AVAILABLE,@Spark}
[INFO][2018-05-25 11:17:09,976][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@184497d1{/executors,null,AVAILABLE,@Spark}
[INFO][2018-05-25 11:17:09,977][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6ffab045{/executors/json,null,AVAILABLE,@Spark}
[INFO][2018-05-25 11:17:09,978][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3e2943ab{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2018-05-25 11:17:09,979][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4a9f80d3{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2018-05-25 11:17:09,993][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@41fe9859{/static,null,AVAILABLE,@Spark}
[INFO][2018-05-25 11:17:09,994][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4602c2a9{/,null,AVAILABLE,@Spark}
[INFO][2018-05-25 11:17:09,996][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3e2822{/api,null,AVAILABLE,@Spark}
[INFO][2018-05-25 11:17:09,996][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5fcacc0{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-25 11:17:09,997][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6d1d4d7{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-25 11:17:10,001][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://10.194.32.157:4040
[INFO][2018-05-25 11:17:10,109][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
[INFO][2018-05-25 11:17:10,142][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 53389.
[INFO][2018-05-25 11:17:10,146][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 10.194.32.157:53389
[INFO][2018-05-25 11:17:10,149][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2018-05-25 11:17:10,151][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 10.194.32.157, 53389, None)
[INFO][2018-05-25 11:17:10,155][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 10.194.32.157:53389 with 912.3 MB RAM, BlockManagerId(driver, 10.194.32.157, 53389, None)
[INFO][2018-05-25 11:17:10,158][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 10.194.32.157, 53389, None)
[INFO][2018-05-25 11:17:10,159][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 10.194.32.157, 53389, None)
[INFO][2018-05-25 11:17:10,466][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@18ca3c62{/metrics/json,null,AVAILABLE,@Spark}
[WARN][2018-05-25 11:17:10,854][org.apache.spark.streaming.kafka010.KafkaUtils]overriding enable.auto.commit to false for executor
[WARN][2018-05-25 11:17:10,854][org.apache.spark.streaming.kafka010.KafkaUtils]overriding auto.offset.reset to none for executor
[WARN][2018-05-25 11:17:10,855][org.apache.spark.streaming.kafka010.KafkaUtils]overriding executor group.id to spark-executor-use_a_separate_group_id_for_each_stream
[WARN][2018-05-25 11:17:10,856][org.apache.spark.streaming.kafka010.KafkaUtils]overriding receive.buffer.bytes to 65536 see KAFKA-3135
[INFO][2018-05-25 11:17:10,957][org.apache.spark.streaming.kafka010.DirectKafkaInputDStream]Slide time = 5000 ms
[INFO][2018-05-25 11:17:10,958][org.apache.spark.streaming.kafka010.DirectKafkaInputDStream]Storage level = Serialized 1x Replicated
[INFO][2018-05-25 11:17:10,959][org.apache.spark.streaming.kafka010.DirectKafkaInputDStream]Checkpoint interval = null
[INFO][2018-05-25 11:17:10,959][org.apache.spark.streaming.kafka010.DirectKafkaInputDStream]Remember interval = 5000 ms
[INFO][2018-05-25 11:17:10,960][org.apache.spark.streaming.kafka010.DirectKafkaInputDStream]Initialized and validated org.apache.spark.streaming.kafka010.DirectKafkaInputDStream@50b1dedb
[INFO][2018-05-25 11:17:10,960][org.apache.spark.streaming.dstream.MappedDStream]Slide time = 5000 ms
[INFO][2018-05-25 11:17:10,960][org.apache.spark.streaming.dstream.MappedDStream]Storage level = Serialized 1x Replicated
[INFO][2018-05-25 11:17:10,960][org.apache.spark.streaming.dstream.MappedDStream]Checkpoint interval = null
[INFO][2018-05-25 11:17:10,961][org.apache.spark.streaming.dstream.MappedDStream]Remember interval = 5000 ms
[INFO][2018-05-25 11:17:10,961][org.apache.spark.streaming.dstream.MappedDStream]Initialized and validated org.apache.spark.streaming.dstream.MappedDStream@5af0fc3a
[INFO][2018-05-25 11:17:10,961][org.apache.spark.streaming.dstream.ForEachDStream]Slide time = 5000 ms
[INFO][2018-05-25 11:17:10,961][org.apache.spark.streaming.dstream.ForEachDStream]Storage level = Serialized 1x Replicated
[INFO][2018-05-25 11:17:10,961][org.apache.spark.streaming.dstream.ForEachDStream]Checkpoint interval = null
[INFO][2018-05-25 11:17:10,961][org.apache.spark.streaming.dstream.ForEachDStream]Remember interval = 5000 ms
[INFO][2018-05-25 11:17:10,961][org.apache.spark.streaming.dstream.ForEachDStream]Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@323fc5be
[INFO][2018-05-25 11:17:11,040][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	metric.reporters = []
	metadata.max.age.ms = 300000
	partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor]
	reconnect.backoff.ms = 50
	sasl.kerberos.ticket.renew.window.factor = 0.8
	max.partition.fetch.bytes = 1048576
	bootstrap.servers = [vm-xaj-bigdata-da-d01:9092, vm-xaj-bigdata-da-d02:9092, vm-xaj-bigdata-da-d03:9092, vm-xaj-bigdata-da-d04:9092, vm-xaj-bigdata-da-d05:9092, vm-xaj-bigdata-da-d06:9092, vm-xaj-bigdata-da-d07:9092]
	ssl.keystore.type = JKS
	enable.auto.commit = false
	sasl.mechanism = GSSAPI
	interceptor.classes = null
	exclude.internal.topics = true
	ssl.truststore.password = null
	client.id = 
	ssl.endpoint.identification.algorithm = null
	max.poll.records = 2147483647
	check.crcs = true
	request.timeout.ms = 40000
	heartbeat.interval.ms = 3000
	auto.commit.interval.ms = 5000
	receive.buffer.bytes = 65536
	ssl.truststore.type = JKS
	ssl.truststore.location = null
	ssl.keystore.password = null
	fetch.min.bytes = 1
	send.buffer.bytes = 131072
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	group.id = use_a_separate_group_id_for_each_stream
	retry.backoff.ms = 100
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	ssl.trustmanager.algorithm = PKIX
	ssl.key.password = null
	fetch.max.wait.ms = 500
	sasl.kerberos.min.time.before.relogin = 60000
	connections.max.idle.ms = 540000
	session.timeout.ms = 30000
	metrics.num.samples = 2
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	ssl.protocol = TLS
	ssl.provider = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.keystore.location = null
	ssl.cipher.suites = null
	security.protocol = PLAINTEXT
	ssl.keymanager.algorithm = SunX509
	metrics.sample.window.ms = 30000
	auto.offset.reset = latest

[INFO][2018-05-25 11:17:15,748][org.apache.spark.SparkContext]Invoking stop() from shutdown hook
[INFO][2018-05-25 11:17:15,760][org.spark_project.jetty.server.AbstractConnector]Stopped Spark@24faea88{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-05-25 11:17:15,763][org.apache.spark.ui.SparkUI]Stopped Spark web UI at http://10.194.32.157:4040
[INFO][2018-05-25 11:17:15,782][org.apache.spark.MapOutputTrackerMasterEndpoint]MapOutputTrackerMasterEndpoint stopped!
[INFO][2018-05-25 11:17:15,809][org.apache.spark.storage.memory.MemoryStore]MemoryStore cleared
[INFO][2018-05-25 11:17:15,809][org.apache.spark.storage.BlockManager]BlockManager stopped
[INFO][2018-05-25 11:17:15,820][org.apache.spark.storage.BlockManagerMaster]BlockManagerMaster stopped
[INFO][2018-05-25 11:17:15,832][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint]OutputCommitCoordinator stopped!
[INFO][2018-05-25 11:17:15,834][org.apache.spark.SparkContext]Successfully stopped SparkContext
[INFO][2018-05-25 11:17:15,835][org.apache.spark.util.ShutdownHookManager]Shutdown hook called
[INFO][2018-05-25 11:17:15,838][org.apache.spark.util.ShutdownHookManager]Deleting directory /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/spark-b605a54e-0de9-405f-a49b-c655860612be
[INFO][2018-05-25 11:18:09,031][org.apache.spark.SparkContext]Running Spark version 2.2.0
[INFO][2018-05-25 11:18:09,928][org.apache.spark.SparkContext]Submitted application: ReceiveKafkaData$
[INFO][2018-05-25 11:18:09,966][org.apache.spark.SecurityManager]Changing view acls to: seven
[INFO][2018-05-25 11:18:09,967][org.apache.spark.SecurityManager]Changing modify acls to: seven
[INFO][2018-05-25 11:18:09,967][org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO][2018-05-25 11:18:09,968][org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO][2018-05-25 11:18:09,969][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(seven); groups with view permissions: Set(); users  with modify permissions: Set(seven); groups with modify permissions: Set()
[INFO][2018-05-25 11:18:10,246][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 53403.
[INFO][2018-05-25 11:18:10,272][org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO][2018-05-25 11:18:10,290][org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO][2018-05-25 11:18:10,293][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2018-05-25 11:18:10,294][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
[INFO][2018-05-25 11:18:10,304][org.apache.spark.storage.DiskBlockManager]Created local directory at /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/blockmgr-85e7aa67-3c49-43e3-b86c-cb48ed3d6b02
[INFO][2018-05-25 11:18:10,318][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 912.3 MB
[INFO][2018-05-25 11:18:10,413][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO][2018-05-25 11:18:10,522][org.spark_project.jetty.util.log]Logging initialized @2740ms
[INFO][2018-05-25 11:18:10,613][org.spark_project.jetty.server.Server]jetty-9.3.z-SNAPSHOT
[INFO][2018-05-25 11:18:10,643][org.spark_project.jetty.server.Server]Started @2865ms
[INFO][2018-05-25 11:18:10,676][org.spark_project.jetty.server.AbstractConnector]Started ServerConnector@349fcd3b{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-05-25 11:18:10,677][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4040.
[INFO][2018-05-25 11:18:10,716][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@649725e3{/jobs,null,AVAILABLE,@Spark}
[INFO][2018-05-25 11:18:10,717][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1a1da881{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2018-05-25 11:18:10,720][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7fd4acee{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2018-05-25 11:18:10,722][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2f058b8a{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2018-05-25 11:18:10,723][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3f2ef586{/stages,null,AVAILABLE,@Spark}
[INFO][2018-05-25 11:18:10,724][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@76c7beb3{/stages/json,null,AVAILABLE,@Spark}
[INFO][2018-05-25 11:18:10,727][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2cf92cc7{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2018-05-25 11:18:10,729][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@611df6e3{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-25 11:18:10,740][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6273c5a4{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2018-05-25 11:18:10,747][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@53e211ee{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2018-05-25 11:18:10,751][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3d8bbcdc{/storage,null,AVAILABLE,@Spark}
[INFO][2018-05-25 11:18:10,752][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@117e0fe5{/storage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-25 11:18:10,753][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@78aea4b9{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2018-05-25 11:18:10,755][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4b85880b{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2018-05-25 11:18:10,756][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4215838f{/environment,null,AVAILABLE,@Spark}
[INFO][2018-05-25 11:18:10,757][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2289aca5{/environment/json,null,AVAILABLE,@Spark}
[INFO][2018-05-25 11:18:10,758][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@184497d1{/executors,null,AVAILABLE,@Spark}
[INFO][2018-05-25 11:18:10,759][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6ffab045{/executors/json,null,AVAILABLE,@Spark}
[INFO][2018-05-25 11:18:10,763][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3e2943ab{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2018-05-25 11:18:10,763][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4a9f80d3{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2018-05-25 11:18:10,783][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@41fe9859{/static,null,AVAILABLE,@Spark}
[INFO][2018-05-25 11:18:10,784][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4602c2a9{/,null,AVAILABLE,@Spark}
[INFO][2018-05-25 11:18:10,785][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3e2822{/api,null,AVAILABLE,@Spark}
[INFO][2018-05-25 11:18:10,786][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5fcacc0{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-25 11:18:10,787][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6d1d4d7{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-25 11:18:10,789][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://10.194.32.157:4040
[INFO][2018-05-25 11:18:10,951][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
[INFO][2018-05-25 11:18:11,004][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 53404.
[INFO][2018-05-25 11:18:11,005][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 10.194.32.157:53404
[INFO][2018-05-25 11:18:11,006][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2018-05-25 11:18:11,020][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 10.194.32.157, 53404, None)
[INFO][2018-05-25 11:18:11,026][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 10.194.32.157:53404 with 912.3 MB RAM, BlockManagerId(driver, 10.194.32.157, 53404, None)
[INFO][2018-05-25 11:18:11,043][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 10.194.32.157, 53404, None)
[INFO][2018-05-25 11:18:11,045][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 10.194.32.157, 53404, None)
[INFO][2018-05-25 11:18:11,322][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@18ca3c62{/metrics/json,null,AVAILABLE,@Spark}
[WARN][2018-05-25 11:18:11,845][org.apache.spark.streaming.kafka010.KafkaUtils]overriding enable.auto.commit to false for executor
[WARN][2018-05-25 11:18:11,846][org.apache.spark.streaming.kafka010.KafkaUtils]overriding auto.offset.reset to none for executor
[WARN][2018-05-25 11:18:11,846][org.apache.spark.streaming.kafka010.KafkaUtils]overriding executor group.id to spark-executor-use_a_separate_group_id_for_each_stream
[WARN][2018-05-25 11:18:11,847][org.apache.spark.streaming.kafka010.KafkaUtils]overriding receive.buffer.bytes to 65536 see KAFKA-3135
[INFO][2018-05-25 11:18:11,936][org.apache.spark.streaming.kafka010.DirectKafkaInputDStream]Slide time = 5000 ms
[INFO][2018-05-25 11:18:11,936][org.apache.spark.streaming.kafka010.DirectKafkaInputDStream]Storage level = Serialized 1x Replicated
[INFO][2018-05-25 11:18:11,937][org.apache.spark.streaming.kafka010.DirectKafkaInputDStream]Checkpoint interval = null
[INFO][2018-05-25 11:18:11,937][org.apache.spark.streaming.kafka010.DirectKafkaInputDStream]Remember interval = 5000 ms
[INFO][2018-05-25 11:18:11,938][org.apache.spark.streaming.kafka010.DirectKafkaInputDStream]Initialized and validated org.apache.spark.streaming.kafka010.DirectKafkaInputDStream@459785c2
[INFO][2018-05-25 11:18:11,938][org.apache.spark.streaming.dstream.MappedDStream]Slide time = 5000 ms
[INFO][2018-05-25 11:18:11,938][org.apache.spark.streaming.dstream.MappedDStream]Storage level = Serialized 1x Replicated
[INFO][2018-05-25 11:18:11,939][org.apache.spark.streaming.dstream.MappedDStream]Checkpoint interval = null
[INFO][2018-05-25 11:18:11,939][org.apache.spark.streaming.dstream.MappedDStream]Remember interval = 5000 ms
[INFO][2018-05-25 11:18:11,939][org.apache.spark.streaming.dstream.MappedDStream]Initialized and validated org.apache.spark.streaming.dstream.MappedDStream@44a5c307
[INFO][2018-05-25 11:18:11,939][org.apache.spark.streaming.dstream.ForEachDStream]Slide time = 5000 ms
[INFO][2018-05-25 11:18:11,939][org.apache.spark.streaming.dstream.ForEachDStream]Storage level = Serialized 1x Replicated
[INFO][2018-05-25 11:18:11,939][org.apache.spark.streaming.dstream.ForEachDStream]Checkpoint interval = null
[INFO][2018-05-25 11:18:11,939][org.apache.spark.streaming.dstream.ForEachDStream]Remember interval = 5000 ms
[INFO][2018-05-25 11:18:11,939][org.apache.spark.streaming.dstream.ForEachDStream]Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@15cf6afa
[INFO][2018-05-25 11:18:12,001][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	metric.reporters = []
	metadata.max.age.ms = 300000
	partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor]
	reconnect.backoff.ms = 50
	sasl.kerberos.ticket.renew.window.factor = 0.8
	max.partition.fetch.bytes = 1048576
	bootstrap.servers = [vm-xaj-bigdata-da-d01:9092, vm-xaj-bigdata-da-d02:9092, vm-xaj-bigdata-da-d03:9092, vm-xaj-bigdata-da-d04:9092, vm-xaj-bigdata-da-d05:9092, vm-xaj-bigdata-da-d06:9092, vm-xaj-bigdata-da-d07:9092]
	ssl.keystore.type = JKS
	enable.auto.commit = false
	sasl.mechanism = GSSAPI
	interceptor.classes = null
	exclude.internal.topics = true
	ssl.truststore.password = null
	client.id = 
	ssl.endpoint.identification.algorithm = null
	max.poll.records = 2147483647
	check.crcs = true
	request.timeout.ms = 40000
	heartbeat.interval.ms = 3000
	auto.commit.interval.ms = 5000
	receive.buffer.bytes = 65536
	ssl.truststore.type = JKS
	ssl.truststore.location = null
	ssl.keystore.password = null
	fetch.min.bytes = 1
	send.buffer.bytes = 131072
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	group.id = use_a_separate_group_id_for_each_stream
	retry.backoff.ms = 100
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	ssl.trustmanager.algorithm = PKIX
	ssl.key.password = null
	fetch.max.wait.ms = 500
	sasl.kerberos.min.time.before.relogin = 60000
	connections.max.idle.ms = 540000
	session.timeout.ms = 30000
	metrics.num.samples = 2
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	ssl.protocol = TLS
	ssl.provider = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.keystore.location = null
	ssl.cipher.suites = null
	security.protocol = PLAINTEXT
	ssl.keymanager.algorithm = SunX509
	metrics.sample.window.ms = 30000
	auto.offset.reset = latest

[INFO][2018-05-25 11:18:15,134][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	metric.reporters = []
	metadata.max.age.ms = 300000
	partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor]
	reconnect.backoff.ms = 50
	sasl.kerberos.ticket.renew.window.factor = 0.8
	max.partition.fetch.bytes = 1048576
	bootstrap.servers = [vm-xaj-bigdata-da-d01:9092, vm-xaj-bigdata-da-d02:9092, vm-xaj-bigdata-da-d03:9092, vm-xaj-bigdata-da-d04:9092, vm-xaj-bigdata-da-d05:9092, vm-xaj-bigdata-da-d06:9092, vm-xaj-bigdata-da-d07:9092]
	ssl.keystore.type = JKS
	enable.auto.commit = false
	sasl.mechanism = GSSAPI
	interceptor.classes = null
	exclude.internal.topics = true
	ssl.truststore.password = null
	client.id = consumer-1
	ssl.endpoint.identification.algorithm = null
	max.poll.records = 2147483647
	check.crcs = true
	request.timeout.ms = 40000
	heartbeat.interval.ms = 3000
	auto.commit.interval.ms = 5000
	receive.buffer.bytes = 65536
	ssl.truststore.type = JKS
	ssl.truststore.location = null
	ssl.keystore.password = null
	fetch.min.bytes = 1
	send.buffer.bytes = 131072
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	group.id = use_a_separate_group_id_for_each_stream
	retry.backoff.ms = 100
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	ssl.trustmanager.algorithm = PKIX
	ssl.key.password = null
	fetch.max.wait.ms = 500
	sasl.kerberos.min.time.before.relogin = 60000
	connections.max.idle.ms = 540000
	session.timeout.ms = 30000
	metrics.num.samples = 2
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	ssl.protocol = TLS
	ssl.provider = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.keystore.location = null
	ssl.cipher.suites = null
	security.protocol = PLAINTEXT
	ssl.keymanager.algorithm = SunX509
	metrics.sample.window.ms = 30000
	auto.offset.reset = latest

[INFO][2018-05-25 11:18:15,170][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 0.10.0.1
[INFO][2018-05-25 11:18:15,171][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a7a17cdec9eaa6c5
[INFO][2018-05-25 11:18:15,555][org.apache.kafka.clients.consumer.internals.AbstractCoordinator]Discovered coordinator vm-xaj-bigdata-da-d03:9092 (id: 2147483531 rack: null) for group use_a_separate_group_id_for_each_stream.
[INFO][2018-05-25 11:18:15,556][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]Revoking previously assigned partitions [] for group use_a_separate_group_id_for_each_stream
[INFO][2018-05-25 11:18:15,556][org.apache.kafka.clients.consumer.internals.AbstractCoordinator](Re-)joining group use_a_separate_group_id_for_each_stream
[INFO][2018-05-25 11:18:19,109][org.apache.kafka.clients.consumer.internals.AbstractCoordinator]Successfully joined group use_a_separate_group_id_for_each_stream with generation 1
[INFO][2018-05-25 11:18:19,111][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]Setting newly assigned partitions [seven-0] for group use_a_separate_group_id_for_each_stream
[INFO][2018-05-25 11:18:19,175][org.apache.spark.streaming.util.RecurringTimer]Started timer for JobGenerator at time 1527218295000
[INFO][2018-05-25 11:18:19,176][org.apache.spark.streaming.scheduler.JobGenerator]Started JobGenerator at 1527218295000 ms
[INFO][2018-05-25 11:18:19,178][org.apache.spark.streaming.scheduler.JobScheduler]Started JobScheduler
[INFO][2018-05-25 11:18:19,194][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@58399d82{/streaming,null,AVAILABLE,@Spark}
[INFO][2018-05-25 11:18:19,194][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@46d8f407{/streaming/json,null,AVAILABLE,@Spark}
[INFO][2018-05-25 11:18:19,195][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@78e89bfe{/streaming/batch,null,AVAILABLE,@Spark}
[INFO][2018-05-25 11:18:19,196][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@522ba524{/streaming/batch/json,null,AVAILABLE,@Spark}
[INFO][2018-05-25 11:18:19,200][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@185f7840{/static/streaming,null,AVAILABLE,@Spark}
[INFO][2018-05-25 11:18:19,202][org.apache.spark.streaming.StreamingContext]StreamingContext started
[INFO][2018-05-25 11:18:19,754][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218295000 ms
[INFO][2018-05-25 11:18:19,771][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218295000 ms.0 from job set of time 1527218295000 ms
[INFO][2018-05-25 11:18:19,878][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:18:19,909][org.apache.spark.scheduler.DAGScheduler]Got job 0 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:18:19,910][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 0 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:18:19,910][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:18:19,912][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:18:19,929][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 0 (MapPartitionsRDD[1] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:18:20,021][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218300000 ms
[INFO][2018-05-25 11:18:20,084][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-25 11:18:20,141][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0_piece0 stored as bytes in memory (estimated size 1959.0 B, free 912.3 MB)
[INFO][2018-05-25 11:18:20,142][org.apache.spark.storage.BlockManagerInfo]Added broadcast_0_piece0 in memory on 10.194.32.157:53404 (size: 1959.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:18:20,145][org.apache.spark.SparkContext]Created broadcast 0 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:18:20,176][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:18:20,177][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 0.0 with 1 tasks
[INFO][2018-05-25 11:18:20,243][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:18:20,270][org.apache.spark.executor.Executor]Running task 0.0 in stage 0.0 (TID 0)
[INFO][2018-05-25 11:18:20,320][org.apache.spark.streaming.kafka010.KafkaRDD]Beginning offset 12304 is the same as ending offset skipping seven 0
[INFO][2018-05-25 11:18:20,343][org.apache.spark.executor.Executor]Finished task 0.0 in stage 0.0 (TID 0). 708 bytes result sent to driver
[INFO][2018-05-25 11:18:20,357][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 0.0 (TID 0) in 134 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:18:20,360][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:18:20,365][org.apache.spark.scheduler.DAGScheduler]ResultStage 0 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.167 s
[INFO][2018-05-25 11:18:20,372][org.apache.spark.scheduler.DAGScheduler]Job 0 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.493910 s
[INFO][2018-05-25 11:18:20,376][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218295000 ms.0 from job set of time 1527218295000 ms
[INFO][2018-05-25 11:18:20,378][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 5.375 s for time 1527218295000 ms (execution: 0.618 s)
[INFO][2018-05-25 11:18:20,378][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218300000 ms.0 from job set of time 1527218300000 ms
[INFO][2018-05-25 11:18:20,386][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:18:20,388][org.apache.spark.scheduler.DAGScheduler]Got job 1 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:18:20,388][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 1 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:18:20,388][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:18:20,388][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:18:20,389][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 1 (MapPartitionsRDD[3] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:18:20,389][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:18:20,396][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-25 11:18:20,399][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1_piece0 stored as bytes in memory (estimated size 1960.0 B, free 912.3 MB)
[INFO][2018-05-25 11:18:20,401][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 
[INFO][2018-05-25 11:18:20,401][org.apache.spark.storage.BlockManagerInfo]Added broadcast_1_piece0 in memory on 10.194.32.157:53404 (size: 1960.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:18:20,403][org.apache.spark.SparkContext]Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:18:20,404][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:18:20,404][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 1.0 with 1 tasks
[INFO][2018-05-25 11:18:20,406][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:18:20,408][org.apache.spark.executor.Executor]Running task 0.0 in stage 1.0 (TID 1)
[INFO][2018-05-25 11:18:20,412][org.apache.spark.streaming.kafka010.KafkaRDD]Beginning offset 12304 is the same as ending offset skipping seven 0
[INFO][2018-05-25 11:18:20,414][org.apache.spark.executor.Executor]Finished task 0.0 in stage 1.0 (TID 1). 708 bytes result sent to driver
[INFO][2018-05-25 11:18:20,415][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 1.0 (TID 1) in 9 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:18:20,415][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:18:20,416][org.apache.spark.scheduler.DAGScheduler]ResultStage 1 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.011 s
[INFO][2018-05-25 11:18:20,417][org.apache.spark.scheduler.DAGScheduler]Job 1 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.030263 s
[INFO][2018-05-25 11:18:20,418][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218300000 ms.0 from job set of time 1527218300000 ms
[INFO][2018-05-25 11:18:20,419][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.418 s for time 1527218300000 ms (execution: 0.040 s)
[INFO][2018-05-25 11:18:20,420][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 1 from persistence list
[INFO][2018-05-25 11:18:20,432][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 0 from persistence list
[INFO][2018-05-25 11:18:20,436][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:18:20,436][org.apache.spark.storage.BlockManager]Removing RDD 1
[INFO][2018-05-25 11:18:20,436][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 
[INFO][2018-05-25 11:18:20,436][org.apache.spark.storage.BlockManager]Removing RDD 0
[INFO][2018-05-25 11:18:25,034][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218305000 ms
[INFO][2018-05-25 11:18:25,034][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218305000 ms.0 from job set of time 1527218305000 ms
[INFO][2018-05-25 11:18:25,042][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:18:25,042][org.apache.spark.scheduler.DAGScheduler]Got job 2 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:18:25,042][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 2 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:18:25,042][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:18:25,043][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:18:25,043][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 2 (MapPartitionsRDD[5] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:18:25,046][org.apache.spark.storage.memory.MemoryStore]Block broadcast_2 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-25 11:18:25,048][org.apache.spark.storage.memory.MemoryStore]Block broadcast_2_piece0 stored as bytes in memory (estimated size 1960.0 B, free 912.3 MB)
[INFO][2018-05-25 11:18:25,050][org.apache.spark.storage.BlockManagerInfo]Added broadcast_2_piece0 in memory on 10.194.32.157:53404 (size: 1960.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:18:25,051][org.apache.spark.SparkContext]Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:18:25,052][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[5] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:18:25,052][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 2.0 with 1 tasks
[INFO][2018-05-25 11:18:25,053][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:18:25,054][org.apache.spark.executor.Executor]Running task 0.0 in stage 2.0 (TID 2)
[INFO][2018-05-25 11:18:25,056][org.apache.spark.streaming.kafka010.KafkaRDD]Beginning offset 12304 is the same as ending offset skipping seven 0
[INFO][2018-05-25 11:18:25,057][org.apache.spark.executor.Executor]Finished task 0.0 in stage 2.0 (TID 2). 708 bytes result sent to driver
[INFO][2018-05-25 11:18:25,058][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 2.0 (TID 2) in 5 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:18:25,059][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:18:25,059][org.apache.spark.scheduler.DAGScheduler]ResultStage 2 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.007 s
[INFO][2018-05-25 11:18:25,059][org.apache.spark.scheduler.DAGScheduler]Job 2 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.017640 s
[INFO][2018-05-25 11:18:25,060][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218305000 ms.0 from job set of time 1527218305000 ms
[INFO][2018-05-25 11:18:25,060][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.060 s for time 1527218305000 ms (execution: 0.026 s)
[INFO][2018-05-25 11:18:25,060][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 3 from persistence list
[INFO][2018-05-25 11:18:25,061][org.apache.spark.storage.BlockManager]Removing RDD 3
[INFO][2018-05-25 11:18:25,061][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 2 from persistence list
[INFO][2018-05-25 11:18:25,062][org.apache.spark.storage.BlockManager]Removing RDD 2
[INFO][2018-05-25 11:18:25,062][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:18:25,062][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218295000 ms
[INFO][2018-05-25 11:18:30,021][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218310000 ms
[INFO][2018-05-25 11:18:30,022][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218310000 ms.0 from job set of time 1527218310000 ms
[INFO][2018-05-25 11:18:30,036][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:18:30,037][org.apache.spark.scheduler.DAGScheduler]Got job 3 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:18:30,037][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 3 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:18:30,037][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:18:30,038][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:18:30,038][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 3 (MapPartitionsRDD[7] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:18:30,042][org.apache.spark.storage.memory.MemoryStore]Block broadcast_3 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-25 11:18:30,044][org.apache.spark.storage.memory.MemoryStore]Block broadcast_3_piece0 stored as bytes in memory (estimated size 1958.0 B, free 912.3 MB)
[INFO][2018-05-25 11:18:30,044][org.apache.spark.storage.BlockManagerInfo]Added broadcast_3_piece0 in memory on 10.194.32.157:53404 (size: 1958.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:18:30,045][org.apache.spark.SparkContext]Created broadcast 3 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:18:30,046][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[7] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:18:30,046][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 3.0 with 1 tasks
[INFO][2018-05-25 11:18:30,050][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 3.0 (TID 3, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:18:30,051][org.apache.spark.executor.Executor]Running task 0.0 in stage 3.0 (TID 3)
[INFO][2018-05-25 11:18:30,057][org.apache.spark.streaming.kafka010.KafkaRDD]Beginning offset 12304 is the same as ending offset skipping seven 0
[INFO][2018-05-25 11:18:30,060][org.apache.spark.executor.Executor]Finished task 0.0 in stage 3.0 (TID 3). 708 bytes result sent to driver
[INFO][2018-05-25 11:18:30,061][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 3.0 (TID 3) in 12 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:18:30,061][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 3.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:18:30,062][org.apache.spark.scheduler.DAGScheduler]ResultStage 3 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.013 s
[INFO][2018-05-25 11:18:30,063][org.apache.spark.scheduler.DAGScheduler]Job 3 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.026055 s
[INFO][2018-05-25 11:18:30,064][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218310000 ms.0 from job set of time 1527218310000 ms
[INFO][2018-05-25 11:18:30,065][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 5 from persistence list
[INFO][2018-05-25 11:18:30,066][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.064 s for time 1527218310000 ms (execution: 0.042 s)
[INFO][2018-05-25 11:18:30,068][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 4 from persistence list
[INFO][2018-05-25 11:18:30,068][org.apache.spark.storage.BlockManager]Removing RDD 5
[INFO][2018-05-25 11:18:30,069][org.apache.spark.storage.BlockManager]Removing RDD 4
[INFO][2018-05-25 11:18:30,069][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:18:30,070][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218300000 ms
[INFO][2018-05-25 11:18:35,019][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218315000 ms
[INFO][2018-05-25 11:18:35,021][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218315000 ms.0 from job set of time 1527218315000 ms
[INFO][2018-05-25 11:18:35,029][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:18:35,031][org.apache.spark.scheduler.DAGScheduler]Got job 4 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:18:35,031][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 4 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:18:35,032][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:18:35,032][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:18:35,034][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 4 (MapPartitionsRDD[9] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:18:35,043][org.apache.spark.storage.memory.MemoryStore]Block broadcast_4 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-25 11:18:35,045][org.apache.spark.storage.memory.MemoryStore]Block broadcast_4_piece0 stored as bytes in memory (estimated size 1960.0 B, free 912.3 MB)
[INFO][2018-05-25 11:18:35,045][org.apache.spark.storage.BlockManagerInfo]Added broadcast_4_piece0 in memory on 10.194.32.157:53404 (size: 1960.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:18:35,046][org.apache.spark.SparkContext]Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:18:35,048][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[9] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:18:35,049][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 4.0 with 1 tasks
[INFO][2018-05-25 11:18:35,053][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 4.0 (TID 4, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:18:35,054][org.apache.spark.executor.Executor]Running task 0.0 in stage 4.0 (TID 4)
[INFO][2018-05-25 11:18:35,058][org.apache.spark.streaming.kafka010.KafkaRDD]Beginning offset 12304 is the same as ending offset skipping seven 0
[INFO][2018-05-25 11:18:35,058][org.apache.spark.executor.Executor]Finished task 0.0 in stage 4.0 (TID 4). 708 bytes result sent to driver
[INFO][2018-05-25 11:18:35,059][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 4.0 (TID 4) in 7 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:18:35,059][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 4.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:18:35,060][org.apache.spark.scheduler.DAGScheduler]ResultStage 4 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.008 s
[INFO][2018-05-25 11:18:35,060][org.apache.spark.scheduler.DAGScheduler]Job 4 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.030659 s
[INFO][2018-05-25 11:18:35,061][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218315000 ms.0 from job set of time 1527218315000 ms
[INFO][2018-05-25 11:18:35,061][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 7 from persistence list
[INFO][2018-05-25 11:18:35,061][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.061 s for time 1527218315000 ms (execution: 0.040 s)
[INFO][2018-05-25 11:18:35,061][org.apache.spark.storage.BlockManager]Removing RDD 7
[INFO][2018-05-25 11:18:35,061][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 6 from persistence list
[INFO][2018-05-25 11:18:35,061][org.apache.spark.storage.BlockManager]Removing RDD 6
[INFO][2018-05-25 11:18:35,062][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:18:35,062][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218305000 ms
[INFO][2018-05-25 11:18:40,019][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218320000 ms
[INFO][2018-05-25 11:18:40,019][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218320000 ms.0 from job set of time 1527218320000 ms
[INFO][2018-05-25 11:18:40,029][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:18:40,029][org.apache.spark.scheduler.DAGScheduler]Got job 5 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:18:40,030][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 5 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:18:40,030][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:18:40,030][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:18:40,030][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 5 (MapPartitionsRDD[11] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:18:40,032][org.apache.spark.storage.memory.MemoryStore]Block broadcast_5 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-25 11:18:40,034][org.apache.spark.storage.memory.MemoryStore]Block broadcast_5_piece0 stored as bytes in memory (estimated size 1960.0 B, free 912.3 MB)
[INFO][2018-05-25 11:18:40,036][org.apache.spark.storage.BlockManagerInfo]Added broadcast_5_piece0 in memory on 10.194.32.157:53404 (size: 1960.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:18:40,038][org.apache.spark.SparkContext]Created broadcast 5 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:18:40,040][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[11] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:18:40,040][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 5.0 with 1 tasks
[INFO][2018-05-25 11:18:40,041][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 5.0 (TID 5, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:18:40,042][org.apache.spark.executor.Executor]Running task 0.0 in stage 5.0 (TID 5)
[INFO][2018-05-25 11:18:40,044][org.apache.spark.streaming.kafka010.KafkaRDD]Beginning offset 12304 is the same as ending offset skipping seven 0
[INFO][2018-05-25 11:18:40,045][org.apache.spark.executor.Executor]Finished task 0.0 in stage 5.0 (TID 5). 708 bytes result sent to driver
[INFO][2018-05-25 11:18:40,046][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 5.0 (TID 5) in 5 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:18:40,047][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 5.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:18:40,047][org.apache.spark.scheduler.DAGScheduler]ResultStage 5 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.006 s
[INFO][2018-05-25 11:18:40,048][org.apache.spark.scheduler.DAGScheduler]Job 5 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.018707 s
[INFO][2018-05-25 11:18:40,048][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218320000 ms.0 from job set of time 1527218320000 ms
[INFO][2018-05-25 11:18:40,048][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 9 from persistence list
[INFO][2018-05-25 11:18:40,048][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.048 s for time 1527218320000 ms (execution: 0.029 s)
[INFO][2018-05-25 11:18:40,049][org.apache.spark.storage.BlockManager]Removing RDD 9
[INFO][2018-05-25 11:18:40,049][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 8 from persistence list
[INFO][2018-05-25 11:18:40,049][org.apache.spark.storage.BlockManager]Removing RDD 8
[INFO][2018-05-25 11:18:40,049][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:18:40,049][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218310000 ms
[INFO][2018-05-25 11:18:45,027][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218325000 ms
[INFO][2018-05-25 11:18:45,027][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218325000 ms.0 from job set of time 1527218325000 ms
[INFO][2018-05-25 11:18:45,034][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:18:45,035][org.apache.spark.scheduler.DAGScheduler]Got job 6 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:18:45,035][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 6 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:18:45,035][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:18:45,036][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:18:45,036][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 6 (MapPartitionsRDD[13] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:18:45,038][org.apache.spark.storage.memory.MemoryStore]Block broadcast_6 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-25 11:18:45,041][org.apache.spark.storage.memory.MemoryStore]Block broadcast_6_piece0 stored as bytes in memory (estimated size 1960.0 B, free 912.3 MB)
[INFO][2018-05-25 11:18:45,042][org.apache.spark.storage.BlockManagerInfo]Added broadcast_6_piece0 in memory on 10.194.32.157:53404 (size: 1960.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:18:45,042][org.apache.spark.SparkContext]Created broadcast 6 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:18:45,043][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[13] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:18:45,043][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 6.0 with 1 tasks
[INFO][2018-05-25 11:18:45,044][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 6.0 (TID 6, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:18:45,045][org.apache.spark.executor.Executor]Running task 0.0 in stage 6.0 (TID 6)
[INFO][2018-05-25 11:18:45,047][org.apache.spark.streaming.kafka010.KafkaRDD]Beginning offset 12304 is the same as ending offset skipping seven 0
[INFO][2018-05-25 11:18:45,049][org.apache.spark.executor.Executor]Finished task 0.0 in stage 6.0 (TID 6). 708 bytes result sent to driver
[INFO][2018-05-25 11:18:45,050][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 6.0 (TID 6) in 6 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:18:45,050][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 6.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:18:45,050][org.apache.spark.scheduler.DAGScheduler]ResultStage 6 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.006 s
[INFO][2018-05-25 11:18:45,051][org.apache.spark.scheduler.DAGScheduler]Job 6 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.016141 s
[INFO][2018-05-25 11:18:45,051][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218325000 ms.0 from job set of time 1527218325000 ms
[INFO][2018-05-25 11:18:45,051][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.051 s for time 1527218325000 ms (execution: 0.024 s)
[INFO][2018-05-25 11:18:45,052][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 11 from persistence list
[INFO][2018-05-25 11:18:45,052][org.apache.spark.storage.BlockManager]Removing RDD 11
[INFO][2018-05-25 11:18:45,052][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 10 from persistence list
[INFO][2018-05-25 11:18:45,052][org.apache.spark.storage.BlockManager]Removing RDD 10
[INFO][2018-05-25 11:18:45,053][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:18:45,053][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218315000 ms
[INFO][2018-05-25 11:18:50,020][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218330000 ms
[INFO][2018-05-25 11:18:50,022][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218330000 ms.0 from job set of time 1527218330000 ms
[INFO][2018-05-25 11:18:50,033][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:18:50,035][org.apache.spark.scheduler.DAGScheduler]Got job 7 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:18:50,035][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 7 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:18:50,036][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:18:50,036][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:18:50,036][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 7 (MapPartitionsRDD[15] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:18:50,039][org.apache.spark.storage.memory.MemoryStore]Block broadcast_7 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-25 11:18:50,046][org.apache.spark.storage.memory.MemoryStore]Block broadcast_7_piece0 stored as bytes in memory (estimated size 1960.0 B, free 912.3 MB)
[INFO][2018-05-25 11:18:50,047][org.apache.spark.storage.BlockManagerInfo]Added broadcast_7_piece0 in memory on 10.194.32.157:53404 (size: 1960.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:18:50,047][org.apache.spark.SparkContext]Created broadcast 7 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:18:50,048][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[15] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:18:50,048][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 7.0 with 1 tasks
[INFO][2018-05-25 11:18:50,049][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 7.0 (TID 7, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:18:50,052][org.apache.spark.executor.Executor]Running task 0.0 in stage 7.0 (TID 7)
[INFO][2018-05-25 11:18:50,064][org.apache.spark.streaming.kafka010.KafkaRDD]Beginning offset 12304 is the same as ending offset skipping seven 0
[INFO][2018-05-25 11:18:50,065][org.apache.spark.executor.Executor]Finished task 0.0 in stage 7.0 (TID 7). 708 bytes result sent to driver
[INFO][2018-05-25 11:18:50,070][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 7.0 (TID 7) in 21 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:18:50,071][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 7.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:18:50,072][org.apache.spark.scheduler.DAGScheduler]ResultStage 7 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.023 s
[INFO][2018-05-25 11:18:50,075][org.apache.spark.scheduler.DAGScheduler]Job 7 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.040567 s
[INFO][2018-05-25 11:18:50,081][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218330000 ms.0 from job set of time 1527218330000 ms
[INFO][2018-05-25 11:18:50,082][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 13 from persistence list
[INFO][2018-05-25 11:18:50,082][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.081 s for time 1527218330000 ms (execution: 0.059 s)
[INFO][2018-05-25 11:18:50,082][org.apache.spark.storage.BlockManager]Removing RDD 13
[INFO][2018-05-25 11:18:50,082][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 12 from persistence list
[INFO][2018-05-25 11:18:50,083][org.apache.spark.storage.BlockManager]Removing RDD 12
[INFO][2018-05-25 11:18:50,083][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:18:50,083][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218320000 ms
[INFO][2018-05-25 11:18:55,017][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218335000 ms
[INFO][2018-05-25 11:18:55,018][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218335000 ms.0 from job set of time 1527218335000 ms
[INFO][2018-05-25 11:18:55,025][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:18:55,026][org.apache.spark.scheduler.DAGScheduler]Got job 8 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:18:55,026][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 8 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:18:55,026][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:18:55,026][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:18:55,026][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 8 (MapPartitionsRDD[17] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:18:55,029][org.apache.spark.storage.memory.MemoryStore]Block broadcast_8 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-25 11:18:55,034][org.apache.spark.storage.memory.MemoryStore]Block broadcast_8_piece0 stored as bytes in memory (estimated size 1969.0 B, free 912.3 MB)
[INFO][2018-05-25 11:18:55,035][org.apache.spark.storage.BlockManagerInfo]Added broadcast_8_piece0 in memory on 10.194.32.157:53404 (size: 1969.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:18:55,035][org.apache.spark.SparkContext]Created broadcast 8 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:18:55,036][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[17] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:18:55,036][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 8.0 with 1 tasks
[INFO][2018-05-25 11:18:55,037][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 8.0 (TID 8, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:18:55,038][org.apache.spark.executor.Executor]Running task 0.0 in stage 8.0 (TID 8)
[INFO][2018-05-25 11:18:55,040][org.apache.spark.streaming.kafka010.KafkaRDD]Beginning offset 12304 is the same as ending offset skipping seven 0
[INFO][2018-05-25 11:18:55,041][org.apache.spark.executor.Executor]Finished task 0.0 in stage 8.0 (TID 8). 708 bytes result sent to driver
[INFO][2018-05-25 11:18:55,041][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 8.0 (TID 8) in 4 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:18:55,041][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 8.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:18:55,042][org.apache.spark.scheduler.DAGScheduler]ResultStage 8 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.005 s
[INFO][2018-05-25 11:18:55,042][org.apache.spark.scheduler.DAGScheduler]Job 8 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.017119 s
[INFO][2018-05-25 11:18:55,043][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218335000 ms.0 from job set of time 1527218335000 ms
[INFO][2018-05-25 11:18:55,043][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 15 from persistence list
[INFO][2018-05-25 11:18:55,043][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.043 s for time 1527218335000 ms (execution: 0.026 s)
[INFO][2018-05-25 11:18:55,044][org.apache.spark.storage.BlockManager]Removing RDD 15
[INFO][2018-05-25 11:18:55,044][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 14 from persistence list
[INFO][2018-05-25 11:18:55,044][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:18:55,045][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218325000 ms
[INFO][2018-05-25 11:18:55,045][org.apache.spark.storage.BlockManager]Removing RDD 14
[INFO][2018-05-25 11:19:00,019][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218340000 ms
[INFO][2018-05-25 11:19:00,019][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218340000 ms.0 from job set of time 1527218340000 ms
[INFO][2018-05-25 11:19:00,026][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:19:00,026][org.apache.spark.scheduler.DAGScheduler]Got job 9 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:19:00,026][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 9 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:19:00,026][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:19:00,027][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:19:00,027][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 9 (MapPartitionsRDD[19] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:19:00,029][org.apache.spark.storage.memory.MemoryStore]Block broadcast_9 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-25 11:19:00,036][org.apache.spark.storage.memory.MemoryStore]Block broadcast_9_piece0 stored as bytes in memory (estimated size 1960.0 B, free 912.3 MB)
[INFO][2018-05-25 11:19:00,039][org.apache.spark.storage.BlockManagerInfo]Added broadcast_9_piece0 in memory on 10.194.32.157:53404 (size: 1960.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:19:00,040][org.apache.spark.SparkContext]Created broadcast 9 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:19:00,040][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[19] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:19:00,040][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 9.0 with 1 tasks
[INFO][2018-05-25 11:19:00,041][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 9.0 (TID 9, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:19:00,042][org.apache.spark.executor.Executor]Running task 0.0 in stage 9.0 (TID 9)
[INFO][2018-05-25 11:19:00,045][org.apache.spark.streaming.kafka010.KafkaRDD]Beginning offset 12304 is the same as ending offset skipping seven 0
[INFO][2018-05-25 11:19:00,046][org.apache.spark.executor.Executor]Finished task 0.0 in stage 9.0 (TID 9). 708 bytes result sent to driver
[INFO][2018-05-25 11:19:00,047][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 9.0 (TID 9) in 6 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:19:00,048][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 9.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:19:00,049][org.apache.spark.scheduler.DAGScheduler]ResultStage 9 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.008 s
[INFO][2018-05-25 11:19:00,053][org.apache.spark.scheduler.DAGScheduler]Job 9 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.027117 s
[INFO][2018-05-25 11:19:00,055][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218340000 ms.0 from job set of time 1527218340000 ms
[INFO][2018-05-25 11:19:00,056][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.055 s for time 1527218340000 ms (execution: 0.036 s)
[INFO][2018-05-25 11:19:00,056][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 17 from persistence list
[INFO][2018-05-25 11:19:00,058][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 16 from persistence list
[INFO][2018-05-25 11:19:00,058][org.apache.spark.storage.BlockManager]Removing RDD 17
[INFO][2018-05-25 11:19:00,060][org.apache.spark.storage.BlockManager]Removing RDD 16
[INFO][2018-05-25 11:19:00,060][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:19:00,060][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218330000 ms
[INFO][2018-05-25 11:19:00,065][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_8_piece0 on 10.194.32.157:53404 in memory (size: 1969.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:19:00,068][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_6_piece0 on 10.194.32.157:53404 in memory (size: 1960.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:19:00,069][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_7_piece0 on 10.194.32.157:53404 in memory (size: 1960.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:19:00,070][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_5_piece0 on 10.194.32.157:53404 in memory (size: 1960.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:19:00,072][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_1_piece0 on 10.194.32.157:53404 in memory (size: 1960.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:19:00,073][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_4_piece0 on 10.194.32.157:53404 in memory (size: 1960.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:19:00,074][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_2_piece0 on 10.194.32.157:53404 in memory (size: 1960.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:19:00,075][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_3_piece0 on 10.194.32.157:53404 in memory (size: 1958.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:19:00,077][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_0_piece0 on 10.194.32.157:53404 in memory (size: 1959.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:19:05,024][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218345000 ms
[INFO][2018-05-25 11:19:05,024][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218345000 ms.0 from job set of time 1527218345000 ms
[INFO][2018-05-25 11:19:05,030][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:19:05,032][org.apache.spark.scheduler.DAGScheduler]Got job 10 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:19:05,032][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 10 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:19:05,033][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:19:05,033][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:19:05,035][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 10 (MapPartitionsRDD[21] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:19:05,040][org.apache.spark.storage.memory.MemoryStore]Block broadcast_10 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-25 11:19:05,044][org.apache.spark.storage.memory.MemoryStore]Block broadcast_10_piece0 stored as bytes in memory (estimated size 1960.0 B, free 912.3 MB)
[INFO][2018-05-25 11:19:05,045][org.apache.spark.storage.BlockManagerInfo]Added broadcast_10_piece0 in memory on 10.194.32.157:53404 (size: 1960.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:19:05,045][org.apache.spark.SparkContext]Created broadcast 10 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:19:05,046][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[21] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:19:05,046][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 10.0 with 1 tasks
[INFO][2018-05-25 11:19:05,047][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 10.0 (TID 10, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:19:05,047][org.apache.spark.executor.Executor]Running task 0.0 in stage 10.0 (TID 10)
[INFO][2018-05-25 11:19:05,051][org.apache.spark.streaming.kafka010.KafkaRDD]Beginning offset 12304 is the same as ending offset skipping seven 0
[INFO][2018-05-25 11:19:05,053][org.apache.spark.executor.Executor]Finished task 0.0 in stage 10.0 (TID 10). 708 bytes result sent to driver
[INFO][2018-05-25 11:19:05,054][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 10.0 (TID 10) in 7 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:19:05,054][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 10.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:19:05,055][org.apache.spark.scheduler.DAGScheduler]ResultStage 10 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.008 s
[INFO][2018-05-25 11:19:05,055][org.apache.spark.scheduler.DAGScheduler]Job 10 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.024554 s
[INFO][2018-05-25 11:19:05,056][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218345000 ms.0 from job set of time 1527218345000 ms
[INFO][2018-05-25 11:19:05,056][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.056 s for time 1527218345000 ms (execution: 0.032 s)
[INFO][2018-05-25 11:19:05,056][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 19 from persistence list
[INFO][2018-05-25 11:19:05,056][org.apache.spark.storage.BlockManager]Removing RDD 19
[INFO][2018-05-25 11:19:05,057][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 18 from persistence list
[INFO][2018-05-25 11:19:05,057][org.apache.spark.storage.BlockManager]Removing RDD 18
[INFO][2018-05-25 11:19:05,057][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:19:05,057][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218335000 ms
[INFO][2018-05-25 11:19:10,015][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218350000 ms
[INFO][2018-05-25 11:19:10,015][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218350000 ms.0 from job set of time 1527218350000 ms
[INFO][2018-05-25 11:19:10,021][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:19:10,022][org.apache.spark.scheduler.DAGScheduler]Got job 11 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:19:10,022][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 11 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:19:10,022][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:19:10,022][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:19:10,023][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 11 (MapPartitionsRDD[23] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:19:10,024][org.apache.spark.storage.memory.MemoryStore]Block broadcast_11 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-25 11:19:10,025][org.apache.spark.storage.memory.MemoryStore]Block broadcast_11_piece0 stored as bytes in memory (estimated size 1960.0 B, free 912.3 MB)
[INFO][2018-05-25 11:19:10,026][org.apache.spark.storage.BlockManagerInfo]Added broadcast_11_piece0 in memory on 10.194.32.157:53404 (size: 1960.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:19:10,026][org.apache.spark.SparkContext]Created broadcast 11 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:19:10,027][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[23] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:19:10,027][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 11.0 with 1 tasks
[INFO][2018-05-25 11:19:10,027][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 11.0 (TID 11, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:19:10,028][org.apache.spark.executor.Executor]Running task 0.0 in stage 11.0 (TID 11)
[INFO][2018-05-25 11:19:10,029][org.apache.spark.streaming.kafka010.KafkaRDD]Beginning offset 12304 is the same as ending offset skipping seven 0
[INFO][2018-05-25 11:19:10,029][org.apache.spark.executor.Executor]Finished task 0.0 in stage 11.0 (TID 11). 665 bytes result sent to driver
[INFO][2018-05-25 11:19:10,030][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 11.0 (TID 11) in 3 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:19:10,030][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 11.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:19:10,030][org.apache.spark.scheduler.DAGScheduler]ResultStage 11 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.003 s
[INFO][2018-05-25 11:19:10,031][org.apache.spark.scheduler.DAGScheduler]Job 11 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.009192 s
[INFO][2018-05-25 11:19:10,031][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218350000 ms.0 from job set of time 1527218350000 ms
[INFO][2018-05-25 11:19:10,031][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 21 from persistence list
[INFO][2018-05-25 11:19:10,031][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.031 s for time 1527218350000 ms (execution: 0.016 s)
[INFO][2018-05-25 11:19:10,032][org.apache.spark.storage.BlockManager]Removing RDD 21
[INFO][2018-05-25 11:19:10,032][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 20 from persistence list
[INFO][2018-05-25 11:19:10,032][org.apache.spark.storage.BlockManager]Removing RDD 20
[INFO][2018-05-25 11:19:10,032][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:19:10,032][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218340000 ms
[INFO][2018-05-25 11:19:15,018][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218355000 ms
[INFO][2018-05-25 11:19:15,018][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218355000 ms.0 from job set of time 1527218355000 ms
[INFO][2018-05-25 11:19:15,024][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:19:15,024][org.apache.spark.scheduler.DAGScheduler]Got job 12 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:19:15,024][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 12 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:19:15,024][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:19:15,025][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:19:15,025][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 12 (MapPartitionsRDD[25] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:19:15,026][org.apache.spark.storage.memory.MemoryStore]Block broadcast_12 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-25 11:19:15,027][org.apache.spark.storage.memory.MemoryStore]Block broadcast_12_piece0 stored as bytes in memory (estimated size 1960.0 B, free 912.3 MB)
[INFO][2018-05-25 11:19:15,028][org.apache.spark.storage.BlockManagerInfo]Added broadcast_12_piece0 in memory on 10.194.32.157:53404 (size: 1960.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:19:15,029][org.apache.spark.SparkContext]Created broadcast 12 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:19:15,030][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[25] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:19:15,030][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 12.0 with 1 tasks
[INFO][2018-05-25 11:19:15,031][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 12.0 (TID 12, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:19:15,031][org.apache.spark.executor.Executor]Running task 0.0 in stage 12.0 (TID 12)
[INFO][2018-05-25 11:19:15,033][org.apache.spark.streaming.kafka010.KafkaRDD]Beginning offset 12304 is the same as ending offset skipping seven 0
[INFO][2018-05-25 11:19:15,033][org.apache.spark.executor.Executor]Finished task 0.0 in stage 12.0 (TID 12). 708 bytes result sent to driver
[INFO][2018-05-25 11:19:15,034][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 12.0 (TID 12) in 4 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:19:15,034][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 12.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:19:15,034][org.apache.spark.scheduler.DAGScheduler]ResultStage 12 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.004 s
[INFO][2018-05-25 11:19:15,035][org.apache.spark.scheduler.DAGScheduler]Job 12 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.010636 s
[INFO][2018-05-25 11:19:15,035][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218355000 ms.0 from job set of time 1527218355000 ms
[INFO][2018-05-25 11:19:15,035][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.035 s for time 1527218355000 ms (execution: 0.017 s)
[INFO][2018-05-25 11:19:15,035][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 23 from persistence list
[INFO][2018-05-25 11:19:15,036][org.apache.spark.storage.BlockManager]Removing RDD 23
[INFO][2018-05-25 11:19:15,036][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 22 from persistence list
[INFO][2018-05-25 11:19:15,036][org.apache.spark.storage.BlockManager]Removing RDD 22
[INFO][2018-05-25 11:19:15,036][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:19:15,037][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218345000 ms
[INFO][2018-05-25 11:19:20,018][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218360000 ms
[INFO][2018-05-25 11:19:20,019][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218360000 ms.0 from job set of time 1527218360000 ms
[INFO][2018-05-25 11:19:20,024][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:19:20,025][org.apache.spark.scheduler.DAGScheduler]Got job 13 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:19:20,025][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 13 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:19:20,025][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:19:20,025][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:19:20,025][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 13 (MapPartitionsRDD[27] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:19:20,027][org.apache.spark.storage.memory.MemoryStore]Block broadcast_13 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-25 11:19:20,028][org.apache.spark.storage.memory.MemoryStore]Block broadcast_13_piece0 stored as bytes in memory (estimated size 1960.0 B, free 912.3 MB)
[INFO][2018-05-25 11:19:20,028][org.apache.spark.storage.BlockManagerInfo]Added broadcast_13_piece0 in memory on 10.194.32.157:53404 (size: 1960.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:19:20,029][org.apache.spark.SparkContext]Created broadcast 13 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:19:20,029][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[27] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:19:20,029][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 13.0 with 1 tasks
[INFO][2018-05-25 11:19:20,030][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 13.0 (TID 13, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:19:20,031][org.apache.spark.executor.Executor]Running task 0.0 in stage 13.0 (TID 13)
[INFO][2018-05-25 11:19:20,033][org.apache.spark.streaming.kafka010.KafkaRDD]Beginning offset 12304 is the same as ending offset skipping seven 0
[INFO][2018-05-25 11:19:20,033][org.apache.spark.executor.Executor]Finished task 0.0 in stage 13.0 (TID 13). 708 bytes result sent to driver
[INFO][2018-05-25 11:19:20,035][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 13.0 (TID 13) in 4 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:19:20,035][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 13.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:19:20,036][org.apache.spark.scheduler.DAGScheduler]ResultStage 13 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.006 s
[INFO][2018-05-25 11:19:20,036][org.apache.spark.scheduler.DAGScheduler]Job 13 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.012031 s
[INFO][2018-05-25 11:19:20,037][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218360000 ms.0 from job set of time 1527218360000 ms
[INFO][2018-05-25 11:19:20,037][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 25 from persistence list
[INFO][2018-05-25 11:19:20,037][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.037 s for time 1527218360000 ms (execution: 0.019 s)
[INFO][2018-05-25 11:19:20,037][org.apache.spark.storage.BlockManager]Removing RDD 25
[INFO][2018-05-25 11:19:20,038][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 24 from persistence list
[INFO][2018-05-25 11:19:20,039][org.apache.spark.storage.BlockManager]Removing RDD 24
[INFO][2018-05-25 11:19:20,039][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:19:20,039][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218350000 ms
[INFO][2018-05-25 11:19:25,015][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218365000 ms
[INFO][2018-05-25 11:19:25,016][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218365000 ms.0 from job set of time 1527218365000 ms
[INFO][2018-05-25 11:19:25,021][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:19:25,022][org.apache.spark.scheduler.DAGScheduler]Got job 14 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:19:25,022][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 14 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:19:25,022][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:19:25,022][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:19:25,024][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 14 (MapPartitionsRDD[29] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:19:25,026][org.apache.spark.storage.memory.MemoryStore]Block broadcast_14 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-25 11:19:25,027][org.apache.spark.storage.memory.MemoryStore]Block broadcast_14_piece0 stored as bytes in memory (estimated size 1960.0 B, free 912.3 MB)
[INFO][2018-05-25 11:19:25,028][org.apache.spark.storage.BlockManagerInfo]Added broadcast_14_piece0 in memory on 10.194.32.157:53404 (size: 1960.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:19:25,029][org.apache.spark.SparkContext]Created broadcast 14 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:19:25,029][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[29] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:19:25,029][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 14.0 with 1 tasks
[INFO][2018-05-25 11:19:25,030][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 14.0 (TID 14, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:19:25,030][org.apache.spark.executor.Executor]Running task 0.0 in stage 14.0 (TID 14)
[INFO][2018-05-25 11:19:25,031][org.apache.spark.streaming.kafka010.KafkaRDD]Beginning offset 12304 is the same as ending offset skipping seven 0
[INFO][2018-05-25 11:19:25,032][org.apache.spark.executor.Executor]Finished task 0.0 in stage 14.0 (TID 14). 665 bytes result sent to driver
[INFO][2018-05-25 11:19:25,032][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 14.0 (TID 14) in 3 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:19:25,032][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 14.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:19:25,033][org.apache.spark.scheduler.DAGScheduler]ResultStage 14 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.004 s
[INFO][2018-05-25 11:19:25,033][org.apache.spark.scheduler.DAGScheduler]Job 14 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.011762 s
[INFO][2018-05-25 11:19:25,034][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218365000 ms.0 from job set of time 1527218365000 ms
[INFO][2018-05-25 11:19:25,034][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.034 s for time 1527218365000 ms (execution: 0.018 s)
[INFO][2018-05-25 11:19:25,034][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 27 from persistence list
[INFO][2018-05-25 11:19:25,036][org.apache.spark.storage.BlockManager]Removing RDD 27
[INFO][2018-05-25 11:19:25,037][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 26 from persistence list
[INFO][2018-05-25 11:19:25,037][org.apache.spark.storage.BlockManager]Removing RDD 26
[INFO][2018-05-25 11:19:25,038][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:19:25,038][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218355000 ms
[INFO][2018-05-25 11:19:30,020][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218370000 ms
[INFO][2018-05-25 11:19:30,020][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218370000 ms.0 from job set of time 1527218370000 ms
[INFO][2018-05-25 11:19:30,026][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:19:30,026][org.apache.spark.scheduler.DAGScheduler]Got job 15 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:19:30,026][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 15 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:19:30,026][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:19:30,026][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:19:30,027][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 15 (MapPartitionsRDD[31] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:19:30,029][org.apache.spark.storage.memory.MemoryStore]Block broadcast_15 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-25 11:19:30,030][org.apache.spark.storage.memory.MemoryStore]Block broadcast_15_piece0 stored as bytes in memory (estimated size 1960.0 B, free 912.3 MB)
[INFO][2018-05-25 11:19:30,031][org.apache.spark.storage.BlockManagerInfo]Added broadcast_15_piece0 in memory on 10.194.32.157:53404 (size: 1960.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:19:30,031][org.apache.spark.SparkContext]Created broadcast 15 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:19:30,032][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 15 (MapPartitionsRDD[31] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:19:30,032][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 15.0 with 1 tasks
[INFO][2018-05-25 11:19:30,033][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 15.0 (TID 15, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:19:30,033][org.apache.spark.executor.Executor]Running task 0.0 in stage 15.0 (TID 15)
[INFO][2018-05-25 11:19:30,035][org.apache.spark.streaming.kafka010.KafkaRDD]Beginning offset 12304 is the same as ending offset skipping seven 0
[INFO][2018-05-25 11:19:30,036][org.apache.spark.executor.Executor]Finished task 0.0 in stage 15.0 (TID 15). 708 bytes result sent to driver
[INFO][2018-05-25 11:19:30,036][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 15.0 (TID 15) in 3 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:19:30,037][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 15.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:19:30,037][org.apache.spark.scheduler.DAGScheduler]ResultStage 15 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.005 s
[INFO][2018-05-25 11:19:30,038][org.apache.spark.scheduler.DAGScheduler]Job 15 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.011822 s
[INFO][2018-05-25 11:19:30,038][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218370000 ms.0 from job set of time 1527218370000 ms
[INFO][2018-05-25 11:19:30,038][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.038 s for time 1527218370000 ms (execution: 0.018 s)
[INFO][2018-05-25 11:19:30,039][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 29 from persistence list
[INFO][2018-05-25 11:19:30,039][org.apache.spark.storage.BlockManager]Removing RDD 29
[INFO][2018-05-25 11:19:30,039][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 28 from persistence list
[INFO][2018-05-25 11:19:30,039][org.apache.spark.storage.BlockManager]Removing RDD 28
[INFO][2018-05-25 11:19:30,039][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:19:30,040][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218360000 ms
[INFO][2018-05-25 11:19:35,015][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218375000 ms
[INFO][2018-05-25 11:19:35,016][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218375000 ms.0 from job set of time 1527218375000 ms
[INFO][2018-05-25 11:19:35,023][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:19:35,026][org.apache.spark.scheduler.DAGScheduler]Got job 16 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:19:35,027][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 16 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:19:35,027][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:19:35,028][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:19:35,029][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 16 (MapPartitionsRDD[33] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:19:35,033][org.apache.spark.storage.memory.MemoryStore]Block broadcast_16 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-25 11:19:35,035][org.apache.spark.storage.memory.MemoryStore]Block broadcast_16_piece0 stored as bytes in memory (estimated size 1960.0 B, free 912.3 MB)
[INFO][2018-05-25 11:19:35,036][org.apache.spark.storage.BlockManagerInfo]Added broadcast_16_piece0 in memory on 10.194.32.157:53404 (size: 1960.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:19:35,037][org.apache.spark.SparkContext]Created broadcast 16 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:19:35,037][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 16 (MapPartitionsRDD[33] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:19:35,037][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 16.0 with 1 tasks
[INFO][2018-05-25 11:19:35,038][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 16.0 (TID 16, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:19:35,039][org.apache.spark.executor.Executor]Running task 0.0 in stage 16.0 (TID 16)
[INFO][2018-05-25 11:19:35,041][org.apache.spark.streaming.kafka010.KafkaRDD]Beginning offset 12304 is the same as ending offset skipping seven 0
[INFO][2018-05-25 11:19:35,043][org.apache.spark.executor.Executor]Finished task 0.0 in stage 16.0 (TID 16). 708 bytes result sent to driver
[INFO][2018-05-25 11:19:35,045][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 16.0 (TID 16) in 7 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:19:35,046][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 16.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:19:35,047][org.apache.spark.scheduler.DAGScheduler]ResultStage 16 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.009 s
[INFO][2018-05-25 11:19:35,048][org.apache.spark.scheduler.DAGScheduler]Job 16 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.024352 s
[INFO][2018-05-25 11:19:35,048][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218375000 ms.0 from job set of time 1527218375000 ms
[INFO][2018-05-25 11:19:35,048][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.048 s for time 1527218375000 ms (execution: 0.032 s)
[INFO][2018-05-25 11:19:35,049][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 31 from persistence list
[INFO][2018-05-25 11:19:35,050][org.apache.spark.storage.BlockManager]Removing RDD 31
[INFO][2018-05-25 11:19:35,050][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 30 from persistence list
[INFO][2018-05-25 11:19:35,050][org.apache.spark.storage.BlockManager]Removing RDD 30
[INFO][2018-05-25 11:19:35,051][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:19:35,051][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218365000 ms
[INFO][2018-05-25 11:19:40,024][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218380000 ms
[INFO][2018-05-25 11:19:40,025][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218380000 ms.0 from job set of time 1527218380000 ms
[INFO][2018-05-25 11:19:40,031][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:19:40,032][org.apache.spark.scheduler.DAGScheduler]Got job 17 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:19:40,032][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 17 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:19:40,032][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:19:40,032][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:19:40,032][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 17 (MapPartitionsRDD[35] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:19:40,034][org.apache.spark.storage.memory.MemoryStore]Block broadcast_17 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-25 11:19:40,036][org.apache.spark.storage.memory.MemoryStore]Block broadcast_17_piece0 stored as bytes in memory (estimated size 1960.0 B, free 912.3 MB)
[INFO][2018-05-25 11:19:40,039][org.apache.spark.storage.BlockManagerInfo]Added broadcast_17_piece0 in memory on 10.194.32.157:53404 (size: 1960.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:19:40,040][org.apache.spark.SparkContext]Created broadcast 17 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:19:40,041][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[35] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:19:40,041][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 17.0 with 1 tasks
[INFO][2018-05-25 11:19:40,042][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 17.0 (TID 17, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:19:40,042][org.apache.spark.executor.Executor]Running task 0.0 in stage 17.0 (TID 17)
[INFO][2018-05-25 11:19:40,044][org.apache.spark.streaming.kafka010.KafkaRDD]Beginning offset 12304 is the same as ending offset skipping seven 0
[INFO][2018-05-25 11:19:40,044][org.apache.spark.executor.Executor]Finished task 0.0 in stage 17.0 (TID 17). 665 bytes result sent to driver
[INFO][2018-05-25 11:19:40,045][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 17.0 (TID 17) in 3 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:19:40,045][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 17.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:19:40,045][org.apache.spark.scheduler.DAGScheduler]ResultStage 17 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.004 s
[INFO][2018-05-25 11:19:40,046][org.apache.spark.scheduler.DAGScheduler]Job 17 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.014068 s
[INFO][2018-05-25 11:19:40,046][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218380000 ms.0 from job set of time 1527218380000 ms
[INFO][2018-05-25 11:19:40,046][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 33 from persistence list
[INFO][2018-05-25 11:19:40,046][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.046 s for time 1527218380000 ms (execution: 0.021 s)
[INFO][2018-05-25 11:19:40,047][org.apache.spark.storage.BlockManager]Removing RDD 33
[INFO][2018-05-25 11:19:40,047][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 32 from persistence list
[INFO][2018-05-25 11:19:40,047][org.apache.spark.storage.BlockManager]Removing RDD 32
[INFO][2018-05-25 11:19:40,047][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:19:40,047][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218370000 ms
[INFO][2018-05-25 11:19:44,864][org.apache.spark.streaming.StreamingContext]Invoking stop(stopGracefully=false) from shutdown hook
[INFO][2018-05-25 11:19:44,866][org.apache.spark.streaming.scheduler.ReceiverTracker]ReceiverTracker stopped
[INFO][2018-05-25 11:19:44,867][org.apache.spark.streaming.scheduler.JobGenerator]Stopping JobGenerator immediately
[INFO][2018-05-25 11:19:44,868][org.apache.spark.streaming.util.RecurringTimer]Stopped timer for JobGenerator after time 1527218380000
[INFO][2018-05-25 11:19:44,883][org.apache.spark.streaming.scheduler.JobGenerator]Stopped JobGenerator
[INFO][2018-05-25 11:19:44,885][org.apache.spark.streaming.scheduler.JobScheduler]Stopped JobScheduler
[INFO][2018-05-25 11:19:44,894][org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@58399d82{/streaming,null,UNAVAILABLE,@Spark}
[INFO][2018-05-25 11:19:44,895][org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@78e89bfe{/streaming/batch,null,UNAVAILABLE,@Spark}
[INFO][2018-05-25 11:19:44,896][org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@185f7840{/static/streaming,null,UNAVAILABLE,@Spark}
[INFO][2018-05-25 11:19:44,897][org.apache.spark.streaming.StreamingContext]StreamingContext stopped successfully
[INFO][2018-05-25 11:19:44,898][org.apache.spark.SparkContext]Invoking stop() from shutdown hook
[INFO][2018-05-25 11:19:44,903][org.spark_project.jetty.server.AbstractConnector]Stopped Spark@349fcd3b{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-05-25 11:19:44,905][org.apache.spark.ui.SparkUI]Stopped Spark web UI at http://10.194.32.157:4040
[INFO][2018-05-25 11:19:44,912][org.apache.spark.MapOutputTrackerMasterEndpoint]MapOutputTrackerMasterEndpoint stopped!
[INFO][2018-05-25 11:19:44,936][org.apache.spark.storage.memory.MemoryStore]MemoryStore cleared
[INFO][2018-05-25 11:19:44,937][org.apache.spark.storage.BlockManager]BlockManager stopped
[INFO][2018-05-25 11:19:44,937][org.apache.spark.storage.BlockManagerMaster]BlockManagerMaster stopped
[INFO][2018-05-25 11:19:44,939][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint]OutputCommitCoordinator stopped!
[INFO][2018-05-25 11:19:44,940][org.apache.spark.SparkContext]Successfully stopped SparkContext
[INFO][2018-05-25 11:19:44,941][org.apache.spark.util.ShutdownHookManager]Shutdown hook called
[INFO][2018-05-25 11:19:44,941][org.apache.spark.util.ShutdownHookManager]Deleting directory /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/spark-017a23da-86fb-4cf8-add2-e64803dfee44
[INFO][2018-05-25 11:20:43,216][org.apache.spark.SparkContext]Running Spark version 2.2.0
[INFO][2018-05-25 11:20:44,165][org.apache.spark.SparkContext]Submitted application: ReceiveKafkaData$
[INFO][2018-05-25 11:20:44,190][org.apache.spark.SecurityManager]Changing view acls to: seven
[INFO][2018-05-25 11:20:44,191][org.apache.spark.SecurityManager]Changing modify acls to: seven
[INFO][2018-05-25 11:20:44,192][org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO][2018-05-25 11:20:44,192][org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO][2018-05-25 11:20:44,193][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(seven); groups with view permissions: Set(); users  with modify permissions: Set(seven); groups with modify permissions: Set()
[INFO][2018-05-25 11:20:44,568][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 53451.
[INFO][2018-05-25 11:20:44,598][org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO][2018-05-25 11:20:44,620][org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO][2018-05-25 11:20:44,626][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2018-05-25 11:20:44,627][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
[INFO][2018-05-25 11:20:44,643][org.apache.spark.storage.DiskBlockManager]Created local directory at /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/blockmgr-7e1ac88a-51a0-4c40-bff1-0f44cbdd6eb5
[INFO][2018-05-25 11:20:44,660][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 912.3 MB
[INFO][2018-05-25 11:20:44,771][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO][2018-05-25 11:20:44,900][org.spark_project.jetty.util.log]Logging initialized @2692ms
[INFO][2018-05-25 11:20:44,973][org.spark_project.jetty.server.Server]jetty-9.3.z-SNAPSHOT
[INFO][2018-05-25 11:20:44,997][org.spark_project.jetty.server.Server]Started @2791ms
[INFO][2018-05-25 11:20:45,031][org.spark_project.jetty.server.AbstractConnector]Started ServerConnector@4de00ad1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-05-25 11:20:45,031][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4040.
[INFO][2018-05-25 11:20:45,057][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@c7a975a{/jobs,null,AVAILABLE,@Spark}
[INFO][2018-05-25 11:20:45,058][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@55795845{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2018-05-25 11:20:45,059][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@119f1f2a{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2018-05-25 11:20:45,060][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7fd4acee{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2018-05-25 11:20:45,061][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6175619b{/stages,null,AVAILABLE,@Spark}
[INFO][2018-05-25 11:20:45,061][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@756cf158{/stages/json,null,AVAILABLE,@Spark}
[INFO][2018-05-25 11:20:45,062][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@751d3241{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2018-05-25 11:20:45,063][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@30ea8c23{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-25 11:20:45,064][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4e76dac{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2018-05-25 11:20:45,065][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5f2f577{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2018-05-25 11:20:45,065][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5d465e4b{/storage,null,AVAILABLE,@Spark}
[INFO][2018-05-25 11:20:45,066][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@41a90fa8{/storage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-25 11:20:45,067][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@52500920{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2018-05-25 11:20:45,068][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@18a3962d{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2018-05-25 11:20:45,069][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2a65bb85{/environment,null,AVAILABLE,@Spark}
[INFO][2018-05-25 11:20:45,069][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4f936da8{/environment/json,null,AVAILABLE,@Spark}
[INFO][2018-05-25 11:20:45,070][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@452ba1db{/executors,null,AVAILABLE,@Spark}
[INFO][2018-05-25 11:20:45,071][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@76a36b71{/executors/json,null,AVAILABLE,@Spark}
[INFO][2018-05-25 11:20:45,072][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@f9d87b{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2018-05-25 11:20:45,073][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@26fb628{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2018-05-25 11:20:45,088][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@70dd7e15{/static,null,AVAILABLE,@Spark}
[INFO][2018-05-25 11:20:45,090][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@60d8c0dc{/,null,AVAILABLE,@Spark}
[INFO][2018-05-25 11:20:45,092][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6a62689d{/api,null,AVAILABLE,@Spark}
[INFO][2018-05-25 11:20:45,093][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1849db1a{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-25 11:20:45,094][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1ca25c47{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-25 11:20:45,097][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://10.194.32.157:4040
[INFO][2018-05-25 11:20:45,181][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
[INFO][2018-05-25 11:20:45,201][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 53453.
[INFO][2018-05-25 11:20:45,204][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 10.194.32.157:53453
[INFO][2018-05-25 11:20:45,206][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2018-05-25 11:20:45,209][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 10.194.32.157, 53453, None)
[INFO][2018-05-25 11:20:45,213][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 10.194.32.157:53453 with 912.3 MB RAM, BlockManagerId(driver, 10.194.32.157, 53453, None)
[INFO][2018-05-25 11:20:45,218][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 10.194.32.157, 53453, None)
[INFO][2018-05-25 11:20:45,219][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 10.194.32.157, 53453, None)
[INFO][2018-05-25 11:20:45,428][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@17f460bb{/metrics/json,null,AVAILABLE,@Spark}
[WARN][2018-05-25 11:20:45,862][org.apache.spark.streaming.kafka010.KafkaUtils]overriding enable.auto.commit to false for executor
[WARN][2018-05-25 11:20:45,863][org.apache.spark.streaming.kafka010.KafkaUtils]overriding auto.offset.reset to none for executor
[WARN][2018-05-25 11:20:45,864][org.apache.spark.streaming.kafka010.KafkaUtils]overriding executor group.id to spark-executor-use_a_separate_group_id_for_each_stream
[WARN][2018-05-25 11:20:45,865][org.apache.spark.streaming.kafka010.KafkaUtils]overriding receive.buffer.bytes to 65536 see KAFKA-3135
[INFO][2018-05-25 11:20:45,960][org.apache.spark.streaming.kafka010.DirectKafkaInputDStream]Slide time = 5000 ms
[INFO][2018-05-25 11:20:45,961][org.apache.spark.streaming.kafka010.DirectKafkaInputDStream]Storage level = Serialized 1x Replicated
[INFO][2018-05-25 11:20:45,961][org.apache.spark.streaming.kafka010.DirectKafkaInputDStream]Checkpoint interval = null
[INFO][2018-05-25 11:20:45,962][org.apache.spark.streaming.kafka010.DirectKafkaInputDStream]Remember interval = 5000 ms
[INFO][2018-05-25 11:20:45,962][org.apache.spark.streaming.kafka010.DirectKafkaInputDStream]Initialized and validated org.apache.spark.streaming.kafka010.DirectKafkaInputDStream@5ed2d187
[INFO][2018-05-25 11:20:45,963][org.apache.spark.streaming.dstream.MappedDStream]Slide time = 5000 ms
[INFO][2018-05-25 11:20:45,963][org.apache.spark.streaming.dstream.MappedDStream]Storage level = Serialized 1x Replicated
[INFO][2018-05-25 11:20:45,963][org.apache.spark.streaming.dstream.MappedDStream]Checkpoint interval = null
[INFO][2018-05-25 11:20:45,963][org.apache.spark.streaming.dstream.MappedDStream]Remember interval = 5000 ms
[INFO][2018-05-25 11:20:45,963][org.apache.spark.streaming.dstream.MappedDStream]Initialized and validated org.apache.spark.streaming.dstream.MappedDStream@5431d733
[INFO][2018-05-25 11:20:45,963][org.apache.spark.streaming.dstream.ForEachDStream]Slide time = 5000 ms
[INFO][2018-05-25 11:20:45,963][org.apache.spark.streaming.dstream.ForEachDStream]Storage level = Serialized 1x Replicated
[INFO][2018-05-25 11:20:45,963][org.apache.spark.streaming.dstream.ForEachDStream]Checkpoint interval = null
[INFO][2018-05-25 11:20:45,963][org.apache.spark.streaming.dstream.ForEachDStream]Remember interval = 5000 ms
[INFO][2018-05-25 11:20:45,964][org.apache.spark.streaming.dstream.ForEachDStream]Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@3cb3ce72
[INFO][2018-05-25 11:20:46,038][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	metric.reporters = []
	metadata.max.age.ms = 300000
	partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor]
	reconnect.backoff.ms = 50
	sasl.kerberos.ticket.renew.window.factor = 0.8
	max.partition.fetch.bytes = 1048576
	bootstrap.servers = [vm-xaj-bigdata-da-d01:9092, vm-xaj-bigdata-da-d02:9092, vm-xaj-bigdata-da-d03:9092, vm-xaj-bigdata-da-d04:9092, vm-xaj-bigdata-da-d05:9092, vm-xaj-bigdata-da-d06:9092, vm-xaj-bigdata-da-d07:9092]
	ssl.keystore.type = JKS
	enable.auto.commit = false
	sasl.mechanism = GSSAPI
	interceptor.classes = null
	exclude.internal.topics = true
	ssl.truststore.password = null
	client.id = 
	ssl.endpoint.identification.algorithm = null
	max.poll.records = 2147483647
	check.crcs = true
	request.timeout.ms = 40000
	heartbeat.interval.ms = 3000
	auto.commit.interval.ms = 5000
	receive.buffer.bytes = 65536
	ssl.truststore.type = JKS
	ssl.truststore.location = null
	ssl.keystore.password = null
	fetch.min.bytes = 1
	send.buffer.bytes = 131072
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	group.id = use_a_separate_group_id_for_each_stream
	retry.backoff.ms = 100
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	ssl.trustmanager.algorithm = PKIX
	ssl.key.password = null
	fetch.max.wait.ms = 500
	sasl.kerberos.min.time.before.relogin = 60000
	connections.max.idle.ms = 540000
	session.timeout.ms = 30000
	metrics.num.samples = 2
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	ssl.protocol = TLS
	ssl.provider = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.keystore.location = null
	ssl.cipher.suites = null
	security.protocol = PLAINTEXT
	ssl.keymanager.algorithm = SunX509
	metrics.sample.window.ms = 30000
	auto.offset.reset = latest

[INFO][2018-05-25 11:20:53,230][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	metric.reporters = []
	metadata.max.age.ms = 300000
	partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor]
	reconnect.backoff.ms = 50
	sasl.kerberos.ticket.renew.window.factor = 0.8
	max.partition.fetch.bytes = 1048576
	bootstrap.servers = [vm-xaj-bigdata-da-d01:9092, vm-xaj-bigdata-da-d02:9092, vm-xaj-bigdata-da-d03:9092, vm-xaj-bigdata-da-d04:9092, vm-xaj-bigdata-da-d05:9092, vm-xaj-bigdata-da-d06:9092, vm-xaj-bigdata-da-d07:9092]
	ssl.keystore.type = JKS
	enable.auto.commit = false
	sasl.mechanism = GSSAPI
	interceptor.classes = null
	exclude.internal.topics = true
	ssl.truststore.password = null
	client.id = consumer-1
	ssl.endpoint.identification.algorithm = null
	max.poll.records = 2147483647
	check.crcs = true
	request.timeout.ms = 40000
	heartbeat.interval.ms = 3000
	auto.commit.interval.ms = 5000
	receive.buffer.bytes = 65536
	ssl.truststore.type = JKS
	ssl.truststore.location = null
	ssl.keystore.password = null
	fetch.min.bytes = 1
	send.buffer.bytes = 131072
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	group.id = use_a_separate_group_id_for_each_stream
	retry.backoff.ms = 100
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	ssl.trustmanager.algorithm = PKIX
	ssl.key.password = null
	fetch.max.wait.ms = 500
	sasl.kerberos.min.time.before.relogin = 60000
	connections.max.idle.ms = 540000
	session.timeout.ms = 30000
	metrics.num.samples = 2
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	ssl.protocol = TLS
	ssl.provider = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.keystore.location = null
	ssl.cipher.suites = null
	security.protocol = PLAINTEXT
	ssl.keymanager.algorithm = SunX509
	metrics.sample.window.ms = 30000
	auto.offset.reset = latest

[INFO][2018-05-25 11:20:53,259][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 0.10.0.1
[INFO][2018-05-25 11:20:53,260][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a7a17cdec9eaa6c5
[INFO][2018-05-25 11:20:53,455][org.apache.kafka.clients.consumer.internals.AbstractCoordinator]Discovered coordinator vm-xaj-bigdata-da-d03:9092 (id: 2147483531 rack: null) for group use_a_separate_group_id_for_each_stream.
[INFO][2018-05-25 11:20:53,455][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]Revoking previously assigned partitions [] for group use_a_separate_group_id_for_each_stream
[INFO][2018-05-25 11:20:53,455][org.apache.kafka.clients.consumer.internals.AbstractCoordinator](Re-)joining group use_a_separate_group_id_for_each_stream
[INFO][2018-05-25 11:20:56,523][org.apache.kafka.clients.consumer.internals.AbstractCoordinator]Successfully joined group use_a_separate_group_id_for_each_stream with generation 1
[INFO][2018-05-25 11:20:56,526][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]Setting newly assigned partitions [seven-0] for group use_a_separate_group_id_for_each_stream
[INFO][2018-05-25 11:20:56,569][org.apache.spark.streaming.util.RecurringTimer]Started timer for JobGenerator at time 1527218450000
[INFO][2018-05-25 11:20:56,570][org.apache.spark.streaming.scheduler.JobGenerator]Started JobGenerator at 1527218450000 ms
[INFO][2018-05-25 11:20:56,571][org.apache.spark.streaming.scheduler.JobScheduler]Started JobScheduler
[INFO][2018-05-25 11:20:56,574][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5c645b43{/streaming,null,AVAILABLE,@Spark}
[INFO][2018-05-25 11:20:56,574][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@298d9a05{/streaming/json,null,AVAILABLE,@Spark}
[INFO][2018-05-25 11:20:56,575][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@17814b1c{/streaming/batch,null,AVAILABLE,@Spark}
[INFO][2018-05-25 11:20:56,576][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7bb004b8{/streaming/batch/json,null,AVAILABLE,@Spark}
[INFO][2018-05-25 11:20:56,577][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@29c5ee1d{/static/streaming,null,AVAILABLE,@Spark}
[INFO][2018-05-25 11:20:56,578][org.apache.spark.streaming.StreamingContext]StreamingContext started
[INFO][2018-05-25 11:20:57,153][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218450000 ms
[INFO][2018-05-25 11:20:57,155][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218450000 ms.0 from job set of time 1527218450000 ms
[INFO][2018-05-25 11:20:57,170][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218455000 ms
[INFO][2018-05-25 11:20:57,275][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:20:57,305][org.apache.spark.scheduler.DAGScheduler]Got job 0 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:20:57,306][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 0 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:20:57,307][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:20:57,308][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:20:57,374][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 0 (MapPartitionsRDD[1] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:20:57,495][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-25 11:20:57,620][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0_piece0 stored as bytes in memory (estimated size 1959.0 B, free 912.3 MB)
[INFO][2018-05-25 11:20:57,622][org.apache.spark.storage.BlockManagerInfo]Added broadcast_0_piece0 in memory on 10.194.32.157:53453 (size: 1959.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:20:57,638][org.apache.spark.SparkContext]Created broadcast 0 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:20:57,690][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:20:57,691][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 0.0 with 1 tasks
[INFO][2018-05-25 11:20:57,770][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:20:57,783][org.apache.spark.executor.Executor]Running task 0.0 in stage 0.0 (TID 0)
[INFO][2018-05-25 11:20:57,837][org.apache.spark.streaming.kafka010.KafkaRDD]Beginning offset 12304 is the same as ending offset skipping seven 0
[INFO][2018-05-25 11:20:57,860][org.apache.spark.executor.Executor]Finished task 0.0 in stage 0.0 (TID 0). 751 bytes result sent to driver
[INFO][2018-05-25 11:20:57,868][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 0.0 (TID 0) in 110 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:20:57,873][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:20:57,896][org.apache.spark.scheduler.DAGScheduler]ResultStage 0 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.148 s
[INFO][2018-05-25 11:20:57,913][org.apache.spark.scheduler.DAGScheduler]Job 0 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.637515 s
[INFO][2018-05-25 11:20:57,919][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218450000 ms.0 from job set of time 1527218450000 ms
[INFO][2018-05-25 11:20:57,920][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 7.918 s for time 1527218450000 ms (execution: 0.764 s)
[INFO][2018-05-25 11:20:57,920][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218455000 ms.0 from job set of time 1527218455000 ms
[INFO][2018-05-25 11:20:57,931][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:20:57,934][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:20:57,935][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 
[INFO][2018-05-25 11:20:57,952][org.apache.spark.scheduler.DAGScheduler]Got job 1 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:20:57,953][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 1 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:20:57,953][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:20:57,953][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:20:57,954][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 1 (MapPartitionsRDD[3] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:20:57,958][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-25 11:20:57,965][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1_piece0 stored as bytes in memory (estimated size 1960.0 B, free 912.3 MB)
[INFO][2018-05-25 11:20:57,967][org.apache.spark.storage.BlockManagerInfo]Added broadcast_1_piece0 in memory on 10.194.32.157:53453 (size: 1960.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:20:57,968][org.apache.spark.SparkContext]Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:20:57,971][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:20:57,972][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 1.0 with 1 tasks
[INFO][2018-05-25 11:20:57,975][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:20:57,980][org.apache.spark.executor.Executor]Running task 0.0 in stage 1.0 (TID 1)
[INFO][2018-05-25 11:20:57,986][org.apache.spark.streaming.kafka010.KafkaRDD]Beginning offset 12304 is the same as ending offset skipping seven 0
[INFO][2018-05-25 11:20:57,989][org.apache.spark.executor.Executor]Finished task 0.0 in stage 1.0 (TID 1). 708 bytes result sent to driver
[INFO][2018-05-25 11:20:57,990][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 1.0 (TID 1) in 17 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:20:57,991][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:20:57,999][org.apache.spark.scheduler.DAGScheduler]ResultStage 1 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.026 s
[INFO][2018-05-25 11:20:57,999][org.apache.spark.scheduler.DAGScheduler]Job 1 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.064824 s
[INFO][2018-05-25 11:20:58,000][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218455000 ms.0 from job set of time 1527218455000 ms
[INFO][2018-05-25 11:20:58,001][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 3.000 s for time 1527218455000 ms (execution: 0.080 s)
[INFO][2018-05-25 11:20:58,001][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 1 from persistence list
[INFO][2018-05-25 11:20:58,007][org.apache.spark.storage.BlockManager]Removing RDD 1
[INFO][2018-05-25 11:20:58,007][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 0 from persistence list
[INFO][2018-05-25 11:20:58,008][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:20:58,008][org.apache.spark.storage.BlockManager]Removing RDD 0
[INFO][2018-05-25 11:20:58,008][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 
[INFO][2018-05-25 11:21:00,021][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218460000 ms
[INFO][2018-05-25 11:21:00,022][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218460000 ms.0 from job set of time 1527218460000 ms
[INFO][2018-05-25 11:21:00,030][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:21:00,031][org.apache.spark.scheduler.DAGScheduler]Got job 2 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:21:00,031][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 2 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:21:00,031][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:21:00,031][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:21:00,032][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 2 (MapPartitionsRDD[5] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:21:00,037][org.apache.spark.storage.memory.MemoryStore]Block broadcast_2 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-25 11:21:00,039][org.apache.spark.storage.memory.MemoryStore]Block broadcast_2_piece0 stored as bytes in memory (estimated size 1960.0 B, free 912.3 MB)
[INFO][2018-05-25 11:21:00,040][org.apache.spark.storage.BlockManagerInfo]Added broadcast_2_piece0 in memory on 10.194.32.157:53453 (size: 1960.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:21:00,041][org.apache.spark.SparkContext]Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:21:00,042][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[5] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:21:00,042][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 2.0 with 1 tasks
[INFO][2018-05-25 11:21:00,044][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:21:00,044][org.apache.spark.executor.Executor]Running task 0.0 in stage 2.0 (TID 2)
[INFO][2018-05-25 11:21:00,049][org.apache.spark.streaming.kafka010.KafkaRDD]Beginning offset 12304 is the same as ending offset skipping seven 0
[INFO][2018-05-25 11:21:00,050][org.apache.spark.executor.Executor]Finished task 0.0 in stage 2.0 (TID 2). 708 bytes result sent to driver
[INFO][2018-05-25 11:21:00,051][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 2.0 (TID 2) in 8 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:21:00,052][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:21:00,053][org.apache.spark.scheduler.DAGScheduler]ResultStage 2 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.009 s
[INFO][2018-05-25 11:21:00,053][org.apache.spark.scheduler.DAGScheduler]Job 2 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.022978 s
[INFO][2018-05-25 11:21:00,054][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218460000 ms.0 from job set of time 1527218460000 ms
[INFO][2018-05-25 11:21:00,054][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.054 s for time 1527218460000 ms (execution: 0.032 s)
[INFO][2018-05-25 11:21:00,054][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 3 from persistence list
[INFO][2018-05-25 11:21:00,054][org.apache.spark.storage.BlockManager]Removing RDD 3
[INFO][2018-05-25 11:21:00,055][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 2 from persistence list
[INFO][2018-05-25 11:21:00,055][org.apache.spark.storage.BlockManager]Removing RDD 2
[INFO][2018-05-25 11:21:00,055][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:21:00,055][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218450000 ms
[INFO][2018-05-25 11:21:00,426][org.apache.spark.SparkContext]Running Spark version 2.2.0
[INFO][2018-05-25 11:21:01,640][org.apache.spark.SparkContext]Submitted application: SimulationKafkaSendOutData$
[INFO][2018-05-25 11:21:01,680][org.apache.spark.SecurityManager]Changing view acls to: seven
[INFO][2018-05-25 11:21:01,681][org.apache.spark.SecurityManager]Changing modify acls to: seven
[INFO][2018-05-25 11:21:01,682][org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO][2018-05-25 11:21:01,683][org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO][2018-05-25 11:21:01,684][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(seven); groups with view permissions: Set(); users  with modify permissions: Set(seven); groups with modify permissions: Set()
[INFO][2018-05-25 11:21:02,076][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 53467.
[INFO][2018-05-25 11:21:02,122][org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO][2018-05-25 11:21:02,142][org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO][2018-05-25 11:21:02,145][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2018-05-25 11:21:02,145][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
[INFO][2018-05-25 11:21:02,162][org.apache.spark.storage.DiskBlockManager]Created local directory at /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/blockmgr-6157b73c-ecf3-4a86-9ae7-889da651ccf9
[INFO][2018-05-25 11:21:02,192][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 912.3 MB
[INFO][2018-05-25 11:21:02,313][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO][2018-05-25 11:21:02,463][org.spark_project.jetty.util.log]Logging initialized @2985ms
[INFO][2018-05-25 11:21:02,580][org.spark_project.jetty.server.Server]jetty-9.3.z-SNAPSHOT
[INFO][2018-05-25 11:21:02,610][org.spark_project.jetty.server.Server]Started @3132ms
[WARN][2018-05-25 11:21:02,636][org.apache.spark.util.Utils]Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[INFO][2018-05-25 11:21:02,648][org.spark_project.jetty.server.AbstractConnector]Started ServerConnector@326d21ce{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}
[INFO][2018-05-25 11:21:02,648][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4041.
[INFO][2018-05-25 11:21:02,683][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@180e6ac4{/jobs,null,AVAILABLE,@Spark}
[INFO][2018-05-25 11:21:02,684][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@c7a975a{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2018-05-25 11:21:02,685][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@757d6814{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2018-05-25 11:21:02,686][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4c168660{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2018-05-25 11:21:02,688][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@fd0e5b6{/stages,null,AVAILABLE,@Spark}
[INFO][2018-05-25 11:21:02,691][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@36b0fcd5{/stages/json,null,AVAILABLE,@Spark}
[INFO][2018-05-25 11:21:02,694][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@475835b1{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2018-05-25 11:21:02,695][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@77192705{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-25 11:21:02,696][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7e809b79{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2018-05-25 11:21:02,696][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@625e134e{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2018-05-25 11:21:02,697][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@89c10b7{/storage,null,AVAILABLE,@Spark}
[INFO][2018-05-25 11:21:02,697][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4fe89c24{/storage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-25 11:21:02,699][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3d08f3f5{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2018-05-25 11:21:02,700][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1a1da881{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2018-05-25 11:21:02,701][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7fd4acee{/environment,null,AVAILABLE,@Spark}
[INFO][2018-05-25 11:21:02,703][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6175619b{/environment/json,null,AVAILABLE,@Spark}
[INFO][2018-05-25 11:21:02,706][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@756cf158{/executors,null,AVAILABLE,@Spark}
[INFO][2018-05-25 11:21:02,709][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@751d3241{/executors/json,null,AVAILABLE,@Spark}
[INFO][2018-05-25 11:21:02,710][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@64337702{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2018-05-25 11:21:02,711][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@30ea8c23{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2018-05-25 11:21:02,721][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4e76dac{/static,null,AVAILABLE,@Spark}
[INFO][2018-05-25 11:21:02,722][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@f9d87b{/,null,AVAILABLE,@Spark}
[INFO][2018-05-25 11:21:02,729][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@26fb628{/api,null,AVAILABLE,@Spark}
[INFO][2018-05-25 11:21:02,733][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5ac86ba5{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-25 11:21:02,737][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2c9399a4{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-25 11:21:02,739][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://10.194.32.157:4041
[INFO][2018-05-25 11:21:02,978][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
[INFO][2018-05-25 11:21:03,017][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 53468.
[INFO][2018-05-25 11:21:03,018][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 10.194.32.157:53468
[INFO][2018-05-25 11:21:03,033][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2018-05-25 11:21:03,035][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 10.194.32.157, 53468, None)
[INFO][2018-05-25 11:21:03,039][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 10.194.32.157:53468 with 912.3 MB RAM, BlockManagerId(driver, 10.194.32.157, 53468, None)
[INFO][2018-05-25 11:21:03,053][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 10.194.32.157, 53468, None)
[INFO][2018-05-25 11:21:03,054][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 10.194.32.157, 53468, None)
[INFO][2018-05-25 11:21:03,470][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@405325cf{/metrics/json,null,AVAILABLE,@Spark}
[INFO][2018-05-25 11:21:04,405][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0 stored as values in memory (estimated size 228.1 KB, free 912.1 MB)
[INFO][2018-05-25 11:21:04,466][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0_piece0 stored as bytes in memory (estimated size 22.1 KB, free 912.1 MB)
[INFO][2018-05-25 11:21:04,469][org.apache.spark.storage.BlockManagerInfo]Added broadcast_0_piece0 in memory on 10.194.32.157:53468 (size: 22.1 KB, free: 912.3 MB)
[INFO][2018-05-25 11:21:04,472][org.apache.spark.SparkContext]Created broadcast 0 from textFile at SimulationKafkaSendOutData.scala:25
[INFO][2018-05-25 11:21:05,018][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218465000 ms
[INFO][2018-05-25 11:21:05,019][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218465000 ms.0 from job set of time 1527218465000 ms
[INFO][2018-05-25 11:21:05,029][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:21:05,031][org.apache.spark.scheduler.DAGScheduler]Got job 3 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:21:05,032][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 3 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:21:05,032][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:21:05,032][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:21:05,033][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 3 (MapPartitionsRDD[7] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:21:05,036][org.apache.spark.storage.memory.MemoryStore]Block broadcast_3 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-25 11:21:05,043][org.apache.spark.storage.memory.MemoryStore]Block broadcast_3_piece0 stored as bytes in memory (estimated size 1958.0 B, free 912.3 MB)
[INFO][2018-05-25 11:21:05,044][org.apache.spark.storage.BlockManagerInfo]Added broadcast_3_piece0 in memory on 10.194.32.157:53453 (size: 1958.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:21:05,044][org.apache.spark.SparkContext]Created broadcast 3 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:21:05,045][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[7] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:21:05,046][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 3.0 with 1 tasks
[INFO][2018-05-25 11:21:05,047][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 3.0 (TID 3, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:21:05,047][org.apache.spark.executor.Executor]Running task 0.0 in stage 3.0 (TID 3)
[INFO][2018-05-25 11:21:05,052][org.apache.spark.streaming.kafka010.KafkaRDD]Beginning offset 12304 is the same as ending offset skipping seven 0
[INFO][2018-05-25 11:21:05,053][org.apache.spark.executor.Executor]Finished task 0.0 in stage 3.0 (TID 3). 708 bytes result sent to driver
[INFO][2018-05-25 11:21:05,056][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 3.0 (TID 3) in 9 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:21:05,056][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 3.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:21:05,056][org.apache.spark.scheduler.DAGScheduler]ResultStage 3 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.010 s
[INFO][2018-05-25 11:21:05,057][org.apache.spark.scheduler.DAGScheduler]Job 3 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.026969 s
[INFO][2018-05-25 11:21:05,057][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218465000 ms.0 from job set of time 1527218465000 ms
[INFO][2018-05-25 11:21:05,058][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 5 from persistence list
[INFO][2018-05-25 11:21:05,058][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.057 s for time 1527218465000 ms (execution: 0.038 s)
[INFO][2018-05-25 11:21:05,058][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 4 from persistence list
[INFO][2018-05-25 11:21:05,059][org.apache.spark.storage.BlockManager]Removing RDD 5
[INFO][2018-05-25 11:21:05,059][org.apache.spark.storage.BlockManager]Removing RDD 4
[INFO][2018-05-25 11:21:05,059][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:21:05,060][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218455000 ms
[WARN][2018-05-25 11:21:05,077][org.apache.hadoop.util.NativeCodeLoader]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO][2018-05-25 11:21:05,274][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 1
[INFO][2018-05-25 11:21:05,516][org.apache.spark.SparkContext]Starting job: collect at SimulationKafkaSendOutData.scala:25
[INFO][2018-05-25 11:21:05,538][org.apache.spark.scheduler.DAGScheduler]Got job 0 (collect at SimulationKafkaSendOutData.scala:25) with 2 output partitions
[INFO][2018-05-25 11:21:05,539][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 0 (collect at SimulationKafkaSendOutData.scala:25)
[INFO][2018-05-25 11:21:05,540][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:21:05,541][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:21:05,557][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 0 (hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/* MapPartitionsRDD[1] at textFile at SimulationKafkaSendOutData.scala:25), which has no missing parents
[INFO][2018-05-25 11:21:05,622][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1 stored as values in memory (estimated size 3.3 KB, free 912.1 MB)
[INFO][2018-05-25 11:21:05,625][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1_piece0 stored as bytes in memory (estimated size 1969.0 B, free 912.1 MB)
[INFO][2018-05-25 11:21:05,626][org.apache.spark.storage.BlockManagerInfo]Added broadcast_1_piece0 in memory on 10.194.32.157:53468 (size: 1969.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:21:05,627][org.apache.spark.SparkContext]Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:21:05,644][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ResultStage 0 (hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/* MapPartitionsRDD[1] at textFile at SimulationKafkaSendOutData.scala:25) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2018-05-25 11:21:05,645][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 0.0 with 2 tasks
[INFO][2018-05-25 11:21:05,699][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 4883 bytes)
[INFO][2018-05-25 11:21:05,701][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 4883 bytes)
[INFO][2018-05-25 11:21:05,709][org.apache.spark.executor.Executor]Running task 0.0 in stage 0.0 (TID 0)
[INFO][2018-05-25 11:21:05,710][org.apache.spark.executor.Executor]Running task 1.0 in stage 0.0 (TID 1)
[INFO][2018-05-25 11:21:05,766][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/part-00000:10912993+10912994
[INFO][2018-05-25 11:21:05,768][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/part-00000:0+10912993
[INFO][2018-05-25 11:21:10,021][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218470000 ms
[INFO][2018-05-25 11:21:10,022][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218470000 ms.0 from job set of time 1527218470000 ms
[INFO][2018-05-25 11:21:10,033][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:21:10,035][org.apache.spark.scheduler.DAGScheduler]Got job 4 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:21:10,035][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 4 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:21:10,035][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:21:10,035][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:21:10,036][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 4 (MapPartitionsRDD[9] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:21:10,038][org.apache.spark.storage.memory.MemoryStore]Block broadcast_4 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-25 11:21:10,042][org.apache.spark.storage.memory.MemoryStore]Block broadcast_4_piece0 stored as bytes in memory (estimated size 1960.0 B, free 912.3 MB)
[INFO][2018-05-25 11:21:10,043][org.apache.spark.storage.BlockManagerInfo]Added broadcast_4_piece0 in memory on 10.194.32.157:53453 (size: 1960.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:21:10,043][org.apache.spark.SparkContext]Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:21:10,044][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[9] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:21:10,044][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 4.0 with 1 tasks
[INFO][2018-05-25 11:21:10,045][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 4.0 (TID 4, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:21:10,046][org.apache.spark.executor.Executor]Running task 0.0 in stage 4.0 (TID 4)
[INFO][2018-05-25 11:21:10,049][org.apache.spark.streaming.kafka010.KafkaRDD]Beginning offset 12304 is the same as ending offset skipping seven 0
[INFO][2018-05-25 11:21:10,050][org.apache.spark.executor.Executor]Finished task 0.0 in stage 4.0 (TID 4). 708 bytes result sent to driver
[INFO][2018-05-25 11:21:10,051][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 4.0 (TID 4) in 6 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:21:10,052][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 4.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:21:10,052][org.apache.spark.scheduler.DAGScheduler]ResultStage 4 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.007 s
[INFO][2018-05-25 11:21:10,052][org.apache.spark.scheduler.DAGScheduler]Job 4 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.019275 s
[INFO][2018-05-25 11:21:10,053][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218470000 ms.0 from job set of time 1527218470000 ms
[INFO][2018-05-25 11:21:10,053][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 7 from persistence list
[INFO][2018-05-25 11:21:10,053][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.053 s for time 1527218470000 ms (execution: 0.031 s)
[INFO][2018-05-25 11:21:10,054][org.apache.spark.storage.BlockManager]Removing RDD 7
[INFO][2018-05-25 11:21:10,054][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 6 from persistence list
[INFO][2018-05-25 11:21:10,054][org.apache.spark.storage.BlockManager]Removing RDD 6
[INFO][2018-05-25 11:21:10,054][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:21:10,055][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218460000 ms
[INFO][2018-05-25 11:21:12,779][org.apache.spark.storage.memory.MemoryStore]Block taskresult_1 stored as bytes in memory (estimated size 10.5 MB, free 901.5 MB)
[INFO][2018-05-25 11:21:12,783][org.apache.spark.storage.BlockManagerInfo]Added taskresult_1 in memory on 10.194.32.157:53468 (size: 10.5 MB, free: 901.7 MB)
[INFO][2018-05-25 11:21:12,783][org.apache.spark.executor.Executor]Finished task 1.0 in stage 0.0 (TID 1). 11054080 bytes result sent via BlockManager)
[INFO][2018-05-25 11:21:12,830][org.apache.spark.network.client.TransportClientFactory]Successfully created connection to /10.194.32.157:53468 after 26 ms (0 ms spent in bootstraps)
[INFO][2018-05-25 11:21:13,212][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 0.0 (TID 1) in 7509 ms on localhost (executor driver) (1/2)
[INFO][2018-05-25 11:21:13,215][org.apache.spark.storage.BlockManagerInfo]Removed taskresult_1 on 10.194.32.157:53468 in memory (size: 10.5 MB, free: 912.3 MB)
[INFO][2018-05-25 11:21:13,737][org.apache.spark.storage.memory.MemoryStore]Block taskresult_0 stored as bytes in memory (estimated size 10.5 MB, free 901.5 MB)
[INFO][2018-05-25 11:21:13,739][org.apache.spark.storage.BlockManagerInfo]Added taskresult_0 in memory on 10.194.32.157:53468 (size: 10.5 MB, free: 901.7 MB)
[INFO][2018-05-25 11:21:13,740][org.apache.spark.executor.Executor]Finished task 0.0 in stage 0.0 (TID 0). 11054321 bytes result sent via BlockManager)
[INFO][2018-05-25 11:21:13,823][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 0.0 (TID 0) in 8140 ms on localhost (executor driver) (2/2)
[INFO][2018-05-25 11:21:13,823][org.apache.spark.storage.BlockManagerInfo]Removed taskresult_0 on 10.194.32.157:53468 in memory (size: 10.5 MB, free: 912.3 MB)
[INFO][2018-05-25 11:21:13,824][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:21:13,825][org.apache.spark.scheduler.DAGScheduler]ResultStage 0 (collect at SimulationKafkaSendOutData.scala:25) finished in 8.159 s
[INFO][2018-05-25 11:21:13,830][org.apache.spark.scheduler.DAGScheduler]Job 0 finished: collect at SimulationKafkaSendOutData.scala:25, took 8.313373 s
[INFO][2018-05-25 11:21:14,007][org.spark_project.jetty.server.AbstractConnector]Stopped Spark@326d21ce{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}
[INFO][2018-05-25 11:21:14,009][org.apache.spark.ui.SparkUI]Stopped Spark web UI at http://10.194.32.157:4041
[INFO][2018-05-25 11:21:14,016][org.apache.spark.MapOutputTrackerMasterEndpoint]MapOutputTrackerMasterEndpoint stopped!
[INFO][2018-05-25 11:21:14,029][org.apache.spark.storage.memory.MemoryStore]MemoryStore cleared
[INFO][2018-05-25 11:21:14,029][org.apache.spark.storage.BlockManager]BlockManager stopped
[INFO][2018-05-25 11:21:14,030][org.apache.spark.storage.BlockManagerMaster]BlockManagerMaster stopped
[INFO][2018-05-25 11:21:14,032][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint]OutputCommitCoordinator stopped!
[INFO][2018-05-25 11:21:14,034][org.apache.spark.SparkContext]Successfully stopped SparkContext
[INFO][2018-05-25 11:21:14,050][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	metric.reporters = []
	metadata.max.age.ms = 300000
	reconnect.backoff.ms = 50
	sasl.kerberos.ticket.renew.window.factor = 0.8
	bootstrap.servers = [vm-xaj-bigdata-da-d01:9092, vm-xaj-bigdata-da-d02:9092, vm-xaj-bigdata-da-d03:9092, vm-xaj-bigdata-da-d04:9092, vm-xaj-bigdata-da-d05:9092, vm-xaj-bigdata-da-d06:9092, vm-xaj-bigdata-da-d07:9092]
	ssl.keystore.type = JKS
	sasl.mechanism = GSSAPI
	max.block.ms = 60000
	interceptor.classes = null
	ssl.truststore.password = null
	client.id = 
	ssl.endpoint.identification.algorithm = null
	request.timeout.ms = 30000
	acks = all
	receive.buffer.bytes = 32768
	ssl.truststore.type = JKS
	retries = 0
	ssl.truststore.location = null
	ssl.keystore.password = null
	send.buffer.bytes = 131072
	compression.type = none
	metadata.fetch.timeout.ms = 60000
	retry.backoff.ms = 100
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	buffer.memory = 33554432
	timeout.ms = 30000
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	ssl.trustmanager.algorithm = PKIX
	block.on.buffer.full = false
	ssl.key.password = null
	sasl.kerberos.min.time.before.relogin = 60000
	connections.max.idle.ms = 540000
	max.in.flight.requests.per.connection = 5
	metrics.num.samples = 2
	ssl.protocol = TLS
	ssl.provider = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	batch.size = 16384
	ssl.keystore.location = null
	ssl.cipher.suites = null
	security.protocol = PLAINTEXT
	max.request.size = 1048576
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
	ssl.keymanager.algorithm = SunX509
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	linger.ms = 1

[INFO][2018-05-25 11:21:14,086][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	metric.reporters = []
	metadata.max.age.ms = 300000
	reconnect.backoff.ms = 50
	sasl.kerberos.ticket.renew.window.factor = 0.8
	bootstrap.servers = [vm-xaj-bigdata-da-d01:9092, vm-xaj-bigdata-da-d02:9092, vm-xaj-bigdata-da-d03:9092, vm-xaj-bigdata-da-d04:9092, vm-xaj-bigdata-da-d05:9092, vm-xaj-bigdata-da-d06:9092, vm-xaj-bigdata-da-d07:9092]
	ssl.keystore.type = JKS
	sasl.mechanism = GSSAPI
	max.block.ms = 60000
	interceptor.classes = null
	ssl.truststore.password = null
	client.id = producer-1
	ssl.endpoint.identification.algorithm = null
	request.timeout.ms = 30000
	acks = all
	receive.buffer.bytes = 32768
	ssl.truststore.type = JKS
	retries = 0
	ssl.truststore.location = null
	ssl.keystore.password = null
	send.buffer.bytes = 131072
	compression.type = none
	metadata.fetch.timeout.ms = 60000
	retry.backoff.ms = 100
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	buffer.memory = 33554432
	timeout.ms = 30000
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	ssl.trustmanager.algorithm = PKIX
	block.on.buffer.full = false
	ssl.key.password = null
	sasl.kerberos.min.time.before.relogin = 60000
	connections.max.idle.ms = 540000
	max.in.flight.requests.per.connection = 5
	metrics.num.samples = 2
	ssl.protocol = TLS
	ssl.provider = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	batch.size = 16384
	ssl.keystore.location = null
	ssl.cipher.suites = null
	security.protocol = PLAINTEXT
	max.request.size = 1048576
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
	ssl.keymanager.algorithm = SunX509
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	linger.ms = 1

[INFO][2018-05-25 11:21:14,089][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 0.10.0.1
[INFO][2018-05-25 11:21:14,089][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a7a17cdec9eaa6c5
[INFO][2018-05-25 11:21:15,017][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218475000 ms
[INFO][2018-05-25 11:21:15,018][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218475000 ms.0 from job set of time 1527218475000 ms
[INFO][2018-05-25 11:21:15,025][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:21:15,026][org.apache.spark.scheduler.DAGScheduler]Got job 5 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:21:15,026][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 5 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:21:15,026][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:21:15,027][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:21:15,027][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 5 (MapPartitionsRDD[11] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:21:15,029][org.apache.spark.storage.memory.MemoryStore]Block broadcast_5 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-25 11:21:15,033][org.apache.spark.storage.memory.MemoryStore]Block broadcast_5_piece0 stored as bytes in memory (estimated size 1960.0 B, free 912.3 MB)
[INFO][2018-05-25 11:21:15,034][org.apache.spark.storage.BlockManagerInfo]Added broadcast_5_piece0 in memory on 10.194.32.157:53453 (size: 1960.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:21:15,034][org.apache.spark.SparkContext]Created broadcast 5 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:21:15,035][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[11] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:21:15,035][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 5.0 with 1 tasks
[INFO][2018-05-25 11:21:15,035][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 5.0 (TID 5, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:21:15,036][org.apache.spark.executor.Executor]Running task 0.0 in stage 5.0 (TID 5)
[INFO][2018-05-25 11:21:15,044][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12304 -> 12306
[INFO][2018-05-25 11:21:15,046][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initializing cache 16 64 0.75
[INFO][2018-05-25 11:21:15,048][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Cache miss for CacheKey(spark-executor-use_a_separate_group_id_for_each_stream,seven,0)
[INFO][2018-05-25 11:21:15,050][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	metric.reporters = []
	metadata.max.age.ms = 300000
	partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor]
	reconnect.backoff.ms = 50
	sasl.kerberos.ticket.renew.window.factor = 0.8
	max.partition.fetch.bytes = 1048576
	bootstrap.servers = [vm-xaj-bigdata-da-d01:9092, vm-xaj-bigdata-da-d02:9092, vm-xaj-bigdata-da-d03:9092, vm-xaj-bigdata-da-d04:9092, vm-xaj-bigdata-da-d05:9092, vm-xaj-bigdata-da-d06:9092, vm-xaj-bigdata-da-d07:9092]
	ssl.keystore.type = JKS
	enable.auto.commit = false
	sasl.mechanism = GSSAPI
	interceptor.classes = null
	exclude.internal.topics = true
	ssl.truststore.password = null
	client.id = 
	ssl.endpoint.identification.algorithm = null
	max.poll.records = 2147483647
	check.crcs = true
	request.timeout.ms = 40000
	heartbeat.interval.ms = 3000
	auto.commit.interval.ms = 5000
	receive.buffer.bytes = 65536
	ssl.truststore.type = JKS
	ssl.truststore.location = null
	ssl.keystore.password = null
	fetch.min.bytes = 1
	send.buffer.bytes = 131072
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	group.id = spark-executor-use_a_separate_group_id_for_each_stream
	retry.backoff.ms = 100
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	ssl.trustmanager.algorithm = PKIX
	ssl.key.password = null
	fetch.max.wait.ms = 500
	sasl.kerberos.min.time.before.relogin = 60000
	connections.max.idle.ms = 540000
	session.timeout.ms = 30000
	metrics.num.samples = 2
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	ssl.protocol = TLS
	ssl.provider = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.keystore.location = null
	ssl.cipher.suites = null
	security.protocol = PLAINTEXT
	ssl.keymanager.algorithm = SunX509
	metrics.sample.window.ms = 30000
	auto.offset.reset = none

[INFO][2018-05-25 11:21:15,052][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	metric.reporters = []
	metadata.max.age.ms = 300000
	partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor]
	reconnect.backoff.ms = 50
	sasl.kerberos.ticket.renew.window.factor = 0.8
	max.partition.fetch.bytes = 1048576
	bootstrap.servers = [vm-xaj-bigdata-da-d01:9092, vm-xaj-bigdata-da-d02:9092, vm-xaj-bigdata-da-d03:9092, vm-xaj-bigdata-da-d04:9092, vm-xaj-bigdata-da-d05:9092, vm-xaj-bigdata-da-d06:9092, vm-xaj-bigdata-da-d07:9092]
	ssl.keystore.type = JKS
	enable.auto.commit = false
	sasl.mechanism = GSSAPI
	interceptor.classes = null
	exclude.internal.topics = true
	ssl.truststore.password = null
	client.id = consumer-2
	ssl.endpoint.identification.algorithm = null
	max.poll.records = 2147483647
	check.crcs = true
	request.timeout.ms = 40000
	heartbeat.interval.ms = 3000
	auto.commit.interval.ms = 5000
	receive.buffer.bytes = 65536
	ssl.truststore.type = JKS
	ssl.truststore.location = null
	ssl.keystore.password = null
	fetch.min.bytes = 1
	send.buffer.bytes = 131072
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	group.id = spark-executor-use_a_separate_group_id_for_each_stream
	retry.backoff.ms = 100
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	ssl.trustmanager.algorithm = PKIX
	ssl.key.password = null
	fetch.max.wait.ms = 500
	sasl.kerberos.min.time.before.relogin = 60000
	connections.max.idle.ms = 540000
	session.timeout.ms = 30000
	metrics.num.samples = 2
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	ssl.protocol = TLS
	ssl.provider = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.keystore.location = null
	ssl.cipher.suites = null
	security.protocol = PLAINTEXT
	ssl.keymanager.algorithm = SunX509
	metrics.sample.window.ms = 30000
	auto.offset.reset = none

[INFO][2018-05-25 11:21:15,054][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 0.10.0.1
[INFO][2018-05-25 11:21:15,054][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a7a17cdec9eaa6c5
[INFO][2018-05-25 11:21:15,056][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 12304
[INFO][2018-05-25 11:21:15,173][org.apache.kafka.clients.consumer.internals.AbstractCoordinator]Discovered coordinator vm-xaj-bigdata-da-d06:9092 (id: 2147483530 rack: null) for group spark-executor-use_a_separate_group_id_for_each_stream.
[INFO][2018-05-25 11:21:15,211][org.apache.spark.executor.Executor]Finished task 0.0 in stage 5.0 (TID 5). 969 bytes result sent to driver
[INFO][2018-05-25 11:21:15,212][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 5.0 (TID 5) in 177 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:21:15,213][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 5.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:21:15,213][org.apache.spark.scheduler.DAGScheduler]ResultStage 5 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.178 s
[INFO][2018-05-25 11:21:15,213][org.apache.spark.scheduler.DAGScheduler]Job 5 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.187837 s
[INFO][2018-05-25 11:21:15,224][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:73
[INFO][2018-05-25 11:21:15,225][org.apache.spark.scheduler.DAGScheduler]Got job 6 (foreachPartition at ReceiveKafkaData.scala:73) with 1 output partitions
[INFO][2018-05-25 11:21:15,225][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 6 (foreachPartition at ReceiveKafkaData.scala:73)
[INFO][2018-05-25 11:21:15,225][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:21:15,226][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:21:15,226][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 6 (MapPartitionsRDD[11] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:21:15,228][org.apache.spark.storage.memory.MemoryStore]Block broadcast_6 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-25 11:21:15,232][org.apache.spark.storage.memory.MemoryStore]Block broadcast_6_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.3 MB)
[INFO][2018-05-25 11:21:15,232][org.apache.spark.storage.BlockManagerInfo]Added broadcast_6_piece0 in memory on 10.194.32.157:53453 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:21:15,233][org.apache.spark.SparkContext]Created broadcast 6 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:21:15,234][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[11] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:21:15,234][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 6.0 with 1 tasks
[INFO][2018-05-25 11:21:15,235][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 6.0 (TID 6, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:21:15,235][org.apache.spark.executor.Executor]Running task 0.0 in stage 6.0 (TID 6)
[INFO][2018-05-25 11:21:15,238][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12304 -> 12306
[INFO][2018-05-25 11:21:15,238][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 12304
[INFO][2018-05-25 11:21:15,433][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_5_piece0 on 10.194.32.157:53453 in memory (size: 1960.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:21:15,435][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_3_piece0 on 10.194.32.157:53453 in memory (size: 1958.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:21:15,436][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_4_piece0 on 10.194.32.157:53453 in memory (size: 1960.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:21:15,438][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_1_piece0 on 10.194.32.157:53453 in memory (size: 1960.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:21:15,439][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_0_piece0 on 10.194.32.157:53453 in memory (size: 1959.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:21:15,440][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_2_piece0 on 10.194.32.157:53453 in memory (size: 1960.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:21:15,593][org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper]Process identifier=hconnection-0x66a480f0 connecting to ZooKeeper ensemble=vm-xaj-bigdata-da-d01:2181,vm-xaj-bigdata-da-d02:2181,vm-xaj-bigdata-da-d03:2181
[INFO][2018-05-25 11:21:15,604][org.apache.zookeeper.ZooKeeper]Client environment:zookeeper.version=3.4.5-cdh5.7.0--1, built on 04/06/2016 14:24 GMT
[INFO][2018-05-25 11:21:15,604][org.apache.zookeeper.ZooKeeper]Client environment:host.name=10.194.32.157
[INFO][2018-05-25 11:21:15,604][org.apache.zookeeper.ZooKeeper]Client environment:java.version=1.8.0_161
[INFO][2018-05-25 11:21:15,604][org.apache.zookeeper.ZooKeeper]Client environment:java.vendor=Oracle Corporation
[INFO][2018-05-25 11:21:15,604][org.apache.zookeeper.ZooKeeper]Client environment:java.home=/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre
[INFO][2018-05-25 11:21:15,604][org.apache.zookeeper.ZooKeeper]Client environment:java.class.path=/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/charsets.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/deploy.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/cldrdata.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/dnsns.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/jaccess.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/jfxrt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/localedata.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/nashorn.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/sunec.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/sunjce_provider.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/sunpkcs11.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/zipfs.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/javaws.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/jce.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/jfr.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/jfxswt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/jsse.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/management-agent.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/plugin.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/resources.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/rt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/ant-javafx.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/dt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/javafx-mx.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/jconsole.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/packager.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/sa-jdi.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/tools.jar:/Users/seven/project/github/dataMining/target/classes:/Users/seven/software/maven/repository/org/scala-lang/scala-library/2.11.12/scala-library-2.11.12.jar:/Users/seven/software/maven/repository/org/scala-lang/scala-compiler/2.11.12/scala-compiler-2.11.12.jar:/Users/seven/software/maven/repository/org/scala-lang/modules/scala-xml_2.11/1.0.5/scala-xml_2.11-1.0.5.jar:/Users/seven/software/maven/repository/org/scala-lang/modules/scala-parser-combinators_2.11/1.0.4/scala-parser-combinators_2.11-1.0.4.jar:/Users/seven/software/maven/repository/org/scala-lang/scala-reflect/2.11.12/scala-reflect-2.11.12.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-client/2.6.0-cdh5.7.0/hadoop-client-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-common/2.6.0-cdh5.7.0/hadoop-common-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/xmlenc/xmlenc/0.52/xmlenc-0.52.jar:/Users/seven/software/maven/repository/commons-net/commons-net/3.1/commons-net-3.1.jar:/Users/seven/software/maven/repository/commons-configuration/commons-configuration/1.6/commons-configuration-1.6.jar:/Users/seven/software/maven/repository/commons-digester/commons-digester/1.8/commons-digester-1.8.jar:/Users/seven/software/maven/repository/commons-beanutils/commons-beanutils/1.7.0/commons-beanutils-1.7.0.jar:/Users/seven/software/maven/repository/org/slf4j/slf4j-api/1.7.5/slf4j-api-1.7.5.jar:/Users/seven/software/maven/repository/org/slf4j/slf4j-log4j12/1.7.5/slf4j-log4j12-1.7.5.jar:/Users/seven/software/maven/repository/org/apache/avro/avro/1.7.6-cdh5.7.0/avro-1.7.6-cdh5.7.0.jar:/Users/seven/software/maven/repository/com/google/code/gson/gson/2.2.4/gson-2.2.4.jar:/Users/seven/software/maven/repository/org/apache/curator/curator-client/2.7.1/curator-client-2.7.1.jar:/Users/seven/software/maven/repository/org/apache/curator/curator-recipes/2.7.1/curator-recipes-2.7.1.jar:/Users/seven/software/maven/repository/com/google/code/findbugs/jsr305/3.0.0/jsr305-3.0.0.jar:/Users/seven/software/maven/repository/org/apache/htrace/htrace-core4/4.0.1-incubating/htrace-core4-4.0.1-incubating.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-compress/1.4.1/commons-compress-1.4.1.jar:/Users/seven/software/maven/repository/org/tukaani/xz/1.0/xz-1.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-hdfs/2.6.0-cdh5.7.0/hadoop-hdfs-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/io/netty/netty/3.6.2.Final/netty-3.6.2.Final.jar:/Users/seven/software/maven/repository/xerces/xercesImpl/2.9.1/xercesImpl-2.9.1.jar:/Users/seven/software/maven/repository/xml-apis/xml-apis/1.3.04/xml-apis-1.3.04.jar:/Users/seven/software/maven/repository/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-app/2.6.0-cdh5.7.0/hadoop-mapreduce-client-app-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-common/2.6.0-cdh5.7.0/hadoop-mapreduce-client-common-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-yarn-client/2.6.0-cdh5.7.0/hadoop-yarn-client-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-yarn-server-common/2.6.0-cdh5.7.0/hadoop-yarn-server-common-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-shuffle/2.6.0-cdh5.7.0/hadoop-mapreduce-client-shuffle-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-yarn-api/2.6.0-cdh5.7.0/hadoop-yarn-api-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-core/2.6.0-cdh5.7.0/hadoop-mapreduce-client-core-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-yarn-common/2.6.0-cdh5.7.0/hadoop-yarn-common-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/javax/xml/bind/jaxb-api/2.2.2/jaxb-api-2.2.2.jar:/Users/seven/software/maven/repository/javax/xml/stream/stax-api/1.0-2/stax-api-1.0-2.jar:/Users/seven/software/maven/repository/javax/activation/activation/1.1/activation-1.1.jar:/Users/seven/software/maven/repository/com/sun/jersey/jersey-client/1.9/jersey-client-1.9.jar:/Users/seven/software/maven/repository/org/codehaus/jackson/jackson-xc/1.8.8/jackson-xc-1.8.8.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.6.0-cdh5.7.0/hadoop-mapreduce-client-jobclient-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-aws/2.6.0-cdh5.7.0/hadoop-aws-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/com/amazonaws/aws-java-sdk-s3/1.10.6/aws-java-sdk-s3-1.10.6.jar:/Users/seven/software/maven/repository/com/amazonaws/aws-java-sdk-kms/1.10.6/aws-java-sdk-kms-1.10.6.jar:/Users/seven/software/maven/repository/com/amazonaws/aws-java-sdk-core/1.10.6/aws-java-sdk-core-1.10.6.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-annotations/2.6.0-cdh5.7.0/hadoop-annotations-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-client/1.2.0-cdh5.7.0/hbase-client-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-annotations/1.2.0-cdh5.7.0/hbase-annotations-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-common/1.2.0-cdh5.7.0/hbase-common-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-protocol/1.2.0-cdh5.7.0/hbase-protocol-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/commons-codec/commons-codec/1.9/commons-codec-1.9.jar:/Users/seven/software/maven/repository/commons-io/commons-io/2.4/commons-io-2.4.jar:/Users/seven/software/maven/repository/commons-lang/commons-lang/2.6/commons-lang-2.6.jar:/Users/seven/software/maven/repository/commons-logging/commons-logging/1.2/commons-logging-1.2.jar:/Users/seven/software/maven/repository/com/google/guava/guava/12.0.1/guava-12.0.1.jar:/Users/seven/software/maven/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/Users/seven/software/maven/repository/io/netty/netty-all/4.0.23.Final/netty-all-4.0.23.Final.jar:/Users/seven/software/maven/repository/org/apache/zookeeper/zookeeper/3.4.5-cdh5.7.0/zookeeper-3.4.5-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/htrace/htrace-core/3.2.0-incubating/htrace-core-3.2.0-incubating.jar:/Users/seven/software/maven/repository/org/codehaus/jackson/jackson-mapper-asl/1.8.8/jackson-mapper-asl-1.8.8.jar:/Users/seven/software/maven/repository/org/jruby/jcodings/jcodings/1.0.8/jcodings-1.0.8.jar:/Users/seven/software/maven/repository/org/jruby/joni/joni/2.1.2/joni-2.1.2.jar:/Users/seven/software/maven/repository/com/yammer/metrics/metrics-core/2.2.0/metrics-core-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-auth/2.6.0-cdh5.7.0/hadoop-auth-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/httpcomponents/httpclient/4.2.5/httpclient-4.2.5.jar:/Users/seven/software/maven/repository/org/apache/httpcomponents/httpcore/4.2.4/httpcore-4.2.4.jar:/Users/seven/software/maven/repository/org/apache/directory/server/apacheds-kerberos-codec/2.0.0-M15/apacheds-kerberos-codec-2.0.0-M15.jar:/Users/seven/software/maven/repository/org/apache/directory/server/apacheds-i18n/2.0.0-M15/apacheds-i18n-2.0.0-M15.jar:/Users/seven/software/maven/repository/org/apache/directory/api/api-asn1-api/1.0.0-M20/api-asn1-api-1.0.0-M20.jar:/Users/seven/software/maven/repository/org/apache/directory/api/api-util/1.0.0-M20/api-util-1.0.0-M20.jar:/Users/seven/software/maven/repository/org/apache/curator/curator-framework/2.7.1/curator-framework-2.7.1.jar:/Users/seven/software/maven/repository/com/github/stephenc/findbugs/findbugs-annotations/1.3.9-1/findbugs-annotations-1.3.9-1.jar:/Users/seven/software/maven/repository/junit/junit/4.12/junit-4.12.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-server/1.2.0-cdh5.7.0/hbase-server-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-procedure/1.2.0-cdh5.7.0/hbase-procedure-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-common/1.2.0-cdh5.7.0/hbase-common-1.2.0-cdh5.7.0-tests.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-prefix-tree/1.2.0-cdh5.7.0/hbase-prefix-tree-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar:/Users/seven/software/maven/repository/commons-collections/commons-collections/3.2.2/commons-collections-3.2.2.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-hadoop-compat/1.2.0-cdh5.7.0/hbase-hadoop-compat-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-hadoop2-compat/1.2.0-cdh5.7.0/hbase-hadoop2-compat-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/com/sun/jersey/jersey-core/1.9/jersey-core-1.9.jar:/Users/seven/software/maven/repository/com/sun/jersey/jersey-server/1.9/jersey-server-1.9.jar:/Users/seven/software/maven/repository/asm/asm/3.1/asm-3.1.jar:/Users/seven/software/maven/repository/commons-cli/commons-cli/1.2/commons-cli-1.2.jar:/Users/seven/software/maven/repository/com/github/stephenc/high-scale-lib/high-scale-lib/1.1.1/high-scale-lib-1.1.1.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-math/2.1/commons-math-2.1.jar:/Users/seven/software/maven/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jetty/6.1.26.cloudera.4/jetty-6.1.26.cloudera.4.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jetty-util/6.1.26.cloudera.4/jetty-util-6.1.26.cloudera.4.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jetty-sslengine/6.1.26.cloudera.4/jetty-sslengine-6.1.26.cloudera.4.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jsp-2.1/6.1.14/jsp-2.1-6.1.14.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jsp-api-2.1/6.1.14/jsp-api-2.1-6.1.14.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/servlet-api-2.5/6.1.14/servlet-api-2.5-6.1.14.jar:/Users/seven/software/maven/repository/org/codehaus/jackson/jackson-core-asl/1.8.8/jackson-core-asl-1.8.8.jar:/Users/seven/software/maven/repository/org/codehaus/jackson/jackson-jaxrs/1.8.8/jackson-jaxrs-1.8.8.jar:/Users/seven/software/maven/repository/tomcat/jasper-compiler/5.5.23/jasper-compiler-5.5.23.jar:/Users/seven/software/maven/repository/tomcat/jasper-runtime/5.5.23/jasper-runtime-5.5.23.jar:/Users/seven/software/maven/repository/commons-el/commons-el/1.0/commons-el-1.0.jar:/Users/seven/software/maven/repository/org/jamon/jamon-runtime/2.4.1/jamon-runtime-2.4.1.jar:/Users/seven/software/maven/repository/com/lmax/disruptor/3.3.0/disruptor-3.3.0.jar:/Users/seven/software/maven/repository/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar:/Users/seven/software/maven/repository/org/owasp/esapi/esapi/2.1.0/esapi-2.1.0.jar:/Users/seven/software/maven/repository/commons-beanutils/commons-beanutils-core/1.7.0/commons-beanutils-core-1.7.0.jar:/Users/seven/software/maven/repository/commons-fileupload/commons-fileupload/1.2/commons-fileupload-1.2.jar:/Users/seven/software/maven/repository/xom/xom/1.2.5/xom-1.2.5.jar:/Users/seven/software/maven/repository/xalan/xalan/2.7.0/xalan-2.7.0.jar:/Users/seven/software/maven/repository/org/beanshell/bsh-core/2.0b4/bsh-core-2.0b4.jar:/Users/seven/software/maven/repository/org/owasp/antisamy/antisamy/1.4.3/antisamy-1.4.3.jar:/Users/seven/software/maven/repository/org/apache/xmlgraphics/batik-css/1.7/batik-css-1.7.jar:/Users/seven/software/maven/repository/org/apache/xmlgraphics/batik-ext/1.7/batik-ext-1.7.jar:/Users/seven/software/maven/repository/org/apache/xmlgraphics/batik-util/1.7/batik-util-1.7.jar:/Users/seven/software/maven/repository/xml-apis/xml-apis-ext/1.3.04/xml-apis-ext-1.3.04.jar:/Users/seven/software/maven/repository/net/sourceforge/nekohtml/nekohtml/1.9.12/nekohtml-1.9.12.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-core/2.6.0-mr1-cdh5.7.0/hadoop-core-2.6.0-mr1-cdh5.7.0.jar:/Users/seven/software/maven/repository/javax/servlet/jsp/jsp-api/2.1/jsp-api-2.1.jar:/Users/seven/software/maven/repository/net/java/dev/jets3t/jets3t/0.6.1/jets3t-0.6.1.jar:/Users/seven/software/maven/repository/javax/servlet/servlet-api/2.5/servlet-api-2.5.jar:/Users/seven/software/maven/repository/hsqldb/hsqldb/1.8.0.10/hsqldb-1.8.0.10.jar:/Users/seven/software/maven/repository/org/eclipse/jdt/core/3.1.1/core-3.1.1.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-hdfs/2.6.0-cdh5.7.0/hadoop-hdfs-2.6.0-cdh5.7.0-tests.jar:/Users/seven/software/maven/repository/commons-daemon/commons-daemon/1.0.13/commons-daemon-1.0.13.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-lang3/3.5/commons-lang3-3.5.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-pool2/2.4.2/commons-pool2-2.4.2.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-mllib_2.11/2.2.0/spark-mllib_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-core_2.11/2.2.0/spark-core_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/avro/avro-mapred/1.7.7/avro-mapred-1.7.7-hadoop2.jar:/Users/seven/software/maven/repository/org/apache/avro/avro-ipc/1.7.7/avro-ipc-1.7.7-tests.jar:/Users/seven/software/maven/repository/com/twitter/chill_2.11/0.8.0/chill_2.11-0.8.0.jar:/Users/seven/software/maven/repository/com/esotericsoftware/kryo-shaded/3.0.3/kryo-shaded-3.0.3.jar:/Users/seven/software/maven/repository/com/esotericsoftware/minlog/1.3.0/minlog-1.3.0.jar:/Users/seven/software/maven/repository/org/objenesis/objenesis/2.1/objenesis-2.1.jar:/Users/seven/software/maven/repository/com/twitter/chill-java/0.8.0/chill-java-0.8.0.jar:/Users/seven/software/maven/repository/org/apache/xbean/xbean-asm5-shaded/4.4/xbean-asm5-shaded-4.4.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-launcher_2.11/2.2.0/spark-launcher_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-network-common_2.11/2.2.0/spark-network-common_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-network-shuffle_2.11/2.2.0/spark-network-shuffle_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-unsafe_2.11/2.2.0/spark-unsafe_2.11-2.2.0.jar:/Users/seven/software/maven/repository/javax/servlet/javax.servlet-api/3.1.0/javax.servlet-api-3.1.0.jar:/Users/seven/software/maven/repository/org/slf4j/jul-to-slf4j/1.7.16/jul-to-slf4j-1.7.16.jar:/Users/seven/software/maven/repository/org/slf4j/jcl-over-slf4j/1.7.16/jcl-over-slf4j-1.7.16.jar:/Users/seven/software/maven/repository/com/ning/compress-lzf/1.0.3/compress-lzf-1.0.3.jar:/Users/seven/software/maven/repository/org/xerial/snappy/snappy-java/1.1.2.6/snappy-java-1.1.2.6.jar:/Users/seven/software/maven/repository/org/roaringbitmap/RoaringBitmap/0.5.11/RoaringBitmap-0.5.11.jar:/Users/seven/software/maven/repository/org/json4s/json4s-jackson_2.11/3.2.11/json4s-jackson_2.11-3.2.11.jar:/Users/seven/software/maven/repository/org/json4s/json4s-core_2.11/3.2.11/json4s-core_2.11-3.2.11.jar:/Users/seven/software/maven/repository/org/json4s/json4s-ast_2.11/3.2.11/json4s-ast_2.11-3.2.11.jar:/Users/seven/software/maven/repository/org/scala-lang/scalap/2.11.0/scalap-2.11.0.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/core/jersey-client/2.22.2/jersey-client-2.22.2.jar:/Users/seven/software/maven/repository/javax/ws/rs/javax.ws.rs-api/2.0.1/javax.ws.rs-api-2.0.1.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/hk2-api/2.4.0-b34/hk2-api-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/hk2-utils/2.4.0-b34/hk2-utils-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/external/aopalliance-repackaged/2.4.0-b34/aopalliance-repackaged-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/external/javax.inject/2.4.0-b34/javax.inject-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/hk2-locator/2.4.0-b34/hk2-locator-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/javassist/javassist/3.18.1-GA/javassist-3.18.1-GA.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/core/jersey-common/2.22.2/jersey-common-2.22.2.jar:/Users/seven/software/maven/repository/javax/annotation/javax.annotation-api/1.2/javax.annotation-api-1.2.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/bundles/repackaged/jersey-guava/2.22.2/jersey-guava-2.22.2.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/osgi-resource-locator/1.0.1/osgi-resource-locator-1.0.1.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/core/jersey-server/2.22.2/jersey-server-2.22.2.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/media/jersey-media-jaxb/2.22.2/jersey-media-jaxb-2.22.2.jar:/Users/seven/software/maven/repository/javax/validation/validation-api/1.1.0.Final/validation-api-1.1.0.Final.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/containers/jersey-container-servlet/2.22.2/jersey-container-servlet-2.22.2.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/containers/jersey-container-servlet-core/2.22.2/jersey-container-servlet-core-2.22.2.jar:/Users/seven/software/maven/repository/com/clearspring/analytics/stream/2.7.0/stream-2.7.0.jar:/Users/seven/software/maven/repository/io/dropwizard/metrics/metrics-core/3.1.2/metrics-core-3.1.2.jar:/Users/seven/software/maven/repository/io/dropwizard/metrics/metrics-jvm/3.1.2/metrics-jvm-3.1.2.jar:/Users/seven/software/maven/repository/io/dropwizard/metrics/metrics-json/3.1.2/metrics-json-3.1.2.jar:/Users/seven/software/maven/repository/io/dropwizard/metrics/metrics-graphite/3.1.2/metrics-graphite-3.1.2.jar:/Users/seven/software/maven/repository/org/apache/ivy/ivy/2.4.0/ivy-2.4.0.jar:/Users/seven/software/maven/repository/oro/oro/2.0.8/oro-2.0.8.jar:/Users/seven/software/maven/repository/net/razorvine/pyrolite/4.13/pyrolite-4.13.jar:/Users/seven/software/maven/repository/net/sf/py4j/py4j/0.10.4/py4j-0.10.4.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-crypto/1.0.0/commons-crypto-1.0.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-streaming_2.11/2.2.0/spark-streaming_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-sql_2.11/2.2.0/spark-sql_2.11-2.2.0.jar:/Users/seven/software/maven/repository/com/univocity/univocity-parsers/2.2.1/univocity-parsers-2.2.1.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-sketch_2.11/2.2.0/spark-sketch_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-catalyst_2.11/2.2.0/spark-catalyst_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/codehaus/janino/janino/3.0.0/janino-3.0.0.jar:/Users/seven/software/maven/repository/org/codehaus/janino/commons-compiler/3.0.0/commons-compiler-3.0.0.jar:/Users/seven/software/maven/repository/org/antlr/antlr4-runtime/4.5.3/antlr4-runtime-4.5.3.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-column/1.8.2/parquet-column-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-common/1.8.2/parquet-common-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-encoding/1.8.2/parquet-encoding-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-hadoop/1.8.2/parquet-hadoop-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-format/2.3.1/parquet-format-2.3.1.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-jackson/1.8.2/parquet-jackson-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-graphx_2.11/2.2.0/spark-graphx_2.11-2.2.0.jar:/Users/seven/software/maven/repository/com/github/fommil/netlib/core/1.1.2/core-1.1.2.jar:/Users/seven/software/maven/repository/net/sourceforge/f2j/arpack_combined_all/0.1/arpack_combined_all-0.1.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-mllib-local_2.11/2.2.0/spark-mllib-local_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/scalanlp/breeze_2.11/0.13.1/breeze_2.11-0.13.1.jar:/Users/seven/software/maven/repository/org/scalanlp/breeze-macros_2.11/0.13.1/breeze-macros_2.11-0.13.1.jar:/Users/seven/software/maven/repository/net/sf/opencsv/opencsv/2.3/opencsv-2.3.jar:/Users/seven/software/maven/repository/com/github/rwl/jtransforms/2.4.0/jtransforms-2.4.0.jar:/Users/seven/software/maven/repository/org/spire-math/spire_2.11/0.13.0/spire_2.11-0.13.0.jar:/Users/seven/software/maven/repository/org/spire-math/spire-macros_2.11/0.13.0/spire-macros_2.11-0.13.0.jar:/Users/seven/software/maven/repository/org/typelevel/machinist_2.11/0.6.1/machinist_2.11-0.6.1.jar:/Users/seven/software/maven/repository/com/chuusai/shapeless_2.11/2.3.2/shapeless_2.11-2.3.2.jar:/Users/seven/software/maven/repository/org/typelevel/macro-compat_2.11/1.1.1/macro-compat_2.11-1.1.1.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-math3/3.4.1/commons-math3-3.4.1.jar:/Users/seven/software/maven/repository/org/jpmml/pmml-model/1.2.15/pmml-model-1.2.15.jar:/Users/seven/software/maven/repository/org/jpmml/pmml-schema/1.2.15/pmml-schema-1.2.15.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-tags_2.11/2.2.0/spark-tags_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-streaming-kafka-0-10_2.11/2.2.0/spark-streaming-kafka-0-10_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/kafka/kafka_2.11/0.10.0.1/kafka_2.11-0.10.0.1.jar:/Users/seven/software/maven/repository/com/101tec/zkclient/0.8/zkclient-0.8.jar:/Users/seven/software/maven/repository/org/apache/kafka/kafka-clients/0.10.0.1/kafka-clients-0.10.0.1.jar:/Users/seven/software/maven/repository/mysql/mysql-connector-java/5.1.31/mysql-connector-java-5.1.31.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/module/jackson-module-scala_2.11/2.6.5/jackson-module-scala_2.11-2.6.5.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/core/jackson-core/2.6.5/jackson-core-2.6.5.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/core/jackson-annotations/2.6.5/jackson-annotations-2.6.5.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/core/jackson-databind/2.6.5/jackson-databind-2.6.5.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/module/jackson-module-paranamer/2.6.5/jackson-module-paranamer-2.6.5.jar:/Users/seven/software/maven/repository/com/thoughtworks/paranamer/paranamer/2.6/paranamer-2.6.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-streaming-flume_2.11/2.2.0/spark-streaming-flume_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/flume/flume-ng-core/1.6.0/flume-ng-core-1.6.0.jar:/Users/seven/software/maven/repository/org/apache/flume/flume-ng-configuration/1.6.0/flume-ng-configuration-1.6.0.jar:/Users/seven/software/maven/repository/org/apache/avro/avro-ipc/1.7.4/avro-ipc-1.7.4.jar:/Users/seven/software/maven/repository/org/apache/velocity/velocity/1.7/velocity-1.7.jar:/Users/seven/software/maven/repository/joda-time/joda-time/2.1/joda-time-2.1.jar:/Users/seven/software/maven/repository/org/apache/mina/mina-core/2.0.4/mina-core-2.0.4.jar:/Users/seven/software/maven/repository/org/apache/flume/flume-ng-sdk/1.6.0/flume-ng-sdk-1.6.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-streaming-flume-sink_2.11/2.2.0/spark-streaming-flume-sink_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/flume/flume-ng-clients/flume-ng-log4jappender/1.6.0/flume-ng-log4jappender-1.6.0.jar:/Users/seven/software/maven/repository/net/jpountz/lz4/lz4/1.3.0/lz4-1.3.0.jar:/Applications/IntelliJ IDEA.app/Contents/lib/idea_rt.jar
[INFO][2018-05-25 11:21:15,605][org.apache.zookeeper.ZooKeeper]Client environment:java.library.path=/Users/seven/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:.
[INFO][2018-05-25 11:21:15,605][org.apache.zookeeper.ZooKeeper]Client environment:java.io.tmpdir=/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/
[INFO][2018-05-25 11:21:15,605][org.apache.zookeeper.ZooKeeper]Client environment:java.compiler=<NA>
[INFO][2018-05-25 11:21:15,605][org.apache.zookeeper.ZooKeeper]Client environment:os.name=Mac OS X
[INFO][2018-05-25 11:21:15,605][org.apache.zookeeper.ZooKeeper]Client environment:os.arch=x86_64
[INFO][2018-05-25 11:21:15,605][org.apache.zookeeper.ZooKeeper]Client environment:os.version=10.13.4
[INFO][2018-05-25 11:21:15,605][org.apache.zookeeper.ZooKeeper]Client environment:user.name=seven
[INFO][2018-05-25 11:21:15,605][org.apache.zookeeper.ZooKeeper]Client environment:user.home=/Users/seven
[INFO][2018-05-25 11:21:15,605][org.apache.zookeeper.ZooKeeper]Client environment:user.dir=/Users/seven/project/github/dataMining
[INFO][2018-05-25 11:21:15,606][org.apache.zookeeper.ZooKeeper]Initiating client connection, connectString=vm-xaj-bigdata-da-d01:2181,vm-xaj-bigdata-da-d02:2181,vm-xaj-bigdata-da-d03:2181 sessionTimeout=60000 watcher=hconnection-0x66a480f00x0, quorum=vm-xaj-bigdata-da-d01:2181,vm-xaj-bigdata-da-d02:2181,vm-xaj-bigdata-da-d03:2181, baseZNode=/hbase
[INFO][2018-05-25 11:21:16,632][org.apache.zookeeper.ClientCnxn]Opening socket connection to server master/10.213.4.25:2181. Will not attempt to authenticate using SASL (unknown error)
[INFO][2018-05-25 11:21:16,639][org.apache.zookeeper.ClientCnxn]Socket connection established, initiating session, client: /10.194.32.157:53483, server: master/10.213.4.25:2181
[INFO][2018-05-25 11:21:16,656][org.apache.zookeeper.ClientCnxn]Session establishment complete on server master/10.213.4.25:2181, sessionid = 0x162b4dc560963d2, negotiated timeout = 60000
[WARN][2018-05-25 11:21:17,267][org.apache.hadoop.util.NativeCodeLoader]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO][2018-05-25 11:21:17,877][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-25 11:21:17,880][org.apache.spark.executor.Executor]Finished task 0.0 in stage 6.0 (TID 6). 794 bytes result sent to driver
[INFO][2018-05-25 11:21:17,881][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 6.0 (TID 6) in 2646 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:21:17,881][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 6.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:21:17,882][org.apache.spark.scheduler.DAGScheduler]ResultStage 6 (foreachPartition at ReceiveKafkaData.scala:73) finished in 2.647 s
[INFO][2018-05-25 11:21:17,882][org.apache.spark.scheduler.DAGScheduler]Job 6 finished: foreachPartition at ReceiveKafkaData.scala:73, took 2.657602 s
[INFO][2018-05-25 11:21:17,882][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218475000 ms.0 from job set of time 1527218475000 ms
[INFO][2018-05-25 11:21:17,884][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 2.882 s for time 1527218475000 ms (execution: 2.864 s)
[INFO][2018-05-25 11:21:17,884][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 9 from persistence list
[INFO][2018-05-25 11:21:17,885][org.apache.spark.storage.BlockManager]Removing RDD 9
[INFO][2018-05-25 11:21:17,886][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 8 from persistence list
[INFO][2018-05-25 11:21:17,886][org.apache.spark.storage.BlockManager]Removing RDD 8
[INFO][2018-05-25 11:21:17,886][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:21:17,886][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218465000 ms
[INFO][2018-05-25 11:21:20,021][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218480000 ms
[INFO][2018-05-25 11:21:20,021][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218480000 ms.0 from job set of time 1527218480000 ms
[INFO][2018-05-25 11:21:20,027][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:21:20,028][org.apache.spark.scheduler.DAGScheduler]Got job 7 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:21:20,028][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 7 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:21:20,028][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:21:20,028][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:21:20,028][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 7 (MapPartitionsRDD[13] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:21:20,030][org.apache.spark.storage.memory.MemoryStore]Block broadcast_7 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-25 11:21:20,031][org.apache.spark.storage.memory.MemoryStore]Block broadcast_7_piece0 stored as bytes in memory (estimated size 1972.0 B, free 912.3 MB)
[INFO][2018-05-25 11:21:20,032][org.apache.spark.storage.BlockManagerInfo]Added broadcast_7_piece0 in memory on 10.194.32.157:53453 (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:21:20,035][org.apache.spark.SparkContext]Created broadcast 7 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:21:20,036][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[13] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:21:20,036][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 7.0 with 1 tasks
[INFO][2018-05-25 11:21:20,037][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 7.0 (TID 7, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:21:20,038][org.apache.spark.executor.Executor]Running task 0.0 in stage 7.0 (TID 7)
[INFO][2018-05-25 11:21:20,069][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12306 -> 12316
[INFO][2018-05-25 11:21:20,073][org.apache.spark.executor.Executor]Finished task 0.0 in stage 7.0 (TID 7). 959 bytes result sent to driver
[INFO][2018-05-25 11:21:20,077][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 7.0 (TID 7) in 40 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:21:20,078][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 7.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:21:20,079][org.apache.spark.scheduler.DAGScheduler]ResultStage 7 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.042 s
[INFO][2018-05-25 11:21:20,079][org.apache.spark.scheduler.DAGScheduler]Job 7 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.052096 s
[INFO][2018-05-25 11:21:20,085][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:73
[INFO][2018-05-25 11:21:20,086][org.apache.spark.scheduler.DAGScheduler]Got job 8 (foreachPartition at ReceiveKafkaData.scala:73) with 1 output partitions
[INFO][2018-05-25 11:21:20,086][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 8 (foreachPartition at ReceiveKafkaData.scala:73)
[INFO][2018-05-25 11:21:20,086][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:21:20,086][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:21:20,087][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 8 (MapPartitionsRDD[13] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:21:20,088][org.apache.spark.storage.memory.MemoryStore]Block broadcast_8 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-25 11:21:20,090][org.apache.spark.storage.memory.MemoryStore]Block broadcast_8_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.3 MB)
[INFO][2018-05-25 11:21:20,091][org.apache.spark.storage.BlockManagerInfo]Added broadcast_8_piece0 in memory on 10.194.32.157:53453 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:21:20,091][org.apache.spark.SparkContext]Created broadcast 8 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:21:20,093][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[13] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:21:20,094][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 8.0 with 1 tasks
[INFO][2018-05-25 11:21:20,095][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 8.0 (TID 8, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:21:20,095][org.apache.spark.executor.Executor]Running task 0.0 in stage 8.0 (TID 8)
[INFO][2018-05-25 11:21:20,097][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12306 -> 12316
[INFO][2018-05-25 11:21:20,097][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 12306
[INFO][2018-05-25 11:21:20,132][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-25 11:21:20,133][org.apache.spark.executor.Executor]Finished task 0.0 in stage 8.0 (TID 8). 708 bytes result sent to driver
[INFO][2018-05-25 11:21:20,134][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 8.0 (TID 8) in 39 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:21:20,134][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 8.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:21:20,134][org.apache.spark.scheduler.DAGScheduler]ResultStage 8 (foreachPartition at ReceiveKafkaData.scala:73) finished in 0.040 s
[INFO][2018-05-25 11:21:20,135][org.apache.spark.scheduler.DAGScheduler]Job 8 finished: foreachPartition at ReceiveKafkaData.scala:73, took 0.049307 s
[INFO][2018-05-25 11:21:20,135][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218480000 ms.0 from job set of time 1527218480000 ms
[INFO][2018-05-25 11:21:20,135][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 11 from persistence list
[INFO][2018-05-25 11:21:20,135][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.135 s for time 1527218480000 ms (execution: 0.114 s)
[INFO][2018-05-25 11:21:20,135][org.apache.spark.storage.BlockManager]Removing RDD 11
[INFO][2018-05-25 11:21:20,136][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 10 from persistence list
[INFO][2018-05-25 11:21:20,136][org.apache.spark.storage.BlockManager]Removing RDD 10
[INFO][2018-05-25 11:21:20,136][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:21:20,136][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218470000 ms
[INFO][2018-05-25 11:21:25,015][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218485000 ms
[INFO][2018-05-25 11:21:25,016][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218485000 ms.0 from job set of time 1527218485000 ms
[INFO][2018-05-25 11:21:25,021][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:21:25,022][org.apache.spark.scheduler.DAGScheduler]Got job 9 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:21:25,022][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 9 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:21:25,022][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:21:25,022][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:21:25,022][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 9 (MapPartitionsRDD[15] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:21:25,025][org.apache.spark.storage.memory.MemoryStore]Block broadcast_9 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-25 11:21:25,027][org.apache.spark.storage.memory.MemoryStore]Block broadcast_9_piece0 stored as bytes in memory (estimated size 1972.0 B, free 912.3 MB)
[INFO][2018-05-25 11:21:25,028][org.apache.spark.storage.BlockManagerInfo]Added broadcast_9_piece0 in memory on 10.194.32.157:53453 (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:21:25,028][org.apache.spark.SparkContext]Created broadcast 9 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:21:25,029][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[15] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:21:25,029][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 9.0 with 1 tasks
[INFO][2018-05-25 11:21:25,030][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 9.0 (TID 9, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:21:25,031][org.apache.spark.executor.Executor]Running task 0.0 in stage 9.0 (TID 9)
[INFO][2018-05-25 11:21:25,035][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12316 -> 12326
[INFO][2018-05-25 11:21:25,041][org.apache.spark.executor.Executor]Finished task 0.0 in stage 9.0 (TID 9). 973 bytes result sent to driver
[INFO][2018-05-25 11:21:25,042][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 9.0 (TID 9) in 12 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:21:25,043][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 9.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:21:25,043][org.apache.spark.scheduler.DAGScheduler]ResultStage 9 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.013 s
[INFO][2018-05-25 11:21:25,044][org.apache.spark.scheduler.DAGScheduler]Job 9 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.022622 s
[INFO][2018-05-25 11:21:25,049][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:73
[INFO][2018-05-25 11:21:25,050][org.apache.spark.scheduler.DAGScheduler]Got job 10 (foreachPartition at ReceiveKafkaData.scala:73) with 1 output partitions
[INFO][2018-05-25 11:21:25,050][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 10 (foreachPartition at ReceiveKafkaData.scala:73)
[INFO][2018-05-25 11:21:25,050][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:21:25,050][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:21:25,050][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 10 (MapPartitionsRDD[15] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:21:25,053][org.apache.spark.storage.memory.MemoryStore]Block broadcast_10 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-25 11:21:25,054][org.apache.spark.storage.memory.MemoryStore]Block broadcast_10_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.3 MB)
[INFO][2018-05-25 11:21:25,056][org.apache.spark.storage.BlockManagerInfo]Added broadcast_10_piece0 in memory on 10.194.32.157:53453 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:21:25,056][org.apache.spark.SparkContext]Created broadcast 10 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:21:25,057][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[15] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:21:25,057][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 10.0 with 1 tasks
[INFO][2018-05-25 11:21:25,058][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 10.0 (TID 10, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:21:25,058][org.apache.spark.executor.Executor]Running task 0.0 in stage 10.0 (TID 10)
[INFO][2018-05-25 11:21:25,060][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12316 -> 12326
[INFO][2018-05-25 11:21:25,060][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 12316
[INFO][2018-05-25 11:21:25,103][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-25 11:21:25,103][org.apache.spark.executor.Executor]Finished task 0.0 in stage 10.0 (TID 10). 708 bytes result sent to driver
[INFO][2018-05-25 11:21:25,104][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 10.0 (TID 10) in 47 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:21:25,104][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 10.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:21:25,105][org.apache.spark.scheduler.DAGScheduler]ResultStage 10 (foreachPartition at ReceiveKafkaData.scala:73) finished in 0.047 s
[INFO][2018-05-25 11:21:25,105][org.apache.spark.scheduler.DAGScheduler]Job 10 finished: foreachPartition at ReceiveKafkaData.scala:73, took 0.055761 s
[INFO][2018-05-25 11:21:25,106][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218485000 ms.0 from job set of time 1527218485000 ms
[INFO][2018-05-25 11:21:25,106][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 13 from persistence list
[INFO][2018-05-25 11:21:25,106][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.106 s for time 1527218485000 ms (execution: 0.090 s)
[INFO][2018-05-25 11:21:25,106][org.apache.spark.storage.BlockManager]Removing RDD 13
[INFO][2018-05-25 11:21:25,106][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 12 from persistence list
[INFO][2018-05-25 11:21:25,107][org.apache.spark.storage.BlockManager]Removing RDD 12
[INFO][2018-05-25 11:21:25,107][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:21:25,108][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218475000 ms
[INFO][2018-05-25 11:21:30,029][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218490000 ms
[INFO][2018-05-25 11:21:30,030][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218490000 ms.0 from job set of time 1527218490000 ms
[INFO][2018-05-25 11:21:30,036][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:21:30,037][org.apache.spark.scheduler.DAGScheduler]Got job 11 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:21:30,037][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 11 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:21:30,037][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:21:30,037][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:21:30,038][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 11 (MapPartitionsRDD[17] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:21:30,040][org.apache.spark.storage.memory.MemoryStore]Block broadcast_11 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-25 11:21:30,041][org.apache.spark.storage.memory.MemoryStore]Block broadcast_11_piece0 stored as bytes in memory (estimated size 1969.0 B, free 912.3 MB)
[INFO][2018-05-25 11:21:30,041][org.apache.spark.storage.BlockManagerInfo]Added broadcast_11_piece0 in memory on 10.194.32.157:53453 (size: 1969.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:21:30,042][org.apache.spark.SparkContext]Created broadcast 11 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:21:30,043][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[17] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:21:30,044][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 11.0 with 1 tasks
[INFO][2018-05-25 11:21:30,045][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 11.0 (TID 11, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:21:30,045][org.apache.spark.executor.Executor]Running task 0.0 in stage 11.0 (TID 11)
[INFO][2018-05-25 11:21:30,047][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12326 -> 12336
[INFO][2018-05-25 11:21:30,048][org.apache.spark.executor.Executor]Finished task 0.0 in stage 11.0 (TID 11). 972 bytes result sent to driver
[INFO][2018-05-25 11:21:30,050][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 11.0 (TID 11) in 6 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:21:30,051][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 11.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:21:30,052][org.apache.spark.scheduler.DAGScheduler]ResultStage 11 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.007 s
[INFO][2018-05-25 11:21:30,052][org.apache.spark.scheduler.DAGScheduler]Job 11 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.015647 s
[INFO][2018-05-25 11:21:30,063][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:73
[INFO][2018-05-25 11:21:30,064][org.apache.spark.scheduler.DAGScheduler]Got job 12 (foreachPartition at ReceiveKafkaData.scala:73) with 1 output partitions
[INFO][2018-05-25 11:21:30,064][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 12 (foreachPartition at ReceiveKafkaData.scala:73)
[INFO][2018-05-25 11:21:30,064][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:21:30,065][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:21:30,066][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 12 (MapPartitionsRDD[17] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:21:30,068][org.apache.spark.storage.memory.MemoryStore]Block broadcast_12 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-25 11:21:30,070][org.apache.spark.storage.memory.MemoryStore]Block broadcast_12_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.3 MB)
[INFO][2018-05-25 11:21:30,070][org.apache.spark.storage.BlockManagerInfo]Added broadcast_12_piece0 in memory on 10.194.32.157:53453 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:21:30,071][org.apache.spark.SparkContext]Created broadcast 12 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:21:30,072][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[17] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:21:30,072][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 12.0 with 1 tasks
[INFO][2018-05-25 11:21:30,073][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 12.0 (TID 12, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:21:30,073][org.apache.spark.executor.Executor]Running task 0.0 in stage 12.0 (TID 12)
[INFO][2018-05-25 11:21:30,074][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12326 -> 12336
[INFO][2018-05-25 11:21:30,075][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 12326
[INFO][2018-05-25 11:21:30,111][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-25 11:21:30,112][org.apache.spark.executor.Executor]Finished task 0.0 in stage 12.0 (TID 12). 665 bytes result sent to driver
[INFO][2018-05-25 11:21:30,113][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 12.0 (TID 12) in 41 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:21:30,113][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 12.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:21:30,113][org.apache.spark.scheduler.DAGScheduler]ResultStage 12 (foreachPartition at ReceiveKafkaData.scala:73) finished in 0.041 s
[INFO][2018-05-25 11:21:30,114][org.apache.spark.scheduler.DAGScheduler]Job 12 finished: foreachPartition at ReceiveKafkaData.scala:73, took 0.050364 s
[INFO][2018-05-25 11:21:30,114][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218490000 ms.0 from job set of time 1527218490000 ms
[INFO][2018-05-25 11:21:30,114][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 15 from persistence list
[INFO][2018-05-25 11:21:30,114][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.114 s for time 1527218490000 ms (execution: 0.085 s)
[INFO][2018-05-25 11:21:30,114][org.apache.spark.storage.BlockManager]Removing RDD 15
[INFO][2018-05-25 11:21:30,115][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 14 from persistence list
[INFO][2018-05-25 11:21:30,115][org.apache.spark.storage.BlockManager]Removing RDD 14
[INFO][2018-05-25 11:21:30,115][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:21:30,115][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218480000 ms
[INFO][2018-05-25 11:21:35,021][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218495000 ms
[INFO][2018-05-25 11:21:35,022][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218495000 ms.0 from job set of time 1527218495000 ms
[INFO][2018-05-25 11:21:35,028][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:21:35,028][org.apache.spark.scheduler.DAGScheduler]Got job 13 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:21:35,028][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 13 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:21:35,028][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:21:35,029][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:21:35,029][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 13 (MapPartitionsRDD[19] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:21:35,031][org.apache.spark.storage.memory.MemoryStore]Block broadcast_13 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-25 11:21:35,032][org.apache.spark.storage.memory.MemoryStore]Block broadcast_13_piece0 stored as bytes in memory (estimated size 1972.0 B, free 912.3 MB)
[INFO][2018-05-25 11:21:35,033][org.apache.spark.storage.BlockManagerInfo]Added broadcast_13_piece0 in memory on 10.194.32.157:53453 (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:21:35,033][org.apache.spark.SparkContext]Created broadcast 13 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:21:35,034][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[19] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:21:35,034][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 13.0 with 1 tasks
[INFO][2018-05-25 11:21:35,036][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 13.0 (TID 13, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:21:35,038][org.apache.spark.executor.Executor]Running task 0.0 in stage 13.0 (TID 13)
[INFO][2018-05-25 11:21:35,039][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12336 -> 12345
[INFO][2018-05-25 11:21:35,040][org.apache.spark.executor.Executor]Finished task 0.0 in stage 13.0 (TID 13). 973 bytes result sent to driver
[INFO][2018-05-25 11:21:35,042][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 13.0 (TID 13) in 7 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:21:35,043][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 13.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:21:35,043][org.apache.spark.scheduler.DAGScheduler]ResultStage 13 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.009 s
[INFO][2018-05-25 11:21:35,044][org.apache.spark.scheduler.DAGScheduler]Job 13 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.015704 s
[INFO][2018-05-25 11:21:35,048][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:73
[INFO][2018-05-25 11:21:35,049][org.apache.spark.scheduler.DAGScheduler]Got job 14 (foreachPartition at ReceiveKafkaData.scala:73) with 1 output partitions
[INFO][2018-05-25 11:21:35,049][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 14 (foreachPartition at ReceiveKafkaData.scala:73)
[INFO][2018-05-25 11:21:35,049][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:21:35,049][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:21:35,049][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 14 (MapPartitionsRDD[19] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:21:35,051][org.apache.spark.storage.memory.MemoryStore]Block broadcast_14 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-25 11:21:35,052][org.apache.spark.storage.memory.MemoryStore]Block broadcast_14_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.3 MB)
[INFO][2018-05-25 11:21:35,053][org.apache.spark.storage.BlockManagerInfo]Added broadcast_14_piece0 in memory on 10.194.32.157:53453 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:21:35,054][org.apache.spark.SparkContext]Created broadcast 14 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:21:35,054][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[19] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:21:35,054][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 14.0 with 1 tasks
[INFO][2018-05-25 11:21:35,055][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 14.0 (TID 14, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:21:35,055][org.apache.spark.executor.Executor]Running task 0.0 in stage 14.0 (TID 14)
[INFO][2018-05-25 11:21:35,056][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12336 -> 12345
[INFO][2018-05-25 11:21:35,056][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 12336
[INFO][2018-05-25 11:21:35,086][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-25 11:21:35,088][org.apache.spark.executor.Executor]Finished task 0.0 in stage 14.0 (TID 14). 665 bytes result sent to driver
[INFO][2018-05-25 11:21:35,089][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 14.0 (TID 14) in 33 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:21:35,089][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 14.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:21:35,089][org.apache.spark.scheduler.DAGScheduler]ResultStage 14 (foreachPartition at ReceiveKafkaData.scala:73) finished in 0.035 s
[INFO][2018-05-25 11:21:35,090][org.apache.spark.scheduler.DAGScheduler]Job 14 finished: foreachPartition at ReceiveKafkaData.scala:73, took 0.041019 s
[INFO][2018-05-25 11:21:35,090][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218495000 ms.0 from job set of time 1527218495000 ms
[INFO][2018-05-25 11:21:35,090][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.090 s for time 1527218495000 ms (execution: 0.068 s)
[INFO][2018-05-25 11:21:35,091][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 17 from persistence list
[INFO][2018-05-25 11:21:35,091][org.apache.spark.storage.BlockManager]Removing RDD 17
[INFO][2018-05-25 11:21:35,091][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 16 from persistence list
[INFO][2018-05-25 11:21:35,092][org.apache.spark.storage.BlockManager]Removing RDD 16
[INFO][2018-05-25 11:21:35,092][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:21:35,092][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218485000 ms
[INFO][2018-05-25 11:21:40,020][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218500000 ms
[INFO][2018-05-25 11:21:40,020][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218500000 ms.0 from job set of time 1527218500000 ms
[INFO][2018-05-25 11:21:40,026][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:21:40,027][org.apache.spark.scheduler.DAGScheduler]Got job 15 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:21:40,027][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 15 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:21:40,027][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:21:40,027][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:21:40,027][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 15 (MapPartitionsRDD[21] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:21:40,030][org.apache.spark.storage.memory.MemoryStore]Block broadcast_15 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-25 11:21:40,031][org.apache.spark.storage.memory.MemoryStore]Block broadcast_15_piece0 stored as bytes in memory (estimated size 1973.0 B, free 912.2 MB)
[INFO][2018-05-25 11:21:40,031][org.apache.spark.storage.BlockManagerInfo]Added broadcast_15_piece0 in memory on 10.194.32.157:53453 (size: 1973.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:21:40,032][org.apache.spark.SparkContext]Created broadcast 15 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:21:40,033][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 15 (MapPartitionsRDD[21] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:21:40,034][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 15.0 with 1 tasks
[INFO][2018-05-25 11:21:40,035][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 15.0 (TID 15, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:21:40,035][org.apache.spark.executor.Executor]Running task 0.0 in stage 15.0 (TID 15)
[INFO][2018-05-25 11:21:40,037][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12345 -> 12355
[INFO][2018-05-25 11:21:40,037][org.apache.spark.executor.Executor]Finished task 0.0 in stage 15.0 (TID 15). 968 bytes result sent to driver
[INFO][2018-05-25 11:21:40,038][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 15.0 (TID 15) in 4 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:21:40,038][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 15.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:21:40,039][org.apache.spark.scheduler.DAGScheduler]ResultStage 15 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.005 s
[INFO][2018-05-25 11:21:40,040][org.apache.spark.scheduler.DAGScheduler]Job 15 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.013515 s
[INFO][2018-05-25 11:21:40,045][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:73
[INFO][2018-05-25 11:21:40,045][org.apache.spark.scheduler.DAGScheduler]Got job 16 (foreachPartition at ReceiveKafkaData.scala:73) with 1 output partitions
[INFO][2018-05-25 11:21:40,045][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 16 (foreachPartition at ReceiveKafkaData.scala:73)
[INFO][2018-05-25 11:21:40,045][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:21:40,045][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:21:40,045][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 16 (MapPartitionsRDD[21] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:21:40,047][org.apache.spark.storage.memory.MemoryStore]Block broadcast_16 stored as values in memory (estimated size 3.4 KB, free 912.2 MB)
[INFO][2018-05-25 11:21:40,048][org.apache.spark.storage.memory.MemoryStore]Block broadcast_16_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.2 MB)
[INFO][2018-05-25 11:21:40,049][org.apache.spark.storage.BlockManagerInfo]Added broadcast_16_piece0 in memory on 10.194.32.157:53453 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:21:40,049][org.apache.spark.SparkContext]Created broadcast 16 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:21:40,049][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 16 (MapPartitionsRDD[21] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:21:40,049][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 16.0 with 1 tasks
[INFO][2018-05-25 11:21:40,050][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 16.0 (TID 16, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:21:40,050][org.apache.spark.executor.Executor]Running task 0.0 in stage 16.0 (TID 16)
[INFO][2018-05-25 11:21:40,051][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12345 -> 12355
[INFO][2018-05-25 11:21:40,051][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 12345
[INFO][2018-05-25 11:21:40,084][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-25 11:21:40,103][org.apache.spark.executor.Executor]Finished task 0.0 in stage 16.0 (TID 16). 665 bytes result sent to driver
[INFO][2018-05-25 11:21:40,106][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 16.0 (TID 16) in 56 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:21:40,106][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 16.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:21:40,107][org.apache.spark.scheduler.DAGScheduler]ResultStage 16 (foreachPartition at ReceiveKafkaData.scala:73) finished in 0.057 s
[INFO][2018-05-25 11:21:40,107][org.apache.spark.scheduler.DAGScheduler]Job 16 finished: foreachPartition at ReceiveKafkaData.scala:73, took 0.062566 s
[INFO][2018-05-25 11:21:40,110][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218500000 ms.0 from job set of time 1527218500000 ms
[INFO][2018-05-25 11:21:40,112][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.110 s for time 1527218500000 ms (execution: 0.090 s)
[INFO][2018-05-25 11:21:40,113][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 19 from persistence list
[INFO][2018-05-25 11:21:40,114][org.apache.spark.storage.BlockManager]Removing RDD 19
[INFO][2018-05-25 11:21:40,114][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 18 from persistence list
[INFO][2018-05-25 11:21:40,115][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:21:40,115][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218490000 ms
[INFO][2018-05-25 11:21:40,116][org.apache.spark.storage.BlockManager]Removing RDD 18
[INFO][2018-05-25 11:21:45,019][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218505000 ms
[INFO][2018-05-25 11:21:45,019][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218505000 ms.0 from job set of time 1527218505000 ms
[INFO][2018-05-25 11:21:45,024][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:21:45,025][org.apache.spark.scheduler.DAGScheduler]Got job 17 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:21:45,025][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 17 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:21:45,025][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:21:45,025][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:21:45,025][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 17 (MapPartitionsRDD[23] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:21:45,027][org.apache.spark.storage.memory.MemoryStore]Block broadcast_17 stored as values in memory (estimated size 3.1 KB, free 912.2 MB)
[INFO][2018-05-25 11:21:45,030][org.apache.spark.storage.memory.MemoryStore]Block broadcast_17_piece0 stored as bytes in memory (estimated size 1972.0 B, free 912.2 MB)
[INFO][2018-05-25 11:21:45,030][org.apache.spark.storage.BlockManagerInfo]Added broadcast_17_piece0 in memory on 10.194.32.157:53453 (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:21:45,031][org.apache.spark.SparkContext]Created broadcast 17 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:21:45,031][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[23] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:21:45,031][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 17.0 with 1 tasks
[INFO][2018-05-25 11:21:45,031][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 17.0 (TID 17, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:21:45,032][org.apache.spark.executor.Executor]Running task 0.0 in stage 17.0 (TID 17)
[INFO][2018-05-25 11:21:45,033][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12355 -> 12365
[INFO][2018-05-25 11:21:45,034][org.apache.spark.executor.Executor]Finished task 0.0 in stage 17.0 (TID 17). 975 bytes result sent to driver
[INFO][2018-05-25 11:21:45,035][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 17.0 (TID 17) in 4 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:21:45,035][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 17.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:21:45,035][org.apache.spark.scheduler.DAGScheduler]ResultStage 17 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.004 s
[INFO][2018-05-25 11:21:45,036][org.apache.spark.scheduler.DAGScheduler]Job 17 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.011332 s
[INFO][2018-05-25 11:21:45,042][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:73
[INFO][2018-05-25 11:21:45,042][org.apache.spark.scheduler.DAGScheduler]Got job 18 (foreachPartition at ReceiveKafkaData.scala:73) with 1 output partitions
[INFO][2018-05-25 11:21:45,042][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 18 (foreachPartition at ReceiveKafkaData.scala:73)
[INFO][2018-05-25 11:21:45,042][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:21:45,043][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:21:45,043][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 18 (MapPartitionsRDD[23] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:21:45,045][org.apache.spark.storage.memory.MemoryStore]Block broadcast_18 stored as values in memory (estimated size 3.4 KB, free 912.2 MB)
[INFO][2018-05-25 11:21:45,049][org.apache.spark.storage.memory.MemoryStore]Block broadcast_18_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.2 MB)
[INFO][2018-05-25 11:21:45,049][org.apache.spark.storage.BlockManagerInfo]Added broadcast_18_piece0 in memory on 10.194.32.157:53453 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:21:45,050][org.apache.spark.SparkContext]Created broadcast 18 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:21:45,051][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 18 (MapPartitionsRDD[23] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:21:45,051][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 18.0 with 1 tasks
[INFO][2018-05-25 11:21:45,051][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 18.0 (TID 18, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:21:45,052][org.apache.spark.executor.Executor]Running task 0.0 in stage 18.0 (TID 18)
[INFO][2018-05-25 11:21:45,053][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12355 -> 12365
[INFO][2018-05-25 11:21:45,053][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 12355
[INFO][2018-05-25 11:21:45,080][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-25 11:21:45,081][org.apache.spark.executor.Executor]Finished task 0.0 in stage 18.0 (TID 18). 708 bytes result sent to driver
[INFO][2018-05-25 11:21:45,081][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 18.0 (TID 18) in 30 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:21:45,081][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 18.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:21:45,082][org.apache.spark.scheduler.DAGScheduler]ResultStage 18 (foreachPartition at ReceiveKafkaData.scala:73) finished in 0.030 s
[INFO][2018-05-25 11:21:45,082][org.apache.spark.scheduler.DAGScheduler]Job 18 finished: foreachPartition at ReceiveKafkaData.scala:73, took 0.039775 s
[INFO][2018-05-25 11:21:45,082][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218505000 ms.0 from job set of time 1527218505000 ms
[INFO][2018-05-25 11:21:45,082][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.082 s for time 1527218505000 ms (execution: 0.063 s)
[INFO][2018-05-25 11:21:45,082][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 21 from persistence list
[INFO][2018-05-25 11:21:45,083][org.apache.spark.storage.BlockManager]Removing RDD 21
[INFO][2018-05-25 11:21:45,083][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 20 from persistence list
[INFO][2018-05-25 11:21:45,083][org.apache.spark.storage.BlockManager]Removing RDD 20
[INFO][2018-05-25 11:21:45,083][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:21:45,083][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218495000 ms
[INFO][2018-05-25 11:21:50,025][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218510000 ms
[INFO][2018-05-25 11:21:50,025][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218510000 ms.0 from job set of time 1527218510000 ms
[INFO][2018-05-25 11:21:50,032][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:21:50,032][org.apache.spark.scheduler.DAGScheduler]Got job 19 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:21:50,033][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 19 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:21:50,033][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:21:50,033][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:21:50,033][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 19 (MapPartitionsRDD[25] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:21:50,035][org.apache.spark.storage.memory.MemoryStore]Block broadcast_19 stored as values in memory (estimated size 3.1 KB, free 912.2 MB)
[INFO][2018-05-25 11:21:50,038][org.apache.spark.storage.memory.MemoryStore]Block broadcast_19_piece0 stored as bytes in memory (estimated size 1972.0 B, free 912.2 MB)
[INFO][2018-05-25 11:21:50,039][org.apache.spark.storage.BlockManagerInfo]Added broadcast_19_piece0 in memory on 10.194.32.157:53453 (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:21:50,039][org.apache.spark.SparkContext]Created broadcast 19 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:21:50,040][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 19 (MapPartitionsRDD[25] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:21:50,040][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 19.0 with 1 tasks
[INFO][2018-05-25 11:21:50,040][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 19.0 (TID 19, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:21:50,041][org.apache.spark.executor.Executor]Running task 0.0 in stage 19.0 (TID 19)
[INFO][2018-05-25 11:21:50,042][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12365 -> 12375
[INFO][2018-05-25 11:21:50,043][org.apache.spark.executor.Executor]Finished task 0.0 in stage 19.0 (TID 19). 974 bytes result sent to driver
[INFO][2018-05-25 11:21:50,044][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 19.0 (TID 19) in 4 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:21:50,044][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 19.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:21:50,044][org.apache.spark.scheduler.DAGScheduler]ResultStage 19 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.004 s
[INFO][2018-05-25 11:21:50,044][org.apache.spark.scheduler.DAGScheduler]Job 19 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.012283 s
[INFO][2018-05-25 11:21:50,049][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:73
[INFO][2018-05-25 11:21:50,049][org.apache.spark.scheduler.DAGScheduler]Got job 20 (foreachPartition at ReceiveKafkaData.scala:73) with 1 output partitions
[INFO][2018-05-25 11:21:50,049][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 20 (foreachPartition at ReceiveKafkaData.scala:73)
[INFO][2018-05-25 11:21:50,049][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:21:50,049][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:21:50,050][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 20 (MapPartitionsRDD[25] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:21:50,052][org.apache.spark.storage.memory.MemoryStore]Block broadcast_20 stored as values in memory (estimated size 3.4 KB, free 912.2 MB)
[INFO][2018-05-25 11:21:50,055][org.apache.spark.storage.memory.MemoryStore]Block broadcast_20_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.2 MB)
[INFO][2018-05-25 11:21:50,056][org.apache.spark.storage.BlockManagerInfo]Added broadcast_20_piece0 in memory on 10.194.32.157:53453 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:21:50,056][org.apache.spark.SparkContext]Created broadcast 20 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:21:50,056][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 20 (MapPartitionsRDD[25] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:21:50,056][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 20.0 with 1 tasks
[INFO][2018-05-25 11:21:50,057][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 20.0 (TID 20, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:21:50,058][org.apache.spark.executor.Executor]Running task 0.0 in stage 20.0 (TID 20)
[INFO][2018-05-25 11:21:50,059][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12365 -> 12375
[INFO][2018-05-25 11:21:50,059][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 12365
[INFO][2018-05-25 11:21:50,092][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-25 11:21:50,093][org.apache.spark.executor.Executor]Finished task 0.0 in stage 20.0 (TID 20). 708 bytes result sent to driver
[INFO][2018-05-25 11:21:50,094][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 20.0 (TID 20) in 37 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:21:50,094][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 20.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:21:50,095][org.apache.spark.scheduler.DAGScheduler]ResultStage 20 (foreachPartition at ReceiveKafkaData.scala:73) finished in 0.038 s
[INFO][2018-05-25 11:21:50,095][org.apache.spark.scheduler.DAGScheduler]Job 20 finished: foreachPartition at ReceiveKafkaData.scala:73, took 0.046215 s
[INFO][2018-05-25 11:21:50,096][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218510000 ms.0 from job set of time 1527218510000 ms
[INFO][2018-05-25 11:21:50,096][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.096 s for time 1527218510000 ms (execution: 0.071 s)
[INFO][2018-05-25 11:21:50,096][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 23 from persistence list
[INFO][2018-05-25 11:21:50,096][org.apache.spark.storage.BlockManager]Removing RDD 23
[INFO][2018-05-25 11:21:50,096][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 22 from persistence list
[INFO][2018-05-25 11:21:50,097][org.apache.spark.storage.BlockManager]Removing RDD 22
[INFO][2018-05-25 11:21:50,097][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:21:50,097][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218500000 ms
[INFO][2018-05-25 11:21:55,021][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218515000 ms
[INFO][2018-05-25 11:21:55,021][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218515000 ms.0 from job set of time 1527218515000 ms
[INFO][2018-05-25 11:21:55,029][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:21:55,030][org.apache.spark.scheduler.DAGScheduler]Got job 21 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:21:55,030][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 21 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:21:55,030][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:21:55,030][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:21:55,031][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 21 (MapPartitionsRDD[27] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:21:55,033][org.apache.spark.storage.memory.MemoryStore]Block broadcast_21 stored as values in memory (estimated size 3.1 KB, free 912.2 MB)
[INFO][2018-05-25 11:21:55,036][org.apache.spark.storage.memory.MemoryStore]Block broadcast_21_piece0 stored as bytes in memory (estimated size 1972.0 B, free 912.2 MB)
[INFO][2018-05-25 11:21:55,037][org.apache.spark.storage.BlockManagerInfo]Added broadcast_21_piece0 in memory on 10.194.32.157:53453 (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:21:55,037][org.apache.spark.SparkContext]Created broadcast 21 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:21:55,037][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 21 (MapPartitionsRDD[27] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:21:55,037][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 21.0 with 1 tasks
[INFO][2018-05-25 11:21:55,038][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 21.0 (TID 21, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:21:55,038][org.apache.spark.executor.Executor]Running task 0.0 in stage 21.0 (TID 21)
[INFO][2018-05-25 11:21:55,040][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12375 -> 12385
[INFO][2018-05-25 11:21:55,041][org.apache.spark.executor.Executor]Finished task 0.0 in stage 21.0 (TID 21). 959 bytes result sent to driver
[INFO][2018-05-25 11:21:55,041][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 21.0 (TID 21) in 3 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:21:55,041][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 21.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:21:55,042][org.apache.spark.scheduler.DAGScheduler]ResultStage 21 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.003 s
[INFO][2018-05-25 11:21:55,042][org.apache.spark.scheduler.DAGScheduler]Job 21 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.012412 s
[INFO][2018-05-25 11:21:55,057][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:73
[INFO][2018-05-25 11:21:55,067][org.apache.spark.scheduler.DAGScheduler]Got job 22 (foreachPartition at ReceiveKafkaData.scala:73) with 1 output partitions
[INFO][2018-05-25 11:21:55,067][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 22 (foreachPartition at ReceiveKafkaData.scala:73)
[INFO][2018-05-25 11:21:55,067][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:21:55,067][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:21:55,067][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 22 (MapPartitionsRDD[27] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:21:55,069][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_12_piece0 on 10.194.32.157:53453 in memory (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:21:55,070][org.apache.spark.storage.memory.MemoryStore]Block broadcast_22 stored as values in memory (estimated size 3.4 KB, free 912.2 MB)
[INFO][2018-05-25 11:21:55,070][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_13_piece0 on 10.194.32.157:53453 in memory (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:21:55,071][org.apache.spark.storage.memory.MemoryStore]Block broadcast_22_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.2 MB)
[INFO][2018-05-25 11:21:55,071][org.apache.spark.storage.BlockManagerInfo]Added broadcast_22_piece0 in memory on 10.194.32.157:53453 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:21:55,072][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_17_piece0 on 10.194.32.157:53453 in memory (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:21:55,072][org.apache.spark.SparkContext]Created broadcast 22 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:21:55,072][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 22 (MapPartitionsRDD[27] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:21:55,072][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 22.0 with 1 tasks
[INFO][2018-05-25 11:21:55,073][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 22.0 (TID 22, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:21:55,073][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_11_piece0 on 10.194.32.157:53453 in memory (size: 1969.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:21:55,073][org.apache.spark.executor.Executor]Running task 0.0 in stage 22.0 (TID 22)
[INFO][2018-05-25 11:21:55,075][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12375 -> 12385
[INFO][2018-05-25 11:21:55,075][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 12375
[INFO][2018-05-25 11:21:55,076][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_14_piece0 on 10.194.32.157:53453 in memory (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:21:55,077][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_16_piece0 on 10.194.32.157:53453 in memory (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:21:55,079][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_15_piece0 on 10.194.32.157:53453 in memory (size: 1973.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:21:55,080][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_10_piece0 on 10.194.32.157:53453 in memory (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:21:55,081][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_19_piece0 on 10.194.32.157:53453 in memory (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:21:55,082][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_20_piece0 on 10.194.32.157:53453 in memory (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:21:55,083][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_7_piece0 on 10.194.32.157:53453 in memory (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:21:55,084][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_9_piece0 on 10.194.32.157:53453 in memory (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:21:55,085][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_8_piece0 on 10.194.32.157:53453 in memory (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:21:55,086][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_21_piece0 on 10.194.32.157:53453 in memory (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:21:55,086][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_18_piece0 on 10.194.32.157:53453 in memory (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:21:56,118][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-25 11:21:56,121][org.apache.spark.executor.Executor]Finished task 0.0 in stage 22.0 (TID 22). 708 bytes result sent to driver
[INFO][2018-05-25 11:21:56,121][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 22.0 (TID 22) in 1048 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:21:56,121][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 22.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:21:56,122][org.apache.spark.scheduler.DAGScheduler]ResultStage 22 (foreachPartition at ReceiveKafkaData.scala:73) finished in 1.048 s
[INFO][2018-05-25 11:21:56,122][org.apache.spark.scheduler.DAGScheduler]Job 22 finished: foreachPartition at ReceiveKafkaData.scala:73, took 1.064442 s
[INFO][2018-05-25 11:21:56,123][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218515000 ms.0 from job set of time 1527218515000 ms
[INFO][2018-05-25 11:21:56,124][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 25 from persistence list
[INFO][2018-05-25 11:21:56,124][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 1.123 s for time 1527218515000 ms (execution: 1.102 s)
[INFO][2018-05-25 11:21:56,124][org.apache.spark.storage.BlockManager]Removing RDD 25
[INFO][2018-05-25 11:21:56,125][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 24 from persistence list
[INFO][2018-05-25 11:21:56,125][org.apache.spark.storage.BlockManager]Removing RDD 24
[INFO][2018-05-25 11:21:56,125][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:21:56,125][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218505000 ms
[INFO][2018-05-25 11:22:00,018][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218520000 ms
[INFO][2018-05-25 11:22:00,019][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218520000 ms.0 from job set of time 1527218520000 ms
[INFO][2018-05-25 11:22:00,023][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:22:00,024][org.apache.spark.scheduler.DAGScheduler]Got job 23 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:22:00,024][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 23 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:22:00,024][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:22:00,024][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:22:00,024][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 23 (MapPartitionsRDD[29] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:22:00,026][org.apache.spark.storage.memory.MemoryStore]Block broadcast_23 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-25 11:22:00,027][org.apache.spark.storage.memory.MemoryStore]Block broadcast_23_piece0 stored as bytes in memory (estimated size 1972.0 B, free 912.3 MB)
[INFO][2018-05-25 11:22:00,028][org.apache.spark.storage.BlockManagerInfo]Added broadcast_23_piece0 in memory on 10.194.32.157:53453 (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:22:00,028][org.apache.spark.SparkContext]Created broadcast 23 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:22:00,029][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 23 (MapPartitionsRDD[29] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:22:00,029][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 23.0 with 1 tasks
[INFO][2018-05-25 11:22:00,030][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 23.0 (TID 23, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:22:00,030][org.apache.spark.executor.Executor]Running task 0.0 in stage 23.0 (TID 23)
[INFO][2018-05-25 11:22:00,031][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12385 -> 12395
[INFO][2018-05-25 11:22:00,032][org.apache.spark.executor.Executor]Finished task 0.0 in stage 23.0 (TID 23). 930 bytes result sent to driver
[INFO][2018-05-25 11:22:00,033][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 23.0 (TID 23) in 3 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:22:00,033][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 23.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:22:00,033][org.apache.spark.scheduler.DAGScheduler]ResultStage 23 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.004 s
[INFO][2018-05-25 11:22:00,034][org.apache.spark.scheduler.DAGScheduler]Job 23 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.010024 s
[INFO][2018-05-25 11:22:00,038][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:73
[INFO][2018-05-25 11:22:00,039][org.apache.spark.scheduler.DAGScheduler]Got job 24 (foreachPartition at ReceiveKafkaData.scala:73) with 1 output partitions
[INFO][2018-05-25 11:22:00,039][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 24 (foreachPartition at ReceiveKafkaData.scala:73)
[INFO][2018-05-25 11:22:00,039][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:22:00,039][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:22:00,040][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 24 (MapPartitionsRDD[29] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:22:00,041][org.apache.spark.storage.memory.MemoryStore]Block broadcast_24 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-25 11:22:00,043][org.apache.spark.storage.memory.MemoryStore]Block broadcast_24_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.3 MB)
[INFO][2018-05-25 11:22:00,043][org.apache.spark.storage.BlockManagerInfo]Added broadcast_24_piece0 in memory on 10.194.32.157:53453 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:22:00,044][org.apache.spark.SparkContext]Created broadcast 24 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:22:00,044][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 24 (MapPartitionsRDD[29] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:22:00,044][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 24.0 with 1 tasks
[INFO][2018-05-25 11:22:00,045][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 24.0 (TID 24, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:22:00,045][org.apache.spark.executor.Executor]Running task 0.0 in stage 24.0 (TID 24)
[INFO][2018-05-25 11:22:00,046][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12385 -> 12395
[INFO][2018-05-25 11:22:00,047][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 12385
[INFO][2018-05-25 11:22:00,074][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-25 11:22:00,075][org.apache.spark.executor.Executor]Finished task 0.0 in stage 24.0 (TID 24). 708 bytes result sent to driver
[INFO][2018-05-25 11:22:00,075][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 24.0 (TID 24) in 31 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:22:00,075][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 24.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:22:00,076][org.apache.spark.scheduler.DAGScheduler]ResultStage 24 (foreachPartition at ReceiveKafkaData.scala:73) finished in 0.032 s
[INFO][2018-05-25 11:22:00,076][org.apache.spark.scheduler.DAGScheduler]Job 24 finished: foreachPartition at ReceiveKafkaData.scala:73, took 0.037876 s
[INFO][2018-05-25 11:22:00,076][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218520000 ms.0 from job set of time 1527218520000 ms
[INFO][2018-05-25 11:22:00,076][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.076 s for time 1527218520000 ms (execution: 0.057 s)
[INFO][2018-05-25 11:22:00,076][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 27 from persistence list
[INFO][2018-05-25 11:22:00,077][org.apache.spark.storage.BlockManager]Removing RDD 27
[INFO][2018-05-25 11:22:00,077][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 26 from persistence list
[INFO][2018-05-25 11:22:00,077][org.apache.spark.storage.BlockManager]Removing RDD 26
[INFO][2018-05-25 11:22:00,077][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:22:00,077][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218510000 ms
[INFO][2018-05-25 11:22:05,017][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218525000 ms
[INFO][2018-05-25 11:22:05,017][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218525000 ms.0 from job set of time 1527218525000 ms
[INFO][2018-05-25 11:22:05,022][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:22:05,023][org.apache.spark.scheduler.DAGScheduler]Got job 25 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:22:05,023][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 25 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:22:05,023][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:22:05,023][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:22:05,023][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 25 (MapPartitionsRDD[31] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:22:05,025][org.apache.spark.storage.memory.MemoryStore]Block broadcast_25 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-25 11:22:05,026][org.apache.spark.storage.memory.MemoryStore]Block broadcast_25_piece0 stored as bytes in memory (estimated size 1972.0 B, free 912.3 MB)
[INFO][2018-05-25 11:22:05,026][org.apache.spark.storage.BlockManagerInfo]Added broadcast_25_piece0 in memory on 10.194.32.157:53453 (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:22:05,026][org.apache.spark.SparkContext]Created broadcast 25 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:22:05,027][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 25 (MapPartitionsRDD[31] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:22:05,027][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 25.0 with 1 tasks
[INFO][2018-05-25 11:22:05,027][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 25.0 (TID 25, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:22:05,028][org.apache.spark.executor.Executor]Running task 0.0 in stage 25.0 (TID 25)
[INFO][2018-05-25 11:22:05,029][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12395 -> 12405
[INFO][2018-05-25 11:22:05,031][org.apache.spark.executor.Executor]Finished task 0.0 in stage 25.0 (TID 25). 910 bytes result sent to driver
[INFO][2018-05-25 11:22:05,031][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 25.0 (TID 25) in 4 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:22:05,032][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 25.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:22:05,032][org.apache.spark.scheduler.DAGScheduler]ResultStage 25 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.005 s
[INFO][2018-05-25 11:22:05,033][org.apache.spark.scheduler.DAGScheduler]Job 25 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.010381 s
[INFO][2018-05-25 11:22:05,037][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:73
[INFO][2018-05-25 11:22:05,038][org.apache.spark.scheduler.DAGScheduler]Got job 26 (foreachPartition at ReceiveKafkaData.scala:73) with 1 output partitions
[INFO][2018-05-25 11:22:05,038][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 26 (foreachPartition at ReceiveKafkaData.scala:73)
[INFO][2018-05-25 11:22:05,038][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:22:05,038][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:22:05,038][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 26 (MapPartitionsRDD[31] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:22:05,039][org.apache.spark.storage.memory.MemoryStore]Block broadcast_26 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-25 11:22:05,040][org.apache.spark.storage.memory.MemoryStore]Block broadcast_26_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.3 MB)
[INFO][2018-05-25 11:22:05,040][org.apache.spark.storage.BlockManagerInfo]Added broadcast_26_piece0 in memory on 10.194.32.157:53453 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:22:05,041][org.apache.spark.SparkContext]Created broadcast 26 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:22:05,041][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 26 (MapPartitionsRDD[31] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:22:05,041][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 26.0 with 1 tasks
[INFO][2018-05-25 11:22:05,041][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 26.0 (TID 26, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:22:05,042][org.apache.spark.executor.Executor]Running task 0.0 in stage 26.0 (TID 26)
[INFO][2018-05-25 11:22:05,042][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12395 -> 12405
[INFO][2018-05-25 11:22:05,042][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 12395
[INFO][2018-05-25 11:22:05,080][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-25 11:22:05,080][org.apache.spark.executor.Executor]Finished task 0.0 in stage 26.0 (TID 26). 708 bytes result sent to driver
[INFO][2018-05-25 11:22:05,081][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 26.0 (TID 26) in 40 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:22:05,081][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 26.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:22:05,081][org.apache.spark.scheduler.DAGScheduler]ResultStage 26 (foreachPartition at ReceiveKafkaData.scala:73) finished in 0.040 s
[INFO][2018-05-25 11:22:05,082][org.apache.spark.scheduler.DAGScheduler]Job 26 finished: foreachPartition at ReceiveKafkaData.scala:73, took 0.044246 s
[INFO][2018-05-25 11:22:05,082][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218525000 ms.0 from job set of time 1527218525000 ms
[INFO][2018-05-25 11:22:05,082][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.082 s for time 1527218525000 ms (execution: 0.065 s)
[INFO][2018-05-25 11:22:05,082][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 29 from persistence list
[INFO][2018-05-25 11:22:05,083][org.apache.spark.storage.BlockManager]Removing RDD 29
[INFO][2018-05-25 11:22:05,083][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 28 from persistence list
[INFO][2018-05-25 11:22:05,083][org.apache.spark.storage.BlockManager]Removing RDD 28
[INFO][2018-05-25 11:22:05,084][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:22:05,084][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218515000 ms
[INFO][2018-05-25 11:22:10,025][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218530000 ms
[INFO][2018-05-25 11:22:10,025][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218530000 ms.0 from job set of time 1527218530000 ms
[INFO][2018-05-25 11:22:10,030][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:22:10,030][org.apache.spark.scheduler.DAGScheduler]Got job 27 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:22:10,030][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 27 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:22:10,030][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:22:10,030][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:22:10,031][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 27 (MapPartitionsRDD[33] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:22:10,032][org.apache.spark.storage.memory.MemoryStore]Block broadcast_27 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-25 11:22:10,034][org.apache.spark.storage.memory.MemoryStore]Block broadcast_27_piece0 stored as bytes in memory (estimated size 1972.0 B, free 912.3 MB)
[INFO][2018-05-25 11:22:10,034][org.apache.spark.storage.BlockManagerInfo]Added broadcast_27_piece0 in memory on 10.194.32.157:53453 (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:22:10,034][org.apache.spark.SparkContext]Created broadcast 27 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:22:10,035][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 27 (MapPartitionsRDD[33] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:22:10,035][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 27.0 with 1 tasks
[INFO][2018-05-25 11:22:10,035][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 27.0 (TID 27, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:22:10,036][org.apache.spark.executor.Executor]Running task 0.0 in stage 27.0 (TID 27)
[INFO][2018-05-25 11:22:10,037][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12405 -> 12415
[INFO][2018-05-25 11:22:10,038][org.apache.spark.executor.Executor]Finished task 0.0 in stage 27.0 (TID 27). 974 bytes result sent to driver
[INFO][2018-05-25 11:22:10,039][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 27.0 (TID 27) in 4 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:22:10,039][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 27.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:22:10,039][org.apache.spark.scheduler.DAGScheduler]ResultStage 27 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.004 s
[INFO][2018-05-25 11:22:10,040][org.apache.spark.scheduler.DAGScheduler]Job 27 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.009671 s
[INFO][2018-05-25 11:22:10,044][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:73
[INFO][2018-05-25 11:22:10,045][org.apache.spark.scheduler.DAGScheduler]Got job 28 (foreachPartition at ReceiveKafkaData.scala:73) with 1 output partitions
[INFO][2018-05-25 11:22:10,046][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 28 (foreachPartition at ReceiveKafkaData.scala:73)
[INFO][2018-05-25 11:22:10,046][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:22:10,046][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:22:10,046][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 28 (MapPartitionsRDD[33] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:22:10,054][org.apache.spark.storage.memory.MemoryStore]Block broadcast_28 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-25 11:22:10,058][org.apache.spark.storage.memory.MemoryStore]Block broadcast_28_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.3 MB)
[INFO][2018-05-25 11:22:10,059][org.apache.spark.storage.BlockManagerInfo]Added broadcast_28_piece0 in memory on 10.194.32.157:53453 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:22:10,060][org.apache.spark.SparkContext]Created broadcast 28 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:22:10,061][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 28 (MapPartitionsRDD[33] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:22:10,061][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 28.0 with 1 tasks
[INFO][2018-05-25 11:22:10,062][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 28.0 (TID 28, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:22:10,062][org.apache.spark.executor.Executor]Running task 0.0 in stage 28.0 (TID 28)
[INFO][2018-05-25 11:22:10,063][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12405 -> 12415
[INFO][2018-05-25 11:22:10,063][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 12405
[INFO][2018-05-25 11:22:10,100][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-25 11:22:10,101][org.apache.spark.executor.Executor]Finished task 0.0 in stage 28.0 (TID 28). 665 bytes result sent to driver
[INFO][2018-05-25 11:22:10,101][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 28.0 (TID 28) in 40 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:22:10,102][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 28.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:22:10,102][org.apache.spark.scheduler.DAGScheduler]ResultStage 28 (foreachPartition at ReceiveKafkaData.scala:73) finished in 0.041 s
[INFO][2018-05-25 11:22:10,103][org.apache.spark.scheduler.DAGScheduler]Job 28 finished: foreachPartition at ReceiveKafkaData.scala:73, took 0.058011 s
[INFO][2018-05-25 11:22:10,103][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218530000 ms.0 from job set of time 1527218530000 ms
[INFO][2018-05-25 11:22:10,116][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.103 s for time 1527218530000 ms (execution: 0.078 s)
[INFO][2018-05-25 11:22:10,116][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 31 from persistence list
[INFO][2018-05-25 11:22:10,117][org.apache.spark.storage.BlockManager]Removing RDD 31
[INFO][2018-05-25 11:22:10,117][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 30 from persistence list
[INFO][2018-05-25 11:22:10,117][org.apache.spark.storage.BlockManager]Removing RDD 30
[INFO][2018-05-25 11:22:10,117][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:22:10,117][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218520000 ms
[INFO][2018-05-25 11:22:15,017][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218535000 ms
[INFO][2018-05-25 11:22:15,017][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218535000 ms.0 from job set of time 1527218535000 ms
[INFO][2018-05-25 11:22:15,023][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:22:15,024][org.apache.spark.scheduler.DAGScheduler]Got job 29 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:22:15,024][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 29 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:22:15,024][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:22:15,025][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:22:15,025][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 29 (MapPartitionsRDD[35] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:22:15,026][org.apache.spark.storage.memory.MemoryStore]Block broadcast_29 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-25 11:22:15,027][org.apache.spark.storage.memory.MemoryStore]Block broadcast_29_piece0 stored as bytes in memory (estimated size 1972.0 B, free 912.3 MB)
[INFO][2018-05-25 11:22:15,027][org.apache.spark.storage.BlockManagerInfo]Added broadcast_29_piece0 in memory on 10.194.32.157:53453 (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:22:15,027][org.apache.spark.SparkContext]Created broadcast 29 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:22:15,027][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 29 (MapPartitionsRDD[35] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:22:15,027][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 29.0 with 1 tasks
[INFO][2018-05-25 11:22:15,028][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 29.0 (TID 29, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:22:15,028][org.apache.spark.executor.Executor]Running task 0.0 in stage 29.0 (TID 29)
[INFO][2018-05-25 11:22:15,029][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12415 -> 12425
[INFO][2018-05-25 11:22:15,030][org.apache.spark.executor.Executor]Finished task 0.0 in stage 29.0 (TID 29). 971 bytes result sent to driver
[INFO][2018-05-25 11:22:15,031][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 29.0 (TID 29) in 3 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:22:15,031][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 29.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:22:15,031][org.apache.spark.scheduler.DAGScheduler]ResultStage 29 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.003 s
[INFO][2018-05-25 11:22:15,031][org.apache.spark.scheduler.DAGScheduler]Job 29 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.007809 s
[INFO][2018-05-25 11:22:15,035][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:73
[INFO][2018-05-25 11:22:15,035][org.apache.spark.scheduler.DAGScheduler]Got job 30 (foreachPartition at ReceiveKafkaData.scala:73) with 1 output partitions
[INFO][2018-05-25 11:22:15,035][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 30 (foreachPartition at ReceiveKafkaData.scala:73)
[INFO][2018-05-25 11:22:15,035][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:22:15,035][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:22:15,035][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 30 (MapPartitionsRDD[35] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:22:15,036][org.apache.spark.storage.memory.MemoryStore]Block broadcast_30 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-25 11:22:15,037][org.apache.spark.storage.memory.MemoryStore]Block broadcast_30_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.2 MB)
[INFO][2018-05-25 11:22:15,038][org.apache.spark.storage.BlockManagerInfo]Added broadcast_30_piece0 in memory on 10.194.32.157:53453 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:22:15,038][org.apache.spark.SparkContext]Created broadcast 30 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:22:15,038][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 30 (MapPartitionsRDD[35] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:22:15,038][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 30.0 with 1 tasks
[INFO][2018-05-25 11:22:15,039][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 30.0 (TID 30, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:22:15,039][org.apache.spark.executor.Executor]Running task 0.0 in stage 30.0 (TID 30)
[INFO][2018-05-25 11:22:15,040][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12415 -> 12425
[INFO][2018-05-25 11:22:15,040][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 12415
[INFO][2018-05-25 11:22:15,080][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-25 11:22:15,081][org.apache.spark.executor.Executor]Finished task 0.0 in stage 30.0 (TID 30). 708 bytes result sent to driver
[INFO][2018-05-25 11:22:15,082][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 30.0 (TID 30) in 43 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:22:15,082][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 30.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:22:15,082][org.apache.spark.scheduler.DAGScheduler]ResultStage 30 (foreachPartition at ReceiveKafkaData.scala:73) finished in 0.043 s
[INFO][2018-05-25 11:22:15,083][org.apache.spark.scheduler.DAGScheduler]Job 30 finished: foreachPartition at ReceiveKafkaData.scala:73, took 0.047998 s
[INFO][2018-05-25 11:22:15,084][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218535000 ms.0 from job set of time 1527218535000 ms
[INFO][2018-05-25 11:22:15,085][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 33 from persistence list
[INFO][2018-05-25 11:22:15,085][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.084 s for time 1527218535000 ms (execution: 0.067 s)
[INFO][2018-05-25 11:22:15,087][org.apache.spark.storage.BlockManager]Removing RDD 33
[INFO][2018-05-25 11:22:15,087][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 32 from persistence list
[INFO][2018-05-25 11:22:15,088][org.apache.spark.storage.BlockManager]Removing RDD 32
[INFO][2018-05-25 11:22:15,088][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:22:15,088][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218525000 ms
[INFO][2018-05-25 11:22:20,014][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218540000 ms
[INFO][2018-05-25 11:22:20,018][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218540000 ms.0 from job set of time 1527218540000 ms
[INFO][2018-05-25 11:22:20,019][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:22:20,021][org.apache.spark.scheduler.DAGScheduler]Got job 31 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:22:20,021][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 31 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:22:20,021][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:22:20,021][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:22:20,021][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 31 (MapPartitionsRDD[37] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:22:20,023][org.apache.spark.storage.memory.MemoryStore]Block broadcast_31 stored as values in memory (estimated size 3.1 KB, free 912.2 MB)
[INFO][2018-05-25 11:22:20,024][org.apache.spark.storage.memory.MemoryStore]Block broadcast_31_piece0 stored as bytes in memory (estimated size 1972.0 B, free 912.2 MB)
[INFO][2018-05-25 11:22:20,024][org.apache.spark.storage.BlockManagerInfo]Added broadcast_31_piece0 in memory on 10.194.32.157:53453 (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:22:20,024][org.apache.spark.SparkContext]Created broadcast 31 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:22:20,025][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 31 (MapPartitionsRDD[37] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:22:20,025][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 31.0 with 1 tasks
[INFO][2018-05-25 11:22:20,025][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 31.0 (TID 31, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:22:20,025][org.apache.spark.executor.Executor]Running task 0.0 in stage 31.0 (TID 31)
[INFO][2018-05-25 11:22:20,027][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12425 -> 12435
[INFO][2018-05-25 11:22:20,028][org.apache.spark.executor.Executor]Finished task 0.0 in stage 31.0 (TID 31). 974 bytes result sent to driver
[INFO][2018-05-25 11:22:20,029][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 31.0 (TID 31) in 4 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:22:20,030][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 31.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:22:20,030][org.apache.spark.scheduler.DAGScheduler]ResultStage 31 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.005 s
[INFO][2018-05-25 11:22:20,030][org.apache.spark.scheduler.DAGScheduler]Job 31 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.011512 s
[INFO][2018-05-25 11:22:20,035][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:73
[INFO][2018-05-25 11:22:20,036][org.apache.spark.scheduler.DAGScheduler]Got job 32 (foreachPartition at ReceiveKafkaData.scala:73) with 1 output partitions
[INFO][2018-05-25 11:22:20,036][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 32 (foreachPartition at ReceiveKafkaData.scala:73)
[INFO][2018-05-25 11:22:20,036][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:22:20,036][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:22:20,036][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 32 (MapPartitionsRDD[37] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:22:20,037][org.apache.spark.storage.memory.MemoryStore]Block broadcast_32 stored as values in memory (estimated size 3.4 KB, free 912.2 MB)
[INFO][2018-05-25 11:22:20,041][org.apache.spark.storage.memory.MemoryStore]Block broadcast_32_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.2 MB)
[INFO][2018-05-25 11:22:20,041][org.apache.spark.storage.BlockManagerInfo]Added broadcast_32_piece0 in memory on 10.194.32.157:53453 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:22:20,041][org.apache.spark.SparkContext]Created broadcast 32 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:22:20,042][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 32 (MapPartitionsRDD[37] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:22:20,042][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 32.0 with 1 tasks
[INFO][2018-05-25 11:22:20,042][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 32.0 (TID 32, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:22:20,043][org.apache.spark.executor.Executor]Running task 0.0 in stage 32.0 (TID 32)
[INFO][2018-05-25 11:22:20,045][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12425 -> 12435
[INFO][2018-05-25 11:22:20,045][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 12425
[INFO][2018-05-25 11:22:20,084][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-25 11:22:20,085][org.apache.spark.executor.Executor]Finished task 0.0 in stage 32.0 (TID 32). 708 bytes result sent to driver
[INFO][2018-05-25 11:22:20,085][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 32.0 (TID 32) in 43 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:22:20,085][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 32.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:22:20,086][org.apache.spark.scheduler.DAGScheduler]ResultStage 32 (foreachPartition at ReceiveKafkaData.scala:73) finished in 0.044 s
[INFO][2018-05-25 11:22:20,086][org.apache.spark.scheduler.DAGScheduler]Job 32 finished: foreachPartition at ReceiveKafkaData.scala:73, took 0.050580 s
[INFO][2018-05-25 11:22:20,087][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218540000 ms.0 from job set of time 1527218540000 ms
[INFO][2018-05-25 11:22:20,087][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.087 s for time 1527218540000 ms (execution: 0.071 s)
[INFO][2018-05-25 11:22:20,087][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 35 from persistence list
[INFO][2018-05-25 11:22:20,087][org.apache.spark.storage.BlockManager]Removing RDD 35
[INFO][2018-05-25 11:22:20,087][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 34 from persistence list
[INFO][2018-05-25 11:22:20,088][org.apache.spark.storage.BlockManager]Removing RDD 34
[INFO][2018-05-25 11:22:20,088][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:22:20,089][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218530000 ms
[INFO][2018-05-25 11:22:25,016][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218545000 ms
[INFO][2018-05-25 11:22:25,016][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218545000 ms.0 from job set of time 1527218545000 ms
[INFO][2018-05-25 11:22:25,021][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:22:25,021][org.apache.spark.scheduler.DAGScheduler]Got job 33 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:22:25,021][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 33 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:22:25,021][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:22:25,021][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:22:25,022][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 33 (MapPartitionsRDD[39] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:22:25,023][org.apache.spark.storage.memory.MemoryStore]Block broadcast_33 stored as values in memory (estimated size 3.1 KB, free 912.2 MB)
[INFO][2018-05-25 11:22:25,024][org.apache.spark.storage.memory.MemoryStore]Block broadcast_33_piece0 stored as bytes in memory (estimated size 1972.0 B, free 912.2 MB)
[INFO][2018-05-25 11:22:25,024][org.apache.spark.storage.BlockManagerInfo]Added broadcast_33_piece0 in memory on 10.194.32.157:53453 (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:22:25,025][org.apache.spark.SparkContext]Created broadcast 33 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:22:25,025][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 33 (MapPartitionsRDD[39] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:22:25,025][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 33.0 with 1 tasks
[INFO][2018-05-25 11:22:25,025][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 33.0 (TID 33, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:22:25,026][org.apache.spark.executor.Executor]Running task 0.0 in stage 33.0 (TID 33)
[INFO][2018-05-25 11:22:25,027][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12435 -> 12445
[INFO][2018-05-25 11:22:25,029][org.apache.spark.executor.Executor]Finished task 0.0 in stage 33.0 (TID 33). 972 bytes result sent to driver
[INFO][2018-05-25 11:22:25,029][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 33.0 (TID 33) in 4 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:22:25,029][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 33.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:22:25,030][org.apache.spark.scheduler.DAGScheduler]ResultStage 33 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.004 s
[INFO][2018-05-25 11:22:25,030][org.apache.spark.scheduler.DAGScheduler]Job 33 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.009091 s
[INFO][2018-05-25 11:22:25,034][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:73
[INFO][2018-05-25 11:22:25,035][org.apache.spark.scheduler.DAGScheduler]Got job 34 (foreachPartition at ReceiveKafkaData.scala:73) with 1 output partitions
[INFO][2018-05-25 11:22:25,035][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 34 (foreachPartition at ReceiveKafkaData.scala:73)
[INFO][2018-05-25 11:22:25,035][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:22:25,035][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:22:25,035][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 34 (MapPartitionsRDD[39] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:22:25,036][org.apache.spark.storage.memory.MemoryStore]Block broadcast_34 stored as values in memory (estimated size 3.4 KB, free 912.2 MB)
[INFO][2018-05-25 11:22:25,037][org.apache.spark.storage.memory.MemoryStore]Block broadcast_34_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.2 MB)
[INFO][2018-05-25 11:22:25,037][org.apache.spark.storage.BlockManagerInfo]Added broadcast_34_piece0 in memory on 10.194.32.157:53453 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:22:25,038][org.apache.spark.SparkContext]Created broadcast 34 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:22:25,039][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 34 (MapPartitionsRDD[39] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:22:25,039][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 34.0 with 1 tasks
[INFO][2018-05-25 11:22:25,040][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 34.0 (TID 34, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:22:25,040][org.apache.spark.executor.Executor]Running task 0.0 in stage 34.0 (TID 34)
[INFO][2018-05-25 11:22:25,041][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12435 -> 12445
[INFO][2018-05-25 11:22:25,041][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 12435
[INFO][2018-05-25 11:22:25,071][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-25 11:22:25,071][org.apache.spark.executor.Executor]Finished task 0.0 in stage 34.0 (TID 34). 665 bytes result sent to driver
[INFO][2018-05-25 11:22:25,072][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 34.0 (TID 34) in 32 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:22:25,072][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 34.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:22:25,072][org.apache.spark.scheduler.DAGScheduler]ResultStage 34 (foreachPartition at ReceiveKafkaData.scala:73) finished in 0.032 s
[INFO][2018-05-25 11:22:25,072][org.apache.spark.scheduler.DAGScheduler]Job 34 finished: foreachPartition at ReceiveKafkaData.scala:73, took 0.037866 s
[INFO][2018-05-25 11:22:25,072][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218545000 ms.0 from job set of time 1527218545000 ms
[INFO][2018-05-25 11:22:25,073][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.072 s for time 1527218545000 ms (execution: 0.056 s)
[INFO][2018-05-25 11:22:25,073][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 37 from persistence list
[INFO][2018-05-25 11:22:25,073][org.apache.spark.storage.BlockManager]Removing RDD 37
[INFO][2018-05-25 11:22:25,073][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 36 from persistence list
[INFO][2018-05-25 11:22:25,073][org.apache.spark.storage.BlockManager]Removing RDD 36
[INFO][2018-05-25 11:22:25,074][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:22:25,074][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218535000 ms
[INFO][2018-05-25 11:22:30,021][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218550000 ms
[INFO][2018-05-25 11:22:30,022][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218550000 ms.0 from job set of time 1527218550000 ms
[INFO][2018-05-25 11:22:30,026][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:22:30,026][org.apache.spark.scheduler.DAGScheduler]Got job 35 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:22:30,026][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 35 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:22:30,026][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:22:30,027][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:22:30,027][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 35 (MapPartitionsRDD[41] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:22:30,028][org.apache.spark.storage.memory.MemoryStore]Block broadcast_35 stored as values in memory (estimated size 3.1 KB, free 912.2 MB)
[INFO][2018-05-25 11:22:30,029][org.apache.spark.storage.memory.MemoryStore]Block broadcast_35_piece0 stored as bytes in memory (estimated size 1972.0 B, free 912.2 MB)
[INFO][2018-05-25 11:22:30,030][org.apache.spark.storage.BlockManagerInfo]Added broadcast_35_piece0 in memory on 10.194.32.157:53453 (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:22:30,030][org.apache.spark.SparkContext]Created broadcast 35 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:22:30,030][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 35 (MapPartitionsRDD[41] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:22:30,030][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 35.0 with 1 tasks
[INFO][2018-05-25 11:22:30,031][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 35.0 (TID 35, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:22:30,031][org.apache.spark.executor.Executor]Running task 0.0 in stage 35.0 (TID 35)
[INFO][2018-05-25 11:22:30,032][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12445 -> 12455
[INFO][2018-05-25 11:22:30,033][org.apache.spark.executor.Executor]Finished task 0.0 in stage 35.0 (TID 35). 972 bytes result sent to driver
[INFO][2018-05-25 11:22:30,033][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 35.0 (TID 35) in 2 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:22:30,033][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 35.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:22:30,034][org.apache.spark.scheduler.DAGScheduler]ResultStage 35 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.003 s
[INFO][2018-05-25 11:22:30,034][org.apache.spark.scheduler.DAGScheduler]Job 35 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.007882 s
[INFO][2018-05-25 11:22:30,037][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:73
[INFO][2018-05-25 11:22:30,038][org.apache.spark.scheduler.DAGScheduler]Got job 36 (foreachPartition at ReceiveKafkaData.scala:73) with 1 output partitions
[INFO][2018-05-25 11:22:30,038][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 36 (foreachPartition at ReceiveKafkaData.scala:73)
[INFO][2018-05-25 11:22:30,038][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:22:30,038][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:22:30,038][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 36 (MapPartitionsRDD[41] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:22:30,039][org.apache.spark.storage.memory.MemoryStore]Block broadcast_36 stored as values in memory (estimated size 3.4 KB, free 912.2 MB)
[INFO][2018-05-25 11:22:30,040][org.apache.spark.storage.memory.MemoryStore]Block broadcast_36_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.2 MB)
[INFO][2018-05-25 11:22:30,040][org.apache.spark.storage.BlockManagerInfo]Added broadcast_36_piece0 in memory on 10.194.32.157:53453 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:22:30,040][org.apache.spark.SparkContext]Created broadcast 36 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:22:30,040][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 36 (MapPartitionsRDD[41] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:22:30,040][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 36.0 with 1 tasks
[INFO][2018-05-25 11:22:30,041][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 36.0 (TID 36, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:22:30,041][org.apache.spark.executor.Executor]Running task 0.0 in stage 36.0 (TID 36)
[INFO][2018-05-25 11:22:30,042][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12445 -> 12455
[INFO][2018-05-25 11:22:30,042][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 12445
[INFO][2018-05-25 11:22:30,077][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-25 11:22:30,077][org.apache.spark.executor.Executor]Finished task 0.0 in stage 36.0 (TID 36). 665 bytes result sent to driver
[INFO][2018-05-25 11:22:30,078][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 36.0 (TID 36) in 37 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:22:30,078][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 36.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:22:30,079][org.apache.spark.scheduler.DAGScheduler]ResultStage 36 (foreachPartition at ReceiveKafkaData.scala:73) finished in 0.037 s
[INFO][2018-05-25 11:22:30,079][org.apache.spark.scheduler.DAGScheduler]Job 36 finished: foreachPartition at ReceiveKafkaData.scala:73, took 0.041254 s
[INFO][2018-05-25 11:22:30,079][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218550000 ms.0 from job set of time 1527218550000 ms
[INFO][2018-05-25 11:22:30,079][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.079 s for time 1527218550000 ms (execution: 0.058 s)
[INFO][2018-05-25 11:22:30,079][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 39 from persistence list
[INFO][2018-05-25 11:22:30,079][org.apache.spark.storage.BlockManager]Removing RDD 39
[INFO][2018-05-25 11:22:30,080][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 38 from persistence list
[INFO][2018-05-25 11:22:30,080][org.apache.spark.storage.BlockManager]Removing RDD 38
[INFO][2018-05-25 11:22:30,080][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:22:30,080][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218540000 ms
[INFO][2018-05-25 11:22:35,024][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218555000 ms
[INFO][2018-05-25 11:22:35,025][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218555000 ms.0 from job set of time 1527218555000 ms
[INFO][2018-05-25 11:22:35,030][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:22:35,030][org.apache.spark.scheduler.DAGScheduler]Got job 37 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:22:35,030][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 37 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:22:35,030][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:22:35,030][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:22:35,031][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 37 (MapPartitionsRDD[43] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:22:35,032][org.apache.spark.storage.memory.MemoryStore]Block broadcast_37 stored as values in memory (estimated size 3.1 KB, free 912.2 MB)
[INFO][2018-05-25 11:22:35,033][org.apache.spark.storage.memory.MemoryStore]Block broadcast_37_piece0 stored as bytes in memory (estimated size 1972.0 B, free 912.2 MB)
[INFO][2018-05-25 11:22:35,034][org.apache.spark.storage.BlockManagerInfo]Added broadcast_37_piece0 in memory on 10.194.32.157:53453 (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:22:35,034][org.apache.spark.SparkContext]Created broadcast 37 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:22:35,035][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 37 (MapPartitionsRDD[43] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:22:35,035][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 37.0 with 1 tasks
[INFO][2018-05-25 11:22:35,035][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 37.0 (TID 37, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:22:35,036][org.apache.spark.executor.Executor]Running task 0.0 in stage 37.0 (TID 37)
[INFO][2018-05-25 11:22:35,037][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12455 -> 12465
[INFO][2018-05-25 11:22:35,038][org.apache.spark.executor.Executor]Finished task 0.0 in stage 37.0 (TID 37). 968 bytes result sent to driver
[INFO][2018-05-25 11:22:35,039][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 37.0 (TID 37) in 4 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:22:35,039][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 37.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:22:35,039][org.apache.spark.scheduler.DAGScheduler]ResultStage 37 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.004 s
[INFO][2018-05-25 11:22:35,040][org.apache.spark.scheduler.DAGScheduler]Job 37 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.010006 s
[INFO][2018-05-25 11:22:35,043][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:73
[INFO][2018-05-25 11:22:35,044][org.apache.spark.scheduler.DAGScheduler]Got job 38 (foreachPartition at ReceiveKafkaData.scala:73) with 1 output partitions
[INFO][2018-05-25 11:22:35,044][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 38 (foreachPartition at ReceiveKafkaData.scala:73)
[INFO][2018-05-25 11:22:35,044][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:22:35,044][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:22:35,044][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 38 (MapPartitionsRDD[43] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:22:35,045][org.apache.spark.storage.memory.MemoryStore]Block broadcast_38 stored as values in memory (estimated size 3.4 KB, free 912.2 MB)
[INFO][2018-05-25 11:22:35,046][org.apache.spark.storage.memory.MemoryStore]Block broadcast_38_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.2 MB)
[INFO][2018-05-25 11:22:35,046][org.apache.spark.storage.BlockManagerInfo]Added broadcast_38_piece0 in memory on 10.194.32.157:53453 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:22:35,046][org.apache.spark.SparkContext]Created broadcast 38 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:22:35,047][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 38 (MapPartitionsRDD[43] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:22:35,047][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 38.0 with 1 tasks
[INFO][2018-05-25 11:22:35,047][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 38.0 (TID 38, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:22:35,047][org.apache.spark.executor.Executor]Running task 0.0 in stage 38.0 (TID 38)
[INFO][2018-05-25 11:22:35,048][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12455 -> 12465
[INFO][2018-05-25 11:22:35,048][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 12455
[INFO][2018-05-25 11:22:35,089][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-25 11:22:35,091][org.apache.spark.executor.Executor]Finished task 0.0 in stage 38.0 (TID 38). 665 bytes result sent to driver
[INFO][2018-05-25 11:22:35,091][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 38.0 (TID 38) in 44 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:22:35,091][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 38.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:22:35,092][org.apache.spark.scheduler.DAGScheduler]ResultStage 38 (foreachPartition at ReceiveKafkaData.scala:73) finished in 0.045 s
[INFO][2018-05-25 11:22:35,093][org.apache.spark.scheduler.DAGScheduler]Job 38 finished: foreachPartition at ReceiveKafkaData.scala:73, took 0.049162 s
[INFO][2018-05-25 11:22:35,093][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218555000 ms.0 from job set of time 1527218555000 ms
[INFO][2018-05-25 11:22:35,093][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.093 s for time 1527218555000 ms (execution: 0.068 s)
[INFO][2018-05-25 11:22:35,093][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 41 from persistence list
[INFO][2018-05-25 11:22:35,094][org.apache.spark.storage.BlockManager]Removing RDD 41
[INFO][2018-05-25 11:22:35,094][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 40 from persistence list
[INFO][2018-05-25 11:22:35,094][org.apache.spark.storage.BlockManager]Removing RDD 40
[INFO][2018-05-25 11:22:35,094][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:22:35,094][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218545000 ms
[INFO][2018-05-25 11:22:40,013][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218560000 ms
[INFO][2018-05-25 11:22:40,014][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218560000 ms.0 from job set of time 1527218560000 ms
[INFO][2018-05-25 11:22:40,018][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:22:40,018][org.apache.spark.scheduler.DAGScheduler]Got job 39 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:22:40,018][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 39 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:22:40,018][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:22:40,018][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:22:40,018][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 39 (MapPartitionsRDD[45] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:22:40,019][org.apache.spark.storage.memory.MemoryStore]Block broadcast_39 stored as values in memory (estimated size 3.1 KB, free 912.2 MB)
[INFO][2018-05-25 11:22:40,020][org.apache.spark.storage.memory.MemoryStore]Block broadcast_39_piece0 stored as bytes in memory (estimated size 1972.0 B, free 912.2 MB)
[INFO][2018-05-25 11:22:40,021][org.apache.spark.storage.BlockManagerInfo]Added broadcast_39_piece0 in memory on 10.194.32.157:53453 (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:22:40,021][org.apache.spark.SparkContext]Created broadcast 39 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:22:40,021][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 39 (MapPartitionsRDD[45] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:22:40,021][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 39.0 with 1 tasks
[INFO][2018-05-25 11:22:40,022][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 39.0 (TID 39, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:22:40,022][org.apache.spark.executor.Executor]Running task 0.0 in stage 39.0 (TID 39)
[INFO][2018-05-25 11:22:40,023][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12465 -> 12475
[INFO][2018-05-25 11:22:40,025][org.apache.spark.executor.Executor]Finished task 0.0 in stage 39.0 (TID 39). 970 bytes result sent to driver
[INFO][2018-05-25 11:22:40,026][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 39.0 (TID 39) in 3 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:22:40,026][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 39.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:22:40,026][org.apache.spark.scheduler.DAGScheduler]ResultStage 39 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.004 s
[INFO][2018-05-25 11:22:40,027][org.apache.spark.scheduler.DAGScheduler]Job 39 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.008894 s
[INFO][2018-05-25 11:22:40,031][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:73
[INFO][2018-05-25 11:22:40,031][org.apache.spark.scheduler.DAGScheduler]Got job 40 (foreachPartition at ReceiveKafkaData.scala:73) with 1 output partitions
[INFO][2018-05-25 11:22:40,031][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 40 (foreachPartition at ReceiveKafkaData.scala:73)
[INFO][2018-05-25 11:22:40,031][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:22:40,031][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:22:40,032][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 40 (MapPartitionsRDD[45] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:22:40,033][org.apache.spark.storage.memory.MemoryStore]Block broadcast_40 stored as values in memory (estimated size 3.4 KB, free 912.2 MB)
[INFO][2018-05-25 11:22:40,034][org.apache.spark.storage.memory.MemoryStore]Block broadcast_40_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.2 MB)
[INFO][2018-05-25 11:22:40,034][org.apache.spark.storage.BlockManagerInfo]Added broadcast_40_piece0 in memory on 10.194.32.157:53453 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:22:40,035][org.apache.spark.SparkContext]Created broadcast 40 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:22:40,035][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 40 (MapPartitionsRDD[45] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:22:40,035][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 40.0 with 1 tasks
[INFO][2018-05-25 11:22:40,036][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 40.0 (TID 40, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:22:40,036][org.apache.spark.executor.Executor]Running task 0.0 in stage 40.0 (TID 40)
[INFO][2018-05-25 11:22:40,037][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12465 -> 12475
[INFO][2018-05-25 11:22:40,037][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 12465
[INFO][2018-05-25 11:22:40,067][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-25 11:22:40,067][org.apache.spark.executor.Executor]Finished task 0.0 in stage 40.0 (TID 40). 665 bytes result sent to driver
[INFO][2018-05-25 11:22:40,068][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 40.0 (TID 40) in 32 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:22:40,068][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 40.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:22:40,068][org.apache.spark.scheduler.DAGScheduler]ResultStage 40 (foreachPartition at ReceiveKafkaData.scala:73) finished in 0.033 s
[INFO][2018-05-25 11:22:40,069][org.apache.spark.scheduler.DAGScheduler]Job 40 finished: foreachPartition at ReceiveKafkaData.scala:73, took 0.037562 s
[INFO][2018-05-25 11:22:40,069][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218560000 ms.0 from job set of time 1527218560000 ms
[INFO][2018-05-25 11:22:40,069][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 43 from persistence list
[INFO][2018-05-25 11:22:40,070][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.069 s for time 1527218560000 ms (execution: 0.055 s)
[INFO][2018-05-25 11:22:40,070][org.apache.spark.storage.BlockManager]Removing RDD 43
[INFO][2018-05-25 11:22:40,070][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 42 from persistence list
[INFO][2018-05-25 11:22:40,071][org.apache.spark.storage.BlockManager]Removing RDD 42
[INFO][2018-05-25 11:22:40,071][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:22:40,071][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218550000 ms
[INFO][2018-05-25 11:22:45,019][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218565000 ms
[INFO][2018-05-25 11:22:45,020][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218565000 ms.0 from job set of time 1527218565000 ms
[INFO][2018-05-25 11:22:45,024][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:22:45,025][org.apache.spark.scheduler.DAGScheduler]Got job 41 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:22:45,025][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 41 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:22:45,025][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:22:45,025][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:22:45,025][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 41 (MapPartitionsRDD[47] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:22:45,026][org.apache.spark.storage.memory.MemoryStore]Block broadcast_41 stored as values in memory (estimated size 3.1 KB, free 912.2 MB)
[INFO][2018-05-25 11:22:45,028][org.apache.spark.storage.memory.MemoryStore]Block broadcast_41_piece0 stored as bytes in memory (estimated size 1972.0 B, free 912.2 MB)
[INFO][2018-05-25 11:22:45,028][org.apache.spark.storage.BlockManagerInfo]Added broadcast_41_piece0 in memory on 10.194.32.157:53453 (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:22:45,028][org.apache.spark.SparkContext]Created broadcast 41 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:22:45,029][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 41 (MapPartitionsRDD[47] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:22:45,029][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 41.0 with 1 tasks
[INFO][2018-05-25 11:22:45,029][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 41.0 (TID 41, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:22:45,029][org.apache.spark.executor.Executor]Running task 0.0 in stage 41.0 (TID 41)
[INFO][2018-05-25 11:22:45,031][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12475 -> 12484
[INFO][2018-05-25 11:22:45,031][org.apache.spark.executor.Executor]Finished task 0.0 in stage 41.0 (TID 41). 960 bytes result sent to driver
[INFO][2018-05-25 11:22:45,032][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 41.0 (TID 41) in 3 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:22:45,032][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 41.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:22:45,032][org.apache.spark.scheduler.DAGScheduler]ResultStage 41 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.003 s
[INFO][2018-05-25 11:22:45,033][org.apache.spark.scheduler.DAGScheduler]Job 41 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.008379 s
[INFO][2018-05-25 11:22:45,036][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:73
[INFO][2018-05-25 11:22:45,037][org.apache.spark.scheduler.DAGScheduler]Got job 42 (foreachPartition at ReceiveKafkaData.scala:73) with 1 output partitions
[INFO][2018-05-25 11:22:45,037][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 42 (foreachPartition at ReceiveKafkaData.scala:73)
[INFO][2018-05-25 11:22:45,037][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:22:45,037][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:22:45,037][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 42 (MapPartitionsRDD[47] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:22:45,039][org.apache.spark.storage.memory.MemoryStore]Block broadcast_42 stored as values in memory (estimated size 3.4 KB, free 912.2 MB)
[INFO][2018-05-25 11:22:45,040][org.apache.spark.storage.memory.MemoryStore]Block broadcast_42_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.2 MB)
[INFO][2018-05-25 11:22:45,040][org.apache.spark.storage.BlockManagerInfo]Added broadcast_42_piece0 in memory on 10.194.32.157:53453 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:22:45,041][org.apache.spark.SparkContext]Created broadcast 42 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:22:45,041][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 42 (MapPartitionsRDD[47] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:22:45,041][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 42.0 with 1 tasks
[INFO][2018-05-25 11:22:45,042][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 42.0 (TID 42, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:22:45,042][org.apache.spark.executor.Executor]Running task 0.0 in stage 42.0 (TID 42)
[INFO][2018-05-25 11:22:45,043][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12475 -> 12484
[INFO][2018-05-25 11:22:45,043][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 12475
[INFO][2018-05-25 11:22:45,078][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-25 11:22:45,078][org.apache.spark.executor.Executor]Finished task 0.0 in stage 42.0 (TID 42). 708 bytes result sent to driver
[INFO][2018-05-25 11:22:45,079][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 42.0 (TID 42) in 37 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:22:45,079][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 42.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:22:45,080][org.apache.spark.scheduler.DAGScheduler]ResultStage 42 (foreachPartition at ReceiveKafkaData.scala:73) finished in 0.038 s
[INFO][2018-05-25 11:22:45,080][org.apache.spark.scheduler.DAGScheduler]Job 42 finished: foreachPartition at ReceiveKafkaData.scala:73, took 0.043203 s
[INFO][2018-05-25 11:22:45,080][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218565000 ms.0 from job set of time 1527218565000 ms
[INFO][2018-05-25 11:22:45,080][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.080 s for time 1527218565000 ms (execution: 0.060 s)
[INFO][2018-05-25 11:22:45,080][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 45 from persistence list
[INFO][2018-05-25 11:22:45,081][org.apache.spark.storage.BlockManager]Removing RDD 45
[INFO][2018-05-25 11:22:45,081][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 44 from persistence list
[INFO][2018-05-25 11:22:45,081][org.apache.spark.storage.BlockManager]Removing RDD 44
[INFO][2018-05-25 11:22:45,081][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:22:45,081][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218555000 ms
[INFO][2018-05-25 11:22:50,016][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218570000 ms
[INFO][2018-05-25 11:22:50,016][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218570000 ms.0 from job set of time 1527218570000 ms
[INFO][2018-05-25 11:22:50,020][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:22:50,021][org.apache.spark.scheduler.DAGScheduler]Got job 43 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:22:50,021][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 43 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:22:50,021][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:22:50,021][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:22:50,021][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 43 (MapPartitionsRDD[49] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:22:50,023][org.apache.spark.storage.memory.MemoryStore]Block broadcast_43 stored as values in memory (estimated size 3.1 KB, free 912.2 MB)
[INFO][2018-05-25 11:22:50,024][org.apache.spark.storage.memory.MemoryStore]Block broadcast_43_piece0 stored as bytes in memory (estimated size 1971.0 B, free 912.2 MB)
[INFO][2018-05-25 11:22:50,025][org.apache.spark.storage.BlockManagerInfo]Added broadcast_43_piece0 in memory on 10.194.32.157:53453 (size: 1971.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:22:50,025][org.apache.spark.SparkContext]Created broadcast 43 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:22:50,025][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 43 (MapPartitionsRDD[49] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:22:50,025][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 43.0 with 1 tasks
[INFO][2018-05-25 11:22:50,026][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 43.0 (TID 43, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:22:50,026][org.apache.spark.executor.Executor]Running task 0.0 in stage 43.0 (TID 43)
[INFO][2018-05-25 11:22:50,027][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12484 -> 12494
[INFO][2018-05-25 11:22:50,027][org.apache.spark.executor.Executor]Finished task 0.0 in stage 43.0 (TID 43). 932 bytes result sent to driver
[INFO][2018-05-25 11:22:50,028][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 43.0 (TID 43) in 2 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:22:50,028][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 43.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:22:50,028][org.apache.spark.scheduler.DAGScheduler]ResultStage 43 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.002 s
[INFO][2018-05-25 11:22:50,028][org.apache.spark.scheduler.DAGScheduler]Job 43 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.007870 s
[INFO][2018-05-25 11:22:50,033][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:73
[INFO][2018-05-25 11:22:50,034][org.apache.spark.scheduler.DAGScheduler]Got job 44 (foreachPartition at ReceiveKafkaData.scala:73) with 1 output partitions
[INFO][2018-05-25 11:22:50,034][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 44 (foreachPartition at ReceiveKafkaData.scala:73)
[INFO][2018-05-25 11:22:50,034][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:22:50,034][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:22:50,034][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 44 (MapPartitionsRDD[49] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:22:50,035][org.apache.spark.storage.memory.MemoryStore]Block broadcast_44 stored as values in memory (estimated size 3.4 KB, free 912.2 MB)
[INFO][2018-05-25 11:22:50,041][org.apache.spark.storage.memory.MemoryStore]Block broadcast_44_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.2 MB)
[INFO][2018-05-25 11:22:50,041][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_39_piece0 on 10.194.32.157:53453 in memory (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:22:50,041][org.apache.spark.storage.BlockManagerInfo]Added broadcast_44_piece0 in memory on 10.194.32.157:53453 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:22:50,042][org.apache.spark.SparkContext]Created broadcast 44 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:22:50,042][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 44 (MapPartitionsRDD[49] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:22:50,042][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 44.0 with 1 tasks
[INFO][2018-05-25 11:22:50,042][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 44.0 (TID 44, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:22:50,043][org.apache.spark.executor.Executor]Running task 0.0 in stage 44.0 (TID 44)
[INFO][2018-05-25 11:22:50,043][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_31_piece0 on 10.194.32.157:53453 in memory (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:22:50,045][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12484 -> 12494
[INFO][2018-05-25 11:22:50,045][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 12484
[INFO][2018-05-25 11:22:50,045][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_34_piece0 on 10.194.32.157:53453 in memory (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:22:50,047][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_23_piece0 on 10.194.32.157:53453 in memory (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:22:50,048][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_32_piece0 on 10.194.32.157:53453 in memory (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:22:50,049][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_35_piece0 on 10.194.32.157:53453 in memory (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:22:50,050][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_27_piece0 on 10.194.32.157:53453 in memory (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:22:50,050][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_38_piece0 on 10.194.32.157:53453 in memory (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:22:50,051][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_41_piece0 on 10.194.32.157:53453 in memory (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:22:50,052][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_26_piece0 on 10.194.32.157:53453 in memory (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:22:50,052][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_33_piece0 on 10.194.32.157:53453 in memory (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:22:50,053][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_43_piece0 on 10.194.32.157:53453 in memory (size: 1971.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:22:50,054][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_22_piece0 on 10.194.32.157:53453 in memory (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:22:50,055][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_42_piece0 on 10.194.32.157:53453 in memory (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:22:50,057][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_30_piece0 on 10.194.32.157:53453 in memory (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:22:50,059][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_40_piece0 on 10.194.32.157:53453 in memory (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:22:50,060][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_37_piece0 on 10.194.32.157:53453 in memory (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:22:50,061][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_24_piece0 on 10.194.32.157:53453 in memory (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:22:50,062][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_28_piece0 on 10.194.32.157:53453 in memory (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:22:50,063][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_25_piece0 on 10.194.32.157:53453 in memory (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:22:50,065][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_36_piece0 on 10.194.32.157:53453 in memory (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:22:50,066][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_29_piece0 on 10.194.32.157:53453 in memory (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:22:50,076][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-25 11:22:50,077][org.apache.spark.executor.Executor]Finished task 0.0 in stage 44.0 (TID 44). 708 bytes result sent to driver
[INFO][2018-05-25 11:22:50,077][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 44.0 (TID 44) in 35 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:22:50,077][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 44.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:22:50,078][org.apache.spark.scheduler.DAGScheduler]ResultStage 44 (foreachPartition at ReceiveKafkaData.scala:73) finished in 0.035 s
[INFO][2018-05-25 11:22:50,078][org.apache.spark.scheduler.DAGScheduler]Job 44 finished: foreachPartition at ReceiveKafkaData.scala:73, took 0.044636 s
[INFO][2018-05-25 11:22:50,078][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218570000 ms.0 from job set of time 1527218570000 ms
[INFO][2018-05-25 11:22:50,078][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 47 from persistence list
[INFO][2018-05-25 11:22:50,078][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.078 s for time 1527218570000 ms (execution: 0.062 s)
[INFO][2018-05-25 11:22:50,079][org.apache.spark.storage.BlockManager]Removing RDD 47
[INFO][2018-05-25 11:22:50,079][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 46 from persistence list
[INFO][2018-05-25 11:22:50,079][org.apache.spark.storage.BlockManager]Removing RDD 46
[INFO][2018-05-25 11:22:50,079][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:22:50,079][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218560000 ms
[INFO][2018-05-25 11:22:55,020][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218575000 ms
[INFO][2018-05-25 11:22:55,020][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218575000 ms.0 from job set of time 1527218575000 ms
[INFO][2018-05-25 11:22:55,024][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:22:55,024][org.apache.spark.scheduler.DAGScheduler]Got job 45 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:22:55,024][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 45 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:22:55,024][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:22:55,024][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:22:55,025][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 45 (MapPartitionsRDD[51] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:22:55,026][org.apache.spark.storage.memory.MemoryStore]Block broadcast_45 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-25 11:22:55,027][org.apache.spark.storage.memory.MemoryStore]Block broadcast_45_piece0 stored as bytes in memory (estimated size 1972.0 B, free 912.3 MB)
[INFO][2018-05-25 11:22:55,028][org.apache.spark.storage.BlockManagerInfo]Added broadcast_45_piece0 in memory on 10.194.32.157:53453 (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:22:55,028][org.apache.spark.SparkContext]Created broadcast 45 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:22:55,028][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 45 (MapPartitionsRDD[51] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:22:55,028][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 45.0 with 1 tasks
[INFO][2018-05-25 11:22:55,029][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 45.0 (TID 45, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:22:55,029][org.apache.spark.executor.Executor]Running task 0.0 in stage 45.0 (TID 45)
[INFO][2018-05-25 11:22:55,030][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12494 -> 12504
[INFO][2018-05-25 11:22:55,030][org.apache.spark.executor.Executor]Finished task 0.0 in stage 45.0 (TID 45). 931 bytes result sent to driver
[INFO][2018-05-25 11:22:55,031][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 45.0 (TID 45) in 3 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:22:55,031][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 45.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:22:55,031][org.apache.spark.scheduler.DAGScheduler]ResultStage 45 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.003 s
[INFO][2018-05-25 11:22:55,031][org.apache.spark.scheduler.DAGScheduler]Job 45 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.007624 s
[INFO][2018-05-25 11:22:55,035][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:73
[INFO][2018-05-25 11:22:55,036][org.apache.spark.scheduler.DAGScheduler]Got job 46 (foreachPartition at ReceiveKafkaData.scala:73) with 1 output partitions
[INFO][2018-05-25 11:22:55,036][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 46 (foreachPartition at ReceiveKafkaData.scala:73)
[INFO][2018-05-25 11:22:55,036][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:22:55,036][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:22:55,036][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 46 (MapPartitionsRDD[51] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:22:55,037][org.apache.spark.storage.memory.MemoryStore]Block broadcast_46 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-25 11:22:55,038][org.apache.spark.storage.memory.MemoryStore]Block broadcast_46_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.3 MB)
[INFO][2018-05-25 11:22:55,038][org.apache.spark.storage.BlockManagerInfo]Added broadcast_46_piece0 in memory on 10.194.32.157:53453 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:22:55,039][org.apache.spark.SparkContext]Created broadcast 46 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:22:55,039][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 46 (MapPartitionsRDD[51] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:22:55,039][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 46.0 with 1 tasks
[INFO][2018-05-25 11:22:55,039][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 46.0 (TID 46, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:22:55,039][org.apache.spark.executor.Executor]Running task 0.0 in stage 46.0 (TID 46)
[INFO][2018-05-25 11:22:55,041][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12494 -> 12504
[INFO][2018-05-25 11:22:55,041][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 12494
[INFO][2018-05-25 11:22:55,073][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-25 11:22:55,074][org.apache.spark.executor.Executor]Finished task 0.0 in stage 46.0 (TID 46). 708 bytes result sent to driver
[INFO][2018-05-25 11:22:55,074][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 46.0 (TID 46) in 35 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:22:55,074][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 46.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:22:55,075][org.apache.spark.scheduler.DAGScheduler]ResultStage 46 (foreachPartition at ReceiveKafkaData.scala:73) finished in 0.036 s
[INFO][2018-05-25 11:22:55,075][org.apache.spark.scheduler.DAGScheduler]Job 46 finished: foreachPartition at ReceiveKafkaData.scala:73, took 0.039422 s
[INFO][2018-05-25 11:22:55,075][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218575000 ms.0 from job set of time 1527218575000 ms
[INFO][2018-05-25 11:22:55,075][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.075 s for time 1527218575000 ms (execution: 0.055 s)
[INFO][2018-05-25 11:22:55,075][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 49 from persistence list
[INFO][2018-05-25 11:22:55,076][org.apache.spark.storage.BlockManager]Removing RDD 49
[INFO][2018-05-25 11:22:55,076][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 48 from persistence list
[INFO][2018-05-25 11:22:55,076][org.apache.spark.storage.BlockManager]Removing RDD 48
[INFO][2018-05-25 11:22:55,076][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:22:55,077][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218565000 ms
[INFO][2018-05-25 11:23:00,016][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218580000 ms
[INFO][2018-05-25 11:23:00,018][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218580000 ms.0 from job set of time 1527218580000 ms
[INFO][2018-05-25 11:23:00,022][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:23:00,023][org.apache.spark.scheduler.DAGScheduler]Got job 47 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:23:00,023][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 47 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:23:00,023][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:23:00,023][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:23:00,023][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 47 (MapPartitionsRDD[53] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:23:00,025][org.apache.spark.storage.memory.MemoryStore]Block broadcast_47 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-25 11:23:00,026][org.apache.spark.storage.memory.MemoryStore]Block broadcast_47_piece0 stored as bytes in memory (estimated size 1972.0 B, free 912.3 MB)
[INFO][2018-05-25 11:23:00,026][org.apache.spark.storage.BlockManagerInfo]Added broadcast_47_piece0 in memory on 10.194.32.157:53453 (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:23:00,026][org.apache.spark.SparkContext]Created broadcast 47 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:23:00,027][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 47 (MapPartitionsRDD[53] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:23:00,027][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 47.0 with 1 tasks
[INFO][2018-05-25 11:23:00,028][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 47.0 (TID 47, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:23:00,028][org.apache.spark.executor.Executor]Running task 0.0 in stage 47.0 (TID 47)
[INFO][2018-05-25 11:23:00,029][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12504 -> 12514
[INFO][2018-05-25 11:23:00,030][org.apache.spark.executor.Executor]Finished task 0.0 in stage 47.0 (TID 47). 930 bytes result sent to driver
[INFO][2018-05-25 11:23:00,030][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 47.0 (TID 47) in 2 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:23:00,030][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 47.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:23:00,030][org.apache.spark.scheduler.DAGScheduler]ResultStage 47 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.003 s
[INFO][2018-05-25 11:23:00,030][org.apache.spark.scheduler.DAGScheduler]Job 47 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.007757 s
[INFO][2018-05-25 11:23:00,033][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:73
[INFO][2018-05-25 11:23:00,034][org.apache.spark.scheduler.DAGScheduler]Got job 48 (foreachPartition at ReceiveKafkaData.scala:73) with 1 output partitions
[INFO][2018-05-25 11:23:00,034][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 48 (foreachPartition at ReceiveKafkaData.scala:73)
[INFO][2018-05-25 11:23:00,034][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:23:00,034][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:23:00,034][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 48 (MapPartitionsRDD[53] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:23:00,035][org.apache.spark.storage.memory.MemoryStore]Block broadcast_48 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-25 11:23:00,036][org.apache.spark.storage.memory.MemoryStore]Block broadcast_48_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.3 MB)
[INFO][2018-05-25 11:23:00,036][org.apache.spark.storage.BlockManagerInfo]Added broadcast_48_piece0 in memory on 10.194.32.157:53453 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:23:00,036][org.apache.spark.SparkContext]Created broadcast 48 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:23:00,037][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 48 (MapPartitionsRDD[53] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:23:00,037][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 48.0 with 1 tasks
[INFO][2018-05-25 11:23:00,037][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 48.0 (TID 48, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:23:00,038][org.apache.spark.executor.Executor]Running task 0.0 in stage 48.0 (TID 48)
[INFO][2018-05-25 11:23:00,039][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12504 -> 12514
[INFO][2018-05-25 11:23:00,039][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 12504
[INFO][2018-05-25 11:23:00,076][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-25 11:23:00,076][org.apache.spark.executor.Executor]Finished task 0.0 in stage 48.0 (TID 48). 708 bytes result sent to driver
[INFO][2018-05-25 11:23:00,077][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 48.0 (TID 48) in 40 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:23:00,077][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 48.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:23:00,077][org.apache.spark.scheduler.DAGScheduler]ResultStage 48 (foreachPartition at ReceiveKafkaData.scala:73) finished in 0.040 s
[INFO][2018-05-25 11:23:00,077][org.apache.spark.scheduler.DAGScheduler]Job 48 finished: foreachPartition at ReceiveKafkaData.scala:73, took 0.044002 s
[INFO][2018-05-25 11:23:00,078][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218580000 ms.0 from job set of time 1527218580000 ms
[INFO][2018-05-25 11:23:00,078][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.078 s for time 1527218580000 ms (execution: 0.061 s)
[INFO][2018-05-25 11:23:00,078][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 51 from persistence list
[INFO][2018-05-25 11:23:00,078][org.apache.spark.storage.BlockManager]Removing RDD 51
[INFO][2018-05-25 11:23:00,078][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 50 from persistence list
[INFO][2018-05-25 11:23:00,079][org.apache.spark.storage.BlockManager]Removing RDD 50
[INFO][2018-05-25 11:23:00,079][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:23:00,079][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218570000 ms
[INFO][2018-05-25 11:23:05,023][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218585000 ms
[INFO][2018-05-25 11:23:05,023][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218585000 ms.0 from job set of time 1527218585000 ms
[INFO][2018-05-25 11:23:05,028][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:23:05,028][org.apache.spark.scheduler.DAGScheduler]Got job 49 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:23:05,028][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 49 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:23:05,028][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:23:05,029][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:23:05,029][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 49 (MapPartitionsRDD[55] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:23:05,030][org.apache.spark.storage.memory.MemoryStore]Block broadcast_49 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-25 11:23:05,032][org.apache.spark.storage.memory.MemoryStore]Block broadcast_49_piece0 stored as bytes in memory (estimated size 1972.0 B, free 912.3 MB)
[INFO][2018-05-25 11:23:05,032][org.apache.spark.storage.BlockManagerInfo]Added broadcast_49_piece0 in memory on 10.194.32.157:53453 (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:23:05,033][org.apache.spark.SparkContext]Created broadcast 49 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:23:05,033][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 49 (MapPartitionsRDD[55] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:23:05,033][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 49.0 with 1 tasks
[INFO][2018-05-25 11:23:05,034][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 49.0 (TID 49, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:23:05,037][org.apache.spark.executor.Executor]Running task 0.0 in stage 49.0 (TID 49)
[INFO][2018-05-25 11:23:05,039][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12514 -> 12524
[INFO][2018-05-25 11:23:05,041][org.apache.spark.executor.Executor]Finished task 0.0 in stage 49.0 (TID 49). 973 bytes result sent to driver
[INFO][2018-05-25 11:23:05,042][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 49.0 (TID 49) in 8 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:23:05,042][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 49.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:23:05,043][org.apache.spark.scheduler.DAGScheduler]ResultStage 49 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.008 s
[INFO][2018-05-25 11:23:05,044][org.apache.spark.scheduler.DAGScheduler]Job 49 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.015991 s
[INFO][2018-05-25 11:23:05,048][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:73
[INFO][2018-05-25 11:23:05,048][org.apache.spark.scheduler.DAGScheduler]Got job 50 (foreachPartition at ReceiveKafkaData.scala:73) with 1 output partitions
[INFO][2018-05-25 11:23:05,048][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 50 (foreachPartition at ReceiveKafkaData.scala:73)
[INFO][2018-05-25 11:23:05,048][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:23:05,048][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:23:05,048][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 50 (MapPartitionsRDD[55] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:23:05,050][org.apache.spark.storage.memory.MemoryStore]Block broadcast_50 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-25 11:23:05,051][org.apache.spark.storage.memory.MemoryStore]Block broadcast_50_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.3 MB)
[INFO][2018-05-25 11:23:05,051][org.apache.spark.storage.BlockManagerInfo]Added broadcast_50_piece0 in memory on 10.194.32.157:53453 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:23:05,051][org.apache.spark.SparkContext]Created broadcast 50 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:23:05,052][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 50 (MapPartitionsRDD[55] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:23:05,052][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 50.0 with 1 tasks
[INFO][2018-05-25 11:23:05,052][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 50.0 (TID 50, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:23:05,052][org.apache.spark.executor.Executor]Running task 0.0 in stage 50.0 (TID 50)
[INFO][2018-05-25 11:23:05,053][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12514 -> 12524
[INFO][2018-05-25 11:23:05,053][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 12514
[INFO][2018-05-25 11:23:06,098][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-25 11:23:06,098][org.apache.spark.executor.Executor]Finished task 0.0 in stage 50.0 (TID 50). 708 bytes result sent to driver
[INFO][2018-05-25 11:23:06,099][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 50.0 (TID 50) in 1047 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:23:06,099][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 50.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:23:06,099][org.apache.spark.scheduler.DAGScheduler]ResultStage 50 (foreachPartition at ReceiveKafkaData.scala:73) finished in 1.047 s
[INFO][2018-05-25 11:23:06,099][org.apache.spark.scheduler.DAGScheduler]Job 50 finished: foreachPartition at ReceiveKafkaData.scala:73, took 1.051534 s
[INFO][2018-05-25 11:23:06,100][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218585000 ms.0 from job set of time 1527218585000 ms
[INFO][2018-05-25 11:23:06,100][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 1.100 s for time 1527218585000 ms (execution: 1.077 s)
[INFO][2018-05-25 11:23:06,100][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 53 from persistence list
[INFO][2018-05-25 11:23:06,100][org.apache.spark.storage.BlockManager]Removing RDD 53
[INFO][2018-05-25 11:23:06,100][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 52 from persistence list
[INFO][2018-05-25 11:23:06,100][org.apache.spark.storage.BlockManager]Removing RDD 52
[INFO][2018-05-25 11:23:06,100][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:23:06,100][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218575000 ms
[INFO][2018-05-25 11:23:10,023][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218590000 ms
[INFO][2018-05-25 11:23:10,023][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218590000 ms.0 from job set of time 1527218590000 ms
[INFO][2018-05-25 11:23:10,027][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:23:10,028][org.apache.spark.scheduler.DAGScheduler]Got job 51 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:23:10,028][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 51 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:23:10,028][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:23:10,028][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:23:10,028][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 51 (MapPartitionsRDD[57] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:23:10,030][org.apache.spark.storage.memory.MemoryStore]Block broadcast_51 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-25 11:23:10,031][org.apache.spark.storage.memory.MemoryStore]Block broadcast_51_piece0 stored as bytes in memory (estimated size 1972.0 B, free 912.3 MB)
[INFO][2018-05-25 11:23:10,031][org.apache.spark.storage.BlockManagerInfo]Added broadcast_51_piece0 in memory on 10.194.32.157:53453 (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:23:10,031][org.apache.spark.SparkContext]Created broadcast 51 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:23:10,032][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 51 (MapPartitionsRDD[57] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:23:10,032][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 51.0 with 1 tasks
[INFO][2018-05-25 11:23:10,032][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 51.0 (TID 51, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:23:10,032][org.apache.spark.executor.Executor]Running task 0.0 in stage 51.0 (TID 51)
[INFO][2018-05-25 11:23:10,034][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12524 -> 12534
[INFO][2018-05-25 11:23:10,035][org.apache.spark.executor.Executor]Finished task 0.0 in stage 51.0 (TID 51). 928 bytes result sent to driver
[INFO][2018-05-25 11:23:10,035][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 51.0 (TID 51) in 3 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:23:10,035][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 51.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:23:10,035][org.apache.spark.scheduler.DAGScheduler]ResultStage 51 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.003 s
[INFO][2018-05-25 11:23:10,036][org.apache.spark.scheduler.DAGScheduler]Job 51 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.007927 s
[INFO][2018-05-25 11:23:10,039][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:73
[INFO][2018-05-25 11:23:10,039][org.apache.spark.scheduler.DAGScheduler]Got job 52 (foreachPartition at ReceiveKafkaData.scala:73) with 1 output partitions
[INFO][2018-05-25 11:23:10,039][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 52 (foreachPartition at ReceiveKafkaData.scala:73)
[INFO][2018-05-25 11:23:10,039][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:23:10,040][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:23:10,040][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 52 (MapPartitionsRDD[57] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:23:10,040][org.apache.spark.storage.memory.MemoryStore]Block broadcast_52 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-25 11:23:10,041][org.apache.spark.storage.memory.MemoryStore]Block broadcast_52_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.2 MB)
[INFO][2018-05-25 11:23:10,041][org.apache.spark.storage.BlockManagerInfo]Added broadcast_52_piece0 in memory on 10.194.32.157:53453 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:23:10,041][org.apache.spark.SparkContext]Created broadcast 52 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:23:10,042][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 52 (MapPartitionsRDD[57] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:23:10,042][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 52.0 with 1 tasks
[INFO][2018-05-25 11:23:10,042][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 52.0 (TID 52, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:23:10,042][org.apache.spark.executor.Executor]Running task 0.0 in stage 52.0 (TID 52)
[INFO][2018-05-25 11:23:10,043][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12524 -> 12534
[INFO][2018-05-25 11:23:10,043][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 12524
[INFO][2018-05-25 11:23:10,083][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-25 11:23:10,084][org.apache.spark.executor.Executor]Finished task 0.0 in stage 52.0 (TID 52). 708 bytes result sent to driver
[INFO][2018-05-25 11:23:10,084][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 52.0 (TID 52) in 42 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:23:10,085][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 52.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:23:10,085][org.apache.spark.scheduler.DAGScheduler]ResultStage 52 (foreachPartition at ReceiveKafkaData.scala:73) finished in 0.043 s
[INFO][2018-05-25 11:23:10,085][org.apache.spark.scheduler.DAGScheduler]Job 52 finished: foreachPartition at ReceiveKafkaData.scala:73, took 0.046052 s
[INFO][2018-05-25 11:23:10,086][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218590000 ms.0 from job set of time 1527218590000 ms
[INFO][2018-05-25 11:23:10,086][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.085 s for time 1527218590000 ms (execution: 0.062 s)
[INFO][2018-05-25 11:23:10,086][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 55 from persistence list
[INFO][2018-05-25 11:23:10,086][org.apache.spark.storage.BlockManager]Removing RDD 55
[INFO][2018-05-25 11:23:10,086][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 54 from persistence list
[INFO][2018-05-25 11:23:10,087][org.apache.spark.storage.BlockManager]Removing RDD 54
[INFO][2018-05-25 11:23:10,087][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:23:10,087][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218580000 ms
[INFO][2018-05-25 11:23:15,021][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218595000 ms
[INFO][2018-05-25 11:23:15,021][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218595000 ms.0 from job set of time 1527218595000 ms
[INFO][2018-05-25 11:23:15,026][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:23:15,026][org.apache.spark.scheduler.DAGScheduler]Got job 53 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:23:15,026][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 53 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:23:15,026][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:23:15,026][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:23:15,027][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 53 (MapPartitionsRDD[59] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:23:15,028][org.apache.spark.storage.memory.MemoryStore]Block broadcast_53 stored as values in memory (estimated size 3.1 KB, free 912.2 MB)
[INFO][2018-05-25 11:23:15,029][org.apache.spark.storage.memory.MemoryStore]Block broadcast_53_piece0 stored as bytes in memory (estimated size 1972.0 B, free 912.2 MB)
[INFO][2018-05-25 11:23:15,029][org.apache.spark.storage.BlockManagerInfo]Added broadcast_53_piece0 in memory on 10.194.32.157:53453 (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:23:15,029][org.apache.spark.SparkContext]Created broadcast 53 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:23:15,030][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 53 (MapPartitionsRDD[59] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:23:15,030][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 53.0 with 1 tasks
[INFO][2018-05-25 11:23:15,030][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 53.0 (TID 53, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:23:15,030][org.apache.spark.executor.Executor]Running task 0.0 in stage 53.0 (TID 53)
[INFO][2018-05-25 11:23:15,032][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12534 -> 12544
[INFO][2018-05-25 11:23:15,033][org.apache.spark.executor.Executor]Finished task 0.0 in stage 53.0 (TID 53). 972 bytes result sent to driver
[INFO][2018-05-25 11:23:15,033][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 53.0 (TID 53) in 3 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:23:15,033][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 53.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:23:15,034][org.apache.spark.scheduler.DAGScheduler]ResultStage 53 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.004 s
[INFO][2018-05-25 11:23:15,034][org.apache.spark.scheduler.DAGScheduler]Job 53 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.007831 s
[INFO][2018-05-25 11:23:15,037][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:73
[INFO][2018-05-25 11:23:15,038][org.apache.spark.scheduler.DAGScheduler]Got job 54 (foreachPartition at ReceiveKafkaData.scala:73) with 1 output partitions
[INFO][2018-05-25 11:23:15,038][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 54 (foreachPartition at ReceiveKafkaData.scala:73)
[INFO][2018-05-25 11:23:15,038][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:23:15,038][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:23:15,038][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 54 (MapPartitionsRDD[59] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:23:15,039][org.apache.spark.storage.memory.MemoryStore]Block broadcast_54 stored as values in memory (estimated size 3.4 KB, free 912.2 MB)
[INFO][2018-05-25 11:23:15,040][org.apache.spark.storage.memory.MemoryStore]Block broadcast_54_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.2 MB)
[INFO][2018-05-25 11:23:15,040][org.apache.spark.storage.BlockManagerInfo]Added broadcast_54_piece0 in memory on 10.194.32.157:53453 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:23:15,041][org.apache.spark.SparkContext]Created broadcast 54 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:23:15,041][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 54 (MapPartitionsRDD[59] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:23:15,041][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 54.0 with 1 tasks
[INFO][2018-05-25 11:23:15,042][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 54.0 (TID 54, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:23:15,042][org.apache.spark.executor.Executor]Running task 0.0 in stage 54.0 (TID 54)
[INFO][2018-05-25 11:23:15,043][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12534 -> 12544
[INFO][2018-05-25 11:23:15,043][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 12534
[INFO][2018-05-25 11:23:15,079][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-25 11:23:15,080][org.apache.spark.executor.Executor]Finished task 0.0 in stage 54.0 (TID 54). 708 bytes result sent to driver
[INFO][2018-05-25 11:23:15,080][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 54.0 (TID 54) in 39 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:23:15,081][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 54.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:23:15,081][org.apache.spark.scheduler.DAGScheduler]ResultStage 54 (foreachPartition at ReceiveKafkaData.scala:73) finished in 0.040 s
[INFO][2018-05-25 11:23:15,081][org.apache.spark.scheduler.DAGScheduler]Job 54 finished: foreachPartition at ReceiveKafkaData.scala:73, took 0.043854 s
[INFO][2018-05-25 11:23:15,082][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218595000 ms.0 from job set of time 1527218595000 ms
[INFO][2018-05-25 11:23:15,082][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 57 from persistence list
[INFO][2018-05-25 11:23:15,082][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.082 s for time 1527218595000 ms (execution: 0.061 s)
[INFO][2018-05-25 11:23:15,082][org.apache.spark.storage.BlockManager]Removing RDD 57
[INFO][2018-05-25 11:23:15,082][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 56 from persistence list
[INFO][2018-05-25 11:23:15,083][org.apache.spark.storage.BlockManager]Removing RDD 56
[INFO][2018-05-25 11:23:15,083][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:23:15,083][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218585000 ms
[INFO][2018-05-25 11:23:20,019][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218600000 ms
[INFO][2018-05-25 11:23:20,020][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218600000 ms.0 from job set of time 1527218600000 ms
[INFO][2018-05-25 11:23:20,023][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:23:20,024][org.apache.spark.scheduler.DAGScheduler]Got job 55 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:23:20,024][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 55 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:23:20,024][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:23:20,024][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:23:20,024][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 55 (MapPartitionsRDD[61] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:23:20,025][org.apache.spark.storage.memory.MemoryStore]Block broadcast_55 stored as values in memory (estimated size 3.1 KB, free 912.2 MB)
[INFO][2018-05-25 11:23:20,026][org.apache.spark.storage.memory.MemoryStore]Block broadcast_55_piece0 stored as bytes in memory (estimated size 1971.0 B, free 912.2 MB)
[INFO][2018-05-25 11:23:20,026][org.apache.spark.storage.BlockManagerInfo]Added broadcast_55_piece0 in memory on 10.194.32.157:53453 (size: 1971.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:23:20,027][org.apache.spark.SparkContext]Created broadcast 55 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:23:20,027][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 55 (MapPartitionsRDD[61] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:23:20,027][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 55.0 with 1 tasks
[INFO][2018-05-25 11:23:20,028][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 55.0 (TID 55, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:23:20,028][org.apache.spark.executor.Executor]Running task 0.0 in stage 55.0 (TID 55)
[INFO][2018-05-25 11:23:20,029][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12544 -> 12554
[INFO][2018-05-25 11:23:20,029][org.apache.spark.executor.Executor]Finished task 0.0 in stage 55.0 (TID 55). 929 bytes result sent to driver
[INFO][2018-05-25 11:23:20,030][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 55.0 (TID 55) in 3 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:23:20,030][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 55.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:23:20,030][org.apache.spark.scheduler.DAGScheduler]ResultStage 55 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.003 s
[INFO][2018-05-25 11:23:20,030][org.apache.spark.scheduler.DAGScheduler]Job 55 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.006785 s
[INFO][2018-05-25 11:23:20,034][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:73
[INFO][2018-05-25 11:23:20,034][org.apache.spark.scheduler.DAGScheduler]Got job 56 (foreachPartition at ReceiveKafkaData.scala:73) with 1 output partitions
[INFO][2018-05-25 11:23:20,034][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 56 (foreachPartition at ReceiveKafkaData.scala:73)
[INFO][2018-05-25 11:23:20,034][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:23:20,034][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:23:20,034][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 56 (MapPartitionsRDD[61] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:23:20,035][org.apache.spark.storage.memory.MemoryStore]Block broadcast_56 stored as values in memory (estimated size 3.4 KB, free 912.2 MB)
[INFO][2018-05-25 11:23:20,036][org.apache.spark.storage.memory.MemoryStore]Block broadcast_56_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.2 MB)
[INFO][2018-05-25 11:23:20,036][org.apache.spark.storage.BlockManagerInfo]Added broadcast_56_piece0 in memory on 10.194.32.157:53453 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:23:20,036][org.apache.spark.SparkContext]Created broadcast 56 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:23:20,036][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 56 (MapPartitionsRDD[61] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:23:20,036][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 56.0 with 1 tasks
[INFO][2018-05-25 11:23:20,037][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 56.0 (TID 56, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:23:20,037][org.apache.spark.executor.Executor]Running task 0.0 in stage 56.0 (TID 56)
[INFO][2018-05-25 11:23:20,038][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12544 -> 12554
[INFO][2018-05-25 11:23:20,038][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 12544
[INFO][2018-05-25 11:23:20,075][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-25 11:23:20,076][org.apache.spark.executor.Executor]Finished task 0.0 in stage 56.0 (TID 56). 708 bytes result sent to driver
[INFO][2018-05-25 11:23:20,076][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 56.0 (TID 56) in 39 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:23:20,076][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 56.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:23:20,077][org.apache.spark.scheduler.DAGScheduler]ResultStage 56 (foreachPartition at ReceiveKafkaData.scala:73) finished in 0.040 s
[INFO][2018-05-25 11:23:20,077][org.apache.spark.scheduler.DAGScheduler]Job 56 finished: foreachPartition at ReceiveKafkaData.scala:73, took 0.043117 s
[INFO][2018-05-25 11:23:20,077][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218600000 ms.0 from job set of time 1527218600000 ms
[INFO][2018-05-25 11:23:20,078][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.077 s for time 1527218600000 ms (execution: 0.057 s)
[INFO][2018-05-25 11:23:20,078][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 59 from persistence list
[INFO][2018-05-25 11:23:20,078][org.apache.spark.storage.BlockManager]Removing RDD 59
[INFO][2018-05-25 11:23:20,078][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 58 from persistence list
[INFO][2018-05-25 11:23:20,078][org.apache.spark.storage.BlockManager]Removing RDD 58
[INFO][2018-05-25 11:23:20,078][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:23:20,078][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218590000 ms
[INFO][2018-05-25 11:23:25,015][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218605000 ms
[INFO][2018-05-25 11:23:25,016][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218605000 ms.0 from job set of time 1527218605000 ms
[INFO][2018-05-25 11:23:25,020][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:23:25,020][org.apache.spark.scheduler.DAGScheduler]Got job 57 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:23:25,020][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 57 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:23:25,020][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:23:25,020][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:23:25,020][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 57 (MapPartitionsRDD[63] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:23:25,021][org.apache.spark.storage.memory.MemoryStore]Block broadcast_57 stored as values in memory (estimated size 3.1 KB, free 912.2 MB)
[INFO][2018-05-25 11:23:25,022][org.apache.spark.storage.memory.MemoryStore]Block broadcast_57_piece0 stored as bytes in memory (estimated size 1972.0 B, free 912.2 MB)
[INFO][2018-05-25 11:23:25,022][org.apache.spark.storage.BlockManagerInfo]Added broadcast_57_piece0 in memory on 10.194.32.157:53453 (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:23:25,023][org.apache.spark.SparkContext]Created broadcast 57 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:23:25,023][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 57 (MapPartitionsRDD[63] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:23:25,023][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 57.0 with 1 tasks
[INFO][2018-05-25 11:23:25,024][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 57.0 (TID 57, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:23:25,024][org.apache.spark.executor.Executor]Running task 0.0 in stage 57.0 (TID 57)
[INFO][2018-05-25 11:23:25,025][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12554 -> 12564
[INFO][2018-05-25 11:23:25,026][org.apache.spark.executor.Executor]Finished task 0.0 in stage 57.0 (TID 57). 974 bytes result sent to driver
[INFO][2018-05-25 11:23:25,026][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 57.0 (TID 57) in 3 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:23:25,026][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 57.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:23:25,027][org.apache.spark.scheduler.DAGScheduler]ResultStage 57 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.004 s
[INFO][2018-05-25 11:23:25,027][org.apache.spark.scheduler.DAGScheduler]Job 57 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.007056 s
[INFO][2018-05-25 11:23:25,030][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:73
[INFO][2018-05-25 11:23:25,031][org.apache.spark.scheduler.DAGScheduler]Got job 58 (foreachPartition at ReceiveKafkaData.scala:73) with 1 output partitions
[INFO][2018-05-25 11:23:25,031][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 58 (foreachPartition at ReceiveKafkaData.scala:73)
[INFO][2018-05-25 11:23:25,031][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:23:25,031][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:23:25,031][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 58 (MapPartitionsRDD[63] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:23:25,032][org.apache.spark.storage.memory.MemoryStore]Block broadcast_58 stored as values in memory (estimated size 3.4 KB, free 912.2 MB)
[INFO][2018-05-25 11:23:25,033][org.apache.spark.storage.memory.MemoryStore]Block broadcast_58_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.2 MB)
[INFO][2018-05-25 11:23:25,033][org.apache.spark.storage.BlockManagerInfo]Added broadcast_58_piece0 in memory on 10.194.32.157:53453 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:23:25,034][org.apache.spark.SparkContext]Created broadcast 58 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:23:25,034][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 58 (MapPartitionsRDD[63] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:23:25,034][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 58.0 with 1 tasks
[INFO][2018-05-25 11:23:25,034][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 58.0 (TID 58, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:23:25,035][org.apache.spark.executor.Executor]Running task 0.0 in stage 58.0 (TID 58)
[INFO][2018-05-25 11:23:25,035][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12554 -> 12564
[INFO][2018-05-25 11:23:25,035][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 12554
[INFO][2018-05-25 11:23:25,068][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-25 11:23:25,068][org.apache.spark.executor.Executor]Finished task 0.0 in stage 58.0 (TID 58). 665 bytes result sent to driver
[INFO][2018-05-25 11:23:25,069][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 58.0 (TID 58) in 35 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:23:25,069][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 58.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:23:25,069][org.apache.spark.scheduler.DAGScheduler]ResultStage 58 (foreachPartition at ReceiveKafkaData.scala:73) finished in 0.035 s
[INFO][2018-05-25 11:23:25,069][org.apache.spark.scheduler.DAGScheduler]Job 58 finished: foreachPartition at ReceiveKafkaData.scala:73, took 0.038745 s
[INFO][2018-05-25 11:23:25,069][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218605000 ms.0 from job set of time 1527218605000 ms
[INFO][2018-05-25 11:23:25,069][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.069 s for time 1527218605000 ms (execution: 0.053 s)
[INFO][2018-05-25 11:23:25,069][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 61 from persistence list
[INFO][2018-05-25 11:23:25,070][org.apache.spark.storage.BlockManager]Removing RDD 61
[INFO][2018-05-25 11:23:25,070][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 60 from persistence list
[INFO][2018-05-25 11:23:25,070][org.apache.spark.storage.BlockManager]Removing RDD 60
[INFO][2018-05-25 11:23:25,070][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:23:25,070][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218595000 ms
[INFO][2018-05-25 11:23:30,013][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218610000 ms
[INFO][2018-05-25 11:23:30,013][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218610000 ms.0 from job set of time 1527218610000 ms
[INFO][2018-05-25 11:23:30,017][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:23:30,017][org.apache.spark.scheduler.DAGScheduler]Got job 59 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:23:30,018][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 59 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:23:30,018][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:23:30,018][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:23:30,018][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 59 (MapPartitionsRDD[65] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:23:30,019][org.apache.spark.storage.memory.MemoryStore]Block broadcast_59 stored as values in memory (estimated size 3.1 KB, free 912.2 MB)
[INFO][2018-05-25 11:23:30,020][org.apache.spark.storage.memory.MemoryStore]Block broadcast_59_piece0 stored as bytes in memory (estimated size 1972.0 B, free 912.2 MB)
[INFO][2018-05-25 11:23:30,020][org.apache.spark.storage.BlockManagerInfo]Added broadcast_59_piece0 in memory on 10.194.32.157:53453 (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:23:30,020][org.apache.spark.SparkContext]Created broadcast 59 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:23:30,021][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 59 (MapPartitionsRDD[65] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:23:30,021][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 59.0 with 1 tasks
[INFO][2018-05-25 11:23:30,021][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 59.0 (TID 59, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:23:30,021][org.apache.spark.executor.Executor]Running task 0.0 in stage 59.0 (TID 59)
[INFO][2018-05-25 11:23:30,022][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12564 -> 12574
[INFO][2018-05-25 11:23:30,023][org.apache.spark.executor.Executor]Finished task 0.0 in stage 59.0 (TID 59). 932 bytes result sent to driver
[INFO][2018-05-25 11:23:30,024][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 59.0 (TID 59) in 3 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:23:30,024][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 59.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:23:30,024][org.apache.spark.scheduler.DAGScheduler]ResultStage 59 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.003 s
[INFO][2018-05-25 11:23:30,024][org.apache.spark.scheduler.DAGScheduler]Job 59 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.006982 s
[INFO][2018-05-25 11:23:30,028][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:73
[INFO][2018-05-25 11:23:30,028][org.apache.spark.scheduler.DAGScheduler]Got job 60 (foreachPartition at ReceiveKafkaData.scala:73) with 1 output partitions
[INFO][2018-05-25 11:23:30,028][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 60 (foreachPartition at ReceiveKafkaData.scala:73)
[INFO][2018-05-25 11:23:30,028][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:23:30,028][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:23:30,028][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 60 (MapPartitionsRDD[65] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:23:30,029][org.apache.spark.storage.memory.MemoryStore]Block broadcast_60 stored as values in memory (estimated size 3.4 KB, free 912.2 MB)
[INFO][2018-05-25 11:23:30,030][org.apache.spark.storage.memory.MemoryStore]Block broadcast_60_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.2 MB)
[INFO][2018-05-25 11:23:30,030][org.apache.spark.storage.BlockManagerInfo]Added broadcast_60_piece0 in memory on 10.194.32.157:53453 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:23:30,031][org.apache.spark.SparkContext]Created broadcast 60 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:23:30,031][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 60 (MapPartitionsRDD[65] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:23:30,031][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 60.0 with 1 tasks
[INFO][2018-05-25 11:23:30,031][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 60.0 (TID 60, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:23:30,032][org.apache.spark.executor.Executor]Running task 0.0 in stage 60.0 (TID 60)
[INFO][2018-05-25 11:23:30,032][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12564 -> 12574
[INFO][2018-05-25 11:23:30,032][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 12564
[INFO][2018-05-25 11:23:30,067][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-25 11:23:30,068][org.apache.spark.executor.Executor]Finished task 0.0 in stage 60.0 (TID 60). 665 bytes result sent to driver
[INFO][2018-05-25 11:23:30,068][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 60.0 (TID 60) in 37 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:23:30,068][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 60.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:23:30,069][org.apache.spark.scheduler.DAGScheduler]ResultStage 60 (foreachPartition at ReceiveKafkaData.scala:73) finished in 0.038 s
[INFO][2018-05-25 11:23:30,069][org.apache.spark.scheduler.DAGScheduler]Job 60 finished: foreachPartition at ReceiveKafkaData.scala:73, took 0.041352 s
[INFO][2018-05-25 11:23:30,069][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218610000 ms.0 from job set of time 1527218610000 ms
[INFO][2018-05-25 11:23:30,069][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 63 from persistence list
[INFO][2018-05-25 11:23:30,070][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.069 s for time 1527218610000 ms (execution: 0.056 s)
[INFO][2018-05-25 11:23:30,070][org.apache.spark.storage.BlockManager]Removing RDD 63
[INFO][2018-05-25 11:23:30,070][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 62 from persistence list
[INFO][2018-05-25 11:23:30,070][org.apache.spark.storage.BlockManager]Removing RDD 62
[INFO][2018-05-25 11:23:30,070][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:23:30,071][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218600000 ms
[INFO][2018-05-25 11:23:35,015][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218615000 ms
[INFO][2018-05-25 11:23:35,015][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218615000 ms.0 from job set of time 1527218615000 ms
[INFO][2018-05-25 11:23:35,019][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:23:35,020][org.apache.spark.scheduler.DAGScheduler]Got job 61 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:23:35,020][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 61 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:23:35,020][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:23:35,020][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:23:35,020][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 61 (MapPartitionsRDD[67] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:23:35,021][org.apache.spark.storage.memory.MemoryStore]Block broadcast_61 stored as values in memory (estimated size 3.1 KB, free 912.2 MB)
[INFO][2018-05-25 11:23:35,022][org.apache.spark.storage.memory.MemoryStore]Block broadcast_61_piece0 stored as bytes in memory (estimated size 1972.0 B, free 912.2 MB)
[INFO][2018-05-25 11:23:35,022][org.apache.spark.storage.BlockManagerInfo]Added broadcast_61_piece0 in memory on 10.194.32.157:53453 (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:23:35,022][org.apache.spark.SparkContext]Created broadcast 61 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:23:35,022][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 61 (MapPartitionsRDD[67] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:23:35,022][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 61.0 with 1 tasks
[INFO][2018-05-25 11:23:35,023][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 61.0 (TID 61, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:23:35,023][org.apache.spark.executor.Executor]Running task 0.0 in stage 61.0 (TID 61)
[INFO][2018-05-25 11:23:35,024][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12574 -> 12584
[INFO][2018-05-25 11:23:35,025][org.apache.spark.executor.Executor]Finished task 0.0 in stage 61.0 (TID 61). 973 bytes result sent to driver
[INFO][2018-05-25 11:23:35,025][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 61.0 (TID 61) in 2 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:23:35,025][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 61.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:23:35,025][org.apache.spark.scheduler.DAGScheduler]ResultStage 61 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.002 s
[INFO][2018-05-25 11:23:35,026][org.apache.spark.scheduler.DAGScheduler]Job 61 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.006351 s
[INFO][2018-05-25 11:23:35,029][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:73
[INFO][2018-05-25 11:23:35,029][org.apache.spark.scheduler.DAGScheduler]Got job 62 (foreachPartition at ReceiveKafkaData.scala:73) with 1 output partitions
[INFO][2018-05-25 11:23:35,029][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 62 (foreachPartition at ReceiveKafkaData.scala:73)
[INFO][2018-05-25 11:23:35,029][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:23:35,029][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:23:35,029][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 62 (MapPartitionsRDD[67] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:23:35,030][org.apache.spark.storage.memory.MemoryStore]Block broadcast_62 stored as values in memory (estimated size 3.4 KB, free 912.2 MB)
[INFO][2018-05-25 11:23:35,031][org.apache.spark.storage.memory.MemoryStore]Block broadcast_62_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.2 MB)
[INFO][2018-05-25 11:23:35,031][org.apache.spark.storage.BlockManagerInfo]Added broadcast_62_piece0 in memory on 10.194.32.157:53453 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:23:35,031][org.apache.spark.SparkContext]Created broadcast 62 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:23:35,032][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 62 (MapPartitionsRDD[67] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:23:35,032][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 62.0 with 1 tasks
[INFO][2018-05-25 11:23:35,032][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 62.0 (TID 62, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:23:35,032][org.apache.spark.executor.Executor]Running task 0.0 in stage 62.0 (TID 62)
[INFO][2018-05-25 11:23:35,033][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12574 -> 12584
[INFO][2018-05-25 11:23:35,033][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 12574
[INFO][2018-05-25 11:23:35,065][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-25 11:23:35,066][org.apache.spark.executor.Executor]Finished task 0.0 in stage 62.0 (TID 62). 751 bytes result sent to driver
[INFO][2018-05-25 11:23:35,066][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 62.0 (TID 62) in 34 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:23:35,066][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 62.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:23:35,067][org.apache.spark.scheduler.DAGScheduler]ResultStage 62 (foreachPartition at ReceiveKafkaData.scala:73) finished in 0.035 s
[INFO][2018-05-25 11:23:35,067][org.apache.spark.scheduler.DAGScheduler]Job 62 finished: foreachPartition at ReceiveKafkaData.scala:73, took 0.038089 s
[INFO][2018-05-25 11:23:35,067][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218615000 ms.0 from job set of time 1527218615000 ms
[INFO][2018-05-25 11:23:35,068][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.067 s for time 1527218615000 ms (execution: 0.052 s)
[INFO][2018-05-25 11:23:35,068][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 65 from persistence list
[INFO][2018-05-25 11:23:35,068][org.apache.spark.storage.BlockManager]Removing RDD 65
[INFO][2018-05-25 11:23:35,068][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 64 from persistence list
[INFO][2018-05-25 11:23:35,068][org.apache.spark.storage.BlockManager]Removing RDD 64
[INFO][2018-05-25 11:23:35,068][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:23:35,068][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218605000 ms
[INFO][2018-05-25 11:23:40,015][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218620000 ms
[INFO][2018-05-25 11:23:40,015][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218620000 ms.0 from job set of time 1527218620000 ms
[INFO][2018-05-25 11:23:40,020][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:23:40,021][org.apache.spark.scheduler.DAGScheduler]Got job 63 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:23:40,021][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 63 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:23:40,021][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:23:40,021][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:23:40,021][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 63 (MapPartitionsRDD[69] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:23:40,023][org.apache.spark.storage.memory.MemoryStore]Block broadcast_63 stored as values in memory (estimated size 3.1 KB, free 912.2 MB)
[INFO][2018-05-25 11:23:40,024][org.apache.spark.storage.memory.MemoryStore]Block broadcast_63_piece0 stored as bytes in memory (estimated size 1972.0 B, free 912.2 MB)
[INFO][2018-05-25 11:23:40,025][org.apache.spark.storage.BlockManagerInfo]Added broadcast_63_piece0 in memory on 10.194.32.157:53453 (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:23:40,025][org.apache.spark.SparkContext]Created broadcast 63 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:23:40,025][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 63 (MapPartitionsRDD[69] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:23:40,025][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 63.0 with 1 tasks
[INFO][2018-05-25 11:23:40,026][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 63.0 (TID 63, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:23:40,026][org.apache.spark.executor.Executor]Running task 0.0 in stage 63.0 (TID 63)
[INFO][2018-05-25 11:23:40,027][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12584 -> 12594
[INFO][2018-05-25 11:23:40,028][org.apache.spark.executor.Executor]Finished task 0.0 in stage 63.0 (TID 63). 932 bytes result sent to driver
[INFO][2018-05-25 11:23:40,029][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 63.0 (TID 63) in 3 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:23:40,029][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 63.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:23:40,029][org.apache.spark.scheduler.DAGScheduler]ResultStage 63 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.003 s
[INFO][2018-05-25 11:23:40,030][org.apache.spark.scheduler.DAGScheduler]Job 63 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.009400 s
[INFO][2018-05-25 11:23:40,034][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:73
[INFO][2018-05-25 11:23:40,034][org.apache.spark.scheduler.DAGScheduler]Got job 64 (foreachPartition at ReceiveKafkaData.scala:73) with 1 output partitions
[INFO][2018-05-25 11:23:40,034][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 64 (foreachPartition at ReceiveKafkaData.scala:73)
[INFO][2018-05-25 11:23:40,034][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:23:40,034][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:23:40,035][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 64 (MapPartitionsRDD[69] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:23:40,036][org.apache.spark.storage.memory.MemoryStore]Block broadcast_64 stored as values in memory (estimated size 3.4 KB, free 912.2 MB)
[INFO][2018-05-25 11:23:40,036][org.apache.spark.storage.memory.MemoryStore]Block broadcast_64_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.2 MB)
[INFO][2018-05-25 11:23:40,036][org.apache.spark.storage.BlockManagerInfo]Added broadcast_64_piece0 in memory on 10.194.32.157:53453 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:23:40,037][org.apache.spark.SparkContext]Created broadcast 64 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:23:40,037][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 64 (MapPartitionsRDD[69] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:23:40,037][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 64.0 with 1 tasks
[INFO][2018-05-25 11:23:40,037][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 64.0 (TID 64, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:23:40,038][org.apache.spark.executor.Executor]Running task 0.0 in stage 64.0 (TID 64)
[INFO][2018-05-25 11:23:40,038][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12584 -> 12594
[INFO][2018-05-25 11:23:40,038][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 12584
[INFO][2018-05-25 11:23:40,080][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-25 11:23:40,083][org.apache.spark.executor.Executor]Finished task 0.0 in stage 64.0 (TID 64). 665 bytes result sent to driver
[INFO][2018-05-25 11:23:40,084][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 64.0 (TID 64) in 47 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:23:40,084][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 64.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:23:40,084][org.apache.spark.scheduler.DAGScheduler]ResultStage 64 (foreachPartition at ReceiveKafkaData.scala:73) finished in 0.047 s
[INFO][2018-05-25 11:23:40,085][org.apache.spark.scheduler.DAGScheduler]Job 64 finished: foreachPartition at ReceiveKafkaData.scala:73, took 0.050466 s
[INFO][2018-05-25 11:23:40,085][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218620000 ms.0 from job set of time 1527218620000 ms
[INFO][2018-05-25 11:23:40,085][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.085 s for time 1527218620000 ms (execution: 0.070 s)
[INFO][2018-05-25 11:23:40,085][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 67 from persistence list
[INFO][2018-05-25 11:23:40,086][org.apache.spark.storage.BlockManager]Removing RDD 67
[INFO][2018-05-25 11:23:40,086][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 66 from persistence list
[INFO][2018-05-25 11:23:40,086][org.apache.spark.storage.BlockManager]Removing RDD 66
[INFO][2018-05-25 11:23:40,086][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:23:40,087][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218610000 ms
[INFO][2018-05-25 11:23:45,010][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218625000 ms
[INFO][2018-05-25 11:23:45,011][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218625000 ms.0 from job set of time 1527218625000 ms
[INFO][2018-05-25 11:23:45,015][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:23:45,015][org.apache.spark.scheduler.DAGScheduler]Got job 65 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:23:45,015][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 65 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:23:45,015][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:23:45,015][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:23:45,015][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 65 (MapPartitionsRDD[71] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:23:45,017][org.apache.spark.storage.memory.MemoryStore]Block broadcast_65 stored as values in memory (estimated size 3.1 KB, free 912.2 MB)
[INFO][2018-05-25 11:23:45,018][org.apache.spark.storage.memory.MemoryStore]Block broadcast_65_piece0 stored as bytes in memory (estimated size 1972.0 B, free 912.2 MB)
[INFO][2018-05-25 11:23:45,018][org.apache.spark.storage.BlockManagerInfo]Added broadcast_65_piece0 in memory on 10.194.32.157:53453 (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:23:45,018][org.apache.spark.SparkContext]Created broadcast 65 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:23:45,019][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 65 (MapPartitionsRDD[71] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:23:45,019][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 65.0 with 1 tasks
[INFO][2018-05-25 11:23:45,019][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 65.0 (TID 65, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:23:45,020][org.apache.spark.executor.Executor]Running task 0.0 in stage 65.0 (TID 65)
[INFO][2018-05-25 11:23:45,020][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12594 -> 12604
[INFO][2018-05-25 11:23:45,021][org.apache.spark.executor.Executor]Finished task 0.0 in stage 65.0 (TID 65). 926 bytes result sent to driver
[INFO][2018-05-25 11:23:45,022][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 65.0 (TID 65) in 3 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:23:45,022][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 65.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:23:45,022][org.apache.spark.scheduler.DAGScheduler]ResultStage 65 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.003 s
[INFO][2018-05-25 11:23:45,022][org.apache.spark.scheduler.DAGScheduler]Job 65 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.007482 s
[INFO][2018-05-25 11:23:45,029][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_57_piece0 on 10.194.32.157:53453 in memory (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:23:45,030][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:73
[INFO][2018-05-25 11:23:45,030][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_52_piece0 on 10.194.32.157:53453 in memory (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:23:45,030][org.apache.spark.scheduler.DAGScheduler]Got job 66 (foreachPartition at ReceiveKafkaData.scala:73) with 1 output partitions
[INFO][2018-05-25 11:23:45,030][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 66 (foreachPartition at ReceiveKafkaData.scala:73)
[INFO][2018-05-25 11:23:45,030][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:23:45,030][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:23:45,030][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 66 (MapPartitionsRDD[71] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:23:45,031][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_44_piece0 on 10.194.32.157:53453 in memory (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:23:45,031][org.apache.spark.storage.memory.MemoryStore]Block broadcast_66 stored as values in memory (estimated size 3.4 KB, free 912.2 MB)
[INFO][2018-05-25 11:23:45,031][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_60_piece0 on 10.194.32.157:53453 in memory (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:23:45,032][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_58_piece0 on 10.194.32.157:53453 in memory (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:23:45,032][org.apache.spark.storage.memory.MemoryStore]Block broadcast_66_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.2 MB)
[INFO][2018-05-25 11:23:45,032][org.apache.spark.storage.BlockManagerInfo]Added broadcast_66_piece0 in memory on 10.194.32.157:53453 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:23:45,033][org.apache.spark.SparkContext]Created broadcast 66 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:23:45,033][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_47_piece0 on 10.194.32.157:53453 in memory (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:23:45,033][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 66 (MapPartitionsRDD[71] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:23:45,033][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 66.0 with 1 tasks
[INFO][2018-05-25 11:23:45,034][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 66.0 (TID 66, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:23:45,034][org.apache.spark.executor.Executor]Running task 0.0 in stage 66.0 (TID 66)
[INFO][2018-05-25 11:23:45,034][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_61_piece0 on 10.194.32.157:53453 in memory (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:23:45,035][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_56_piece0 on 10.194.32.157:53453 in memory (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:23:45,035][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12594 -> 12604
[INFO][2018-05-25 11:23:45,035][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 12594
[INFO][2018-05-25 11:23:45,036][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_51_piece0 on 10.194.32.157:53453 in memory (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:23:45,036][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_59_piece0 on 10.194.32.157:53453 in memory (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:23:45,037][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_48_piece0 on 10.194.32.157:53453 in memory (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:23:45,038][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_54_piece0 on 10.194.32.157:53453 in memory (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:23:45,038][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_65_piece0 on 10.194.32.157:53453 in memory (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:23:45,039][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_53_piece0 on 10.194.32.157:53453 in memory (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:23:45,040][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_49_piece0 on 10.194.32.157:53453 in memory (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:23:45,040][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_55_piece0 on 10.194.32.157:53453 in memory (size: 1971.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:23:45,041][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_64_piece0 on 10.194.32.157:53453 in memory (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:23:45,042][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_50_piece0 on 10.194.32.157:53453 in memory (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:23:45,043][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_46_piece0 on 10.194.32.157:53453 in memory (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:23:45,043][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_62_piece0 on 10.194.32.157:53453 in memory (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:23:45,044][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_45_piece0 on 10.194.32.157:53453 in memory (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:23:45,045][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_63_piece0 on 10.194.32.157:53453 in memory (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:23:45,066][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-25 11:23:45,067][org.apache.spark.executor.Executor]Finished task 0.0 in stage 66.0 (TID 66). 665 bytes result sent to driver
[INFO][2018-05-25 11:23:45,067][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 66.0 (TID 66) in 33 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:23:45,067][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 66.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:23:45,067][org.apache.spark.scheduler.DAGScheduler]ResultStage 66 (foreachPartition at ReceiveKafkaData.scala:73) finished in 0.034 s
[INFO][2018-05-25 11:23:45,068][org.apache.spark.scheduler.DAGScheduler]Job 66 finished: foreachPartition at ReceiveKafkaData.scala:73, took 0.037767 s
[INFO][2018-05-25 11:23:45,068][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218625000 ms.0 from job set of time 1527218625000 ms
[INFO][2018-05-25 11:23:45,068][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.068 s for time 1527218625000 ms (execution: 0.057 s)
[INFO][2018-05-25 11:23:45,068][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 69 from persistence list
[INFO][2018-05-25 11:23:45,068][org.apache.spark.storage.BlockManager]Removing RDD 69
[INFO][2018-05-25 11:23:45,069][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 68 from persistence list
[INFO][2018-05-25 11:23:45,069][org.apache.spark.storage.BlockManager]Removing RDD 68
[INFO][2018-05-25 11:23:45,069][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:23:45,069][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218615000 ms
[INFO][2018-05-25 11:23:50,015][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218630000 ms
[INFO][2018-05-25 11:23:50,016][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218630000 ms.0 from job set of time 1527218630000 ms
[INFO][2018-05-25 11:23:50,020][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:23:50,020][org.apache.spark.scheduler.DAGScheduler]Got job 67 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:23:50,020][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 67 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:23:50,020][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:23:50,020][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:23:50,021][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 67 (MapPartitionsRDD[73] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:23:50,022][org.apache.spark.storage.memory.MemoryStore]Block broadcast_67 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-25 11:23:50,023][org.apache.spark.storage.memory.MemoryStore]Block broadcast_67_piece0 stored as bytes in memory (estimated size 1972.0 B, free 912.3 MB)
[INFO][2018-05-25 11:23:50,023][org.apache.spark.storage.BlockManagerInfo]Added broadcast_67_piece0 in memory on 10.194.32.157:53453 (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:23:50,023][org.apache.spark.SparkContext]Created broadcast 67 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:23:50,024][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 67 (MapPartitionsRDD[73] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:23:50,024][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 67.0 with 1 tasks
[INFO][2018-05-25 11:23:50,024][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 67.0 (TID 67, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:23:50,024][org.apache.spark.executor.Executor]Running task 0.0 in stage 67.0 (TID 67)
[INFO][2018-05-25 11:23:50,025][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12604 -> 12613
[INFO][2018-05-25 11:23:50,026][org.apache.spark.executor.Executor]Finished task 0.0 in stage 67.0 (TID 67). 973 bytes result sent to driver
[INFO][2018-05-25 11:23:50,026][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 67.0 (TID 67) in 2 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:23:50,027][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 67.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:23:50,027][org.apache.spark.scheduler.DAGScheduler]ResultStage 67 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.003 s
[INFO][2018-05-25 11:23:50,028][org.apache.spark.scheduler.DAGScheduler]Job 67 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.007063 s
[INFO][2018-05-25 11:23:50,031][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:73
[INFO][2018-05-25 11:23:50,032][org.apache.spark.scheduler.DAGScheduler]Got job 68 (foreachPartition at ReceiveKafkaData.scala:73) with 1 output partitions
[INFO][2018-05-25 11:23:50,032][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 68 (foreachPartition at ReceiveKafkaData.scala:73)
[INFO][2018-05-25 11:23:50,032][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:23:50,032][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:23:50,032][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 68 (MapPartitionsRDD[73] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:23:50,033][org.apache.spark.storage.memory.MemoryStore]Block broadcast_68 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-25 11:23:50,033][org.apache.spark.storage.memory.MemoryStore]Block broadcast_68_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.3 MB)
[INFO][2018-05-25 11:23:50,034][org.apache.spark.storage.BlockManagerInfo]Added broadcast_68_piece0 in memory on 10.194.32.157:53453 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:23:50,034][org.apache.spark.SparkContext]Created broadcast 68 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:23:50,034][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 68 (MapPartitionsRDD[73] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:23:50,034][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 68.0 with 1 tasks
[INFO][2018-05-25 11:23:50,035][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 68.0 (TID 68, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:23:50,035][org.apache.spark.executor.Executor]Running task 0.0 in stage 68.0 (TID 68)
[INFO][2018-05-25 11:23:50,036][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12604 -> 12613
[INFO][2018-05-25 11:23:50,036][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 12604
[INFO][2018-05-25 11:23:50,069][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-25 11:23:50,070][org.apache.spark.executor.Executor]Finished task 0.0 in stage 68.0 (TID 68). 708 bytes result sent to driver
[INFO][2018-05-25 11:23:50,070][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 68.0 (TID 68) in 35 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:23:50,070][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 68.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:23:50,071][org.apache.spark.scheduler.DAGScheduler]ResultStage 68 (foreachPartition at ReceiveKafkaData.scala:73) finished in 0.036 s
[INFO][2018-05-25 11:23:50,071][org.apache.spark.scheduler.DAGScheduler]Job 68 finished: foreachPartition at ReceiveKafkaData.scala:73, took 0.039303 s
[INFO][2018-05-25 11:23:50,071][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218630000 ms.0 from job set of time 1527218630000 ms
[INFO][2018-05-25 11:23:50,071][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.071 s for time 1527218630000 ms (execution: 0.055 s)
[INFO][2018-05-25 11:23:50,071][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 71 from persistence list
[INFO][2018-05-25 11:23:50,072][org.apache.spark.storage.BlockManager]Removing RDD 71
[INFO][2018-05-25 11:23:50,072][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 70 from persistence list
[INFO][2018-05-25 11:23:50,072][org.apache.spark.storage.BlockManager]Removing RDD 70
[INFO][2018-05-25 11:23:50,073][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:23:50,073][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218620000 ms
[INFO][2018-05-25 11:23:55,017][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218635000 ms
[INFO][2018-05-25 11:23:55,017][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218635000 ms.0 from job set of time 1527218635000 ms
[INFO][2018-05-25 11:23:55,021][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:23:55,022][org.apache.spark.scheduler.DAGScheduler]Got job 69 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:23:55,022][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 69 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:23:55,022][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:23:55,022][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:23:55,022][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 69 (MapPartitionsRDD[75] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:23:55,023][org.apache.spark.storage.memory.MemoryStore]Block broadcast_69 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-25 11:23:55,024][org.apache.spark.storage.memory.MemoryStore]Block broadcast_69_piece0 stored as bytes in memory (estimated size 1972.0 B, free 912.3 MB)
[INFO][2018-05-25 11:23:55,024][org.apache.spark.storage.BlockManagerInfo]Added broadcast_69_piece0 in memory on 10.194.32.157:53453 (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:23:55,025][org.apache.spark.SparkContext]Created broadcast 69 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:23:55,025][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 69 (MapPartitionsRDD[75] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:23:55,025][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 69.0 with 1 tasks
[INFO][2018-05-25 11:23:55,025][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 69.0 (TID 69, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:23:55,026][org.apache.spark.executor.Executor]Running task 0.0 in stage 69.0 (TID 69)
[INFO][2018-05-25 11:23:55,026][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12613 -> 12623
[INFO][2018-05-25 11:23:55,027][org.apache.spark.executor.Executor]Finished task 0.0 in stage 69.0 (TID 69). 928 bytes result sent to driver
[INFO][2018-05-25 11:23:55,027][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 69.0 (TID 69) in 2 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:23:55,027][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 69.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:23:55,028][org.apache.spark.scheduler.DAGScheduler]ResultStage 69 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.002 s
[INFO][2018-05-25 11:23:55,028][org.apache.spark.scheduler.DAGScheduler]Job 69 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.006547 s
[INFO][2018-05-25 11:23:55,032][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:73
[INFO][2018-05-25 11:23:55,033][org.apache.spark.scheduler.DAGScheduler]Got job 70 (foreachPartition at ReceiveKafkaData.scala:73) with 1 output partitions
[INFO][2018-05-25 11:23:55,033][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 70 (foreachPartition at ReceiveKafkaData.scala:73)
[INFO][2018-05-25 11:23:55,033][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:23:55,033][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:23:55,033][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 70 (MapPartitionsRDD[75] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:23:55,034][org.apache.spark.storage.memory.MemoryStore]Block broadcast_70 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-25 11:23:55,035][org.apache.spark.storage.memory.MemoryStore]Block broadcast_70_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.3 MB)
[INFO][2018-05-25 11:23:55,035][org.apache.spark.storage.BlockManagerInfo]Added broadcast_70_piece0 in memory on 10.194.32.157:53453 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:23:55,036][org.apache.spark.SparkContext]Created broadcast 70 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:23:55,036][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 70 (MapPartitionsRDD[75] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:23:55,036][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 70.0 with 1 tasks
[INFO][2018-05-25 11:23:55,036][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 70.0 (TID 70, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:23:55,037][org.apache.spark.executor.Executor]Running task 0.0 in stage 70.0 (TID 70)
[INFO][2018-05-25 11:23:55,038][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12613 -> 12623
[INFO][2018-05-25 11:23:55,038][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 12613
[INFO][2018-05-25 11:23:55,065][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-25 11:23:55,065][org.apache.spark.executor.Executor]Finished task 0.0 in stage 70.0 (TID 70). 665 bytes result sent to driver
[INFO][2018-05-25 11:23:55,066][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 70.0 (TID 70) in 30 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:23:55,066][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 70.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:23:55,066][org.apache.spark.scheduler.DAGScheduler]ResultStage 70 (foreachPartition at ReceiveKafkaData.scala:73) finished in 0.030 s
[INFO][2018-05-25 11:23:55,066][org.apache.spark.scheduler.DAGScheduler]Job 70 finished: foreachPartition at ReceiveKafkaData.scala:73, took 0.034032 s
[INFO][2018-05-25 11:23:55,067][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218635000 ms.0 from job set of time 1527218635000 ms
[INFO][2018-05-25 11:23:55,067][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.067 s for time 1527218635000 ms (execution: 0.050 s)
[INFO][2018-05-25 11:23:55,067][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 73 from persistence list
[INFO][2018-05-25 11:23:55,067][org.apache.spark.storage.BlockManager]Removing RDD 73
[INFO][2018-05-25 11:23:55,067][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 72 from persistence list
[INFO][2018-05-25 11:23:55,067][org.apache.spark.storage.BlockManager]Removing RDD 72
[INFO][2018-05-25 11:23:55,067][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:23:55,067][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218625000 ms
[INFO][2018-05-25 11:24:00,019][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218640000 ms
[INFO][2018-05-25 11:24:00,019][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218640000 ms.0 from job set of time 1527218640000 ms
[INFO][2018-05-25 11:24:00,024][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:24:00,024][org.apache.spark.scheduler.DAGScheduler]Got job 71 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:24:00,024][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 71 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:24:00,024][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:24:00,024][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:24:00,024][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 71 (MapPartitionsRDD[77] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:24:00,025][org.apache.spark.storage.memory.MemoryStore]Block broadcast_71 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-25 11:24:00,026][org.apache.spark.storage.memory.MemoryStore]Block broadcast_71_piece0 stored as bytes in memory (estimated size 1972.0 B, free 912.3 MB)
[INFO][2018-05-25 11:24:00,026][org.apache.spark.storage.BlockManagerInfo]Added broadcast_71_piece0 in memory on 10.194.32.157:53453 (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:24:00,026][org.apache.spark.SparkContext]Created broadcast 71 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:24:00,027][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 71 (MapPartitionsRDD[77] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:24:00,027][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 71.0 with 1 tasks
[INFO][2018-05-25 11:24:00,027][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 71.0 (TID 71, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:24:00,027][org.apache.spark.executor.Executor]Running task 0.0 in stage 71.0 (TID 71)
[INFO][2018-05-25 11:24:00,028][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12623 -> 12633
[INFO][2018-05-25 11:24:00,029][org.apache.spark.executor.Executor]Finished task 0.0 in stage 71.0 (TID 71). 932 bytes result sent to driver
[INFO][2018-05-25 11:24:00,030][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 71.0 (TID 71) in 3 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:24:00,030][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 71.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:24:00,030][org.apache.spark.scheduler.DAGScheduler]ResultStage 71 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.003 s
[INFO][2018-05-25 11:24:00,030][org.apache.spark.scheduler.DAGScheduler]Job 71 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.006361 s
[INFO][2018-05-25 11:24:00,034][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:73
[INFO][2018-05-25 11:24:00,034][org.apache.spark.scheduler.DAGScheduler]Got job 72 (foreachPartition at ReceiveKafkaData.scala:73) with 1 output partitions
[INFO][2018-05-25 11:24:00,034][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 72 (foreachPartition at ReceiveKafkaData.scala:73)
[INFO][2018-05-25 11:24:00,034][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:24:00,034][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:24:00,034][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 72 (MapPartitionsRDD[77] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:24:00,035][org.apache.spark.storage.memory.MemoryStore]Block broadcast_72 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-25 11:24:00,036][org.apache.spark.storage.memory.MemoryStore]Block broadcast_72_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.3 MB)
[INFO][2018-05-25 11:24:00,036][org.apache.spark.storage.BlockManagerInfo]Added broadcast_72_piece0 in memory on 10.194.32.157:53453 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:24:00,036][org.apache.spark.SparkContext]Created broadcast 72 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:24:00,036][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 72 (MapPartitionsRDD[77] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:24:00,036][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 72.0 with 1 tasks
[INFO][2018-05-25 11:24:00,037][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 72.0 (TID 72, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:24:00,037][org.apache.spark.executor.Executor]Running task 0.0 in stage 72.0 (TID 72)
[INFO][2018-05-25 11:24:00,037][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12623 -> 12633
[INFO][2018-05-25 11:24:00,038][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 12623
[INFO][2018-05-25 11:24:00,084][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-25 11:24:00,085][org.apache.spark.executor.Executor]Finished task 0.0 in stage 72.0 (TID 72). 665 bytes result sent to driver
[INFO][2018-05-25 11:24:00,085][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 72.0 (TID 72) in 48 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:24:00,085][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 72.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:24:00,085][org.apache.spark.scheduler.DAGScheduler]ResultStage 72 (foreachPartition at ReceiveKafkaData.scala:73) finished in 0.049 s
[INFO][2018-05-25 11:24:00,085][org.apache.spark.scheduler.DAGScheduler]Job 72 finished: foreachPartition at ReceiveKafkaData.scala:73, took 0.051461 s
[INFO][2018-05-25 11:24:00,085][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218640000 ms.0 from job set of time 1527218640000 ms
[INFO][2018-05-25 11:24:00,086][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.085 s for time 1527218640000 ms (execution: 0.066 s)
[INFO][2018-05-25 11:24:00,086][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 75 from persistence list
[INFO][2018-05-25 11:24:00,086][org.apache.spark.storage.BlockManager]Removing RDD 75
[INFO][2018-05-25 11:24:00,086][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 74 from persistence list
[INFO][2018-05-25 11:24:00,086][org.apache.spark.storage.BlockManager]Removing RDD 74
[INFO][2018-05-25 11:24:00,086][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:24:00,086][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218630000 ms
[INFO][2018-05-25 11:24:05,018][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218645000 ms
[INFO][2018-05-25 11:24:05,019][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218645000 ms.0 from job set of time 1527218645000 ms
[INFO][2018-05-25 11:24:05,023][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:24:05,023][org.apache.spark.scheduler.DAGScheduler]Got job 73 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:24:05,023][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 73 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:24:05,023][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:24:05,023][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:24:05,024][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 73 (MapPartitionsRDD[79] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:24:05,025][org.apache.spark.storage.memory.MemoryStore]Block broadcast_73 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-25 11:24:05,026][org.apache.spark.storage.memory.MemoryStore]Block broadcast_73_piece0 stored as bytes in memory (estimated size 1972.0 B, free 912.3 MB)
[INFO][2018-05-25 11:24:05,026][org.apache.spark.storage.BlockManagerInfo]Added broadcast_73_piece0 in memory on 10.194.32.157:53453 (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:24:05,026][org.apache.spark.SparkContext]Created broadcast 73 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:24:05,026][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 73 (MapPartitionsRDD[79] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:24:05,026][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 73.0 with 1 tasks
[INFO][2018-05-25 11:24:05,027][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 73.0 (TID 73, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:24:05,027][org.apache.spark.executor.Executor]Running task 0.0 in stage 73.0 (TID 73)
[INFO][2018-05-25 11:24:05,028][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12633 -> 12643
[INFO][2018-05-25 11:24:05,029][org.apache.spark.executor.Executor]Finished task 0.0 in stage 73.0 (TID 73). 919 bytes result sent to driver
[INFO][2018-05-25 11:24:05,030][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 73.0 (TID 73) in 2 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:24:05,030][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 73.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:24:05,030][org.apache.spark.scheduler.DAGScheduler]ResultStage 73 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.003 s
[INFO][2018-05-25 11:24:05,030][org.apache.spark.scheduler.DAGScheduler]Job 73 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.007060 s
[INFO][2018-05-25 11:24:05,035][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:73
[INFO][2018-05-25 11:24:05,035][org.apache.spark.scheduler.DAGScheduler]Got job 74 (foreachPartition at ReceiveKafkaData.scala:73) with 1 output partitions
[INFO][2018-05-25 11:24:05,035][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 74 (foreachPartition at ReceiveKafkaData.scala:73)
[INFO][2018-05-25 11:24:05,035][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:24:05,035][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:24:05,035][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 74 (MapPartitionsRDD[79] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:24:05,036][org.apache.spark.storage.memory.MemoryStore]Block broadcast_74 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-25 11:24:05,037][org.apache.spark.storage.memory.MemoryStore]Block broadcast_74_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.2 MB)
[INFO][2018-05-25 11:24:05,038][org.apache.spark.storage.BlockManagerInfo]Added broadcast_74_piece0 in memory on 10.194.32.157:53453 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:24:05,038][org.apache.spark.SparkContext]Created broadcast 74 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:24:05,038][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 74 (MapPartitionsRDD[79] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:24:05,038][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 74.0 with 1 tasks
[INFO][2018-05-25 11:24:05,039][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 74.0 (TID 74, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:24:05,039][org.apache.spark.executor.Executor]Running task 0.0 in stage 74.0 (TID 74)
[INFO][2018-05-25 11:24:05,040][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12633 -> 12643
[INFO][2018-05-25 11:24:05,040][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 12633
[INFO][2018-05-25 11:24:05,068][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-25 11:24:05,069][org.apache.spark.executor.Executor]Finished task 0.0 in stage 74.0 (TID 74). 708 bytes result sent to driver
[INFO][2018-05-25 11:24:05,069][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 74.0 (TID 74) in 31 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:24:05,069][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 74.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:24:05,069][org.apache.spark.scheduler.DAGScheduler]ResultStage 74 (foreachPartition at ReceiveKafkaData.scala:73) finished in 0.031 s
[INFO][2018-05-25 11:24:05,070][org.apache.spark.scheduler.DAGScheduler]Job 74 finished: foreachPartition at ReceiveKafkaData.scala:73, took 0.035027 s
[INFO][2018-05-25 11:24:05,070][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218645000 ms.0 from job set of time 1527218645000 ms
[INFO][2018-05-25 11:24:05,070][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.070 s for time 1527218645000 ms (execution: 0.051 s)
[INFO][2018-05-25 11:24:05,070][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 77 from persistence list
[INFO][2018-05-25 11:24:05,070][org.apache.spark.storage.BlockManager]Removing RDD 77
[INFO][2018-05-25 11:24:05,071][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 76 from persistence list
[INFO][2018-05-25 11:24:05,071][org.apache.spark.storage.BlockManager]Removing RDD 76
[INFO][2018-05-25 11:24:05,071][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:24:05,071][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218635000 ms
[INFO][2018-05-25 11:24:10,015][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218650000 ms
[INFO][2018-05-25 11:24:10,015][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218650000 ms.0 from job set of time 1527218650000 ms
[INFO][2018-05-25 11:24:10,019][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:24:10,020][org.apache.spark.scheduler.DAGScheduler]Got job 75 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:24:10,020][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 75 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:24:10,020][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:24:10,020][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:24:10,020][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 75 (MapPartitionsRDD[81] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:24:10,022][org.apache.spark.storage.memory.MemoryStore]Block broadcast_75 stored as values in memory (estimated size 3.1 KB, free 912.2 MB)
[INFO][2018-05-25 11:24:10,022][org.apache.spark.storage.memory.MemoryStore]Block broadcast_75_piece0 stored as bytes in memory (estimated size 1972.0 B, free 912.2 MB)
[INFO][2018-05-25 11:24:10,023][org.apache.spark.storage.BlockManagerInfo]Added broadcast_75_piece0 in memory on 10.194.32.157:53453 (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:24:10,023][org.apache.spark.SparkContext]Created broadcast 75 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:24:10,023][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 75 (MapPartitionsRDD[81] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:24:10,023][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 75.0 with 1 tasks
[INFO][2018-05-25 11:24:10,024][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 75.0 (TID 75, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:24:10,024][org.apache.spark.executor.Executor]Running task 0.0 in stage 75.0 (TID 75)
[INFO][2018-05-25 11:24:10,025][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12643 -> 12653
[INFO][2018-05-25 11:24:10,025][org.apache.spark.executor.Executor]Finished task 0.0 in stage 75.0 (TID 75). 930 bytes result sent to driver
[INFO][2018-05-25 11:24:10,026][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 75.0 (TID 75) in 2 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:24:10,026][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 75.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:24:10,026][org.apache.spark.scheduler.DAGScheduler]ResultStage 75 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.003 s
[INFO][2018-05-25 11:24:10,026][org.apache.spark.scheduler.DAGScheduler]Job 75 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.006930 s
[INFO][2018-05-25 11:24:10,029][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:73
[INFO][2018-05-25 11:24:10,029][org.apache.spark.scheduler.DAGScheduler]Got job 76 (foreachPartition at ReceiveKafkaData.scala:73) with 1 output partitions
[INFO][2018-05-25 11:24:10,029][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 76 (foreachPartition at ReceiveKafkaData.scala:73)
[INFO][2018-05-25 11:24:10,029][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:24:10,029][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:24:10,030][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 76 (MapPartitionsRDD[81] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:24:10,030][org.apache.spark.storage.memory.MemoryStore]Block broadcast_76 stored as values in memory (estimated size 3.4 KB, free 912.2 MB)
[INFO][2018-05-25 11:24:10,031][org.apache.spark.storage.memory.MemoryStore]Block broadcast_76_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.2 MB)
[INFO][2018-05-25 11:24:10,031][org.apache.spark.storage.BlockManagerInfo]Added broadcast_76_piece0 in memory on 10.194.32.157:53453 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:24:10,032][org.apache.spark.SparkContext]Created broadcast 76 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:24:10,032][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 76 (MapPartitionsRDD[81] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:24:10,032][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 76.0 with 1 tasks
[INFO][2018-05-25 11:24:10,032][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 76.0 (TID 76, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:24:10,032][org.apache.spark.executor.Executor]Running task 0.0 in stage 76.0 (TID 76)
[INFO][2018-05-25 11:24:10,042][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12643 -> 12653
[INFO][2018-05-25 11:24:10,043][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 12643
[INFO][2018-05-25 11:24:11,084][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-25 11:24:11,086][org.apache.spark.executor.Executor]Finished task 0.0 in stage 76.0 (TID 76). 708 bytes result sent to driver
[INFO][2018-05-25 11:24:11,087][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 76.0 (TID 76) in 1055 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:24:11,087][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 76.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:24:11,087][org.apache.spark.scheduler.DAGScheduler]ResultStage 76 (foreachPartition at ReceiveKafkaData.scala:73) finished in 1.055 s
[INFO][2018-05-25 11:24:11,088][org.apache.spark.scheduler.DAGScheduler]Job 76 finished: foreachPartition at ReceiveKafkaData.scala:73, took 1.058591 s
[INFO][2018-05-25 11:24:11,088][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218650000 ms.0 from job set of time 1527218650000 ms
[INFO][2018-05-25 11:24:11,088][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 1.088 s for time 1527218650000 ms (execution: 1.073 s)
[INFO][2018-05-25 11:24:11,088][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 79 from persistence list
[INFO][2018-05-25 11:24:11,088][org.apache.spark.storage.BlockManager]Removing RDD 79
[INFO][2018-05-25 11:24:11,089][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 78 from persistence list
[INFO][2018-05-25 11:24:11,089][org.apache.spark.storage.BlockManager]Removing RDD 78
[INFO][2018-05-25 11:24:11,089][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:24:11,089][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218640000 ms
[INFO][2018-05-25 11:24:15,014][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218655000 ms
[INFO][2018-05-25 11:24:15,014][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218655000 ms.0 from job set of time 1527218655000 ms
[INFO][2018-05-25 11:24:15,020][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:24:15,020][org.apache.spark.scheduler.DAGScheduler]Got job 77 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:24:15,020][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 77 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:24:15,020][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:24:15,020][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:24:15,021][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 77 (MapPartitionsRDD[83] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:24:15,022][org.apache.spark.storage.memory.MemoryStore]Block broadcast_77 stored as values in memory (estimated size 3.1 KB, free 912.2 MB)
[INFO][2018-05-25 11:24:15,023][org.apache.spark.storage.memory.MemoryStore]Block broadcast_77_piece0 stored as bytes in memory (estimated size 1972.0 B, free 912.2 MB)
[INFO][2018-05-25 11:24:15,023][org.apache.spark.storage.BlockManagerInfo]Added broadcast_77_piece0 in memory on 10.194.32.157:53453 (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:24:15,023][org.apache.spark.SparkContext]Created broadcast 77 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:24:15,024][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 77 (MapPartitionsRDD[83] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:24:15,024][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 77.0 with 1 tasks
[INFO][2018-05-25 11:24:15,024][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 77.0 (TID 77, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:24:15,025][org.apache.spark.executor.Executor]Running task 0.0 in stage 77.0 (TID 77)
[INFO][2018-05-25 11:24:15,026][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12653 -> 12663
[INFO][2018-05-25 11:24:15,026][org.apache.spark.executor.Executor]Finished task 0.0 in stage 77.0 (TID 77). 931 bytes result sent to driver
[INFO][2018-05-25 11:24:15,027][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 77.0 (TID 77) in 3 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:24:15,027][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 77.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:24:15,027][org.apache.spark.scheduler.DAGScheduler]ResultStage 77 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.003 s
[INFO][2018-05-25 11:24:15,027][org.apache.spark.scheduler.DAGScheduler]Job 77 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.007510 s
[INFO][2018-05-25 11:24:15,030][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:73
[INFO][2018-05-25 11:24:15,031][org.apache.spark.scheduler.DAGScheduler]Got job 78 (foreachPartition at ReceiveKafkaData.scala:73) with 1 output partitions
[INFO][2018-05-25 11:24:15,031][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 78 (foreachPartition at ReceiveKafkaData.scala:73)
[INFO][2018-05-25 11:24:15,031][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:24:15,031][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:24:15,031][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 78 (MapPartitionsRDD[83] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:24:15,032][org.apache.spark.storage.memory.MemoryStore]Block broadcast_78 stored as values in memory (estimated size 3.4 KB, free 912.2 MB)
[INFO][2018-05-25 11:24:15,032][org.apache.spark.storage.memory.MemoryStore]Block broadcast_78_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.2 MB)
[INFO][2018-05-25 11:24:15,032][org.apache.spark.storage.BlockManagerInfo]Added broadcast_78_piece0 in memory on 10.194.32.157:53453 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:24:15,033][org.apache.spark.SparkContext]Created broadcast 78 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:24:15,033][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 78 (MapPartitionsRDD[83] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:24:15,033][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 78.0 with 1 tasks
[INFO][2018-05-25 11:24:15,033][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 78.0 (TID 78, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:24:15,033][org.apache.spark.executor.Executor]Running task 0.0 in stage 78.0 (TID 78)
[INFO][2018-05-25 11:24:15,034][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12653 -> 12663
[INFO][2018-05-25 11:24:15,034][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 12653
[INFO][2018-05-25 11:24:15,070][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-25 11:24:15,070][org.apache.spark.executor.Executor]Finished task 0.0 in stage 78.0 (TID 78). 708 bytes result sent to driver
[INFO][2018-05-25 11:24:15,070][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 78.0 (TID 78) in 37 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:24:15,071][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 78.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:24:15,071][org.apache.spark.scheduler.DAGScheduler]ResultStage 78 (foreachPartition at ReceiveKafkaData.scala:73) finished in 0.038 s
[INFO][2018-05-25 11:24:15,071][org.apache.spark.scheduler.DAGScheduler]Job 78 finished: foreachPartition at ReceiveKafkaData.scala:73, took 0.040670 s
[INFO][2018-05-25 11:24:15,071][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218655000 ms.0 from job set of time 1527218655000 ms
[INFO][2018-05-25 11:24:15,072][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.071 s for time 1527218655000 ms (execution: 0.057 s)
[INFO][2018-05-25 11:24:15,072][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 81 from persistence list
[INFO][2018-05-25 11:24:15,072][org.apache.spark.storage.BlockManager]Removing RDD 81
[INFO][2018-05-25 11:24:15,072][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 80 from persistence list
[INFO][2018-05-25 11:24:15,072][org.apache.spark.storage.BlockManager]Removing RDD 80
[INFO][2018-05-25 11:24:15,072][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:24:15,072][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218645000 ms
[INFO][2018-05-25 11:24:20,018][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218660000 ms
[INFO][2018-05-25 11:24:20,019][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218660000 ms.0 from job set of time 1527218660000 ms
[INFO][2018-05-25 11:24:20,024][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:24:20,025][org.apache.spark.scheduler.DAGScheduler]Got job 79 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:24:20,025][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 79 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:24:20,025][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:24:20,025][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:24:20,025][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 79 (MapPartitionsRDD[85] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:24:20,026][org.apache.spark.storage.memory.MemoryStore]Block broadcast_79 stored as values in memory (estimated size 3.1 KB, free 912.2 MB)
[INFO][2018-05-25 11:24:20,027][org.apache.spark.storage.memory.MemoryStore]Block broadcast_79_piece0 stored as bytes in memory (estimated size 1972.0 B, free 912.2 MB)
[INFO][2018-05-25 11:24:20,027][org.apache.spark.storage.BlockManagerInfo]Added broadcast_79_piece0 in memory on 10.194.32.157:53453 (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:24:20,027][org.apache.spark.SparkContext]Created broadcast 79 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:24:20,028][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 79 (MapPartitionsRDD[85] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:24:20,028][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 79.0 with 1 tasks
[INFO][2018-05-25 11:24:20,028][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 79.0 (TID 79, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:24:20,028][org.apache.spark.executor.Executor]Running task 0.0 in stage 79.0 (TID 79)
[INFO][2018-05-25 11:24:20,029][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12663 -> 12673
[INFO][2018-05-25 11:24:20,030][org.apache.spark.executor.Executor]Finished task 0.0 in stage 79.0 (TID 79). 931 bytes result sent to driver
[INFO][2018-05-25 11:24:20,030][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 79.0 (TID 79) in 2 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:24:20,031][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 79.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:24:20,031][org.apache.spark.scheduler.DAGScheduler]ResultStage 79 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.003 s
[INFO][2018-05-25 11:24:20,031][org.apache.spark.scheduler.DAGScheduler]Job 79 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.006994 s
[INFO][2018-05-25 11:24:20,034][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:73
[INFO][2018-05-25 11:24:20,034][org.apache.spark.scheduler.DAGScheduler]Got job 80 (foreachPartition at ReceiveKafkaData.scala:73) with 1 output partitions
[INFO][2018-05-25 11:24:20,034][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 80 (foreachPartition at ReceiveKafkaData.scala:73)
[INFO][2018-05-25 11:24:20,034][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:24:20,034][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:24:20,035][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 80 (MapPartitionsRDD[85] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:24:20,035][org.apache.spark.storage.memory.MemoryStore]Block broadcast_80 stored as values in memory (estimated size 3.4 KB, free 912.2 MB)
[INFO][2018-05-25 11:24:20,036][org.apache.spark.storage.memory.MemoryStore]Block broadcast_80_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.2 MB)
[INFO][2018-05-25 11:24:20,036][org.apache.spark.storage.BlockManagerInfo]Added broadcast_80_piece0 in memory on 10.194.32.157:53453 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:24:20,036][org.apache.spark.SparkContext]Created broadcast 80 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:24:20,037][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 80 (MapPartitionsRDD[85] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:24:20,037][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 80.0 with 1 tasks
[INFO][2018-05-25 11:24:20,037][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 80.0 (TID 80, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:24:20,037][org.apache.spark.executor.Executor]Running task 0.0 in stage 80.0 (TID 80)
[INFO][2018-05-25 11:24:20,038][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12663 -> 12673
[INFO][2018-05-25 11:24:20,038][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 12663
[INFO][2018-05-25 11:24:20,082][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-25 11:24:20,082][org.apache.spark.executor.Executor]Finished task 0.0 in stage 80.0 (TID 80). 708 bytes result sent to driver
[INFO][2018-05-25 11:24:20,083][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 80.0 (TID 80) in 46 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:24:20,083][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 80.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:24:20,083][org.apache.spark.scheduler.DAGScheduler]ResultStage 80 (foreachPartition at ReceiveKafkaData.scala:73) finished in 0.046 s
[INFO][2018-05-25 11:24:20,083][org.apache.spark.scheduler.DAGScheduler]Job 80 finished: foreachPartition at ReceiveKafkaData.scala:73, took 0.049340 s
[INFO][2018-05-25 11:24:20,084][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218660000 ms.0 from job set of time 1527218660000 ms
[INFO][2018-05-25 11:24:20,084][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.084 s for time 1527218660000 ms (execution: 0.065 s)
[INFO][2018-05-25 11:24:20,084][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 83 from persistence list
[INFO][2018-05-25 11:24:20,084][org.apache.spark.storage.BlockManager]Removing RDD 83
[INFO][2018-05-25 11:24:20,084][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 82 from persistence list
[INFO][2018-05-25 11:24:20,085][org.apache.spark.storage.BlockManager]Removing RDD 82
[INFO][2018-05-25 11:24:20,085][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:24:20,085][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218650000 ms
[INFO][2018-05-25 11:24:25,017][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218665000 ms
[INFO][2018-05-25 11:24:25,017][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218665000 ms.0 from job set of time 1527218665000 ms
[INFO][2018-05-25 11:24:25,026][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:24:25,027][org.apache.spark.scheduler.DAGScheduler]Got job 81 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:24:25,027][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 81 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:24:25,028][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:24:25,028][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:24:25,029][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 81 (MapPartitionsRDD[87] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:24:25,031][org.apache.spark.storage.memory.MemoryStore]Block broadcast_81 stored as values in memory (estimated size 3.1 KB, free 912.2 MB)
[INFO][2018-05-25 11:24:25,032][org.apache.spark.storage.memory.MemoryStore]Block broadcast_81_piece0 stored as bytes in memory (estimated size 1972.0 B, free 912.2 MB)
[INFO][2018-05-25 11:24:25,033][org.apache.spark.storage.BlockManagerInfo]Added broadcast_81_piece0 in memory on 10.194.32.157:53453 (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:24:25,033][org.apache.spark.SparkContext]Created broadcast 81 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:24:25,033][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 81 (MapPartitionsRDD[87] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:24:25,033][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 81.0 with 1 tasks
[INFO][2018-05-25 11:24:25,034][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 81.0 (TID 81, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:24:25,034][org.apache.spark.executor.Executor]Running task 0.0 in stage 81.0 (TID 81)
[INFO][2018-05-25 11:24:25,034][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12673 -> 12683
[INFO][2018-05-25 11:24:25,035][org.apache.spark.executor.Executor]Finished task 0.0 in stage 81.0 (TID 81). 929 bytes result sent to driver
[INFO][2018-05-25 11:24:25,035][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 81.0 (TID 81) in 2 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:24:25,035][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 81.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:24:25,036][org.apache.spark.scheduler.DAGScheduler]ResultStage 81 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.003 s
[INFO][2018-05-25 11:24:25,036][org.apache.spark.scheduler.DAGScheduler]Job 81 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.009823 s
[INFO][2018-05-25 11:24:25,039][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:73
[INFO][2018-05-25 11:24:25,039][org.apache.spark.scheduler.DAGScheduler]Got job 82 (foreachPartition at ReceiveKafkaData.scala:73) with 1 output partitions
[INFO][2018-05-25 11:24:25,039][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 82 (foreachPartition at ReceiveKafkaData.scala:73)
[INFO][2018-05-25 11:24:25,039][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:24:25,039][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:24:25,040][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 82 (MapPartitionsRDD[87] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:24:25,041][org.apache.spark.storage.memory.MemoryStore]Block broadcast_82 stored as values in memory (estimated size 3.4 KB, free 912.2 MB)
[INFO][2018-05-25 11:24:25,041][org.apache.spark.storage.memory.MemoryStore]Block broadcast_82_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.2 MB)
[INFO][2018-05-25 11:24:25,041][org.apache.spark.storage.BlockManagerInfo]Added broadcast_82_piece0 in memory on 10.194.32.157:53453 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:24:25,042][org.apache.spark.SparkContext]Created broadcast 82 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:24:25,042][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 82 (MapPartitionsRDD[87] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:24:25,042][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 82.0 with 1 tasks
[INFO][2018-05-25 11:24:25,042][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 82.0 (TID 82, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:24:25,042][org.apache.spark.executor.Executor]Running task 0.0 in stage 82.0 (TID 82)
[INFO][2018-05-25 11:24:25,043][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12673 -> 12683
[INFO][2018-05-25 11:24:25,043][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 12673
[INFO][2018-05-25 11:24:25,076][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-25 11:24:25,076][org.apache.spark.executor.Executor]Finished task 0.0 in stage 82.0 (TID 82). 708 bytes result sent to driver
[INFO][2018-05-25 11:24:25,077][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 82.0 (TID 82) in 35 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:24:25,077][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 82.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:24:25,077][org.apache.spark.scheduler.DAGScheduler]ResultStage 82 (foreachPartition at ReceiveKafkaData.scala:73) finished in 0.035 s
[INFO][2018-05-25 11:24:25,078][org.apache.spark.scheduler.DAGScheduler]Job 82 finished: foreachPartition at ReceiveKafkaData.scala:73, took 0.038883 s
[INFO][2018-05-25 11:24:25,078][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218665000 ms.0 from job set of time 1527218665000 ms
[INFO][2018-05-25 11:24:25,078][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 85 from persistence list
[INFO][2018-05-25 11:24:25,079][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.078 s for time 1527218665000 ms (execution: 0.061 s)
[INFO][2018-05-25 11:24:25,079][org.apache.spark.storage.BlockManager]Removing RDD 85
[INFO][2018-05-25 11:24:25,079][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 84 from persistence list
[INFO][2018-05-25 11:24:25,079][org.apache.spark.storage.BlockManager]Removing RDD 84
[INFO][2018-05-25 11:24:25,079][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:24:25,080][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218655000 ms
[INFO][2018-05-25 11:24:30,016][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218670000 ms
[INFO][2018-05-25 11:24:30,016][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218670000 ms.0 from job set of time 1527218670000 ms
[INFO][2018-05-25 11:24:30,021][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:24:30,021][org.apache.spark.scheduler.DAGScheduler]Got job 83 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:24:30,021][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 83 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:24:30,021][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:24:30,021][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:24:30,022][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 83 (MapPartitionsRDD[89] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:24:30,023][org.apache.spark.storage.memory.MemoryStore]Block broadcast_83 stored as values in memory (estimated size 3.1 KB, free 912.2 MB)
[INFO][2018-05-25 11:24:30,024][org.apache.spark.storage.memory.MemoryStore]Block broadcast_83_piece0 stored as bytes in memory (estimated size 1972.0 B, free 912.2 MB)
[INFO][2018-05-25 11:24:30,024][org.apache.spark.storage.BlockManagerInfo]Added broadcast_83_piece0 in memory on 10.194.32.157:53453 (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:24:30,025][org.apache.spark.SparkContext]Created broadcast 83 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:24:30,025][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 83 (MapPartitionsRDD[89] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:24:30,025][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 83.0 with 1 tasks
[INFO][2018-05-25 11:24:30,027][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 83.0 (TID 83, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:24:30,027][org.apache.spark.executor.Executor]Running task 0.0 in stage 83.0 (TID 83)
[INFO][2018-05-25 11:24:30,028][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12683 -> 12693
[INFO][2018-05-25 11:24:30,029][org.apache.spark.executor.Executor]Finished task 0.0 in stage 83.0 (TID 83). 917 bytes result sent to driver
[INFO][2018-05-25 11:24:30,030][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 83.0 (TID 83) in 4 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:24:30,031][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 83.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:24:30,031][org.apache.spark.scheduler.DAGScheduler]ResultStage 83 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.005 s
[INFO][2018-05-25 11:24:30,031][org.apache.spark.scheduler.DAGScheduler]Job 83 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.010511 s
[INFO][2018-05-25 11:24:30,035][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:73
[INFO][2018-05-25 11:24:30,036][org.apache.spark.scheduler.DAGScheduler]Got job 84 (foreachPartition at ReceiveKafkaData.scala:73) with 1 output partitions
[INFO][2018-05-25 11:24:30,036][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 84 (foreachPartition at ReceiveKafkaData.scala:73)
[INFO][2018-05-25 11:24:30,036][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:24:30,036][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:24:30,036][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 84 (MapPartitionsRDD[89] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:24:30,038][org.apache.spark.storage.memory.MemoryStore]Block broadcast_84 stored as values in memory (estimated size 3.4 KB, free 912.2 MB)
[INFO][2018-05-25 11:24:30,039][org.apache.spark.storage.memory.MemoryStore]Block broadcast_84_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.2 MB)
[INFO][2018-05-25 11:24:30,040][org.apache.spark.storage.BlockManagerInfo]Added broadcast_84_piece0 in memory on 10.194.32.157:53453 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:24:30,040][org.apache.spark.SparkContext]Created broadcast 84 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:24:30,040][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 84 (MapPartitionsRDD[89] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:24:30,040][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 84.0 with 1 tasks
[INFO][2018-05-25 11:24:30,041][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 84.0 (TID 84, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:24:30,041][org.apache.spark.executor.Executor]Running task 0.0 in stage 84.0 (TID 84)
[INFO][2018-05-25 11:24:30,042][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12683 -> 12693
[INFO][2018-05-25 11:24:30,042][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 12683
[INFO][2018-05-25 11:24:30,071][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-25 11:24:30,072][org.apache.spark.executor.Executor]Finished task 0.0 in stage 84.0 (TID 84). 665 bytes result sent to driver
[INFO][2018-05-25 11:24:30,073][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 84.0 (TID 84) in 32 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:24:30,074][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 84.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:24:30,075][org.apache.spark.scheduler.DAGScheduler]ResultStage 84 (foreachPartition at ReceiveKafkaData.scala:73) finished in 0.033 s
[INFO][2018-05-25 11:24:30,075][org.apache.spark.scheduler.DAGScheduler]Job 84 finished: foreachPartition at ReceiveKafkaData.scala:73, took 0.039697 s
[INFO][2018-05-25 11:24:30,076][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218670000 ms.0 from job set of time 1527218670000 ms
[INFO][2018-05-25 11:24:30,077][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.076 s for time 1527218670000 ms (execution: 0.060 s)
[INFO][2018-05-25 11:24:30,077][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 87 from persistence list
[INFO][2018-05-25 11:24:30,078][org.apache.spark.storage.BlockManager]Removing RDD 87
[INFO][2018-05-25 11:24:30,078][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 86 from persistence list
[INFO][2018-05-25 11:24:30,078][org.apache.spark.storage.BlockManager]Removing RDD 86
[INFO][2018-05-25 11:24:30,079][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:24:30,079][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218660000 ms
[INFO][2018-05-25 11:24:35,018][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218675000 ms
[INFO][2018-05-25 11:24:35,019][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218675000 ms.0 from job set of time 1527218675000 ms
[INFO][2018-05-25 11:24:35,022][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:24:35,023][org.apache.spark.scheduler.DAGScheduler]Got job 85 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:24:35,023][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 85 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:24:35,023][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:24:35,023][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:24:35,023][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 85 (MapPartitionsRDD[91] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:24:35,025][org.apache.spark.storage.memory.MemoryStore]Block broadcast_85 stored as values in memory (estimated size 3.1 KB, free 912.2 MB)
[INFO][2018-05-25 11:24:35,026][org.apache.spark.storage.memory.MemoryStore]Block broadcast_85_piece0 stored as bytes in memory (estimated size 1972.0 B, free 912.2 MB)
[INFO][2018-05-25 11:24:35,026][org.apache.spark.storage.BlockManagerInfo]Added broadcast_85_piece0 in memory on 10.194.32.157:53453 (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:24:35,026][org.apache.spark.SparkContext]Created broadcast 85 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:24:35,027][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 85 (MapPartitionsRDD[91] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:24:35,027][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 85.0 with 1 tasks
[INFO][2018-05-25 11:24:35,027][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 85.0 (TID 85, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:24:35,028][org.apache.spark.executor.Executor]Running task 0.0 in stage 85.0 (TID 85)
[INFO][2018-05-25 11:24:35,029][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12693 -> 12703
[INFO][2018-05-25 11:24:35,030][org.apache.spark.executor.Executor]Finished task 0.0 in stage 85.0 (TID 85). 971 bytes result sent to driver
[INFO][2018-05-25 11:24:35,031][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 85.0 (TID 85) in 4 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:24:35,031][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 85.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:24:35,031][org.apache.spark.scheduler.DAGScheduler]ResultStage 85 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.004 s
[INFO][2018-05-25 11:24:35,032][org.apache.spark.scheduler.DAGScheduler]Job 85 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.009338 s
[INFO][2018-05-25 11:24:35,036][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:73
[INFO][2018-05-25 11:24:35,036][org.apache.spark.scheduler.DAGScheduler]Got job 86 (foreachPartition at ReceiveKafkaData.scala:73) with 1 output partitions
[INFO][2018-05-25 11:24:35,036][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 86 (foreachPartition at ReceiveKafkaData.scala:73)
[INFO][2018-05-25 11:24:35,036][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:24:35,036][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:24:35,037][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 86 (MapPartitionsRDD[91] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:24:35,038][org.apache.spark.storage.memory.MemoryStore]Block broadcast_86 stored as values in memory (estimated size 3.4 KB, free 912.2 MB)
[INFO][2018-05-25 11:24:35,038][org.apache.spark.storage.memory.MemoryStore]Block broadcast_86_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.2 MB)
[INFO][2018-05-25 11:24:35,039][org.apache.spark.storage.BlockManagerInfo]Added broadcast_86_piece0 in memory on 10.194.32.157:53453 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:24:35,039][org.apache.spark.SparkContext]Created broadcast 86 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:24:35,039][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 86 (MapPartitionsRDD[91] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:24:35,039][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 86.0 with 1 tasks
[INFO][2018-05-25 11:24:35,040][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 86.0 (TID 86, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:24:35,040][org.apache.spark.executor.Executor]Running task 0.0 in stage 86.0 (TID 86)
[INFO][2018-05-25 11:24:35,041][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12693 -> 12703
[INFO][2018-05-25 11:24:35,041][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 12693
[INFO][2018-05-25 11:24:35,078][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-25 11:24:35,079][org.apache.spark.executor.Executor]Finished task 0.0 in stage 86.0 (TID 86). 708 bytes result sent to driver
[INFO][2018-05-25 11:24:35,079][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 86.0 (TID 86) in 40 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:24:35,079][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 86.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:24:35,080][org.apache.spark.scheduler.DAGScheduler]ResultStage 86 (foreachPartition at ReceiveKafkaData.scala:73) finished in 0.041 s
[INFO][2018-05-25 11:24:35,080][org.apache.spark.scheduler.DAGScheduler]Job 86 finished: foreachPartition at ReceiveKafkaData.scala:73, took 0.043977 s
[INFO][2018-05-25 11:24:35,080][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218675000 ms.0 from job set of time 1527218675000 ms
[INFO][2018-05-25 11:24:35,080][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.080 s for time 1527218675000 ms (execution: 0.061 s)
[INFO][2018-05-25 11:24:35,080][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 89 from persistence list
[INFO][2018-05-25 11:24:35,081][org.apache.spark.storage.BlockManager]Removing RDD 89
[INFO][2018-05-25 11:24:35,081][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 88 from persistence list
[INFO][2018-05-25 11:24:35,081][org.apache.spark.storage.BlockManager]Removing RDD 88
[INFO][2018-05-25 11:24:35,081][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:24:35,081][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218665000 ms
[INFO][2018-05-25 11:24:40,019][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218680000 ms
[INFO][2018-05-25 11:24:40,019][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218680000 ms.0 from job set of time 1527218680000 ms
[INFO][2018-05-25 11:24:40,024][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:24:40,025][org.apache.spark.scheduler.DAGScheduler]Got job 87 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:24:40,025][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 87 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:24:40,025][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:24:40,025][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:24:40,025][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 87 (MapPartitionsRDD[93] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:24:40,026][org.apache.spark.storage.memory.MemoryStore]Block broadcast_87 stored as values in memory (estimated size 3.1 KB, free 912.2 MB)
[INFO][2018-05-25 11:24:40,033][org.apache.spark.storage.memory.MemoryStore]Block broadcast_87_piece0 stored as bytes in memory (estimated size 1972.0 B, free 912.2 MB)
[INFO][2018-05-25 11:24:40,033][org.apache.spark.storage.BlockManagerInfo]Added broadcast_87_piece0 in memory on 10.194.32.157:53453 (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:24:40,034][org.apache.spark.SparkContext]Created broadcast 87 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:24:40,034][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 87 (MapPartitionsRDD[93] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:24:40,034][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 87.0 with 1 tasks
[INFO][2018-05-25 11:24:40,035][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 87.0 (TID 87, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:24:40,035][org.apache.spark.executor.Executor]Running task 0.0 in stage 87.0 (TID 87)
[INFO][2018-05-25 11:24:40,035][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_80_piece0 on 10.194.32.157:53453 in memory (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:24:40,036][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12703 -> 12713
[INFO][2018-05-25 11:24:40,036][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_85_piece0 on 10.194.32.157:53453 in memory (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:24:40,037][org.apache.spark.executor.Executor]Finished task 0.0 in stage 87.0 (TID 87). 929 bytes result sent to driver
[INFO][2018-05-25 11:24:40,037][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_66_piece0 on 10.194.32.157:53453 in memory (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:24:40,038][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 87.0 (TID 87) in 2 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:24:40,038][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 87.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:24:40,038][org.apache.spark.scheduler.DAGScheduler]ResultStage 87 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.004 s
[INFO][2018-05-25 11:24:40,038][org.apache.spark.scheduler.DAGScheduler]Job 87 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.013990 s
[INFO][2018-05-25 11:24:40,039][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_69_piece0 on 10.194.32.157:53453 in memory (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:24:40,040][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_83_piece0 on 10.194.32.157:53453 in memory (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:24:40,041][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_82_piece0 on 10.194.32.157:53453 in memory (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:24:40,042][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_75_piece0 on 10.194.32.157:53453 in memory (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:24:40,042][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:73
[INFO][2018-05-25 11:24:40,042][org.apache.spark.scheduler.DAGScheduler]Got job 88 (foreachPartition at ReceiveKafkaData.scala:73) with 1 output partitions
[INFO][2018-05-25 11:24:40,042][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 88 (foreachPartition at ReceiveKafkaData.scala:73)
[INFO][2018-05-25 11:24:40,042][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:24:40,042][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:24:40,043][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 88 (MapPartitionsRDD[93] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:24:40,043][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_68_piece0 on 10.194.32.157:53453 in memory (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:24:40,045][org.apache.spark.storage.memory.MemoryStore]Block broadcast_88 stored as values in memory (estimated size 3.4 KB, free 912.2 MB)
[INFO][2018-05-25 11:24:40,045][org.apache.spark.storage.memory.MemoryStore]Block broadcast_88_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.2 MB)
[INFO][2018-05-25 11:24:40,046][org.apache.spark.storage.BlockManagerInfo]Added broadcast_88_piece0 in memory on 10.194.32.157:53453 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:24:40,046][org.apache.spark.SparkContext]Created broadcast 88 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:24:40,047][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 88 (MapPartitionsRDD[93] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:24:40,047][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 88.0 with 1 tasks
[INFO][2018-05-25 11:24:40,047][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_86_piece0 on 10.194.32.157:53453 in memory (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:24:40,047][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 88.0 (TID 88, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:24:40,047][org.apache.spark.executor.Executor]Running task 0.0 in stage 88.0 (TID 88)
[INFO][2018-05-25 11:24:40,048][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_84_piece0 on 10.194.32.157:53453 in memory (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:24:40,048][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12703 -> 12713
[INFO][2018-05-25 11:24:40,048][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 12703
[INFO][2018-05-25 11:24:40,049][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_72_piece0 on 10.194.32.157:53453 in memory (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:24:40,050][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_76_piece0 on 10.194.32.157:53453 in memory (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:24:40,051][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_81_piece0 on 10.194.32.157:53453 in memory (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:24:40,052][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_78_piece0 on 10.194.32.157:53453 in memory (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:24:40,053][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_70_piece0 on 10.194.32.157:53453 in memory (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:24:40,054][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_77_piece0 on 10.194.32.157:53453 in memory (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:24:40,054][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_79_piece0 on 10.194.32.157:53453 in memory (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:24:40,055][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_71_piece0 on 10.194.32.157:53453 in memory (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:24:40,056][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_67_piece0 on 10.194.32.157:53453 in memory (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:24:40,056][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_73_piece0 on 10.194.32.157:53453 in memory (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:24:40,057][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_74_piece0 on 10.194.32.157:53453 in memory (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:24:40,078][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-25 11:24:40,081][org.apache.spark.executor.Executor]Finished task 0.0 in stage 88.0 (TID 88). 708 bytes result sent to driver
[INFO][2018-05-25 11:24:40,082][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 88.0 (TID 88) in 35 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:24:40,082][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 88.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:24:40,083][org.apache.spark.scheduler.DAGScheduler]ResultStage 88 (foreachPartition at ReceiveKafkaData.scala:73) finished in 0.036 s
[INFO][2018-05-25 11:24:40,083][org.apache.spark.scheduler.DAGScheduler]Job 88 finished: foreachPartition at ReceiveKafkaData.scala:73, took 0.041307 s
[INFO][2018-05-25 11:24:40,083][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218680000 ms.0 from job set of time 1527218680000 ms
[INFO][2018-05-25 11:24:40,084][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.083 s for time 1527218680000 ms (execution: 0.064 s)
[INFO][2018-05-25 11:24:40,084][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 91 from persistence list
[INFO][2018-05-25 11:24:40,084][org.apache.spark.storage.BlockManager]Removing RDD 91
[INFO][2018-05-25 11:24:40,084][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 90 from persistence list
[INFO][2018-05-25 11:24:40,085][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:24:40,085][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218670000 ms
[INFO][2018-05-25 11:24:40,085][org.apache.spark.storage.BlockManager]Removing RDD 90
[INFO][2018-05-25 11:24:45,016][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218685000 ms
[INFO][2018-05-25 11:24:45,017][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218685000 ms.0 from job set of time 1527218685000 ms
[INFO][2018-05-25 11:24:45,021][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:24:45,021][org.apache.spark.scheduler.DAGScheduler]Got job 89 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:24:45,021][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 89 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:24:45,021][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:24:45,021][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:24:45,021][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 89 (MapPartitionsRDD[95] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:24:45,022][org.apache.spark.storage.memory.MemoryStore]Block broadcast_89 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-25 11:24:45,023][org.apache.spark.storage.memory.MemoryStore]Block broadcast_89_piece0 stored as bytes in memory (estimated size 1972.0 B, free 912.3 MB)
[INFO][2018-05-25 11:24:45,023][org.apache.spark.storage.BlockManagerInfo]Added broadcast_89_piece0 in memory on 10.194.32.157:53453 (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:24:45,023][org.apache.spark.SparkContext]Created broadcast 89 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:24:45,024][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 89 (MapPartitionsRDD[95] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:24:45,024][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 89.0 with 1 tasks
[INFO][2018-05-25 11:24:45,024][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 89.0 (TID 89, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:24:45,024][org.apache.spark.executor.Executor]Running task 0.0 in stage 89.0 (TID 89)
[INFO][2018-05-25 11:24:45,025][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12713 -> 12723
[INFO][2018-05-25 11:24:45,026][org.apache.spark.executor.Executor]Finished task 0.0 in stage 89.0 (TID 89). 976 bytes result sent to driver
[INFO][2018-05-25 11:24:45,026][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 89.0 (TID 89) in 2 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:24:45,026][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 89.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:24:45,027][org.apache.spark.scheduler.DAGScheduler]ResultStage 89 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.002 s
[INFO][2018-05-25 11:24:45,027][org.apache.spark.scheduler.DAGScheduler]Job 89 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.006076 s
[INFO][2018-05-25 11:24:45,030][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:73
[INFO][2018-05-25 11:24:45,030][org.apache.spark.scheduler.DAGScheduler]Got job 90 (foreachPartition at ReceiveKafkaData.scala:73) with 1 output partitions
[INFO][2018-05-25 11:24:45,030][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 90 (foreachPartition at ReceiveKafkaData.scala:73)
[INFO][2018-05-25 11:24:45,030][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:24:45,030][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:24:45,030][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 90 (MapPartitionsRDD[95] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:24:45,031][org.apache.spark.storage.memory.MemoryStore]Block broadcast_90 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-25 11:24:45,031][org.apache.spark.storage.memory.MemoryStore]Block broadcast_90_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.3 MB)
[INFO][2018-05-25 11:24:45,032][org.apache.spark.storage.BlockManagerInfo]Added broadcast_90_piece0 in memory on 10.194.32.157:53453 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:24:45,032][org.apache.spark.SparkContext]Created broadcast 90 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:24:45,032][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 90 (MapPartitionsRDD[95] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:24:45,032][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 90.0 with 1 tasks
[INFO][2018-05-25 11:24:45,033][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 90.0 (TID 90, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:24:45,033][org.apache.spark.executor.Executor]Running task 0.0 in stage 90.0 (TID 90)
[INFO][2018-05-25 11:24:45,034][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12713 -> 12723
[INFO][2018-05-25 11:24:45,034][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 12713
[INFO][2018-05-25 11:24:45,075][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-25 11:24:45,075][org.apache.spark.executor.Executor]Finished task 0.0 in stage 90.0 (TID 90). 708 bytes result sent to driver
[INFO][2018-05-25 11:24:45,076][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 90.0 (TID 90) in 44 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:24:45,076][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 90.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:24:45,076][org.apache.spark.scheduler.DAGScheduler]ResultStage 90 (foreachPartition at ReceiveKafkaData.scala:73) finished in 0.044 s
[INFO][2018-05-25 11:24:45,076][org.apache.spark.scheduler.DAGScheduler]Job 90 finished: foreachPartition at ReceiveKafkaData.scala:73, took 0.046607 s
[INFO][2018-05-25 11:24:45,077][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218685000 ms.0 from job set of time 1527218685000 ms
[INFO][2018-05-25 11:24:45,077][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.077 s for time 1527218685000 ms (execution: 0.060 s)
[INFO][2018-05-25 11:24:45,077][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 93 from persistence list
[INFO][2018-05-25 11:24:45,077][org.apache.spark.storage.BlockManager]Removing RDD 93
[INFO][2018-05-25 11:24:45,077][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 92 from persistence list
[INFO][2018-05-25 11:24:45,077][org.apache.spark.storage.BlockManager]Removing RDD 92
[INFO][2018-05-25 11:24:45,078][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:24:45,078][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218675000 ms
[INFO][2018-05-25 11:24:50,014][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218690000 ms
[INFO][2018-05-25 11:24:50,014][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218690000 ms.0 from job set of time 1527218690000 ms
[INFO][2018-05-25 11:24:50,019][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:24:50,019][org.apache.spark.scheduler.DAGScheduler]Got job 91 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:24:50,019][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 91 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:24:50,019][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:24:50,019][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:24:50,020][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 91 (MapPartitionsRDD[97] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:24:50,020][org.apache.spark.storage.memory.MemoryStore]Block broadcast_91 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-25 11:24:50,021][org.apache.spark.storage.memory.MemoryStore]Block broadcast_91_piece0 stored as bytes in memory (estimated size 1972.0 B, free 912.3 MB)
[INFO][2018-05-25 11:24:50,021][org.apache.spark.storage.BlockManagerInfo]Added broadcast_91_piece0 in memory on 10.194.32.157:53453 (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:24:50,022][org.apache.spark.SparkContext]Created broadcast 91 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:24:50,022][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 91 (MapPartitionsRDD[97] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:24:50,022][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 91.0 with 1 tasks
[INFO][2018-05-25 11:24:50,022][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 91.0 (TID 91, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:24:50,023][org.apache.spark.executor.Executor]Running task 0.0 in stage 91.0 (TID 91)
[INFO][2018-05-25 11:24:50,023][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12723 -> 12733
[INFO][2018-05-25 11:24:50,024][org.apache.spark.executor.Executor]Finished task 0.0 in stage 91.0 (TID 91). 929 bytes result sent to driver
[INFO][2018-05-25 11:24:50,024][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 91.0 (TID 91) in 2 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:24:50,024][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 91.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:24:50,025][org.apache.spark.scheduler.DAGScheduler]ResultStage 91 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.003 s
[INFO][2018-05-25 11:24:50,025][org.apache.spark.scheduler.DAGScheduler]Job 91 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.005812 s
[INFO][2018-05-25 11:24:50,028][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:73
[INFO][2018-05-25 11:24:50,028][org.apache.spark.scheduler.DAGScheduler]Got job 92 (foreachPartition at ReceiveKafkaData.scala:73) with 1 output partitions
[INFO][2018-05-25 11:24:50,028][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 92 (foreachPartition at ReceiveKafkaData.scala:73)
[INFO][2018-05-25 11:24:50,028][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:24:50,028][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:24:50,028][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 92 (MapPartitionsRDD[97] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:24:50,029][org.apache.spark.storage.memory.MemoryStore]Block broadcast_92 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-25 11:24:50,030][org.apache.spark.storage.memory.MemoryStore]Block broadcast_92_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.3 MB)
[INFO][2018-05-25 11:24:50,030][org.apache.spark.storage.BlockManagerInfo]Added broadcast_92_piece0 in memory on 10.194.32.157:53453 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:24:50,030][org.apache.spark.SparkContext]Created broadcast 92 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:24:50,030][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 92 (MapPartitionsRDD[97] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:24:50,030][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 92.0 with 1 tasks
[INFO][2018-05-25 11:24:50,031][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 92.0 (TID 92, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:24:50,031][org.apache.spark.executor.Executor]Running task 0.0 in stage 92.0 (TID 92)
[INFO][2018-05-25 11:24:50,031][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12723 -> 12733
[INFO][2018-05-25 11:24:50,031][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 12723
[INFO][2018-05-25 11:24:50,068][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-25 11:24:50,069][org.apache.spark.executor.Executor]Finished task 0.0 in stage 92.0 (TID 92). 665 bytes result sent to driver
[INFO][2018-05-25 11:24:50,069][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 92.0 (TID 92) in 38 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:24:50,069][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 92.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:24:50,070][org.apache.spark.scheduler.DAGScheduler]ResultStage 92 (foreachPartition at ReceiveKafkaData.scala:73) finished in 0.039 s
[INFO][2018-05-25 11:24:50,070][org.apache.spark.scheduler.DAGScheduler]Job 92 finished: foreachPartition at ReceiveKafkaData.scala:73, took 0.041936 s
[INFO][2018-05-25 11:24:50,070][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218690000 ms.0 from job set of time 1527218690000 ms
[INFO][2018-05-25 11:24:50,070][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.070 s for time 1527218690000 ms (execution: 0.056 s)
[INFO][2018-05-25 11:24:50,070][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 95 from persistence list
[INFO][2018-05-25 11:24:50,071][org.apache.spark.storage.BlockManager]Removing RDD 95
[INFO][2018-05-25 11:24:50,071][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 94 from persistence list
[INFO][2018-05-25 11:24:50,071][org.apache.spark.storage.BlockManager]Removing RDD 94
[INFO][2018-05-25 11:24:50,071][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:24:50,071][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218680000 ms
[INFO][2018-05-25 11:24:55,015][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218695000 ms
[INFO][2018-05-25 11:24:55,016][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218695000 ms.0 from job set of time 1527218695000 ms
[INFO][2018-05-25 11:24:55,020][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:24:55,020][org.apache.spark.scheduler.DAGScheduler]Got job 93 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:24:55,020][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 93 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:24:55,020][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:24:55,021][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:24:55,021][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 93 (MapPartitionsRDD[99] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:24:55,022][org.apache.spark.storage.memory.MemoryStore]Block broadcast_93 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-25 11:24:55,023][org.apache.spark.storage.memory.MemoryStore]Block broadcast_93_piece0 stored as bytes in memory (estimated size 1972.0 B, free 912.3 MB)
[INFO][2018-05-25 11:24:55,023][org.apache.spark.storage.BlockManagerInfo]Added broadcast_93_piece0 in memory on 10.194.32.157:53453 (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:24:55,023][org.apache.spark.SparkContext]Created broadcast 93 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:24:55,024][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 93 (MapPartitionsRDD[99] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:24:55,024][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 93.0 with 1 tasks
[INFO][2018-05-25 11:24:55,024][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 93.0 (TID 93, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:24:55,024][org.apache.spark.executor.Executor]Running task 0.0 in stage 93.0 (TID 93)
[INFO][2018-05-25 11:24:55,025][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12733 -> 12743
[INFO][2018-05-25 11:24:55,026][org.apache.spark.executor.Executor]Finished task 0.0 in stage 93.0 (TID 93). 975 bytes result sent to driver
[INFO][2018-05-25 11:24:55,026][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 93.0 (TID 93) in 2 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:24:55,026][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 93.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:24:55,026][org.apache.spark.scheduler.DAGScheduler]ResultStage 93 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.002 s
[INFO][2018-05-25 11:24:55,027][org.apache.spark.scheduler.DAGScheduler]Job 93 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.006641 s
[INFO][2018-05-25 11:24:55,029][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:73
[INFO][2018-05-25 11:24:55,030][org.apache.spark.scheduler.DAGScheduler]Got job 94 (foreachPartition at ReceiveKafkaData.scala:73) with 1 output partitions
[INFO][2018-05-25 11:24:55,030][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 94 (foreachPartition at ReceiveKafkaData.scala:73)
[INFO][2018-05-25 11:24:55,030][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:24:55,030][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:24:55,030][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 94 (MapPartitionsRDD[99] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:24:55,031][org.apache.spark.storage.memory.MemoryStore]Block broadcast_94 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-25 11:24:55,031][org.apache.spark.storage.memory.MemoryStore]Block broadcast_94_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.3 MB)
[INFO][2018-05-25 11:24:55,032][org.apache.spark.storage.BlockManagerInfo]Added broadcast_94_piece0 in memory on 10.194.32.157:53453 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:24:55,032][org.apache.spark.SparkContext]Created broadcast 94 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:24:55,032][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 94 (MapPartitionsRDD[99] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:24:55,032][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 94.0 with 1 tasks
[INFO][2018-05-25 11:24:55,032][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 94.0 (TID 94, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:24:55,033][org.apache.spark.executor.Executor]Running task 0.0 in stage 94.0 (TID 94)
[INFO][2018-05-25 11:24:55,033][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12733 -> 12743
[INFO][2018-05-25 11:24:55,033][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 12733
[INFO][2018-05-25 11:24:55,072][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-25 11:24:55,072][org.apache.spark.executor.Executor]Finished task 0.0 in stage 94.0 (TID 94). 665 bytes result sent to driver
[INFO][2018-05-25 11:24:55,073][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 94.0 (TID 94) in 41 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:24:55,073][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 94.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:24:55,073][org.apache.spark.scheduler.DAGScheduler]ResultStage 94 (foreachPartition at ReceiveKafkaData.scala:73) finished in 0.041 s
[INFO][2018-05-25 11:24:55,073][org.apache.spark.scheduler.DAGScheduler]Job 94 finished: foreachPartition at ReceiveKafkaData.scala:73, took 0.043963 s
[INFO][2018-05-25 11:24:55,074][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218695000 ms.0 from job set of time 1527218695000 ms
[INFO][2018-05-25 11:24:55,074][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.074 s for time 1527218695000 ms (execution: 0.058 s)
[INFO][2018-05-25 11:24:55,074][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 97 from persistence list
[INFO][2018-05-25 11:24:55,074][org.apache.spark.storage.BlockManager]Removing RDD 97
[INFO][2018-05-25 11:24:55,074][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 96 from persistence list
[INFO][2018-05-25 11:24:55,075][org.apache.spark.storage.BlockManager]Removing RDD 96
[INFO][2018-05-25 11:24:55,075][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:24:55,075][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218685000 ms
[INFO][2018-05-25 11:25:00,012][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218700000 ms
[INFO][2018-05-25 11:25:00,013][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218700000 ms.0 from job set of time 1527218700000 ms
[INFO][2018-05-25 11:25:00,018][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:25:00,019][org.apache.spark.scheduler.DAGScheduler]Got job 95 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:25:00,019][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 95 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:25:00,019][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:25:00,019][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:25:00,019][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 95 (MapPartitionsRDD[101] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:25:00,020][org.apache.spark.storage.memory.MemoryStore]Block broadcast_95 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-25 11:25:00,021][org.apache.spark.storage.memory.MemoryStore]Block broadcast_95_piece0 stored as bytes in memory (estimated size 1972.0 B, free 912.2 MB)
[INFO][2018-05-25 11:25:00,021][org.apache.spark.storage.BlockManagerInfo]Added broadcast_95_piece0 in memory on 10.194.32.157:53453 (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:25:00,022][org.apache.spark.SparkContext]Created broadcast 95 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:25:00,022][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 95 (MapPartitionsRDD[101] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:25:00,022][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 95.0 with 1 tasks
[INFO][2018-05-25 11:25:00,022][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 95.0 (TID 95, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:25:00,022][org.apache.spark.executor.Executor]Running task 0.0 in stage 95.0 (TID 95)
[INFO][2018-05-25 11:25:00,023][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12743 -> 12752
[INFO][2018-05-25 11:25:00,024][org.apache.spark.executor.Executor]Finished task 0.0 in stage 95.0 (TID 95). 973 bytes result sent to driver
[INFO][2018-05-25 11:25:00,024][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 95.0 (TID 95) in 2 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:25:00,024][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 95.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:25:00,025][org.apache.spark.scheduler.DAGScheduler]ResultStage 95 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.002 s
[INFO][2018-05-25 11:25:00,025][org.apache.spark.scheduler.DAGScheduler]Job 95 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.006198 s
[INFO][2018-05-25 11:25:00,028][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:73
[INFO][2018-05-25 11:25:00,028][org.apache.spark.scheduler.DAGScheduler]Got job 96 (foreachPartition at ReceiveKafkaData.scala:73) with 1 output partitions
[INFO][2018-05-25 11:25:00,028][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 96 (foreachPartition at ReceiveKafkaData.scala:73)
[INFO][2018-05-25 11:25:00,028][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:25:00,028][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:25:00,029][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 96 (MapPartitionsRDD[101] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:25:00,029][org.apache.spark.storage.memory.MemoryStore]Block broadcast_96 stored as values in memory (estimated size 3.4 KB, free 912.2 MB)
[INFO][2018-05-25 11:25:00,030][org.apache.spark.storage.memory.MemoryStore]Block broadcast_96_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.2 MB)
[INFO][2018-05-25 11:25:00,030][org.apache.spark.storage.BlockManagerInfo]Added broadcast_96_piece0 in memory on 10.194.32.157:53453 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:25:00,030][org.apache.spark.SparkContext]Created broadcast 96 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:25:00,030][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 96 (MapPartitionsRDD[101] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:25:00,030][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 96.0 with 1 tasks
[INFO][2018-05-25 11:25:00,031][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 96.0 (TID 96, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:25:00,031][org.apache.spark.executor.Executor]Running task 0.0 in stage 96.0 (TID 96)
[INFO][2018-05-25 11:25:00,032][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12743 -> 12752
[INFO][2018-05-25 11:25:00,032][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 12743
[INFO][2018-05-25 11:25:00,071][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-25 11:25:00,072][org.apache.spark.executor.Executor]Finished task 0.0 in stage 96.0 (TID 96). 665 bytes result sent to driver
[INFO][2018-05-25 11:25:00,072][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 96.0 (TID 96) in 41 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:25:00,072][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 96.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:25:00,072][org.apache.spark.scheduler.DAGScheduler]ResultStage 96 (foreachPartition at ReceiveKafkaData.scala:73) finished in 0.041 s
[INFO][2018-05-25 11:25:00,072][org.apache.spark.scheduler.DAGScheduler]Job 96 finished: foreachPartition at ReceiveKafkaData.scala:73, took 0.044338 s
[INFO][2018-05-25 11:25:00,073][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218700000 ms.0 from job set of time 1527218700000 ms
[INFO][2018-05-25 11:25:00,073][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.073 s for time 1527218700000 ms (execution: 0.060 s)
[INFO][2018-05-25 11:25:00,073][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 99 from persistence list
[INFO][2018-05-25 11:25:00,073][org.apache.spark.storage.BlockManager]Removing RDD 99
[INFO][2018-05-25 11:25:00,073][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 98 from persistence list
[INFO][2018-05-25 11:25:00,073][org.apache.spark.storage.BlockManager]Removing RDD 98
[INFO][2018-05-25 11:25:00,073][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:25:00,073][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218690000 ms
[INFO][2018-05-25 11:25:05,020][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218705000 ms
[INFO][2018-05-25 11:25:05,020][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218705000 ms.0 from job set of time 1527218705000 ms
[INFO][2018-05-25 11:25:05,024][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:25:05,024][org.apache.spark.scheduler.DAGScheduler]Got job 97 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:25:05,024][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 97 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:25:05,024][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:25:05,024][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:25:05,024][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 97 (MapPartitionsRDD[103] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:25:05,025][org.apache.spark.storage.memory.MemoryStore]Block broadcast_97 stored as values in memory (estimated size 3.1 KB, free 912.2 MB)
[INFO][2018-05-25 11:25:05,026][org.apache.spark.storage.memory.MemoryStore]Block broadcast_97_piece0 stored as bytes in memory (estimated size 1972.0 B, free 912.2 MB)
[INFO][2018-05-25 11:25:05,026][org.apache.spark.storage.BlockManagerInfo]Added broadcast_97_piece0 in memory on 10.194.32.157:53453 (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:25:05,027][org.apache.spark.SparkContext]Created broadcast 97 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:25:05,027][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 97 (MapPartitionsRDD[103] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:25:05,027][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 97.0 with 1 tasks
[INFO][2018-05-25 11:25:05,028][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 97.0 (TID 97, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:25:05,028][org.apache.spark.executor.Executor]Running task 0.0 in stage 97.0 (TID 97)
[INFO][2018-05-25 11:25:05,028][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12752 -> 12762
[INFO][2018-05-25 11:25:05,029][org.apache.spark.executor.Executor]Finished task 0.0 in stage 97.0 (TID 97). 930 bytes result sent to driver
[INFO][2018-05-25 11:25:05,029][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 97.0 (TID 97) in 2 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:25:05,030][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 97.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:25:05,030][org.apache.spark.scheduler.DAGScheduler]ResultStage 97 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.003 s
[INFO][2018-05-25 11:25:05,030][org.apache.spark.scheduler.DAGScheduler]Job 97 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.006039 s
[INFO][2018-05-25 11:25:05,033][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:73
[INFO][2018-05-25 11:25:05,033][org.apache.spark.scheduler.DAGScheduler]Got job 98 (foreachPartition at ReceiveKafkaData.scala:73) with 1 output partitions
[INFO][2018-05-25 11:25:05,033][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 98 (foreachPartition at ReceiveKafkaData.scala:73)
[INFO][2018-05-25 11:25:05,033][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:25:05,033][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:25:05,034][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 98 (MapPartitionsRDD[103] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:25:05,034][org.apache.spark.storage.memory.MemoryStore]Block broadcast_98 stored as values in memory (estimated size 3.4 KB, free 912.2 MB)
[INFO][2018-05-25 11:25:05,035][org.apache.spark.storage.memory.MemoryStore]Block broadcast_98_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.2 MB)
[INFO][2018-05-25 11:25:05,036][org.apache.spark.storage.BlockManagerInfo]Added broadcast_98_piece0 in memory on 10.194.32.157:53453 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:25:05,036][org.apache.spark.SparkContext]Created broadcast 98 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:25:05,036][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 98 (MapPartitionsRDD[103] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:25:05,036][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 98.0 with 1 tasks
[INFO][2018-05-25 11:25:05,036][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 98.0 (TID 98, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:25:05,036][org.apache.spark.executor.Executor]Running task 0.0 in stage 98.0 (TID 98)
[INFO][2018-05-25 11:25:05,037][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12752 -> 12762
[INFO][2018-05-25 11:25:05,037][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 12752
[INFO][2018-05-25 11:25:05,078][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-25 11:25:05,079][org.apache.spark.executor.Executor]Finished task 0.0 in stage 98.0 (TID 98). 708 bytes result sent to driver
[INFO][2018-05-25 11:25:05,079][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 98.0 (TID 98) in 43 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:25:05,079][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 98.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:25:05,079][org.apache.spark.scheduler.DAGScheduler]ResultStage 98 (foreachPartition at ReceiveKafkaData.scala:73) finished in 0.043 s
[INFO][2018-05-25 11:25:05,080][org.apache.spark.scheduler.DAGScheduler]Job 98 finished: foreachPartition at ReceiveKafkaData.scala:73, took 0.046618 s
[INFO][2018-05-25 11:25:05,080][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218705000 ms.0 from job set of time 1527218705000 ms
[INFO][2018-05-25 11:25:05,080][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.080 s for time 1527218705000 ms (execution: 0.060 s)
[INFO][2018-05-25 11:25:05,080][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 101 from persistence list
[INFO][2018-05-25 11:25:05,081][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 100 from persistence list
[INFO][2018-05-25 11:25:05,081][org.apache.spark.storage.BlockManager]Removing RDD 101
[INFO][2018-05-25 11:25:05,082][org.apache.spark.storage.BlockManager]Removing RDD 100
[INFO][2018-05-25 11:25:05,082][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:25:05,082][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218695000 ms
[INFO][2018-05-25 11:25:10,014][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218710000 ms
[INFO][2018-05-25 11:25:10,015][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218710000 ms.0 from job set of time 1527218710000 ms
[INFO][2018-05-25 11:25:10,019][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:25:10,019][org.apache.spark.scheduler.DAGScheduler]Got job 99 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:25:10,019][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 99 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:25:10,019][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:25:10,019][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:25:10,019][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 99 (MapPartitionsRDD[105] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:25:10,020][org.apache.spark.storage.memory.MemoryStore]Block broadcast_99 stored as values in memory (estimated size 3.1 KB, free 912.2 MB)
[INFO][2018-05-25 11:25:10,021][org.apache.spark.storage.memory.MemoryStore]Block broadcast_99_piece0 stored as bytes in memory (estimated size 1972.0 B, free 912.2 MB)
[INFO][2018-05-25 11:25:10,022][org.apache.spark.storage.BlockManagerInfo]Added broadcast_99_piece0 in memory on 10.194.32.157:53453 (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:25:10,022][org.apache.spark.SparkContext]Created broadcast 99 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:25:10,022][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 99 (MapPartitionsRDD[105] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:25:10,022][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 99.0 with 1 tasks
[INFO][2018-05-25 11:25:10,023][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 99.0 (TID 99, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:25:10,023][org.apache.spark.executor.Executor]Running task 0.0 in stage 99.0 (TID 99)
[INFO][2018-05-25 11:25:10,024][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12762 -> 12772
[INFO][2018-05-25 11:25:10,025][org.apache.spark.executor.Executor]Finished task 0.0 in stage 99.0 (TID 99). 974 bytes result sent to driver
[INFO][2018-05-25 11:25:10,025][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 99.0 (TID 99) in 2 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:25:10,025][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 99.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:25:10,025][org.apache.spark.scheduler.DAGScheduler]ResultStage 99 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.003 s
[INFO][2018-05-25 11:25:10,026][org.apache.spark.scheduler.DAGScheduler]Job 99 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.006772 s
[INFO][2018-05-25 11:25:10,030][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:73
[INFO][2018-05-25 11:25:10,030][org.apache.spark.scheduler.DAGScheduler]Got job 100 (foreachPartition at ReceiveKafkaData.scala:73) with 1 output partitions
[INFO][2018-05-25 11:25:10,030][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 100 (foreachPartition at ReceiveKafkaData.scala:73)
[INFO][2018-05-25 11:25:10,030][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:25:10,030][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:25:10,031][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 100 (MapPartitionsRDD[105] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:25:10,032][org.apache.spark.storage.memory.MemoryStore]Block broadcast_100 stored as values in memory (estimated size 3.4 KB, free 912.2 MB)
[INFO][2018-05-25 11:25:10,032][org.apache.spark.storage.memory.MemoryStore]Block broadcast_100_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.2 MB)
[INFO][2018-05-25 11:25:10,033][org.apache.spark.storage.BlockManagerInfo]Added broadcast_100_piece0 in memory on 10.194.32.157:53453 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:25:10,033][org.apache.spark.SparkContext]Created broadcast 100 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:25:10,033][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 100 (MapPartitionsRDD[105] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:25:10,033][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 100.0 with 1 tasks
[INFO][2018-05-25 11:25:10,034][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 100.0 (TID 100, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:25:10,034][org.apache.spark.executor.Executor]Running task 0.0 in stage 100.0 (TID 100)
[INFO][2018-05-25 11:25:10,035][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12762 -> 12772
[INFO][2018-05-25 11:25:10,035][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 12762
[INFO][2018-05-25 11:25:10,064][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-25 11:25:10,065][org.apache.spark.executor.Executor]Finished task 0.0 in stage 100.0 (TID 100). 665 bytes result sent to driver
[INFO][2018-05-25 11:25:10,066][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 100.0 (TID 100) in 32 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:25:10,066][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 100.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:25:10,066][org.apache.spark.scheduler.DAGScheduler]ResultStage 100 (foreachPartition at ReceiveKafkaData.scala:73) finished in 0.032 s
[INFO][2018-05-25 11:25:10,066][org.apache.spark.scheduler.DAGScheduler]Job 100 finished: foreachPartition at ReceiveKafkaData.scala:73, took 0.036644 s
[INFO][2018-05-25 11:25:10,067][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218710000 ms.0 from job set of time 1527218710000 ms
[INFO][2018-05-25 11:25:10,067][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.067 s for time 1527218710000 ms (execution: 0.052 s)
[INFO][2018-05-25 11:25:10,067][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 103 from persistence list
[INFO][2018-05-25 11:25:10,067][org.apache.spark.storage.BlockManager]Removing RDD 103
[INFO][2018-05-25 11:25:10,067][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 102 from persistence list
[INFO][2018-05-25 11:25:10,067][org.apache.spark.storage.BlockManager]Removing RDD 102
[INFO][2018-05-25 11:25:10,067][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:25:10,068][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218700000 ms
[INFO][2018-05-25 11:25:15,011][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218715000 ms
[INFO][2018-05-25 11:25:15,012][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218715000 ms.0 from job set of time 1527218715000 ms
[INFO][2018-05-25 11:25:15,016][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:25:15,016][org.apache.spark.scheduler.DAGScheduler]Got job 101 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:25:15,016][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 101 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:25:15,016][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:25:15,017][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:25:15,017][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 101 (MapPartitionsRDD[107] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:25:15,017][org.apache.spark.storage.memory.MemoryStore]Block broadcast_101 stored as values in memory (estimated size 3.1 KB, free 912.2 MB)
[INFO][2018-05-25 11:25:15,018][org.apache.spark.storage.memory.MemoryStore]Block broadcast_101_piece0 stored as bytes in memory (estimated size 1972.0 B, free 912.2 MB)
[INFO][2018-05-25 11:25:15,018][org.apache.spark.storage.BlockManagerInfo]Added broadcast_101_piece0 in memory on 10.194.32.157:53453 (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:25:15,019][org.apache.spark.SparkContext]Created broadcast 101 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:25:15,019][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 101 (MapPartitionsRDD[107] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:25:15,019][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 101.0 with 1 tasks
[INFO][2018-05-25 11:25:15,019][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 101.0 (TID 101, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:25:15,019][org.apache.spark.executor.Executor]Running task 0.0 in stage 101.0 (TID 101)
[INFO][2018-05-25 11:25:15,020][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12772 -> 12782
[INFO][2018-05-25 11:25:15,021][org.apache.spark.executor.Executor]Finished task 0.0 in stage 101.0 (TID 101). 973 bytes result sent to driver
[INFO][2018-05-25 11:25:15,021][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 101.0 (TID 101) in 2 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:25:15,021][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 101.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:25:15,021][org.apache.spark.scheduler.DAGScheduler]ResultStage 101 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.002 s
[INFO][2018-05-25 11:25:15,022][org.apache.spark.scheduler.DAGScheduler]Job 101 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.005419 s
[INFO][2018-05-25 11:25:15,024][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:73
[INFO][2018-05-25 11:25:15,024][org.apache.spark.scheduler.DAGScheduler]Got job 102 (foreachPartition at ReceiveKafkaData.scala:73) with 1 output partitions
[INFO][2018-05-25 11:25:15,025][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 102 (foreachPartition at ReceiveKafkaData.scala:73)
[INFO][2018-05-25 11:25:15,025][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:25:15,025][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:25:15,025][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 102 (MapPartitionsRDD[107] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:25:15,025][org.apache.spark.storage.memory.MemoryStore]Block broadcast_102 stored as values in memory (estimated size 3.4 KB, free 912.2 MB)
[INFO][2018-05-25 11:25:15,026][org.apache.spark.storage.memory.MemoryStore]Block broadcast_102_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.2 MB)
[INFO][2018-05-25 11:25:15,026][org.apache.spark.storage.BlockManagerInfo]Added broadcast_102_piece0 in memory on 10.194.32.157:53453 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:25:15,026][org.apache.spark.SparkContext]Created broadcast 102 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:25:15,027][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 102 (MapPartitionsRDD[107] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:25:15,027][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 102.0 with 1 tasks
[INFO][2018-05-25 11:25:15,027][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 102.0 (TID 102, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:25:15,027][org.apache.spark.executor.Executor]Running task 0.0 in stage 102.0 (TID 102)
[INFO][2018-05-25 11:25:15,028][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12772 -> 12782
[INFO][2018-05-25 11:25:15,028][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 12772
[INFO][2018-05-25 11:25:15,071][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-25 11:25:15,071][org.apache.spark.executor.Executor]Finished task 0.0 in stage 102.0 (TID 102). 708 bytes result sent to driver
[INFO][2018-05-25 11:25:15,072][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 102.0 (TID 102) in 45 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:25:15,072][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 102.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:25:15,072][org.apache.spark.scheduler.DAGScheduler]ResultStage 102 (foreachPartition at ReceiveKafkaData.scala:73) finished in 0.045 s
[INFO][2018-05-25 11:25:15,073][org.apache.spark.scheduler.DAGScheduler]Job 102 finished: foreachPartition at ReceiveKafkaData.scala:73, took 0.048246 s
[INFO][2018-05-25 11:25:15,073][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218715000 ms.0 from job set of time 1527218715000 ms
[INFO][2018-05-25 11:25:15,073][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.073 s for time 1527218715000 ms (execution: 0.062 s)
[INFO][2018-05-25 11:25:15,073][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 105 from persistence list
[INFO][2018-05-25 11:25:15,073][org.apache.spark.storage.BlockManager]Removing RDD 105
[INFO][2018-05-25 11:25:15,073][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 104 from persistence list
[INFO][2018-05-25 11:25:15,074][org.apache.spark.storage.BlockManager]Removing RDD 104
[INFO][2018-05-25 11:25:15,074][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:25:15,074][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218705000 ms
[INFO][2018-05-25 11:25:20,020][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218720000 ms
[INFO][2018-05-25 11:25:20,020][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218720000 ms.0 from job set of time 1527218720000 ms
[INFO][2018-05-25 11:25:20,024][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:25:20,025][org.apache.spark.scheduler.DAGScheduler]Got job 103 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:25:20,025][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 103 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:25:20,025][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:25:20,025][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:25:20,025][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 103 (MapPartitionsRDD[109] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:25:20,026][org.apache.spark.storage.memory.MemoryStore]Block broadcast_103 stored as values in memory (estimated size 3.1 KB, free 912.2 MB)
[INFO][2018-05-25 11:25:20,027][org.apache.spark.storage.memory.MemoryStore]Block broadcast_103_piece0 stored as bytes in memory (estimated size 1972.0 B, free 912.2 MB)
[INFO][2018-05-25 11:25:20,027][org.apache.spark.storage.BlockManagerInfo]Added broadcast_103_piece0 in memory on 10.194.32.157:53453 (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:25:20,027][org.apache.spark.SparkContext]Created broadcast 103 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:25:20,027][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 103 (MapPartitionsRDD[109] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:25:20,027][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 103.0 with 1 tasks
[INFO][2018-05-25 11:25:20,028][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 103.0 (TID 103, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:25:20,028][org.apache.spark.executor.Executor]Running task 0.0 in stage 103.0 (TID 103)
[INFO][2018-05-25 11:25:20,029][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12782 -> 12792
[INFO][2018-05-25 11:25:20,030][org.apache.spark.executor.Executor]Finished task 0.0 in stage 103.0 (TID 103). 961 bytes result sent to driver
[INFO][2018-05-25 11:25:20,030][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 103.0 (TID 103) in 2 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:25:20,030][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 103.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:25:20,030][org.apache.spark.scheduler.DAGScheduler]ResultStage 103 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.002 s
[INFO][2018-05-25 11:25:20,031][org.apache.spark.scheduler.DAGScheduler]Job 103 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.006179 s
[INFO][2018-05-25 11:25:20,033][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:73
[INFO][2018-05-25 11:25:20,034][org.apache.spark.scheduler.DAGScheduler]Got job 104 (foreachPartition at ReceiveKafkaData.scala:73) with 1 output partitions
[INFO][2018-05-25 11:25:20,034][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 104 (foreachPartition at ReceiveKafkaData.scala:73)
[INFO][2018-05-25 11:25:20,034][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:25:20,034][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:25:20,034][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 104 (MapPartitionsRDD[109] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:25:20,035][org.apache.spark.storage.memory.MemoryStore]Block broadcast_104 stored as values in memory (estimated size 3.4 KB, free 912.2 MB)
[INFO][2018-05-25 11:25:20,036][org.apache.spark.storage.memory.MemoryStore]Block broadcast_104_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.2 MB)
[INFO][2018-05-25 11:25:20,036][org.apache.spark.storage.BlockManagerInfo]Added broadcast_104_piece0 in memory on 10.194.32.157:53453 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:25:20,036][org.apache.spark.SparkContext]Created broadcast 104 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:25:20,037][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 104 (MapPartitionsRDD[109] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:25:20,037][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 104.0 with 1 tasks
[INFO][2018-05-25 11:25:20,037][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 104.0 (TID 104, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:25:20,037][org.apache.spark.executor.Executor]Running task 0.0 in stage 104.0 (TID 104)
[INFO][2018-05-25 11:25:20,038][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12782 -> 12792
[INFO][2018-05-25 11:25:20,038][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 12782
[INFO][2018-05-25 11:25:21,118][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-25 11:25:21,119][org.apache.spark.executor.Executor]Finished task 0.0 in stage 104.0 (TID 104). 665 bytes result sent to driver
[INFO][2018-05-25 11:25:21,120][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 104.0 (TID 104) in 1083 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:25:21,120][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 104.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:25:21,120][org.apache.spark.scheduler.DAGScheduler]ResultStage 104 (foreachPartition at ReceiveKafkaData.scala:73) finished in 1.083 s
[INFO][2018-05-25 11:25:21,120][org.apache.spark.scheduler.DAGScheduler]Job 104 finished: foreachPartition at ReceiveKafkaData.scala:73, took 1.086905 s
[INFO][2018-05-25 11:25:21,121][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218720000 ms.0 from job set of time 1527218720000 ms
[INFO][2018-05-25 11:25:21,121][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 1.121 s for time 1527218720000 ms (execution: 1.101 s)
[INFO][2018-05-25 11:25:21,121][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 107 from persistence list
[INFO][2018-05-25 11:25:21,121][org.apache.spark.storage.BlockManager]Removing RDD 107
[INFO][2018-05-25 11:25:21,121][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 106 from persistence list
[INFO][2018-05-25 11:25:21,122][org.apache.spark.storage.BlockManager]Removing RDD 106
[INFO][2018-05-25 11:25:21,122][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:25:21,122][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218710000 ms
[INFO][2018-05-25 11:25:25,017][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218725000 ms
[INFO][2018-05-25 11:25:25,018][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218725000 ms.0 from job set of time 1527218725000 ms
[INFO][2018-05-25 11:25:25,021][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:25:25,021][org.apache.spark.scheduler.DAGScheduler]Got job 105 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:25:25,022][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 105 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:25:25,022][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:25:25,022][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:25:25,022][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 105 (MapPartitionsRDD[111] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:25:25,023][org.apache.spark.storage.memory.MemoryStore]Block broadcast_105 stored as values in memory (estimated size 3.1 KB, free 912.2 MB)
[INFO][2018-05-25 11:25:25,023][org.apache.spark.storage.memory.MemoryStore]Block broadcast_105_piece0 stored as bytes in memory (estimated size 1972.0 B, free 912.2 MB)
[INFO][2018-05-25 11:25:25,024][org.apache.spark.storage.BlockManagerInfo]Added broadcast_105_piece0 in memory on 10.194.32.157:53453 (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:25:25,024][org.apache.spark.SparkContext]Created broadcast 105 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:25:25,024][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 105 (MapPartitionsRDD[111] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:25:25,024][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 105.0 with 1 tasks
[INFO][2018-05-25 11:25:25,025][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 105.0 (TID 105, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:25:25,025][org.apache.spark.executor.Executor]Running task 0.0 in stage 105.0 (TID 105)
[INFO][2018-05-25 11:25:25,025][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12792 -> 12802
[INFO][2018-05-25 11:25:25,026][org.apache.spark.executor.Executor]Finished task 0.0 in stage 105.0 (TID 105). 932 bytes result sent to driver
[INFO][2018-05-25 11:25:25,026][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 105.0 (TID 105) in 2 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:25:25,027][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 105.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:25:25,027][org.apache.spark.scheduler.DAGScheduler]ResultStage 105 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.003 s
[INFO][2018-05-25 11:25:25,027][org.apache.spark.scheduler.DAGScheduler]Job 105 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.005740 s
[INFO][2018-05-25 11:25:25,029][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:73
[INFO][2018-05-25 11:25:25,030][org.apache.spark.scheduler.DAGScheduler]Got job 106 (foreachPartition at ReceiveKafkaData.scala:73) with 1 output partitions
[INFO][2018-05-25 11:25:25,030][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 106 (foreachPartition at ReceiveKafkaData.scala:73)
[INFO][2018-05-25 11:25:25,030][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:25:25,030][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:25:25,030][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 106 (MapPartitionsRDD[111] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:25:25,031][org.apache.spark.storage.memory.MemoryStore]Block broadcast_106 stored as values in memory (estimated size 3.4 KB, free 912.2 MB)
[INFO][2018-05-25 11:25:25,032][org.apache.spark.storage.memory.MemoryStore]Block broadcast_106_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.2 MB)
[INFO][2018-05-25 11:25:25,032][org.apache.spark.storage.BlockManagerInfo]Added broadcast_106_piece0 in memory on 10.194.32.157:53453 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:25:25,032][org.apache.spark.SparkContext]Created broadcast 106 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:25:25,032][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 106 (MapPartitionsRDD[111] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:25:25,033][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 106.0 with 1 tasks
[INFO][2018-05-25 11:25:25,033][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 106.0 (TID 106, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:25:25,033][org.apache.spark.executor.Executor]Running task 0.0 in stage 106.0 (TID 106)
[INFO][2018-05-25 11:25:25,034][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12792 -> 12802
[INFO][2018-05-25 11:25:25,034][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 12792
[INFO][2018-05-25 11:25:25,073][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-25 11:25:25,073][org.apache.spark.executor.Executor]Finished task 0.0 in stage 106.0 (TID 106). 708 bytes result sent to driver
[INFO][2018-05-25 11:25:25,074][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 106.0 (TID 106) in 41 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:25:25,074][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 106.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:25:25,074][org.apache.spark.scheduler.DAGScheduler]ResultStage 106 (foreachPartition at ReceiveKafkaData.scala:73) finished in 0.041 s
[INFO][2018-05-25 11:25:25,074][org.apache.spark.scheduler.DAGScheduler]Job 106 finished: foreachPartition at ReceiveKafkaData.scala:73, took 0.044922 s
[INFO][2018-05-25 11:25:25,075][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218725000 ms.0 from job set of time 1527218725000 ms
[INFO][2018-05-25 11:25:25,075][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.075 s for time 1527218725000 ms (execution: 0.057 s)
[INFO][2018-05-25 11:25:25,075][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 109 from persistence list
[INFO][2018-05-25 11:25:25,076][org.apache.spark.storage.BlockManager]Removing RDD 109
[INFO][2018-05-25 11:25:25,076][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 108 from persistence list
[INFO][2018-05-25 11:25:25,076][org.apache.spark.storage.BlockManager]Removing RDD 108
[INFO][2018-05-25 11:25:25,076][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:25:25,076][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218715000 ms
[INFO][2018-05-25 11:25:30,016][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218730000 ms
[INFO][2018-05-25 11:25:30,016][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218730000 ms.0 from job set of time 1527218730000 ms
[INFO][2018-05-25 11:25:30,020][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:25:30,021][org.apache.spark.scheduler.DAGScheduler]Got job 107 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:25:30,021][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 107 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:25:30,021][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:25:30,021][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:25:30,021][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 107 (MapPartitionsRDD[113] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:25:30,022][org.apache.spark.storage.memory.MemoryStore]Block broadcast_107 stored as values in memory (estimated size 3.1 KB, free 912.2 MB)
[INFO][2018-05-25 11:25:30,022][org.apache.spark.storage.memory.MemoryStore]Block broadcast_107_piece0 stored as bytes in memory (estimated size 1972.0 B, free 912.2 MB)
[INFO][2018-05-25 11:25:30,023][org.apache.spark.storage.BlockManagerInfo]Added broadcast_107_piece0 in memory on 10.194.32.157:53453 (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:25:30,023][org.apache.spark.SparkContext]Created broadcast 107 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:25:30,023][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 107 (MapPartitionsRDD[113] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:25:30,023][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 107.0 with 1 tasks
[INFO][2018-05-25 11:25:30,024][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 107.0 (TID 107, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:25:30,024][org.apache.spark.executor.Executor]Running task 0.0 in stage 107.0 (TID 107)
[INFO][2018-05-25 11:25:30,025][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12802 -> 12812
[INFO][2018-05-25 11:25:30,025][org.apache.spark.executor.Executor]Finished task 0.0 in stage 107.0 (TID 107). 946 bytes result sent to driver
[INFO][2018-05-25 11:25:30,026][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 107.0 (TID 107) in 2 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:25:30,026][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 107.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:25:30,026][org.apache.spark.scheduler.DAGScheduler]ResultStage 107 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.003 s
[INFO][2018-05-25 11:25:30,026][org.apache.spark.scheduler.DAGScheduler]Job 107 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.005710 s
[INFO][2018-05-25 11:25:30,033][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_96_piece0 on 10.194.32.157:53453 in memory (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:25:30,034][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_91_piece0 on 10.194.32.157:53453 in memory (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:25:30,034][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:73
[INFO][2018-05-25 11:25:30,034][org.apache.spark.scheduler.DAGScheduler]Got job 108 (foreachPartition at ReceiveKafkaData.scala:73) with 1 output partitions
[INFO][2018-05-25 11:25:30,034][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 108 (foreachPartition at ReceiveKafkaData.scala:73)
[INFO][2018-05-25 11:25:30,034][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:25:30,034][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:25:30,034][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_95_piece0 on 10.194.32.157:53453 in memory (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:25:30,034][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 108 (MapPartitionsRDD[113] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:25:30,035][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_107_piece0 on 10.194.32.157:53453 in memory (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:25:30,035][org.apache.spark.storage.memory.MemoryStore]Block broadcast_108 stored as values in memory (estimated size 3.4 KB, free 912.2 MB)
[INFO][2018-05-25 11:25:30,036][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_87_piece0 on 10.194.32.157:53453 in memory (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:25:30,036][org.apache.spark.storage.memory.MemoryStore]Block broadcast_108_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.2 MB)
[INFO][2018-05-25 11:25:30,036][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_97_piece0 on 10.194.32.157:53453 in memory (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:25:30,037][org.apache.spark.storage.BlockManagerInfo]Added broadcast_108_piece0 in memory on 10.194.32.157:53453 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:25:30,037][org.apache.spark.SparkContext]Created broadcast 108 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:25:30,037][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 108 (MapPartitionsRDD[113] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:25:30,037][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 108.0 with 1 tasks
[INFO][2018-05-25 11:25:30,037][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 108.0 (TID 108, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:25:30,038][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_99_piece0 on 10.194.32.157:53453 in memory (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:25:30,038][org.apache.spark.executor.Executor]Running task 0.0 in stage 108.0 (TID 108)
[INFO][2018-05-25 11:25:30,039][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12802 -> 12812
[INFO][2018-05-25 11:25:30,039][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 12802
[INFO][2018-05-25 11:25:30,039][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_105_piece0 on 10.194.32.157:53453 in memory (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:25:30,040][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_98_piece0 on 10.194.32.157:53453 in memory (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:25:30,041][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_93_piece0 on 10.194.32.157:53453 in memory (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:25:30,041][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_94_piece0 on 10.194.32.157:53453 in memory (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:25:30,042][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_92_piece0 on 10.194.32.157:53453 in memory (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:25:30,042][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_104_piece0 on 10.194.32.157:53453 in memory (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:25:30,043][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_88_piece0 on 10.194.32.157:53453 in memory (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:25:30,043][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_106_piece0 on 10.194.32.157:53453 in memory (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:25:30,044][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_89_piece0 on 10.194.32.157:53453 in memory (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:25:30,044][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_100_piece0 on 10.194.32.157:53453 in memory (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:25:30,044][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_101_piece0 on 10.194.32.157:53453 in memory (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:25:30,045][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_90_piece0 on 10.194.32.157:53453 in memory (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:25:30,045][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_103_piece0 on 10.194.32.157:53453 in memory (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:25:30,046][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_102_piece0 on 10.194.32.157:53453 in memory (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:25:30,072][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-25 11:25:30,073][org.apache.spark.executor.Executor]Finished task 0.0 in stage 108.0 (TID 108). 708 bytes result sent to driver
[INFO][2018-05-25 11:25:30,073][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 108.0 (TID 108) in 36 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:25:30,073][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 108.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:25:30,073][org.apache.spark.scheduler.DAGScheduler]ResultStage 108 (foreachPartition at ReceiveKafkaData.scala:73) finished in 0.036 s
[INFO][2018-05-25 11:25:30,074][org.apache.spark.scheduler.DAGScheduler]Job 108 finished: foreachPartition at ReceiveKafkaData.scala:73, took 0.039659 s
[INFO][2018-05-25 11:25:30,074][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218730000 ms.0 from job set of time 1527218730000 ms
[INFO][2018-05-25 11:25:30,074][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.074 s for time 1527218730000 ms (execution: 0.058 s)
[INFO][2018-05-25 11:25:30,074][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 111 from persistence list
[INFO][2018-05-25 11:25:30,074][org.apache.spark.storage.BlockManager]Removing RDD 111
[INFO][2018-05-25 11:25:30,074][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 110 from persistence list
[INFO][2018-05-25 11:25:30,075][org.apache.spark.storage.BlockManager]Removing RDD 110
[INFO][2018-05-25 11:25:30,075][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:25:30,075][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218720000 ms
[INFO][2018-05-25 11:25:35,023][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218735000 ms
[INFO][2018-05-25 11:25:35,024][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218735000 ms.0 from job set of time 1527218735000 ms
[INFO][2018-05-25 11:25:35,032][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:25:35,033][org.apache.spark.scheduler.DAGScheduler]Got job 109 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:25:35,033][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 109 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:25:35,033][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:25:35,033][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:25:35,033][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 109 (MapPartitionsRDD[115] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:25:35,035][org.apache.spark.storage.memory.MemoryStore]Block broadcast_109 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-25 11:25:35,036][org.apache.spark.storage.memory.MemoryStore]Block broadcast_109_piece0 stored as bytes in memory (estimated size 1972.0 B, free 912.3 MB)
[INFO][2018-05-25 11:25:35,036][org.apache.spark.storage.BlockManagerInfo]Added broadcast_109_piece0 in memory on 10.194.32.157:53453 (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:25:35,036][org.apache.spark.SparkContext]Created broadcast 109 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:25:35,037][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 109 (MapPartitionsRDD[115] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:25:35,038][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 109.0 with 1 tasks
[INFO][2018-05-25 11:25:35,038][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 109.0 (TID 109, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:25:35,038][org.apache.spark.executor.Executor]Running task 0.0 in stage 109.0 (TID 109)
[INFO][2018-05-25 11:25:35,039][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12812 -> 12822
[INFO][2018-05-25 11:25:35,040][org.apache.spark.executor.Executor]Finished task 0.0 in stage 109.0 (TID 109). 972 bytes result sent to driver
[INFO][2018-05-25 11:25:35,040][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 109.0 (TID 109) in 2 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:25:35,040][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 109.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:25:35,041][org.apache.spark.scheduler.DAGScheduler]ResultStage 109 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.003 s
[INFO][2018-05-25 11:25:35,041][org.apache.spark.scheduler.DAGScheduler]Job 109 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.009002 s
[INFO][2018-05-25 11:25:35,043][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:73
[INFO][2018-05-25 11:25:35,044][org.apache.spark.scheduler.DAGScheduler]Got job 110 (foreachPartition at ReceiveKafkaData.scala:73) with 1 output partitions
[INFO][2018-05-25 11:25:35,044][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 110 (foreachPartition at ReceiveKafkaData.scala:73)
[INFO][2018-05-25 11:25:35,044][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:25:35,044][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:25:35,044][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 110 (MapPartitionsRDD[115] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:25:35,045][org.apache.spark.storage.memory.MemoryStore]Block broadcast_110 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-25 11:25:35,046][org.apache.spark.storage.memory.MemoryStore]Block broadcast_110_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.3 MB)
[INFO][2018-05-25 11:25:35,046][org.apache.spark.storage.BlockManagerInfo]Added broadcast_110_piece0 in memory on 10.194.32.157:53453 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:25:35,046][org.apache.spark.SparkContext]Created broadcast 110 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:25:35,047][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 110 (MapPartitionsRDD[115] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:25:35,047][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 110.0 with 1 tasks
[INFO][2018-05-25 11:25:35,047][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 110.0 (TID 110, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:25:35,047][org.apache.spark.executor.Executor]Running task 0.0 in stage 110.0 (TID 110)
[INFO][2018-05-25 11:25:35,049][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12812 -> 12822
[INFO][2018-05-25 11:25:35,050][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 12812
[INFO][2018-05-25 11:25:35,086][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-25 11:25:35,086][org.apache.spark.executor.Executor]Finished task 0.0 in stage 110.0 (TID 110). 708 bytes result sent to driver
[INFO][2018-05-25 11:25:35,087][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 110.0 (TID 110) in 40 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:25:35,087][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 110.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:25:35,087][org.apache.spark.scheduler.DAGScheduler]ResultStage 110 (foreachPartition at ReceiveKafkaData.scala:73) finished in 0.040 s
[INFO][2018-05-25 11:25:35,088][org.apache.spark.scheduler.DAGScheduler]Job 110 finished: foreachPartition at ReceiveKafkaData.scala:73, took 0.043938 s
[INFO][2018-05-25 11:25:35,088][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218735000 ms.0 from job set of time 1527218735000 ms
[INFO][2018-05-25 11:25:35,088][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 113 from persistence list
[INFO][2018-05-25 11:25:35,088][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.088 s for time 1527218735000 ms (execution: 0.064 s)
[INFO][2018-05-25 11:25:35,088][org.apache.spark.storage.BlockManager]Removing RDD 113
[INFO][2018-05-25 11:25:35,088][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 112 from persistence list
[INFO][2018-05-25 11:25:35,089][org.apache.spark.storage.BlockManager]Removing RDD 112
[INFO][2018-05-25 11:25:35,089][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:25:35,089][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218725000 ms
[INFO][2018-05-25 11:25:40,019][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218740000 ms
[INFO][2018-05-25 11:25:40,019][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218740000 ms.0 from job set of time 1527218740000 ms
[INFO][2018-05-25 11:25:40,028][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:25:40,028][org.apache.spark.scheduler.DAGScheduler]Got job 111 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:25:40,028][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 111 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:25:40,028][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:25:40,028][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:25:40,029][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 111 (MapPartitionsRDD[117] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:25:40,030][org.apache.spark.storage.memory.MemoryStore]Block broadcast_111 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-25 11:25:40,036][org.apache.spark.storage.memory.MemoryStore]Block broadcast_111_piece0 stored as bytes in memory (estimated size 1972.0 B, free 912.3 MB)
[INFO][2018-05-25 11:25:40,037][org.apache.spark.storage.BlockManagerInfo]Added broadcast_111_piece0 in memory on 10.194.32.157:53453 (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:25:40,037][org.apache.spark.SparkContext]Created broadcast 111 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:25:40,037][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 111 (MapPartitionsRDD[117] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:25:40,037][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 111.0 with 1 tasks
[INFO][2018-05-25 11:25:40,038][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 111.0 (TID 111, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:25:40,038][org.apache.spark.executor.Executor]Running task 0.0 in stage 111.0 (TID 111)
[INFO][2018-05-25 11:25:40,040][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12822 -> 12832
[INFO][2018-05-25 11:25:40,041][org.apache.spark.executor.Executor]Finished task 0.0 in stage 111.0 (TID 111). 918 bytes result sent to driver
[INFO][2018-05-25 11:25:40,041][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 111.0 (TID 111) in 3 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:25:40,041][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 111.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:25:40,041][org.apache.spark.scheduler.DAGScheduler]ResultStage 111 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.003 s
[INFO][2018-05-25 11:25:40,042][org.apache.spark.scheduler.DAGScheduler]Job 111 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.013815 s
[INFO][2018-05-25 11:25:40,045][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:73
[INFO][2018-05-25 11:25:40,045][org.apache.spark.scheduler.DAGScheduler]Got job 112 (foreachPartition at ReceiveKafkaData.scala:73) with 1 output partitions
[INFO][2018-05-25 11:25:40,045][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 112 (foreachPartition at ReceiveKafkaData.scala:73)
[INFO][2018-05-25 11:25:40,045][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:25:40,046][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:25:40,046][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 112 (MapPartitionsRDD[117] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:25:40,046][org.apache.spark.storage.memory.MemoryStore]Block broadcast_112 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-25 11:25:40,047][org.apache.spark.storage.memory.MemoryStore]Block broadcast_112_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.3 MB)
[INFO][2018-05-25 11:25:40,047][org.apache.spark.storage.BlockManagerInfo]Added broadcast_112_piece0 in memory on 10.194.32.157:53453 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:25:40,048][org.apache.spark.SparkContext]Created broadcast 112 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:25:40,048][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 112 (MapPartitionsRDD[117] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:25:40,048][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 112.0 with 1 tasks
[INFO][2018-05-25 11:25:40,048][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 112.0 (TID 112, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:25:40,048][org.apache.spark.executor.Executor]Running task 0.0 in stage 112.0 (TID 112)
[INFO][2018-05-25 11:25:40,049][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12822 -> 12832
[INFO][2018-05-25 11:25:40,049][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 12822
[INFO][2018-05-25 11:25:40,090][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-25 11:25:40,090][org.apache.spark.executor.Executor]Finished task 0.0 in stage 112.0 (TID 112). 708 bytes result sent to driver
[INFO][2018-05-25 11:25:40,091][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 112.0 (TID 112) in 43 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:25:40,091][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 112.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:25:40,091][org.apache.spark.scheduler.DAGScheduler]ResultStage 112 (foreachPartition at ReceiveKafkaData.scala:73) finished in 0.043 s
[INFO][2018-05-25 11:25:40,091][org.apache.spark.scheduler.DAGScheduler]Job 112 finished: foreachPartition at ReceiveKafkaData.scala:73, took 0.046258 s
[INFO][2018-05-25 11:25:40,092][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218740000 ms.0 from job set of time 1527218740000 ms
[INFO][2018-05-25 11:25:40,092][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.092 s for time 1527218740000 ms (execution: 0.073 s)
[INFO][2018-05-25 11:25:40,092][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 115 from persistence list
[INFO][2018-05-25 11:25:40,092][org.apache.spark.storage.BlockManager]Removing RDD 115
[INFO][2018-05-25 11:25:40,092][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 114 from persistence list
[INFO][2018-05-25 11:25:40,093][org.apache.spark.storage.BlockManager]Removing RDD 114
[INFO][2018-05-25 11:25:40,093][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:25:40,093][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218730000 ms
[INFO][2018-05-25 11:25:45,020][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218745000 ms
[INFO][2018-05-25 11:25:45,020][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218745000 ms.0 from job set of time 1527218745000 ms
[INFO][2018-05-25 11:25:45,025][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:25:45,026][org.apache.spark.scheduler.DAGScheduler]Got job 113 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:25:45,026][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 113 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:25:45,026][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:25:45,026][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:25:45,027][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 113 (MapPartitionsRDD[119] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:25:45,029][org.apache.spark.storage.memory.MemoryStore]Block broadcast_113 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-25 11:25:45,031][org.apache.spark.storage.memory.MemoryStore]Block broadcast_113_piece0 stored as bytes in memory (estimated size 1972.0 B, free 912.3 MB)
[INFO][2018-05-25 11:25:45,032][org.apache.spark.storage.BlockManagerInfo]Added broadcast_113_piece0 in memory on 10.194.32.157:53453 (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:25:45,032][org.apache.spark.SparkContext]Created broadcast 113 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:25:45,032][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 113 (MapPartitionsRDD[119] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:25:45,033][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 113.0 with 1 tasks
[INFO][2018-05-25 11:25:45,033][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 113.0 (TID 113, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:25:45,033][org.apache.spark.executor.Executor]Running task 0.0 in stage 113.0 (TID 113)
[INFO][2018-05-25 11:25:45,034][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12832 -> 12842
[INFO][2018-05-25 11:25:45,035][org.apache.spark.executor.Executor]Finished task 0.0 in stage 113.0 (TID 113). 926 bytes result sent to driver
[INFO][2018-05-25 11:25:45,035][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 113.0 (TID 113) in 2 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:25:45,035][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 113.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:25:45,035][org.apache.spark.scheduler.DAGScheduler]ResultStage 113 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.002 s
[INFO][2018-05-25 11:25:45,035][org.apache.spark.scheduler.DAGScheduler]Job 113 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.009886 s
[INFO][2018-05-25 11:25:45,038][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:73
[INFO][2018-05-25 11:25:45,039][org.apache.spark.scheduler.DAGScheduler]Got job 114 (foreachPartition at ReceiveKafkaData.scala:73) with 1 output partitions
[INFO][2018-05-25 11:25:45,039][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 114 (foreachPartition at ReceiveKafkaData.scala:73)
[INFO][2018-05-25 11:25:45,039][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:25:45,039][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:25:45,039][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 114 (MapPartitionsRDD[119] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:25:45,040][org.apache.spark.storage.memory.MemoryStore]Block broadcast_114 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-25 11:25:45,041][org.apache.spark.storage.memory.MemoryStore]Block broadcast_114_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.3 MB)
[INFO][2018-05-25 11:25:45,041][org.apache.spark.storage.BlockManagerInfo]Added broadcast_114_piece0 in memory on 10.194.32.157:53453 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:25:45,041][org.apache.spark.SparkContext]Created broadcast 114 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:25:45,041][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 114 (MapPartitionsRDD[119] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:25:45,041][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 114.0 with 1 tasks
[INFO][2018-05-25 11:25:45,042][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 114.0 (TID 114, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:25:45,042][org.apache.spark.executor.Executor]Running task 0.0 in stage 114.0 (TID 114)
[INFO][2018-05-25 11:25:45,042][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12832 -> 12842
[INFO][2018-05-25 11:25:45,042][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 12832
[INFO][2018-05-25 11:25:45,098][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-25 11:25:45,099][org.apache.spark.executor.Executor]Finished task 0.0 in stage 114.0 (TID 114). 665 bytes result sent to driver
[INFO][2018-05-25 11:25:45,100][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 114.0 (TID 114) in 59 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:25:45,101][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 114.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:25:45,101][org.apache.spark.scheduler.DAGScheduler]ResultStage 114 (foreachPartition at ReceiveKafkaData.scala:73) finished in 0.060 s
[INFO][2018-05-25 11:25:45,102][org.apache.spark.scheduler.DAGScheduler]Job 114 finished: foreachPartition at ReceiveKafkaData.scala:73, took 0.063259 s
[INFO][2018-05-25 11:25:45,103][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218745000 ms.0 from job set of time 1527218745000 ms
[INFO][2018-05-25 11:25:45,103][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 117 from persistence list
[INFO][2018-05-25 11:25:45,103][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.103 s for time 1527218745000 ms (execution: 0.083 s)
[INFO][2018-05-25 11:25:45,105][org.apache.spark.storage.BlockManager]Removing RDD 117
[INFO][2018-05-25 11:25:45,105][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 116 from persistence list
[INFO][2018-05-25 11:25:45,106][org.apache.spark.storage.BlockManager]Removing RDD 116
[INFO][2018-05-25 11:25:45,106][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:25:45,106][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218735000 ms
[INFO][2018-05-25 11:25:50,017][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218750000 ms
[INFO][2018-05-25 11:25:50,017][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218750000 ms.0 from job set of time 1527218750000 ms
[INFO][2018-05-25 11:25:50,021][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:25:50,021][org.apache.spark.scheduler.DAGScheduler]Got job 115 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:25:50,021][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 115 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:25:50,021][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:25:50,021][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:25:50,022][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 115 (MapPartitionsRDD[121] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:25:50,022][org.apache.spark.storage.memory.MemoryStore]Block broadcast_115 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-25 11:25:50,023][org.apache.spark.storage.memory.MemoryStore]Block broadcast_115_piece0 stored as bytes in memory (estimated size 1972.0 B, free 912.3 MB)
[INFO][2018-05-25 11:25:50,023][org.apache.spark.storage.BlockManagerInfo]Added broadcast_115_piece0 in memory on 10.194.32.157:53453 (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:25:50,024][org.apache.spark.SparkContext]Created broadcast 115 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:25:50,024][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 115 (MapPartitionsRDD[121] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:25:50,024][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 115.0 with 1 tasks
[INFO][2018-05-25 11:25:50,024][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 115.0 (TID 115, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:25:50,025][org.apache.spark.executor.Executor]Running task 0.0 in stage 115.0 (TID 115)
[INFO][2018-05-25 11:25:50,026][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12842 -> 12852
[INFO][2018-05-25 11:25:50,027][org.apache.spark.executor.Executor]Finished task 0.0 in stage 115.0 (TID 115). 932 bytes result sent to driver
[INFO][2018-05-25 11:25:50,027][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 115.0 (TID 115) in 3 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:25:50,027][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 115.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:25:50,027][org.apache.spark.scheduler.DAGScheduler]ResultStage 115 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.003 s
[INFO][2018-05-25 11:25:50,027][org.apache.spark.scheduler.DAGScheduler]Job 115 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.006542 s
[INFO][2018-05-25 11:25:50,031][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:73
[INFO][2018-05-25 11:25:50,032][org.apache.spark.scheduler.DAGScheduler]Got job 116 (foreachPartition at ReceiveKafkaData.scala:73) with 1 output partitions
[INFO][2018-05-25 11:25:50,032][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 116 (foreachPartition at ReceiveKafkaData.scala:73)
[INFO][2018-05-25 11:25:50,032][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:25:50,032][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:25:50,032][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 116 (MapPartitionsRDD[121] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:25:50,033][org.apache.spark.storage.memory.MemoryStore]Block broadcast_116 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-25 11:25:50,035][org.apache.spark.storage.memory.MemoryStore]Block broadcast_116_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.2 MB)
[INFO][2018-05-25 11:25:50,035][org.apache.spark.storage.BlockManagerInfo]Added broadcast_116_piece0 in memory on 10.194.32.157:53453 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:25:50,036][org.apache.spark.SparkContext]Created broadcast 116 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:25:50,036][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 116 (MapPartitionsRDD[121] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:25:50,036][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 116.0 with 1 tasks
[INFO][2018-05-25 11:25:50,036][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 116.0 (TID 116, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:25:50,036][org.apache.spark.executor.Executor]Running task 0.0 in stage 116.0 (TID 116)
[INFO][2018-05-25 11:25:50,037][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12842 -> 12852
[INFO][2018-05-25 11:25:50,037][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 12842
[INFO][2018-05-25 11:25:50,067][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-25 11:25:50,068][org.apache.spark.executor.Executor]Finished task 0.0 in stage 116.0 (TID 116). 708 bytes result sent to driver
[INFO][2018-05-25 11:25:50,068][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 116.0 (TID 116) in 32 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:25:50,068][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 116.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:25:50,068][org.apache.spark.scheduler.DAGScheduler]ResultStage 116 (foreachPartition at ReceiveKafkaData.scala:73) finished in 0.032 s
[INFO][2018-05-25 11:25:50,069][org.apache.spark.scheduler.DAGScheduler]Job 116 finished: foreachPartition at ReceiveKafkaData.scala:73, took 0.037324 s
[INFO][2018-05-25 11:25:50,069][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218750000 ms.0 from job set of time 1527218750000 ms
[INFO][2018-05-25 11:25:50,069][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.069 s for time 1527218750000 ms (execution: 0.052 s)
[INFO][2018-05-25 11:25:50,069][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 119 from persistence list
[INFO][2018-05-25 11:25:50,069][org.apache.spark.storage.BlockManager]Removing RDD 119
[INFO][2018-05-25 11:25:50,069][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 118 from persistence list
[INFO][2018-05-25 11:25:50,070][org.apache.spark.storage.BlockManager]Removing RDD 118
[INFO][2018-05-25 11:25:50,070][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:25:50,070][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218740000 ms
[INFO][2018-05-25 11:25:55,026][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218755000 ms
[INFO][2018-05-25 11:25:55,027][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218755000 ms.0 from job set of time 1527218755000 ms
[INFO][2018-05-25 11:25:55,030][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:25:55,031][org.apache.spark.scheduler.DAGScheduler]Got job 117 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:25:55,031][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 117 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:25:55,031][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:25:55,031][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:25:55,031][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 117 (MapPartitionsRDD[123] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:25:55,032][org.apache.spark.storage.memory.MemoryStore]Block broadcast_117 stored as values in memory (estimated size 3.1 KB, free 912.2 MB)
[INFO][2018-05-25 11:25:55,033][org.apache.spark.storage.memory.MemoryStore]Block broadcast_117_piece0 stored as bytes in memory (estimated size 1972.0 B, free 912.2 MB)
[INFO][2018-05-25 11:25:55,033][org.apache.spark.storage.BlockManagerInfo]Added broadcast_117_piece0 in memory on 10.194.32.157:53453 (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:25:55,034][org.apache.spark.SparkContext]Created broadcast 117 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:25:55,034][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 117 (MapPartitionsRDD[123] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:25:55,034][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 117.0 with 1 tasks
[INFO][2018-05-25 11:25:55,035][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 117.0 (TID 117, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:25:55,035][org.apache.spark.executor.Executor]Running task 0.0 in stage 117.0 (TID 117)
[INFO][2018-05-25 11:25:55,035][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12852 -> 12862
[INFO][2018-05-25 11:25:55,036][org.apache.spark.executor.Executor]Finished task 0.0 in stage 117.0 (TID 117). 930 bytes result sent to driver
[INFO][2018-05-25 11:25:55,036][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 117.0 (TID 117) in 2 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:25:55,037][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 117.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:25:55,037][org.apache.spark.scheduler.DAGScheduler]ResultStage 117 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.003 s
[INFO][2018-05-25 11:25:55,037][org.apache.spark.scheduler.DAGScheduler]Job 117 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.006321 s
[INFO][2018-05-25 11:25:55,041][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:73
[INFO][2018-05-25 11:25:55,042][org.apache.spark.scheduler.DAGScheduler]Got job 118 (foreachPartition at ReceiveKafkaData.scala:73) with 1 output partitions
[INFO][2018-05-25 11:25:55,042][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 118 (foreachPartition at ReceiveKafkaData.scala:73)
[INFO][2018-05-25 11:25:55,042][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:25:55,042][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:25:55,042][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 118 (MapPartitionsRDD[123] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:25:55,043][org.apache.spark.storage.memory.MemoryStore]Block broadcast_118 stored as values in memory (estimated size 3.4 KB, free 912.2 MB)
[INFO][2018-05-25 11:25:55,043][org.apache.spark.storage.memory.MemoryStore]Block broadcast_118_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.2 MB)
[INFO][2018-05-25 11:25:55,044][org.apache.spark.storage.BlockManagerInfo]Added broadcast_118_piece0 in memory on 10.194.32.157:53453 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:25:55,044][org.apache.spark.SparkContext]Created broadcast 118 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:25:55,044][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 118 (MapPartitionsRDD[123] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:25:55,044][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 118.0 with 1 tasks
[INFO][2018-05-25 11:25:55,045][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 118.0 (TID 118, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:25:55,045][org.apache.spark.executor.Executor]Running task 0.0 in stage 118.0 (TID 118)
[INFO][2018-05-25 11:25:55,046][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12852 -> 12862
[INFO][2018-05-25 11:25:55,046][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 12852
[INFO][2018-05-25 11:25:55,087][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-25 11:25:55,087][org.apache.spark.executor.Executor]Finished task 0.0 in stage 118.0 (TID 118). 708 bytes result sent to driver
[INFO][2018-05-25 11:25:55,088][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 118.0 (TID 118) in 43 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:25:55,088][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 118.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:25:55,088][org.apache.spark.scheduler.DAGScheduler]ResultStage 118 (foreachPartition at ReceiveKafkaData.scala:73) finished in 0.044 s
[INFO][2018-05-25 11:25:55,089][org.apache.spark.scheduler.DAGScheduler]Job 118 finished: foreachPartition at ReceiveKafkaData.scala:73, took 0.047224 s
[INFO][2018-05-25 11:25:55,089][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218755000 ms.0 from job set of time 1527218755000 ms
[INFO][2018-05-25 11:25:55,089][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 121 from persistence list
[INFO][2018-05-25 11:25:55,089][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.089 s for time 1527218755000 ms (execution: 0.062 s)
[INFO][2018-05-25 11:25:55,090][org.apache.spark.storage.BlockManager]Removing RDD 121
[INFO][2018-05-25 11:25:55,090][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 120 from persistence list
[INFO][2018-05-25 11:25:55,091][org.apache.spark.storage.BlockManager]Removing RDD 120
[INFO][2018-05-25 11:25:55,091][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:25:55,091][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218745000 ms
[INFO][2018-05-25 11:26:00,012][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218760000 ms
[INFO][2018-05-25 11:26:00,013][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218760000 ms.0 from job set of time 1527218760000 ms
[INFO][2018-05-25 11:26:00,017][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:26:00,017][org.apache.spark.scheduler.DAGScheduler]Got job 119 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:26:00,017][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 119 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:26:00,017][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:26:00,017][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:26:00,017][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 119 (MapPartitionsRDD[125] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:26:00,018][org.apache.spark.storage.memory.MemoryStore]Block broadcast_119 stored as values in memory (estimated size 3.1 KB, free 912.2 MB)
[INFO][2018-05-25 11:26:00,019][org.apache.spark.storage.memory.MemoryStore]Block broadcast_119_piece0 stored as bytes in memory (estimated size 1972.0 B, free 912.2 MB)
[INFO][2018-05-25 11:26:00,019][org.apache.spark.storage.BlockManagerInfo]Added broadcast_119_piece0 in memory on 10.194.32.157:53453 (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:26:00,019][org.apache.spark.SparkContext]Created broadcast 119 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:26:00,020][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 119 (MapPartitionsRDD[125] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:26:00,020][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 119.0 with 1 tasks
[INFO][2018-05-25 11:26:00,020][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 119.0 (TID 119, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:26:00,020][org.apache.spark.executor.Executor]Running task 0.0 in stage 119.0 (TID 119)
[INFO][2018-05-25 11:26:00,021][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12862 -> 12872
[INFO][2018-05-25 11:26:00,022][org.apache.spark.executor.Executor]Finished task 0.0 in stage 119.0 (TID 119). 930 bytes result sent to driver
[INFO][2018-05-25 11:26:00,022][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 119.0 (TID 119) in 2 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:26:00,022][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 119.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:26:00,022][org.apache.spark.scheduler.DAGScheduler]ResultStage 119 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.002 s
[INFO][2018-05-25 11:26:00,022][org.apache.spark.scheduler.DAGScheduler]Job 119 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.005587 s
[INFO][2018-05-25 11:26:00,025][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:73
[INFO][2018-05-25 11:26:00,025][org.apache.spark.scheduler.DAGScheduler]Got job 120 (foreachPartition at ReceiveKafkaData.scala:73) with 1 output partitions
[INFO][2018-05-25 11:26:00,026][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 120 (foreachPartition at ReceiveKafkaData.scala:73)
[INFO][2018-05-25 11:26:00,026][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:26:00,026][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:26:00,026][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 120 (MapPartitionsRDD[125] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:26:00,026][org.apache.spark.storage.memory.MemoryStore]Block broadcast_120 stored as values in memory (estimated size 3.4 KB, free 912.2 MB)
[INFO][2018-05-25 11:26:00,027][org.apache.spark.storage.memory.MemoryStore]Block broadcast_120_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.2 MB)
[INFO][2018-05-25 11:26:00,027][org.apache.spark.storage.BlockManagerInfo]Added broadcast_120_piece0 in memory on 10.194.32.157:53453 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:26:00,028][org.apache.spark.SparkContext]Created broadcast 120 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:26:00,028][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 120 (MapPartitionsRDD[125] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:26:00,028][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 120.0 with 1 tasks
[INFO][2018-05-25 11:26:00,028][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 120.0 (TID 120, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:26:00,028][org.apache.spark.executor.Executor]Running task 0.0 in stage 120.0 (TID 120)
[INFO][2018-05-25 11:26:00,029][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12862 -> 12872
[INFO][2018-05-25 11:26:00,029][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 12862
[INFO][2018-05-25 11:26:00,075][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-25 11:26:00,076][org.apache.spark.executor.Executor]Finished task 0.0 in stage 120.0 (TID 120). 708 bytes result sent to driver
[INFO][2018-05-25 11:26:00,077][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 120.0 (TID 120) in 48 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:26:00,077][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 120.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:26:00,077][org.apache.spark.scheduler.DAGScheduler]ResultStage 120 (foreachPartition at ReceiveKafkaData.scala:73) finished in 0.049 s
[INFO][2018-05-25 11:26:00,077][org.apache.spark.scheduler.DAGScheduler]Job 120 finished: foreachPartition at ReceiveKafkaData.scala:73, took 0.051876 s
[INFO][2018-05-25 11:26:00,077][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218760000 ms.0 from job set of time 1527218760000 ms
[INFO][2018-05-25 11:26:00,078][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.077 s for time 1527218760000 ms (execution: 0.064 s)
[INFO][2018-05-25 11:26:00,078][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 123 from persistence list
[INFO][2018-05-25 11:26:00,078][org.apache.spark.storage.BlockManager]Removing RDD 123
[INFO][2018-05-25 11:26:00,078][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 122 from persistence list
[INFO][2018-05-25 11:26:00,078][org.apache.spark.storage.BlockManager]Removing RDD 122
[INFO][2018-05-25 11:26:00,078][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:26:00,078][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218750000 ms
[INFO][2018-05-25 11:26:05,017][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218765000 ms
[INFO][2018-05-25 11:26:05,017][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218765000 ms.0 from job set of time 1527218765000 ms
[INFO][2018-05-25 11:26:05,021][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:26:05,021][org.apache.spark.scheduler.DAGScheduler]Got job 121 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:26:05,021][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 121 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:26:05,021][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:26:05,022][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:26:05,022][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 121 (MapPartitionsRDD[127] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:26:05,023][org.apache.spark.storage.memory.MemoryStore]Block broadcast_121 stored as values in memory (estimated size 3.1 KB, free 912.2 MB)
[INFO][2018-05-25 11:26:05,023][org.apache.spark.storage.memory.MemoryStore]Block broadcast_121_piece0 stored as bytes in memory (estimated size 1972.0 B, free 912.2 MB)
[INFO][2018-05-25 11:26:05,024][org.apache.spark.storage.BlockManagerInfo]Added broadcast_121_piece0 in memory on 10.194.32.157:53453 (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:26:05,024][org.apache.spark.SparkContext]Created broadcast 121 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:26:05,024][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 121 (MapPartitionsRDD[127] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:26:05,024][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 121.0 with 1 tasks
[INFO][2018-05-25 11:26:05,025][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 121.0 (TID 121, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:26:05,025][org.apache.spark.executor.Executor]Running task 0.0 in stage 121.0 (TID 121)
[INFO][2018-05-25 11:26:05,026][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12872 -> 12882
[INFO][2018-05-25 11:26:05,026][org.apache.spark.executor.Executor]Finished task 0.0 in stage 121.0 (TID 121). 928 bytes result sent to driver
[INFO][2018-05-25 11:26:05,027][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 121.0 (TID 121) in 3 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:26:05,027][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 121.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:26:05,027][org.apache.spark.scheduler.DAGScheduler]ResultStage 121 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.003 s
[INFO][2018-05-25 11:26:05,027][org.apache.spark.scheduler.DAGScheduler]Job 121 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.005973 s
[INFO][2018-05-25 11:26:05,030][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:73
[INFO][2018-05-25 11:26:05,030][org.apache.spark.scheduler.DAGScheduler]Got job 122 (foreachPartition at ReceiveKafkaData.scala:73) with 1 output partitions
[INFO][2018-05-25 11:26:05,030][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 122 (foreachPartition at ReceiveKafkaData.scala:73)
[INFO][2018-05-25 11:26:05,030][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:26:05,030][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:26:05,030][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 122 (MapPartitionsRDD[127] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:26:05,031][org.apache.spark.storage.memory.MemoryStore]Block broadcast_122 stored as values in memory (estimated size 3.4 KB, free 912.2 MB)
[INFO][2018-05-25 11:26:05,032][org.apache.spark.storage.memory.MemoryStore]Block broadcast_122_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.2 MB)
[INFO][2018-05-25 11:26:05,032][org.apache.spark.storage.BlockManagerInfo]Added broadcast_122_piece0 in memory on 10.194.32.157:53453 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:26:05,032][org.apache.spark.SparkContext]Created broadcast 122 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:26:05,032][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 122 (MapPartitionsRDD[127] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:26:05,032][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 122.0 with 1 tasks
[INFO][2018-05-25 11:26:05,033][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 122.0 (TID 122, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:26:05,033][org.apache.spark.executor.Executor]Running task 0.0 in stage 122.0 (TID 122)
[INFO][2018-05-25 11:26:05,033][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12872 -> 12882
[INFO][2018-05-25 11:26:05,033][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 12872
[INFO][2018-05-25 11:26:05,068][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-25 11:26:05,068][org.apache.spark.executor.Executor]Finished task 0.0 in stage 122.0 (TID 122). 665 bytes result sent to driver
[INFO][2018-05-25 11:26:05,069][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 122.0 (TID 122) in 36 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:26:05,069][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 122.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:26:05,069][org.apache.spark.scheduler.DAGScheduler]ResultStage 122 (foreachPartition at ReceiveKafkaData.scala:73) finished in 0.037 s
[INFO][2018-05-25 11:26:05,070][org.apache.spark.scheduler.DAGScheduler]Job 122 finished: foreachPartition at ReceiveKafkaData.scala:73, took 0.040029 s
[INFO][2018-05-25 11:26:05,071][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218765000 ms.0 from job set of time 1527218765000 ms
[INFO][2018-05-25 11:26:05,071][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.071 s for time 1527218765000 ms (execution: 0.054 s)
[INFO][2018-05-25 11:26:05,071][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 125 from persistence list
[INFO][2018-05-25 11:26:05,072][org.apache.spark.storage.BlockManager]Removing RDD 125
[INFO][2018-05-25 11:26:05,073][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 124 from persistence list
[INFO][2018-05-25 11:26:05,073][org.apache.spark.storage.BlockManager]Removing RDD 124
[INFO][2018-05-25 11:26:05,074][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:26:05,074][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218755000 ms
[INFO][2018-05-25 11:26:10,014][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218770000 ms
[INFO][2018-05-25 11:26:10,014][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218770000 ms.0 from job set of time 1527218770000 ms
[INFO][2018-05-25 11:26:10,019][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:26:10,019][org.apache.spark.scheduler.DAGScheduler]Got job 123 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:26:10,019][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 123 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:26:10,019][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:26:10,019][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:26:10,019][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 123 (MapPartitionsRDD[129] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:26:10,020][org.apache.spark.storage.memory.MemoryStore]Block broadcast_123 stored as values in memory (estimated size 3.1 KB, free 912.2 MB)
[INFO][2018-05-25 11:26:10,021][org.apache.spark.storage.memory.MemoryStore]Block broadcast_123_piece0 stored as bytes in memory (estimated size 1972.0 B, free 912.2 MB)
[INFO][2018-05-25 11:26:10,021][org.apache.spark.storage.BlockManagerInfo]Added broadcast_123_piece0 in memory on 10.194.32.157:53453 (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:26:10,021][org.apache.spark.SparkContext]Created broadcast 123 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:26:10,022][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 123 (MapPartitionsRDD[129] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:26:10,022][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 123.0 with 1 tasks
[INFO][2018-05-25 11:26:10,022][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 123.0 (TID 123, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:26:10,022][org.apache.spark.executor.Executor]Running task 0.0 in stage 123.0 (TID 123)
[INFO][2018-05-25 11:26:10,023][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12882 -> 12891
[INFO][2018-05-25 11:26:10,024][org.apache.spark.executor.Executor]Finished task 0.0 in stage 123.0 (TID 123). 960 bytes result sent to driver
[INFO][2018-05-25 11:26:10,024][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 123.0 (TID 123) in 2 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:26:10,024][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 123.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:26:10,025][org.apache.spark.scheduler.DAGScheduler]ResultStage 123 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.003 s
[INFO][2018-05-25 11:26:10,025][org.apache.spark.scheduler.DAGScheduler]Job 123 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.006362 s
[INFO][2018-05-25 11:26:10,028][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:73
[INFO][2018-05-25 11:26:10,028][org.apache.spark.scheduler.DAGScheduler]Got job 124 (foreachPartition at ReceiveKafkaData.scala:73) with 1 output partitions
[INFO][2018-05-25 11:26:10,028][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 124 (foreachPartition at ReceiveKafkaData.scala:73)
[INFO][2018-05-25 11:26:10,028][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:26:10,028][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:26:10,028][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 124 (MapPartitionsRDD[129] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:26:10,029][org.apache.spark.storage.memory.MemoryStore]Block broadcast_124 stored as values in memory (estimated size 3.4 KB, free 912.2 MB)
[INFO][2018-05-25 11:26:10,030][org.apache.spark.storage.memory.MemoryStore]Block broadcast_124_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.2 MB)
[INFO][2018-05-25 11:26:10,030][org.apache.spark.storage.BlockManagerInfo]Added broadcast_124_piece0 in memory on 10.194.32.157:53453 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:26:10,030][org.apache.spark.SparkContext]Created broadcast 124 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:26:10,031][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 124 (MapPartitionsRDD[129] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:26:10,031][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 124.0 with 1 tasks
[INFO][2018-05-25 11:26:10,031][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 124.0 (TID 124, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:26:10,031][org.apache.spark.executor.Executor]Running task 0.0 in stage 124.0 (TID 124)
[INFO][2018-05-25 11:26:10,032][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12882 -> 12891
[INFO][2018-05-25 11:26:10,032][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 12882
[INFO][2018-05-25 11:26:10,073][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-25 11:26:10,073][org.apache.spark.executor.Executor]Finished task 0.0 in stage 124.0 (TID 124). 665 bytes result sent to driver
[INFO][2018-05-25 11:26:10,075][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 124.0 (TID 124) in 43 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:26:10,075][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 124.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:26:10,076][org.apache.spark.scheduler.DAGScheduler]ResultStage 124 (foreachPartition at ReceiveKafkaData.scala:73) finished in 0.045 s
[INFO][2018-05-25 11:26:10,082][org.apache.spark.scheduler.DAGScheduler]Job 124 finished: foreachPartition at ReceiveKafkaData.scala:73, took 0.053815 s
[INFO][2018-05-25 11:26:10,082][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218770000 ms.0 from job set of time 1527218770000 ms
[INFO][2018-05-25 11:26:10,082][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.082 s for time 1527218770000 ms (execution: 0.068 s)
[INFO][2018-05-25 11:26:10,082][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 127 from persistence list
[INFO][2018-05-25 11:26:10,083][org.apache.spark.storage.BlockManager]Removing RDD 127
[INFO][2018-05-25 11:26:10,083][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 126 from persistence list
[INFO][2018-05-25 11:26:10,083][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:26:10,083][org.apache.spark.storage.BlockManager]Removing RDD 126
[INFO][2018-05-25 11:26:10,083][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218760000 ms
[INFO][2018-05-25 11:26:15,018][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218775000 ms
[INFO][2018-05-25 11:26:15,018][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218775000 ms.0 from job set of time 1527218775000 ms
[INFO][2018-05-25 11:26:15,022][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:26:15,023][org.apache.spark.scheduler.DAGScheduler]Got job 125 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:26:15,023][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 125 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:26:15,023][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:26:15,023][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:26:15,023][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 125 (MapPartitionsRDD[131] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:26:15,024][org.apache.spark.storage.memory.MemoryStore]Block broadcast_125 stored as values in memory (estimated size 3.1 KB, free 912.2 MB)
[INFO][2018-05-25 11:26:15,025][org.apache.spark.storage.memory.MemoryStore]Block broadcast_125_piece0 stored as bytes in memory (estimated size 1972.0 B, free 912.2 MB)
[INFO][2018-05-25 11:26:15,025][org.apache.spark.storage.BlockManagerInfo]Added broadcast_125_piece0 in memory on 10.194.32.157:53453 (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:26:15,025][org.apache.spark.SparkContext]Created broadcast 125 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:26:15,025][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 125 (MapPartitionsRDD[131] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:26:15,025][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 125.0 with 1 tasks
[INFO][2018-05-25 11:26:15,026][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 125.0 (TID 125, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:26:15,026][org.apache.spark.executor.Executor]Running task 0.0 in stage 125.0 (TID 125)
[INFO][2018-05-25 11:26:15,027][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12891 -> 12901
[INFO][2018-05-25 11:26:15,027][org.apache.spark.executor.Executor]Finished task 0.0 in stage 125.0 (TID 125). 931 bytes result sent to driver
[INFO][2018-05-25 11:26:15,028][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 125.0 (TID 125) in 2 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:26:15,028][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 125.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:26:15,028][org.apache.spark.scheduler.DAGScheduler]ResultStage 125 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.002 s
[INFO][2018-05-25 11:26:15,028][org.apache.spark.scheduler.DAGScheduler]Job 125 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.005991 s
[INFO][2018-05-25 11:26:15,031][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:73
[INFO][2018-05-25 11:26:15,032][org.apache.spark.scheduler.DAGScheduler]Got job 126 (foreachPartition at ReceiveKafkaData.scala:73) with 1 output partitions
[INFO][2018-05-25 11:26:15,032][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 126 (foreachPartition at ReceiveKafkaData.scala:73)
[INFO][2018-05-25 11:26:15,032][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:26:15,032][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:26:15,032][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 126 (MapPartitionsRDD[131] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:26:15,033][org.apache.spark.storage.memory.MemoryStore]Block broadcast_126 stored as values in memory (estimated size 3.4 KB, free 912.2 MB)
[INFO][2018-05-25 11:26:15,033][org.apache.spark.storage.memory.MemoryStore]Block broadcast_126_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.2 MB)
[INFO][2018-05-25 11:26:15,033][org.apache.spark.storage.BlockManagerInfo]Added broadcast_126_piece0 in memory on 10.194.32.157:53453 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:26:15,034][org.apache.spark.SparkContext]Created broadcast 126 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:26:15,034][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 126 (MapPartitionsRDD[131] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:26:15,034][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 126.0 with 1 tasks
[INFO][2018-05-25 11:26:15,034][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 126.0 (TID 126, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:26:15,035][org.apache.spark.executor.Executor]Running task 0.0 in stage 126.0 (TID 126)
[INFO][2018-05-25 11:26:15,035][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12891 -> 12901
[INFO][2018-05-25 11:26:15,035][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 12891
[INFO][2018-05-25 11:26:15,074][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-25 11:26:15,074][org.apache.spark.executor.Executor]Finished task 0.0 in stage 126.0 (TID 126). 665 bytes result sent to driver
[INFO][2018-05-25 11:26:15,075][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 126.0 (TID 126) in 41 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:26:15,076][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 126.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:26:15,076][org.apache.spark.scheduler.DAGScheduler]ResultStage 126 (foreachPartition at ReceiveKafkaData.scala:73) finished in 0.042 s
[INFO][2018-05-25 11:26:15,076][org.apache.spark.scheduler.DAGScheduler]Job 126 finished: foreachPartition at ReceiveKafkaData.scala:73, took 0.044827 s
[INFO][2018-05-25 11:26:15,076][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218775000 ms.0 from job set of time 1527218775000 ms
[INFO][2018-05-25 11:26:15,077][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.076 s for time 1527218775000 ms (execution: 0.058 s)
[INFO][2018-05-25 11:26:15,077][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 129 from persistence list
[INFO][2018-05-25 11:26:15,077][org.apache.spark.storage.BlockManager]Removing RDD 129
[INFO][2018-05-25 11:26:15,077][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 128 from persistence list
[INFO][2018-05-25 11:26:15,077][org.apache.spark.storage.BlockManager]Removing RDD 128
[INFO][2018-05-25 11:26:15,077][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:26:15,077][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218765000 ms
[INFO][2018-05-25 11:26:20,017][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218780000 ms
[INFO][2018-05-25 11:26:20,017][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218780000 ms.0 from job set of time 1527218780000 ms
[INFO][2018-05-25 11:26:20,021][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:26:20,022][org.apache.spark.scheduler.DAGScheduler]Got job 127 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:26:20,022][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 127 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:26:20,022][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:26:20,022][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:26:20,022][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 127 (MapPartitionsRDD[133] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:26:20,024][org.apache.spark.storage.memory.MemoryStore]Block broadcast_127 stored as values in memory (estimated size 3.1 KB, free 912.2 MB)
[INFO][2018-05-25 11:26:20,025][org.apache.spark.storage.memory.MemoryStore]Block broadcast_127_piece0 stored as bytes in memory (estimated size 1972.0 B, free 912.2 MB)
[INFO][2018-05-25 11:26:20,025][org.apache.spark.storage.BlockManagerInfo]Added broadcast_127_piece0 in memory on 10.194.32.157:53453 (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:26:20,026][org.apache.spark.SparkContext]Created broadcast 127 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:26:20,026][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 127 (MapPartitionsRDD[133] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:26:20,026][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 127.0 with 1 tasks
[INFO][2018-05-25 11:26:20,026][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 127.0 (TID 127, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:26:20,027][org.apache.spark.executor.Executor]Running task 0.0 in stage 127.0 (TID 127)
[INFO][2018-05-25 11:26:20,028][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12901 -> 12911
[INFO][2018-05-25 11:26:21,038][org.apache.spark.executor.Executor]Finished task 0.0 in stage 127.0 (TID 127). 972 bytes result sent to driver
[INFO][2018-05-25 11:26:21,038][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 127.0 (TID 127) in 1012 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:26:21,039][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 127.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:26:21,039][org.apache.spark.scheduler.DAGScheduler]ResultStage 127 (isEmpty at ReceiveKafkaData.scala:72) finished in 1.013 s
[INFO][2018-05-25 11:26:21,039][org.apache.spark.scheduler.DAGScheduler]Job 127 finished: isEmpty at ReceiveKafkaData.scala:72, took 1.018215 s
[INFO][2018-05-25 11:26:21,043][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:73
[INFO][2018-05-25 11:26:21,043][org.apache.spark.scheduler.DAGScheduler]Got job 128 (foreachPartition at ReceiveKafkaData.scala:73) with 1 output partitions
[INFO][2018-05-25 11:26:21,043][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 128 (foreachPartition at ReceiveKafkaData.scala:73)
[INFO][2018-05-25 11:26:21,043][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:26:21,044][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:26:21,044][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 128 (MapPartitionsRDD[133] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:26:21,045][org.apache.spark.storage.memory.MemoryStore]Block broadcast_128 stored as values in memory (estimated size 3.4 KB, free 912.2 MB)
[INFO][2018-05-25 11:26:21,051][org.apache.spark.storage.memory.MemoryStore]Block broadcast_128_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.2 MB)
[INFO][2018-05-25 11:26:21,051][org.apache.spark.storage.BlockManagerInfo]Added broadcast_128_piece0 in memory on 10.194.32.157:53453 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:26:21,051][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_126_piece0 on 10.194.32.157:53453 in memory (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:26:21,051][org.apache.spark.SparkContext]Created broadcast 128 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:26:21,051][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 128 (MapPartitionsRDD[133] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:26:21,052][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 128.0 with 1 tasks
[INFO][2018-05-25 11:26:21,052][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_122_piece0 on 10.194.32.157:53453 in memory (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:26:21,052][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 128.0 (TID 128, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:26:21,052][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_112_piece0 on 10.194.32.157:53453 in memory (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:26:21,052][org.apache.spark.executor.Executor]Running task 0.0 in stage 128.0 (TID 128)
[INFO][2018-05-25 11:26:21,053][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_108_piece0 on 10.194.32.157:53453 in memory (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:26:21,053][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12901 -> 12911
[INFO][2018-05-25 11:26:21,053][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 12901
[INFO][2018-05-25 11:26:21,054][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_118_piece0 on 10.194.32.157:53453 in memory (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:26:21,054][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_111_piece0 on 10.194.32.157:53453 in memory (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:26:21,055][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_125_piece0 on 10.194.32.157:53453 in memory (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:26:21,055][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_109_piece0 on 10.194.32.157:53453 in memory (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:26:21,056][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_119_piece0 on 10.194.32.157:53453 in memory (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:26:21,057][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_114_piece0 on 10.194.32.157:53453 in memory (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:26:21,057][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_116_piece0 on 10.194.32.157:53453 in memory (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:26:21,057][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_127_piece0 on 10.194.32.157:53453 in memory (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:26:21,058][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_110_piece0 on 10.194.32.157:53453 in memory (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:26:21,058][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_117_piece0 on 10.194.32.157:53453 in memory (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:26:21,059][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_123_piece0 on 10.194.32.157:53453 in memory (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:26:21,059][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_120_piece0 on 10.194.32.157:53453 in memory (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:26:21,060][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_113_piece0 on 10.194.32.157:53453 in memory (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:26:21,060][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_124_piece0 on 10.194.32.157:53453 in memory (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:26:21,060][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_121_piece0 on 10.194.32.157:53453 in memory (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:26:21,061][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_115_piece0 on 10.194.32.157:53453 in memory (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:26:21,094][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-25 11:26:21,095][org.apache.spark.executor.Executor]Finished task 0.0 in stage 128.0 (TID 128). 665 bytes result sent to driver
[INFO][2018-05-25 11:26:21,095][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 128.0 (TID 128) in 43 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:26:21,095][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 128.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:26:21,096][org.apache.spark.scheduler.DAGScheduler]ResultStage 128 (foreachPartition at ReceiveKafkaData.scala:73) finished in 0.044 s
[INFO][2018-05-25 11:26:21,096][org.apache.spark.scheduler.DAGScheduler]Job 128 finished: foreachPartition at ReceiveKafkaData.scala:73, took 0.052891 s
[INFO][2018-05-25 11:26:21,096][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218780000 ms.0 from job set of time 1527218780000 ms
[INFO][2018-05-25 11:26:21,096][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 1.096 s for time 1527218780000 ms (execution: 1.079 s)
[INFO][2018-05-25 11:26:21,096][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 131 from persistence list
[INFO][2018-05-25 11:26:21,097][org.apache.spark.storage.BlockManager]Removing RDD 131
[INFO][2018-05-25 11:26:21,097][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 130 from persistence list
[INFO][2018-05-25 11:26:21,097][org.apache.spark.storage.BlockManager]Removing RDD 130
[INFO][2018-05-25 11:26:21,097][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:26:21,097][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218770000 ms
[INFO][2018-05-25 11:26:25,015][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218785000 ms
[INFO][2018-05-25 11:26:25,015][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218785000 ms.0 from job set of time 1527218785000 ms
[INFO][2018-05-25 11:26:25,019][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:26:25,020][org.apache.spark.scheduler.DAGScheduler]Got job 129 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:26:25,020][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 129 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:26:25,020][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:26:25,020][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:26:25,020][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 129 (MapPartitionsRDD[135] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:26:25,021][org.apache.spark.storage.memory.MemoryStore]Block broadcast_129 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-25 11:26:25,022][org.apache.spark.storage.memory.MemoryStore]Block broadcast_129_piece0 stored as bytes in memory (estimated size 1972.0 B, free 912.3 MB)
[INFO][2018-05-25 11:26:25,022][org.apache.spark.storage.BlockManagerInfo]Added broadcast_129_piece0 in memory on 10.194.32.157:53453 (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:26:25,022][org.apache.spark.SparkContext]Created broadcast 129 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:26:25,023][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 129 (MapPartitionsRDD[135] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:26:25,023][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 129.0 with 1 tasks
[INFO][2018-05-25 11:26:25,023][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 129.0 (TID 129, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:26:25,023][org.apache.spark.executor.Executor]Running task 0.0 in stage 129.0 (TID 129)
[INFO][2018-05-25 11:26:25,024][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12911 -> 12921
[INFO][2018-05-25 11:26:25,025][org.apache.spark.executor.Executor]Finished task 0.0 in stage 129.0 (TID 129). 917 bytes result sent to driver
[INFO][2018-05-25 11:26:25,025][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 129.0 (TID 129) in 2 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:26:25,025][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 129.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:26:25,025][org.apache.spark.scheduler.DAGScheduler]ResultStage 129 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.002 s
[INFO][2018-05-25 11:26:25,025][org.apache.spark.scheduler.DAGScheduler]Job 129 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.006249 s
[INFO][2018-05-25 11:26:25,028][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:73
[INFO][2018-05-25 11:26:25,028][org.apache.spark.scheduler.DAGScheduler]Got job 130 (foreachPartition at ReceiveKafkaData.scala:73) with 1 output partitions
[INFO][2018-05-25 11:26:25,029][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 130 (foreachPartition at ReceiveKafkaData.scala:73)
[INFO][2018-05-25 11:26:25,029][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:26:25,029][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:26:25,029][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 130 (MapPartitionsRDD[135] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:26:25,030][org.apache.spark.storage.memory.MemoryStore]Block broadcast_130 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-25 11:26:25,031][org.apache.spark.storage.memory.MemoryStore]Block broadcast_130_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.3 MB)
[INFO][2018-05-25 11:26:25,031][org.apache.spark.storage.BlockManagerInfo]Added broadcast_130_piece0 in memory on 10.194.32.157:53453 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:26:25,031][org.apache.spark.SparkContext]Created broadcast 130 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:26:25,031][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 130 (MapPartitionsRDD[135] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:26:25,032][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 130.0 with 1 tasks
[INFO][2018-05-25 11:26:25,032][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 130.0 (TID 130, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:26:25,032][org.apache.spark.executor.Executor]Running task 0.0 in stage 130.0 (TID 130)
[INFO][2018-05-25 11:26:25,033][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12911 -> 12921
[INFO][2018-05-25 11:26:25,033][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 12911
[INFO][2018-05-25 11:26:25,068][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-25 11:26:25,068][org.apache.spark.executor.Executor]Finished task 0.0 in stage 130.0 (TID 130). 665 bytes result sent to driver
[INFO][2018-05-25 11:26:25,068][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 130.0 (TID 130) in 36 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:26:25,068][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 130.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:26:25,069][org.apache.spark.scheduler.DAGScheduler]ResultStage 130 (foreachPartition at ReceiveKafkaData.scala:73) finished in 0.037 s
[INFO][2018-05-25 11:26:25,069][org.apache.spark.scheduler.DAGScheduler]Job 130 finished: foreachPartition at ReceiveKafkaData.scala:73, took 0.040526 s
[INFO][2018-05-25 11:26:25,069][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218785000 ms.0 from job set of time 1527218785000 ms
[INFO][2018-05-25 11:26:25,069][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.069 s for time 1527218785000 ms (execution: 0.054 s)
[INFO][2018-05-25 11:26:25,069][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 133 from persistence list
[INFO][2018-05-25 11:26:25,069][org.apache.spark.storage.BlockManager]Removing RDD 133
[INFO][2018-05-25 11:26:25,069][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 132 from persistence list
[INFO][2018-05-25 11:26:25,069][org.apache.spark.storage.BlockManager]Removing RDD 132
[INFO][2018-05-25 11:26:25,069][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:26:25,069][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218775000 ms
[INFO][2018-05-25 11:26:30,011][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218790000 ms
[INFO][2018-05-25 11:26:30,013][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218790000 ms.0 from job set of time 1527218790000 ms
[INFO][2018-05-25 11:26:30,015][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:26:30,016][org.apache.spark.scheduler.DAGScheduler]Got job 131 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:26:30,016][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 131 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:26:30,016][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:26:30,016][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:26:30,016][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 131 (MapPartitionsRDD[137] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:26:30,017][org.apache.spark.storage.memory.MemoryStore]Block broadcast_131 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-25 11:26:30,018][org.apache.spark.storage.memory.MemoryStore]Block broadcast_131_piece0 stored as bytes in memory (estimated size 1972.0 B, free 912.3 MB)
[INFO][2018-05-25 11:26:30,018][org.apache.spark.storage.BlockManagerInfo]Added broadcast_131_piece0 in memory on 10.194.32.157:53453 (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:26:30,018][org.apache.spark.SparkContext]Created broadcast 131 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:26:30,019][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 131 (MapPartitionsRDD[137] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:26:30,019][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 131.0 with 1 tasks
[INFO][2018-05-25 11:26:30,019][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 131.0 (TID 131, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:26:30,019][org.apache.spark.executor.Executor]Running task 0.0 in stage 131.0 (TID 131)
[INFO][2018-05-25 11:26:30,020][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12921 -> 12931
[INFO][2018-05-25 11:26:30,021][org.apache.spark.executor.Executor]Finished task 0.0 in stage 131.0 (TID 131). 973 bytes result sent to driver
[INFO][2018-05-25 11:26:30,021][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 131.0 (TID 131) in 2 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:26:30,021][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 131.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:26:30,022][org.apache.spark.scheduler.DAGScheduler]ResultStage 131 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.002 s
[INFO][2018-05-25 11:26:30,022][org.apache.spark.scheduler.DAGScheduler]Job 131 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.006048 s
[INFO][2018-05-25 11:26:30,024][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:73
[INFO][2018-05-25 11:26:30,024][org.apache.spark.scheduler.DAGScheduler]Got job 132 (foreachPartition at ReceiveKafkaData.scala:73) with 1 output partitions
[INFO][2018-05-25 11:26:30,025][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 132 (foreachPartition at ReceiveKafkaData.scala:73)
[INFO][2018-05-25 11:26:30,025][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:26:30,025][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:26:30,025][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 132 (MapPartitionsRDD[137] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:26:30,025][org.apache.spark.storage.memory.MemoryStore]Block broadcast_132 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-25 11:26:30,026][org.apache.spark.storage.memory.MemoryStore]Block broadcast_132_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.3 MB)
[INFO][2018-05-25 11:26:30,026][org.apache.spark.storage.BlockManagerInfo]Added broadcast_132_piece0 in memory on 10.194.32.157:53453 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:26:30,026][org.apache.spark.SparkContext]Created broadcast 132 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:26:30,027][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 132 (MapPartitionsRDD[137] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:26:30,027][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 132.0 with 1 tasks
[INFO][2018-05-25 11:26:30,027][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 132.0 (TID 132, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:26:30,027][org.apache.spark.executor.Executor]Running task 0.0 in stage 132.0 (TID 132)
[INFO][2018-05-25 11:26:30,028][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12921 -> 12931
[INFO][2018-05-25 11:26:30,028][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 12921
[INFO][2018-05-25 11:26:31,084][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-25 11:26:31,084][org.apache.spark.executor.Executor]Finished task 0.0 in stage 132.0 (TID 132). 708 bytes result sent to driver
[INFO][2018-05-25 11:26:31,085][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 132.0 (TID 132) in 1058 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:26:31,085][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 132.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:26:31,085][org.apache.spark.scheduler.DAGScheduler]ResultStage 132 (foreachPartition at ReceiveKafkaData.scala:73) finished in 1.058 s
[INFO][2018-05-25 11:26:31,085][org.apache.spark.scheduler.DAGScheduler]Job 132 finished: foreachPartition at ReceiveKafkaData.scala:73, took 1.060993 s
[INFO][2018-05-25 11:26:31,086][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218790000 ms.0 from job set of time 1527218790000 ms
[INFO][2018-05-25 11:26:31,086][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 135 from persistence list
[INFO][2018-05-25 11:26:31,086][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 1.086 s for time 1527218790000 ms (execution: 1.073 s)
[INFO][2018-05-25 11:26:31,087][org.apache.spark.storage.BlockManager]Removing RDD 135
[INFO][2018-05-25 11:26:31,087][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 134 from persistence list
[INFO][2018-05-25 11:26:31,088][org.apache.spark.storage.BlockManager]Removing RDD 134
[INFO][2018-05-25 11:26:31,088][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:26:31,088][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218780000 ms
[INFO][2018-05-25 11:26:35,015][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218795000 ms
[INFO][2018-05-25 11:26:35,015][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218795000 ms.0 from job set of time 1527218795000 ms
[INFO][2018-05-25 11:26:35,020][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:26:35,020][org.apache.spark.scheduler.DAGScheduler]Got job 133 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:26:35,020][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 133 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:26:35,020][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:26:35,020][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:26:35,020][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 133 (MapPartitionsRDD[139] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:26:35,021][org.apache.spark.storage.memory.MemoryStore]Block broadcast_133 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-25 11:26:35,022][org.apache.spark.storage.memory.MemoryStore]Block broadcast_133_piece0 stored as bytes in memory (estimated size 1972.0 B, free 912.3 MB)
[INFO][2018-05-25 11:26:35,022][org.apache.spark.storage.BlockManagerInfo]Added broadcast_133_piece0 in memory on 10.194.32.157:53453 (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:26:35,022][org.apache.spark.SparkContext]Created broadcast 133 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:26:35,023][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 133 (MapPartitionsRDD[139] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:26:35,023][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 133.0 with 1 tasks
[INFO][2018-05-25 11:26:35,023][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 133.0 (TID 133, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:26:35,023][org.apache.spark.executor.Executor]Running task 0.0 in stage 133.0 (TID 133)
[INFO][2018-05-25 11:26:35,024][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12931 -> 12941
[INFO][2018-05-25 11:26:35,025][org.apache.spark.executor.Executor]Finished task 0.0 in stage 133.0 (TID 133). 930 bytes result sent to driver
[INFO][2018-05-25 11:26:35,025][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 133.0 (TID 133) in 2 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:26:35,025][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 133.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:26:35,025][org.apache.spark.scheduler.DAGScheduler]ResultStage 133 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.002 s
[INFO][2018-05-25 11:26:35,026][org.apache.spark.scheduler.DAGScheduler]Job 133 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.005918 s
[INFO][2018-05-25 11:26:35,028][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:73
[INFO][2018-05-25 11:26:35,029][org.apache.spark.scheduler.DAGScheduler]Got job 134 (foreachPartition at ReceiveKafkaData.scala:73) with 1 output partitions
[INFO][2018-05-25 11:26:35,029][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 134 (foreachPartition at ReceiveKafkaData.scala:73)
[INFO][2018-05-25 11:26:35,029][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:26:35,029][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:26:35,029][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 134 (MapPartitionsRDD[139] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:26:35,029][org.apache.spark.storage.memory.MemoryStore]Block broadcast_134 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-25 11:26:35,030][org.apache.spark.storage.memory.MemoryStore]Block broadcast_134_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.3 MB)
[INFO][2018-05-25 11:26:35,030][org.apache.spark.storage.BlockManagerInfo]Added broadcast_134_piece0 in memory on 10.194.32.157:53453 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:26:35,030][org.apache.spark.SparkContext]Created broadcast 134 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:26:35,031][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 134 (MapPartitionsRDD[139] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:26:35,031][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 134.0 with 1 tasks
[INFO][2018-05-25 11:26:35,031][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 134.0 (TID 134, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:26:35,031][org.apache.spark.executor.Executor]Running task 0.0 in stage 134.0 (TID 134)
[INFO][2018-05-25 11:26:35,032][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12931 -> 12941
[INFO][2018-05-25 11:26:35,032][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 12931
[INFO][2018-05-25 11:26:35,073][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-25 11:26:35,074][org.apache.spark.executor.Executor]Finished task 0.0 in stage 134.0 (TID 134). 708 bytes result sent to driver
[INFO][2018-05-25 11:26:35,074][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 134.0 (TID 134) in 43 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:26:35,074][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 134.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:26:35,075][org.apache.spark.scheduler.DAGScheduler]ResultStage 134 (foreachPartition at ReceiveKafkaData.scala:73) finished in 0.044 s
[INFO][2018-05-25 11:26:35,075][org.apache.spark.scheduler.DAGScheduler]Job 134 finished: foreachPartition at ReceiveKafkaData.scala:73, took 0.046578 s
[INFO][2018-05-25 11:26:35,075][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218795000 ms.0 from job set of time 1527218795000 ms
[INFO][2018-05-25 11:26:35,075][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.075 s for time 1527218795000 ms (execution: 0.060 s)
[INFO][2018-05-25 11:26:35,075][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 137 from persistence list
[INFO][2018-05-25 11:26:35,076][org.apache.spark.storage.BlockManager]Removing RDD 137
[INFO][2018-05-25 11:26:35,076][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 136 from persistence list
[INFO][2018-05-25 11:26:35,076][org.apache.spark.storage.BlockManager]Removing RDD 136
[INFO][2018-05-25 11:26:35,076][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:26:35,076][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218785000 ms
[INFO][2018-05-25 11:26:40,023][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218800000 ms
[INFO][2018-05-25 11:26:40,023][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218800000 ms.0 from job set of time 1527218800000 ms
[INFO][2018-05-25 11:26:40,026][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:26:40,027][org.apache.spark.scheduler.DAGScheduler]Got job 135 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:26:40,027][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 135 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:26:40,027][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:26:40,027][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:26:40,027][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 135 (MapPartitionsRDD[141] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:26:40,028][org.apache.spark.storage.memory.MemoryStore]Block broadcast_135 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-25 11:26:40,029][org.apache.spark.storage.memory.MemoryStore]Block broadcast_135_piece0 stored as bytes in memory (estimated size 1972.0 B, free 912.3 MB)
[INFO][2018-05-25 11:26:40,029][org.apache.spark.storage.BlockManagerInfo]Added broadcast_135_piece0 in memory on 10.194.32.157:53453 (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:26:40,029][org.apache.spark.SparkContext]Created broadcast 135 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:26:40,030][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 135 (MapPartitionsRDD[141] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:26:40,030][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 135.0 with 1 tasks
[INFO][2018-05-25 11:26:40,031][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 135.0 (TID 135, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:26:40,031][org.apache.spark.executor.Executor]Running task 0.0 in stage 135.0 (TID 135)
[INFO][2018-05-25 11:26:40,032][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12941 -> 12951
[INFO][2018-05-25 11:26:40,033][org.apache.spark.executor.Executor]Finished task 0.0 in stage 135.0 (TID 135). 966 bytes result sent to driver
[INFO][2018-05-25 11:26:40,033][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 135.0 (TID 135) in 2 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:26:40,033][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 135.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:26:40,034][org.apache.spark.scheduler.DAGScheduler]ResultStage 135 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.004 s
[INFO][2018-05-25 11:26:40,034][org.apache.spark.scheduler.DAGScheduler]Job 135 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.007514 s
[INFO][2018-05-25 11:26:40,037][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:73
[INFO][2018-05-25 11:26:40,038][org.apache.spark.scheduler.DAGScheduler]Got job 136 (foreachPartition at ReceiveKafkaData.scala:73) with 1 output partitions
[INFO][2018-05-25 11:26:40,038][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 136 (foreachPartition at ReceiveKafkaData.scala:73)
[INFO][2018-05-25 11:26:40,038][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:26:40,038][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:26:40,038][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 136 (MapPartitionsRDD[141] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:26:40,039][org.apache.spark.storage.memory.MemoryStore]Block broadcast_136 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-25 11:26:40,039][org.apache.spark.storage.memory.MemoryStore]Block broadcast_136_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.2 MB)
[INFO][2018-05-25 11:26:40,039][org.apache.spark.storage.BlockManagerInfo]Added broadcast_136_piece0 in memory on 10.194.32.157:53453 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:26:40,040][org.apache.spark.SparkContext]Created broadcast 136 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:26:40,040][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 136 (MapPartitionsRDD[141] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:26:40,040][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 136.0 with 1 tasks
[INFO][2018-05-25 11:26:40,040][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 136.0 (TID 136, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:26:40,040][org.apache.spark.executor.Executor]Running task 0.0 in stage 136.0 (TID 136)
[INFO][2018-05-25 11:26:40,041][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12941 -> 12951
[INFO][2018-05-25 11:26:40,041][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 12941
[INFO][2018-05-25 11:26:40,079][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-25 11:26:40,085][org.apache.spark.executor.Executor]Finished task 0.0 in stage 136.0 (TID 136). 665 bytes result sent to driver
[INFO][2018-05-25 11:26:40,085][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 136.0 (TID 136) in 45 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:26:40,086][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 136.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:26:40,086][org.apache.spark.scheduler.DAGScheduler]ResultStage 136 (foreachPartition at ReceiveKafkaData.scala:73) finished in 0.046 s
[INFO][2018-05-25 11:26:40,086][org.apache.spark.scheduler.DAGScheduler]Job 136 finished: foreachPartition at ReceiveKafkaData.scala:73, took 0.048756 s
[INFO][2018-05-25 11:26:40,086][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218800000 ms.0 from job set of time 1527218800000 ms
[INFO][2018-05-25 11:26:40,086][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.086 s for time 1527218800000 ms (execution: 0.063 s)
[INFO][2018-05-25 11:26:40,087][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 139 from persistence list
[INFO][2018-05-25 11:26:40,087][org.apache.spark.storage.BlockManager]Removing RDD 139
[INFO][2018-05-25 11:26:40,087][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 138 from persistence list
[INFO][2018-05-25 11:26:40,087][org.apache.spark.storage.BlockManager]Removing RDD 138
[INFO][2018-05-25 11:26:40,087][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:26:40,087][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218790000 ms
[INFO][2018-05-25 11:26:45,015][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218805000 ms
[INFO][2018-05-25 11:26:45,015][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218805000 ms.0 from job set of time 1527218805000 ms
[INFO][2018-05-25 11:26:45,019][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:26:45,020][org.apache.spark.scheduler.DAGScheduler]Got job 137 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:26:45,020][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 137 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:26:45,020][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:26:45,020][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:26:45,020][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 137 (MapPartitionsRDD[143] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:26:45,021][org.apache.spark.storage.memory.MemoryStore]Block broadcast_137 stored as values in memory (estimated size 3.1 KB, free 912.2 MB)
[INFO][2018-05-25 11:26:45,021][org.apache.spark.storage.memory.MemoryStore]Block broadcast_137_piece0 stored as bytes in memory (estimated size 1972.0 B, free 912.2 MB)
[INFO][2018-05-25 11:26:45,022][org.apache.spark.storage.BlockManagerInfo]Added broadcast_137_piece0 in memory on 10.194.32.157:53453 (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:26:45,022][org.apache.spark.SparkContext]Created broadcast 137 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:26:45,022][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 137 (MapPartitionsRDD[143] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:26:45,022][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 137.0 with 1 tasks
[INFO][2018-05-25 11:26:45,022][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 137.0 (TID 137, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:26:45,023][org.apache.spark.executor.Executor]Running task 0.0 in stage 137.0 (TID 137)
[INFO][2018-05-25 11:26:45,023][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12951 -> 12961
[INFO][2018-05-25 11:26:45,024][org.apache.spark.executor.Executor]Finished task 0.0 in stage 137.0 (TID 137). 914 bytes result sent to driver
[INFO][2018-05-25 11:26:45,024][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 137.0 (TID 137) in 2 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:26:45,024][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 137.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:26:45,024][org.apache.spark.scheduler.DAGScheduler]ResultStage 137 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.002 s
[INFO][2018-05-25 11:26:45,025][org.apache.spark.scheduler.DAGScheduler]Job 137 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.005166 s
[INFO][2018-05-25 11:26:45,027][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:73
[INFO][2018-05-25 11:26:45,028][org.apache.spark.scheduler.DAGScheduler]Got job 138 (foreachPartition at ReceiveKafkaData.scala:73) with 1 output partitions
[INFO][2018-05-25 11:26:45,028][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 138 (foreachPartition at ReceiveKafkaData.scala:73)
[INFO][2018-05-25 11:26:45,028][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:26:45,028][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:26:45,028][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 138 (MapPartitionsRDD[143] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:26:45,030][org.apache.spark.storage.memory.MemoryStore]Block broadcast_138 stored as values in memory (estimated size 3.4 KB, free 912.2 MB)
[INFO][2018-05-25 11:26:45,031][org.apache.spark.storage.memory.MemoryStore]Block broadcast_138_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.2 MB)
[INFO][2018-05-25 11:26:45,032][org.apache.spark.storage.BlockManagerInfo]Added broadcast_138_piece0 in memory on 10.194.32.157:53453 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:26:45,033][org.apache.spark.SparkContext]Created broadcast 138 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:26:45,033][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 138 (MapPartitionsRDD[143] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:26:45,034][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 138.0 with 1 tasks
[INFO][2018-05-25 11:26:45,035][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 138.0 (TID 138, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:26:45,036][org.apache.spark.executor.Executor]Running task 0.0 in stage 138.0 (TID 138)
[INFO][2018-05-25 11:26:45,038][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12951 -> 12961
[INFO][2018-05-25 11:26:45,038][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 12951
[INFO][2018-05-25 11:26:45,079][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-25 11:26:45,080][org.apache.spark.executor.Executor]Finished task 0.0 in stage 138.0 (TID 138). 751 bytes result sent to driver
[INFO][2018-05-25 11:26:45,080][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 138.0 (TID 138) in 45 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:26:45,080][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 138.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:26:45,081][org.apache.spark.scheduler.DAGScheduler]ResultStage 138 (foreachPartition at ReceiveKafkaData.scala:73) finished in 0.047 s
[INFO][2018-05-25 11:26:45,081][org.apache.spark.scheduler.DAGScheduler]Job 138 finished: foreachPartition at ReceiveKafkaData.scala:73, took 0.053372 s
[INFO][2018-05-25 11:26:45,081][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218805000 ms.0 from job set of time 1527218805000 ms
[INFO][2018-05-25 11:26:45,081][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.081 s for time 1527218805000 ms (execution: 0.066 s)
[INFO][2018-05-25 11:26:45,081][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 141 from persistence list
[INFO][2018-05-25 11:26:45,082][org.apache.spark.storage.BlockManager]Removing RDD 141
[INFO][2018-05-25 11:26:45,082][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 140 from persistence list
[INFO][2018-05-25 11:26:45,082][org.apache.spark.storage.BlockManager]Removing RDD 140
[INFO][2018-05-25 11:26:45,082][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:26:45,082][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218795000 ms
[INFO][2018-05-25 11:26:50,023][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218810000 ms
[INFO][2018-05-25 11:26:50,023][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218810000 ms.0 from job set of time 1527218810000 ms
[INFO][2018-05-25 11:26:50,027][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:26:50,027][org.apache.spark.scheduler.DAGScheduler]Got job 139 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:26:50,027][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 139 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:26:50,027][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:26:50,027][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:26:50,028][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 139 (MapPartitionsRDD[145] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:26:50,028][org.apache.spark.storage.memory.MemoryStore]Block broadcast_139 stored as values in memory (estimated size 3.1 KB, free 912.2 MB)
[INFO][2018-05-25 11:26:50,029][org.apache.spark.storage.memory.MemoryStore]Block broadcast_139_piece0 stored as bytes in memory (estimated size 1972.0 B, free 912.2 MB)
[INFO][2018-05-25 11:26:50,029][org.apache.spark.storage.BlockManagerInfo]Added broadcast_139_piece0 in memory on 10.194.32.157:53453 (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:26:50,030][org.apache.spark.SparkContext]Created broadcast 139 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:26:50,030][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 139 (MapPartitionsRDD[145] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:26:50,030][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 139.0 with 1 tasks
[INFO][2018-05-25 11:26:50,030][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 139.0 (TID 139, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:26:50,031][org.apache.spark.executor.Executor]Running task 0.0 in stage 139.0 (TID 139)
[INFO][2018-05-25 11:26:50,031][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12961 -> 12971
[INFO][2018-05-25 11:26:50,032][org.apache.spark.executor.Executor]Finished task 0.0 in stage 139.0 (TID 139). 938 bytes result sent to driver
[INFO][2018-05-25 11:26:50,034][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 139.0 (TID 139) in 4 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:26:50,034][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 139.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:26:50,035][org.apache.spark.scheduler.DAGScheduler]ResultStage 139 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.005 s
[INFO][2018-05-25 11:26:50,035][org.apache.spark.scheduler.DAGScheduler]Job 139 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.007935 s
[INFO][2018-05-25 11:26:50,042][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:73
[INFO][2018-05-25 11:26:50,043][org.apache.spark.scheduler.DAGScheduler]Got job 140 (foreachPartition at ReceiveKafkaData.scala:73) with 1 output partitions
[INFO][2018-05-25 11:26:50,043][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 140 (foreachPartition at ReceiveKafkaData.scala:73)
[INFO][2018-05-25 11:26:50,043][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:26:50,043][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:26:50,043][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 140 (MapPartitionsRDD[145] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:26:50,044][org.apache.spark.storage.memory.MemoryStore]Block broadcast_140 stored as values in memory (estimated size 3.4 KB, free 912.2 MB)
[INFO][2018-05-25 11:26:50,045][org.apache.spark.storage.memory.MemoryStore]Block broadcast_140_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.2 MB)
[INFO][2018-05-25 11:26:50,045][org.apache.spark.storage.BlockManagerInfo]Added broadcast_140_piece0 in memory on 10.194.32.157:53453 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:26:50,045][org.apache.spark.SparkContext]Created broadcast 140 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:26:50,046][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 140 (MapPartitionsRDD[145] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:26:50,046][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 140.0 with 1 tasks
[INFO][2018-05-25 11:26:50,046][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 140.0 (TID 140, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:26:50,046][org.apache.spark.executor.Executor]Running task 0.0 in stage 140.0 (TID 140)
[INFO][2018-05-25 11:26:50,047][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12961 -> 12971
[INFO][2018-05-25 11:26:50,047][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 12961
[INFO][2018-05-25 11:26:50,088][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-25 11:26:50,088][org.apache.spark.executor.Executor]Finished task 0.0 in stage 140.0 (TID 140). 708 bytes result sent to driver
[INFO][2018-05-25 11:26:50,088][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 140.0 (TID 140) in 42 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:26:50,089][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 140.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:26:50,089][org.apache.spark.scheduler.DAGScheduler]ResultStage 140 (foreachPartition at ReceiveKafkaData.scala:73) finished in 0.043 s
[INFO][2018-05-25 11:26:50,089][org.apache.spark.scheduler.DAGScheduler]Job 140 finished: foreachPartition at ReceiveKafkaData.scala:73, took 0.046536 s
[INFO][2018-05-25 11:26:50,089][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218810000 ms.0 from job set of time 1527218810000 ms
[INFO][2018-05-25 11:26:50,089][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.089 s for time 1527218810000 ms (execution: 0.066 s)
[INFO][2018-05-25 11:26:50,089][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 143 from persistence list
[INFO][2018-05-25 11:26:50,090][org.apache.spark.storage.BlockManager]Removing RDD 143
[INFO][2018-05-25 11:26:50,090][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 142 from persistence list
[INFO][2018-05-25 11:26:50,090][org.apache.spark.storage.BlockManager]Removing RDD 142
[INFO][2018-05-25 11:26:50,090][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:26:50,090][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218800000 ms
[INFO][2018-05-25 11:26:55,017][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218815000 ms
[INFO][2018-05-25 11:26:55,017][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218815000 ms.0 from job set of time 1527218815000 ms
[INFO][2018-05-25 11:26:55,020][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:26:55,020][org.apache.spark.scheduler.DAGScheduler]Got job 141 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:26:55,020][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 141 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:26:55,020][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:26:55,020][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:26:55,020][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 141 (MapPartitionsRDD[147] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:26:55,021][org.apache.spark.storage.memory.MemoryStore]Block broadcast_141 stored as values in memory (estimated size 3.1 KB, free 912.2 MB)
[INFO][2018-05-25 11:26:55,022][org.apache.spark.storage.memory.MemoryStore]Block broadcast_141_piece0 stored as bytes in memory (estimated size 1972.0 B, free 912.2 MB)
[INFO][2018-05-25 11:26:55,022][org.apache.spark.storage.BlockManagerInfo]Added broadcast_141_piece0 in memory on 10.194.32.157:53453 (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:26:55,022][org.apache.spark.SparkContext]Created broadcast 141 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:26:55,022][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 141 (MapPartitionsRDD[147] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:26:55,022][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 141.0 with 1 tasks
[INFO][2018-05-25 11:26:55,023][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 141.0 (TID 141, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:26:55,023][org.apache.spark.executor.Executor]Running task 0.0 in stage 141.0 (TID 141)
[INFO][2018-05-25 11:26:55,024][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12971 -> 12981
[INFO][2018-05-25 11:26:55,024][org.apache.spark.executor.Executor]Finished task 0.0 in stage 141.0 (TID 141). 917 bytes result sent to driver
[INFO][2018-05-25 11:26:55,024][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 141.0 (TID 141) in 1 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:26:55,025][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 141.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:26:55,025][org.apache.spark.scheduler.DAGScheduler]ResultStage 141 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.002 s
[INFO][2018-05-25 11:26:55,025][org.apache.spark.scheduler.DAGScheduler]Job 141 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.004912 s
[INFO][2018-05-25 11:26:55,027][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:73
[INFO][2018-05-25 11:26:55,027][org.apache.spark.scheduler.DAGScheduler]Got job 142 (foreachPartition at ReceiveKafkaData.scala:73) with 1 output partitions
[INFO][2018-05-25 11:26:55,027][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 142 (foreachPartition at ReceiveKafkaData.scala:73)
[INFO][2018-05-25 11:26:55,027][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:26:55,027][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:26:55,028][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 142 (MapPartitionsRDD[147] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:26:55,028][org.apache.spark.storage.memory.MemoryStore]Block broadcast_142 stored as values in memory (estimated size 3.4 KB, free 912.2 MB)
[INFO][2018-05-25 11:26:55,029][org.apache.spark.storage.memory.MemoryStore]Block broadcast_142_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.2 MB)
[INFO][2018-05-25 11:26:55,029][org.apache.spark.storage.BlockManagerInfo]Added broadcast_142_piece0 in memory on 10.194.32.157:53453 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:26:55,029][org.apache.spark.SparkContext]Created broadcast 142 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:26:55,029][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 142 (MapPartitionsRDD[147] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:26:55,029][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 142.0 with 1 tasks
[INFO][2018-05-25 11:26:55,029][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 142.0 (TID 142, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:26:55,030][org.apache.spark.executor.Executor]Running task 0.0 in stage 142.0 (TID 142)
[INFO][2018-05-25 11:26:55,030][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12971 -> 12981
[INFO][2018-05-25 11:26:55,030][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 12971
[INFO][2018-05-25 11:26:55,073][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-25 11:26:55,074][org.apache.spark.executor.Executor]Finished task 0.0 in stage 142.0 (TID 142). 665 bytes result sent to driver
[INFO][2018-05-25 11:26:55,074][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 142.0 (TID 142) in 45 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:26:55,074][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 142.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:26:55,075][org.apache.spark.scheduler.DAGScheduler]ResultStage 142 (foreachPartition at ReceiveKafkaData.scala:73) finished in 0.046 s
[INFO][2018-05-25 11:26:55,075][org.apache.spark.scheduler.DAGScheduler]Job 142 finished: foreachPartition at ReceiveKafkaData.scala:73, took 0.047592 s
[INFO][2018-05-25 11:26:55,075][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218815000 ms.0 from job set of time 1527218815000 ms
[INFO][2018-05-25 11:26:55,075][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.075 s for time 1527218815000 ms (execution: 0.058 s)
[INFO][2018-05-25 11:26:55,075][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 145 from persistence list
[INFO][2018-05-25 11:26:55,075][org.apache.spark.storage.BlockManager]Removing RDD 145
[INFO][2018-05-25 11:26:55,075][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 144 from persistence list
[INFO][2018-05-25 11:26:55,075][org.apache.spark.storage.BlockManager]Removing RDD 144
[INFO][2018-05-25 11:26:55,076][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:26:55,076][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218805000 ms
[INFO][2018-05-25 11:27:00,011][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218820000 ms
[INFO][2018-05-25 11:27:00,012][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218820000 ms.0 from job set of time 1527218820000 ms
[INFO][2018-05-25 11:27:00,015][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:27:00,016][org.apache.spark.scheduler.DAGScheduler]Got job 143 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:27:00,016][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 143 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:27:00,016][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:27:00,016][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:27:00,016][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 143 (MapPartitionsRDD[149] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:27:00,017][org.apache.spark.storage.memory.MemoryStore]Block broadcast_143 stored as values in memory (estimated size 3.1 KB, free 912.2 MB)
[INFO][2018-05-25 11:27:00,017][org.apache.spark.storage.memory.MemoryStore]Block broadcast_143_piece0 stored as bytes in memory (estimated size 1972.0 B, free 912.2 MB)
[INFO][2018-05-25 11:27:00,018][org.apache.spark.storage.BlockManagerInfo]Added broadcast_143_piece0 in memory on 10.194.32.157:53453 (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:27:00,018][org.apache.spark.SparkContext]Created broadcast 143 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:27:00,018][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 143 (MapPartitionsRDD[149] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:27:00,018][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 143.0 with 1 tasks
[INFO][2018-05-25 11:27:00,018][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 143.0 (TID 143, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:27:00,019][org.apache.spark.executor.Executor]Running task 0.0 in stage 143.0 (TID 143)
[INFO][2018-05-25 11:27:00,019][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12981 -> 12991
[INFO][2018-05-25 11:27:00,020][org.apache.spark.executor.Executor]Finished task 0.0 in stage 143.0 (TID 143). 930 bytes result sent to driver
[INFO][2018-05-25 11:27:00,021][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 143.0 (TID 143) in 3 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:27:00,021][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 143.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:27:00,021][org.apache.spark.scheduler.DAGScheduler]ResultStage 143 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.003 s
[INFO][2018-05-25 11:27:00,021][org.apache.spark.scheduler.DAGScheduler]Job 143 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.005514 s
[INFO][2018-05-25 11:27:00,024][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:73
[INFO][2018-05-25 11:27:00,024][org.apache.spark.scheduler.DAGScheduler]Got job 144 (foreachPartition at ReceiveKafkaData.scala:73) with 1 output partitions
[INFO][2018-05-25 11:27:00,024][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 144 (foreachPartition at ReceiveKafkaData.scala:73)
[INFO][2018-05-25 11:27:00,024][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:27:00,024][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:27:00,024][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 144 (MapPartitionsRDD[149] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:27:00,025][org.apache.spark.storage.memory.MemoryStore]Block broadcast_144 stored as values in memory (estimated size 3.4 KB, free 912.2 MB)
[INFO][2018-05-25 11:27:00,026][org.apache.spark.storage.memory.MemoryStore]Block broadcast_144_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.2 MB)
[INFO][2018-05-25 11:27:00,026][org.apache.spark.storage.BlockManagerInfo]Added broadcast_144_piece0 in memory on 10.194.32.157:53453 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:27:00,026][org.apache.spark.SparkContext]Created broadcast 144 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:27:00,026][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 144 (MapPartitionsRDD[149] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:27:00,026][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 144.0 with 1 tasks
[INFO][2018-05-25 11:27:00,027][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 144.0 (TID 144, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:27:00,027][org.apache.spark.executor.Executor]Running task 0.0 in stage 144.0 (TID 144)
[INFO][2018-05-25 11:27:00,027][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12981 -> 12991
[INFO][2018-05-25 11:27:00,027][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 12981
[INFO][2018-05-25 11:27:00,078][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-25 11:27:00,078][org.apache.spark.executor.Executor]Finished task 0.0 in stage 144.0 (TID 144). 665 bytes result sent to driver
[INFO][2018-05-25 11:27:00,079][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 144.0 (TID 144) in 52 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:27:00,079][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 144.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:27:00,079][org.apache.spark.scheduler.DAGScheduler]ResultStage 144 (foreachPartition at ReceiveKafkaData.scala:73) finished in 0.053 s
[INFO][2018-05-25 11:27:00,079][org.apache.spark.scheduler.DAGScheduler]Job 144 finished: foreachPartition at ReceiveKafkaData.scala:73, took 0.055339 s
[INFO][2018-05-25 11:27:00,079][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218820000 ms.0 from job set of time 1527218820000 ms
[INFO][2018-05-25 11:27:00,079][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.079 s for time 1527218820000 ms (execution: 0.067 s)
[INFO][2018-05-25 11:27:00,080][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 147 from persistence list
[INFO][2018-05-25 11:27:00,080][org.apache.spark.storage.BlockManager]Removing RDD 147
[INFO][2018-05-25 11:27:00,080][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 146 from persistence list
[INFO][2018-05-25 11:27:00,080][org.apache.spark.storage.BlockManager]Removing RDD 146
[INFO][2018-05-25 11:27:00,080][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:27:00,080][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218810000 ms
[INFO][2018-05-25 11:27:05,014][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218825000 ms
[INFO][2018-05-25 11:27:05,015][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218825000 ms.0 from job set of time 1527218825000 ms
[INFO][2018-05-25 11:27:05,017][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:27:05,018][org.apache.spark.scheduler.DAGScheduler]Got job 145 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:27:05,018][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 145 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:27:05,018][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:27:05,018][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:27:05,018][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 145 (MapPartitionsRDD[151] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:27:05,019][org.apache.spark.storage.memory.MemoryStore]Block broadcast_145 stored as values in memory (estimated size 3.1 KB, free 912.2 MB)
[INFO][2018-05-25 11:27:05,020][org.apache.spark.storage.memory.MemoryStore]Block broadcast_145_piece0 stored as bytes in memory (estimated size 1972.0 B, free 912.2 MB)
[INFO][2018-05-25 11:27:05,020][org.apache.spark.storage.BlockManagerInfo]Added broadcast_145_piece0 in memory on 10.194.32.157:53453 (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:27:05,020][org.apache.spark.SparkContext]Created broadcast 145 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:27:05,020][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 145 (MapPartitionsRDD[151] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:27:05,020][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 145.0 with 1 tasks
[INFO][2018-05-25 11:27:05,021][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 145.0 (TID 145, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:27:05,021][org.apache.spark.executor.Executor]Running task 0.0 in stage 145.0 (TID 145)
[INFO][2018-05-25 11:27:05,021][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12991 -> 13001
[INFO][2018-05-25 11:27:05,022][org.apache.spark.executor.Executor]Finished task 0.0 in stage 145.0 (TID 145). 928 bytes result sent to driver
[INFO][2018-05-25 11:27:05,022][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 145.0 (TID 145) in 2 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:27:05,022][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 145.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:27:05,023][org.apache.spark.scheduler.DAGScheduler]ResultStage 145 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.003 s
[INFO][2018-05-25 11:27:05,023][org.apache.spark.scheduler.DAGScheduler]Job 145 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.005272 s
[INFO][2018-05-25 11:27:05,026][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:73
[INFO][2018-05-25 11:27:05,026][org.apache.spark.scheduler.DAGScheduler]Got job 146 (foreachPartition at ReceiveKafkaData.scala:73) with 1 output partitions
[INFO][2018-05-25 11:27:05,026][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 146 (foreachPartition at ReceiveKafkaData.scala:73)
[INFO][2018-05-25 11:27:05,026][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:27:05,026][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:27:05,026][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 146 (MapPartitionsRDD[151] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:27:05,027][org.apache.spark.storage.memory.MemoryStore]Block broadcast_146 stored as values in memory (estimated size 3.4 KB, free 912.2 MB)
[INFO][2018-05-25 11:27:05,028][org.apache.spark.storage.memory.MemoryStore]Block broadcast_146_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.2 MB)
[INFO][2018-05-25 11:27:05,028][org.apache.spark.storage.BlockManagerInfo]Added broadcast_146_piece0 in memory on 10.194.32.157:53453 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:27:05,028][org.apache.spark.SparkContext]Created broadcast 146 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:27:05,028][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 146 (MapPartitionsRDD[151] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:27:05,028][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 146.0 with 1 tasks
[INFO][2018-05-25 11:27:05,029][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 146.0 (TID 146, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:27:05,029][org.apache.spark.executor.Executor]Running task 0.0 in stage 146.0 (TID 146)
[INFO][2018-05-25 11:27:05,029][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 12991 -> 13001
[INFO][2018-05-25 11:27:05,029][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 12991
[INFO][2018-05-25 11:27:05,064][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-25 11:27:05,064][org.apache.spark.executor.Executor]Finished task 0.0 in stage 146.0 (TID 146). 665 bytes result sent to driver
[INFO][2018-05-25 11:27:05,065][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 146.0 (TID 146) in 36 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:27:05,065][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 146.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:27:05,065][org.apache.spark.scheduler.DAGScheduler]ResultStage 146 (foreachPartition at ReceiveKafkaData.scala:73) finished in 0.037 s
[INFO][2018-05-25 11:27:05,065][org.apache.spark.scheduler.DAGScheduler]Job 146 finished: foreachPartition at ReceiveKafkaData.scala:73, took 0.039380 s
[INFO][2018-05-25 11:27:05,065][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218825000 ms.0 from job set of time 1527218825000 ms
[INFO][2018-05-25 11:27:05,066][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.065 s for time 1527218825000 ms (execution: 0.050 s)
[INFO][2018-05-25 11:27:05,066][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 149 from persistence list
[INFO][2018-05-25 11:27:05,066][org.apache.spark.storage.BlockManager]Removing RDD 149
[INFO][2018-05-25 11:27:05,066][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 148 from persistence list
[INFO][2018-05-25 11:27:05,066][org.apache.spark.storage.BlockManager]Removing RDD 148
[INFO][2018-05-25 11:27:05,066][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:27:05,066][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218815000 ms
[INFO][2018-05-25 11:27:10,014][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218830000 ms
[INFO][2018-05-25 11:27:10,015][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218830000 ms.0 from job set of time 1527218830000 ms
[INFO][2018-05-25 11:27:10,018][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:27:10,019][org.apache.spark.scheduler.DAGScheduler]Got job 147 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:27:10,019][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 147 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:27:10,019][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:27:10,019][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:27:10,019][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 147 (MapPartitionsRDD[153] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:27:10,020][org.apache.spark.storage.memory.MemoryStore]Block broadcast_147 stored as values in memory (estimated size 3.1 KB, free 912.2 MB)
[INFO][2018-05-25 11:27:10,020][org.apache.spark.storage.memory.MemoryStore]Block broadcast_147_piece0 stored as bytes in memory (estimated size 1972.0 B, free 912.2 MB)
[INFO][2018-05-25 11:27:10,020][org.apache.spark.storage.BlockManagerInfo]Added broadcast_147_piece0 in memory on 10.194.32.157:53453 (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:27:10,021][org.apache.spark.SparkContext]Created broadcast 147 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:27:10,021][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 147 (MapPartitionsRDD[153] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:27:10,021][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 147.0 with 1 tasks
[INFO][2018-05-25 11:27:10,021][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 147.0 (TID 147, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:27:10,028][org.apache.spark.executor.Executor]Running task 0.0 in stage 147.0 (TID 147)
[INFO][2018-05-25 11:27:10,028][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_129_piece0 on 10.194.32.157:53453 in memory (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:27:10,029][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 13001 -> 13011
[INFO][2018-05-25 11:27:10,029][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_139_piece0 on 10.194.32.157:53453 in memory (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:27:10,029][org.apache.spark.executor.Executor]Finished task 0.0 in stage 147.0 (TID 147). 929 bytes result sent to driver
[INFO][2018-05-25 11:27:10,030][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_144_piece0 on 10.194.32.157:53453 in memory (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:27:10,030][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 147.0 (TID 147) in 9 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:27:10,030][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 147.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:27:10,030][org.apache.spark.scheduler.DAGScheduler]ResultStage 147 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.009 s
[INFO][2018-05-25 11:27:10,030][org.apache.spark.scheduler.DAGScheduler]Job 147 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.011917 s
[INFO][2018-05-25 11:27:10,030][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_131_piece0 on 10.194.32.157:53453 in memory (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:27:10,031][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_140_piece0 on 10.194.32.157:53453 in memory (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:27:10,031][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_146_piece0 on 10.194.32.157:53453 in memory (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:27:10,032][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_132_piece0 on 10.194.32.157:53453 in memory (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:27:10,032][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_134_piece0 on 10.194.32.157:53453 in memory (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:27:10,033][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_136_piece0 on 10.194.32.157:53453 in memory (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:27:10,033][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_135_piece0 on 10.194.32.157:53453 in memory (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:27:10,034][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:73
[INFO][2018-05-25 11:27:10,034][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_130_piece0 on 10.194.32.157:53453 in memory (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:27:10,034][org.apache.spark.scheduler.DAGScheduler]Got job 148 (foreachPartition at ReceiveKafkaData.scala:73) with 1 output partitions
[INFO][2018-05-25 11:27:10,034][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 148 (foreachPartition at ReceiveKafkaData.scala:73)
[INFO][2018-05-25 11:27:10,034][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:27:10,034][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:27:10,034][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 148 (MapPartitionsRDD[153] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:27:10,034][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_142_piece0 on 10.194.32.157:53453 in memory (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:27:10,035][org.apache.spark.storage.memory.MemoryStore]Block broadcast_148 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-25 11:27:10,035][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_128_piece0 on 10.194.32.157:53453 in memory (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:27:10,036][org.apache.spark.storage.memory.MemoryStore]Block broadcast_148_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.3 MB)
[INFO][2018-05-25 11:27:10,036][org.apache.spark.storage.BlockManagerInfo]Added broadcast_148_piece0 in memory on 10.194.32.157:53453 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:27:10,036][org.apache.spark.SparkContext]Created broadcast 148 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:27:10,036][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_143_piece0 on 10.194.32.157:53453 in memory (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:27:10,036][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 148 (MapPartitionsRDD[153] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:27:10,036][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 148.0 with 1 tasks
[INFO][2018-05-25 11:27:10,037][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 148.0 (TID 148, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:27:10,037][org.apache.spark.executor.Executor]Running task 0.0 in stage 148.0 (TID 148)
[INFO][2018-05-25 11:27:10,037][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_141_piece0 on 10.194.32.157:53453 in memory (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:27:10,037][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_138_piece0 on 10.194.32.157:53453 in memory (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:27:10,038][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 13001 -> 13011
[INFO][2018-05-25 11:27:10,038][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 13001
[INFO][2018-05-25 11:27:10,038][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_137_piece0 on 10.194.32.157:53453 in memory (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:27:10,038][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_133_piece0 on 10.194.32.157:53453 in memory (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:27:10,039][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_145_piece0 on 10.194.32.157:53453 in memory (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:27:10,076][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-25 11:27:10,077][org.apache.spark.executor.Executor]Finished task 0.0 in stage 148.0 (TID 148). 708 bytes result sent to driver
[INFO][2018-05-25 11:27:10,077][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 148.0 (TID 148) in 40 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:27:10,077][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 148.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:27:10,077][org.apache.spark.scheduler.DAGScheduler]ResultStage 148 (foreachPartition at ReceiveKafkaData.scala:73) finished in 0.040 s
[INFO][2018-05-25 11:27:10,077][org.apache.spark.scheduler.DAGScheduler]Job 148 finished: foreachPartition at ReceiveKafkaData.scala:73, took 0.043740 s
[INFO][2018-05-25 11:27:10,078][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218830000 ms.0 from job set of time 1527218830000 ms
[INFO][2018-05-25 11:27:10,078][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.078 s for time 1527218830000 ms (execution: 0.064 s)
[INFO][2018-05-25 11:27:10,078][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 151 from persistence list
[INFO][2018-05-25 11:27:10,078][org.apache.spark.storage.BlockManager]Removing RDD 151
[INFO][2018-05-25 11:27:10,078][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 150 from persistence list
[INFO][2018-05-25 11:27:10,078][org.apache.spark.storage.BlockManager]Removing RDD 150
[INFO][2018-05-25 11:27:10,078][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:27:10,079][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218820000 ms
[INFO][2018-05-25 11:27:15,019][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218835000 ms
[INFO][2018-05-25 11:27:15,019][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218835000 ms.0 from job set of time 1527218835000 ms
[INFO][2018-05-25 11:27:15,023][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:27:15,023][org.apache.spark.scheduler.DAGScheduler]Got job 149 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:27:15,023][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 149 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:27:15,023][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:27:15,023][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:27:15,024][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 149 (MapPartitionsRDD[155] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:27:15,024][org.apache.spark.storage.memory.MemoryStore]Block broadcast_149 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-25 11:27:15,025][org.apache.spark.storage.memory.MemoryStore]Block broadcast_149_piece0 stored as bytes in memory (estimated size 1972.0 B, free 912.3 MB)
[INFO][2018-05-25 11:27:15,025][org.apache.spark.storage.BlockManagerInfo]Added broadcast_149_piece0 in memory on 10.194.32.157:53453 (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:27:15,026][org.apache.spark.SparkContext]Created broadcast 149 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:27:15,026][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 149 (MapPartitionsRDD[155] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:27:15,026][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 149.0 with 1 tasks
[INFO][2018-05-25 11:27:15,026][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 149.0 (TID 149, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:27:15,027][org.apache.spark.executor.Executor]Running task 0.0 in stage 149.0 (TID 149)
[INFO][2018-05-25 11:27:15,027][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 13011 -> 13020
[INFO][2018-05-25 11:27:15,028][org.apache.spark.executor.Executor]Finished task 0.0 in stage 149.0 (TID 149). 919 bytes result sent to driver
[INFO][2018-05-25 11:27:15,028][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 149.0 (TID 149) in 2 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:27:15,028][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 149.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:27:15,029][org.apache.spark.scheduler.DAGScheduler]ResultStage 149 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.003 s
[INFO][2018-05-25 11:27:15,029][org.apache.spark.scheduler.DAGScheduler]Job 149 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.005799 s
[INFO][2018-05-25 11:27:15,031][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:73
[INFO][2018-05-25 11:27:15,032][org.apache.spark.scheduler.DAGScheduler]Got job 150 (foreachPartition at ReceiveKafkaData.scala:73) with 1 output partitions
[INFO][2018-05-25 11:27:15,032][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 150 (foreachPartition at ReceiveKafkaData.scala:73)
[INFO][2018-05-25 11:27:15,032][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:27:15,032][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:27:15,032][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 150 (MapPartitionsRDD[155] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:27:15,033][org.apache.spark.storage.memory.MemoryStore]Block broadcast_150 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-25 11:27:15,033][org.apache.spark.storage.memory.MemoryStore]Block broadcast_150_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.3 MB)
[INFO][2018-05-25 11:27:15,033][org.apache.spark.storage.BlockManagerInfo]Added broadcast_150_piece0 in memory on 10.194.32.157:53453 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:27:15,034][org.apache.spark.SparkContext]Created broadcast 150 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:27:15,034][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 150 (MapPartitionsRDD[155] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:27:15,034][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 150.0 with 1 tasks
[INFO][2018-05-25 11:27:15,034][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 150.0 (TID 150, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:27:15,034][org.apache.spark.executor.Executor]Running task 0.0 in stage 150.0 (TID 150)
[INFO][2018-05-25 11:27:15,035][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 13011 -> 13020
[INFO][2018-05-25 11:27:15,035][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 13011
[INFO][2018-05-25 11:27:15,074][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-25 11:27:15,075][org.apache.spark.executor.Executor]Finished task 0.0 in stage 150.0 (TID 150). 708 bytes result sent to driver
[INFO][2018-05-25 11:27:15,075][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 150.0 (TID 150) in 41 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:27:15,075][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 150.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:27:15,075][org.apache.spark.scheduler.DAGScheduler]ResultStage 150 (foreachPartition at ReceiveKafkaData.scala:73) finished in 0.041 s
[INFO][2018-05-25 11:27:15,076][org.apache.spark.scheduler.DAGScheduler]Job 150 finished: foreachPartition at ReceiveKafkaData.scala:73, took 0.043927 s
[INFO][2018-05-25 11:27:15,076][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218835000 ms.0 from job set of time 1527218835000 ms
[INFO][2018-05-25 11:27:15,076][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.076 s for time 1527218835000 ms (execution: 0.057 s)
[INFO][2018-05-25 11:27:15,076][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 153 from persistence list
[INFO][2018-05-25 11:27:15,076][org.apache.spark.storage.BlockManager]Removing RDD 153
[INFO][2018-05-25 11:27:15,076][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 152 from persistence list
[INFO][2018-05-25 11:27:15,076][org.apache.spark.storage.BlockManager]Removing RDD 152
[INFO][2018-05-25 11:27:15,076][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:27:15,077][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218825000 ms
[INFO][2018-05-25 11:27:20,018][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218840000 ms
[INFO][2018-05-25 11:27:20,019][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218840000 ms.0 from job set of time 1527218840000 ms
[INFO][2018-05-25 11:27:20,022][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:27:20,023][org.apache.spark.scheduler.DAGScheduler]Got job 151 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:27:20,023][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 151 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:27:20,023][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:27:20,023][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:27:20,023][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 151 (MapPartitionsRDD[157] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:27:20,024][org.apache.spark.storage.memory.MemoryStore]Block broadcast_151 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-25 11:27:20,025][org.apache.spark.storage.memory.MemoryStore]Block broadcast_151_piece0 stored as bytes in memory (estimated size 1972.0 B, free 912.3 MB)
[INFO][2018-05-25 11:27:20,025][org.apache.spark.storage.BlockManagerInfo]Added broadcast_151_piece0 in memory on 10.194.32.157:53453 (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:27:20,025][org.apache.spark.SparkContext]Created broadcast 151 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:27:20,026][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 151 (MapPartitionsRDD[157] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:27:20,026][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 151.0 with 1 tasks
[INFO][2018-05-25 11:27:20,026][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 151.0 (TID 151, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:27:20,026][org.apache.spark.executor.Executor]Running task 0.0 in stage 151.0 (TID 151)
[INFO][2018-05-25 11:27:20,027][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 13020 -> 13030
[INFO][2018-05-25 11:27:20,027][org.apache.spark.executor.Executor]Finished task 0.0 in stage 151.0 (TID 151). 929 bytes result sent to driver
[INFO][2018-05-25 11:27:20,028][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 151.0 (TID 151) in 2 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:27:20,028][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 151.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:27:20,028][org.apache.spark.scheduler.DAGScheduler]ResultStage 151 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.002 s
[INFO][2018-05-25 11:27:20,028][org.apache.spark.scheduler.DAGScheduler]Job 151 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.005503 s
[INFO][2018-05-25 11:27:20,032][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:73
[INFO][2018-05-25 11:27:20,032][org.apache.spark.scheduler.DAGScheduler]Got job 152 (foreachPartition at ReceiveKafkaData.scala:73) with 1 output partitions
[INFO][2018-05-25 11:27:20,032][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 152 (foreachPartition at ReceiveKafkaData.scala:73)
[INFO][2018-05-25 11:27:20,032][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:27:20,033][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:27:20,033][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 152 (MapPartitionsRDD[157] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:27:20,034][org.apache.spark.storage.memory.MemoryStore]Block broadcast_152 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-25 11:27:20,034][org.apache.spark.storage.memory.MemoryStore]Block broadcast_152_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.3 MB)
[INFO][2018-05-25 11:27:20,035][org.apache.spark.storage.BlockManagerInfo]Added broadcast_152_piece0 in memory on 10.194.32.157:53453 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:27:20,035][org.apache.spark.SparkContext]Created broadcast 152 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:27:20,035][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 152 (MapPartitionsRDD[157] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:27:20,035][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 152.0 with 1 tasks
[INFO][2018-05-25 11:27:20,036][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 152.0 (TID 152, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:27:20,036][org.apache.spark.executor.Executor]Running task 0.0 in stage 152.0 (TID 152)
[INFO][2018-05-25 11:27:20,036][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 13020 -> 13030
[INFO][2018-05-25 11:27:20,036][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 13020
[INFO][2018-05-25 11:27:20,069][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-25 11:27:20,069][org.apache.spark.executor.Executor]Finished task 0.0 in stage 152.0 (TID 152). 665 bytes result sent to driver
[INFO][2018-05-25 11:27:20,069][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 152.0 (TID 152) in 33 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:27:20,069][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 152.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:27:20,069][org.apache.spark.scheduler.DAGScheduler]ResultStage 152 (foreachPartition at ReceiveKafkaData.scala:73) finished in 0.034 s
[INFO][2018-05-25 11:27:20,070][org.apache.spark.scheduler.DAGScheduler]Job 152 finished: foreachPartition at ReceiveKafkaData.scala:73, took 0.037517 s
[INFO][2018-05-25 11:27:20,070][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218840000 ms.0 from job set of time 1527218840000 ms
[INFO][2018-05-25 11:27:20,070][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.070 s for time 1527218840000 ms (execution: 0.051 s)
[INFO][2018-05-25 11:27:20,070][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 155 from persistence list
[INFO][2018-05-25 11:27:20,070][org.apache.spark.storage.BlockManager]Removing RDD 155
[INFO][2018-05-25 11:27:20,070][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 154 from persistence list
[INFO][2018-05-25 11:27:20,070][org.apache.spark.storage.BlockManager]Removing RDD 154
[INFO][2018-05-25 11:27:20,070][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:27:20,070][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218830000 ms
[INFO][2018-05-25 11:27:25,013][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218845000 ms
[INFO][2018-05-25 11:27:25,013][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218845000 ms.0 from job set of time 1527218845000 ms
[INFO][2018-05-25 11:27:25,017][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:27:25,017][org.apache.spark.scheduler.DAGScheduler]Got job 153 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:27:25,017][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 153 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:27:25,017][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:27:25,017][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:27:25,017][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 153 (MapPartitionsRDD[159] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:27:25,018][org.apache.spark.storage.memory.MemoryStore]Block broadcast_153 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-25 11:27:25,019][org.apache.spark.storage.memory.MemoryStore]Block broadcast_153_piece0 stored as bytes in memory (estimated size 1972.0 B, free 912.3 MB)
[INFO][2018-05-25 11:27:25,019][org.apache.spark.storage.BlockManagerInfo]Added broadcast_153_piece0 in memory on 10.194.32.157:53453 (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:27:25,019][org.apache.spark.SparkContext]Created broadcast 153 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:27:25,019][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 153 (MapPartitionsRDD[159] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:27:25,019][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 153.0 with 1 tasks
[INFO][2018-05-25 11:27:25,020][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 153.0 (TID 153, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:27:25,020][org.apache.spark.executor.Executor]Running task 0.0 in stage 153.0 (TID 153)
[INFO][2018-05-25 11:27:25,021][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 13030 -> 13040
[INFO][2018-05-25 11:27:25,022][org.apache.spark.executor.Executor]Finished task 0.0 in stage 153.0 (TID 153). 925 bytes result sent to driver
[INFO][2018-05-25 11:27:25,022][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 153.0 (TID 153) in 2 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:27:25,022][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 153.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:27:25,022][org.apache.spark.scheduler.DAGScheduler]ResultStage 153 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.002 s
[INFO][2018-05-25 11:27:25,022][org.apache.spark.scheduler.DAGScheduler]Job 153 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.005535 s
[INFO][2018-05-25 11:27:25,025][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:73
[INFO][2018-05-25 11:27:25,025][org.apache.spark.scheduler.DAGScheduler]Got job 154 (foreachPartition at ReceiveKafkaData.scala:73) with 1 output partitions
[INFO][2018-05-25 11:27:25,025][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 154 (foreachPartition at ReceiveKafkaData.scala:73)
[INFO][2018-05-25 11:27:25,025][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:27:25,025][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:27:25,025][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 154 (MapPartitionsRDD[159] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:27:25,026][org.apache.spark.storage.memory.MemoryStore]Block broadcast_154 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-25 11:27:25,027][org.apache.spark.storage.memory.MemoryStore]Block broadcast_154_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.3 MB)
[INFO][2018-05-25 11:27:25,027][org.apache.spark.storage.BlockManagerInfo]Added broadcast_154_piece0 in memory on 10.194.32.157:53453 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:27:25,027][org.apache.spark.SparkContext]Created broadcast 154 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:27:25,027][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 154 (MapPartitionsRDD[159] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:27:25,027][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 154.0 with 1 tasks
[INFO][2018-05-25 11:27:25,028][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 154.0 (TID 154, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:27:25,028][org.apache.spark.executor.Executor]Running task 0.0 in stage 154.0 (TID 154)
[INFO][2018-05-25 11:27:25,029][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 13030 -> 13040
[INFO][2018-05-25 11:27:25,029][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 13030
[INFO][2018-05-25 11:27:25,067][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-25 11:27:25,068][org.apache.spark.executor.Executor]Finished task 0.0 in stage 154.0 (TID 154). 708 bytes result sent to driver
[INFO][2018-05-25 11:27:25,068][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 154.0 (TID 154) in 40 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:27:25,069][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 154.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:27:25,069][org.apache.spark.scheduler.DAGScheduler]ResultStage 154 (foreachPartition at ReceiveKafkaData.scala:73) finished in 0.041 s
[INFO][2018-05-25 11:27:25,069][org.apache.spark.scheduler.DAGScheduler]Job 154 finished: foreachPartition at ReceiveKafkaData.scala:73, took 0.044039 s
[INFO][2018-05-25 11:27:25,069][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218845000 ms.0 from job set of time 1527218845000 ms
[INFO][2018-05-25 11:27:25,069][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.069 s for time 1527218845000 ms (execution: 0.056 s)
[INFO][2018-05-25 11:27:25,069][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 157 from persistence list
[INFO][2018-05-25 11:27:25,070][org.apache.spark.storage.BlockManager]Removing RDD 157
[INFO][2018-05-25 11:27:25,070][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 156 from persistence list
[INFO][2018-05-25 11:27:25,070][org.apache.spark.storage.BlockManager]Removing RDD 156
[INFO][2018-05-25 11:27:25,070][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:27:25,070][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218835000 ms
[INFO][2018-05-25 11:27:30,014][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218850000 ms
[INFO][2018-05-25 11:27:30,015][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218850000 ms.0 from job set of time 1527218850000 ms
[INFO][2018-05-25 11:27:30,019][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:27:30,019][org.apache.spark.scheduler.DAGScheduler]Got job 155 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:27:30,019][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 155 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:27:30,019][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:27:30,019][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:27:30,019][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 155 (MapPartitionsRDD[161] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:27:30,020][org.apache.spark.storage.memory.MemoryStore]Block broadcast_155 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-25 11:27:30,021][org.apache.spark.storage.memory.MemoryStore]Block broadcast_155_piece0 stored as bytes in memory (estimated size 1972.0 B, free 912.2 MB)
[INFO][2018-05-25 11:27:30,021][org.apache.spark.storage.BlockManagerInfo]Added broadcast_155_piece0 in memory on 10.194.32.157:53453 (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:27:30,021][org.apache.spark.SparkContext]Created broadcast 155 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:27:30,021][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 155 (MapPartitionsRDD[161] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:27:30,022][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 155.0 with 1 tasks
[INFO][2018-05-25 11:27:30,022][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 155.0 (TID 155, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:27:30,022][org.apache.spark.executor.Executor]Running task 0.0 in stage 155.0 (TID 155)
[INFO][2018-05-25 11:27:30,023][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 13040 -> 13050
[INFO][2018-05-25 11:27:30,024][org.apache.spark.executor.Executor]Finished task 0.0 in stage 155.0 (TID 155). 919 bytes result sent to driver
[INFO][2018-05-25 11:27:30,024][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 155.0 (TID 155) in 2 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:27:30,024][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 155.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:27:30,024][org.apache.spark.scheduler.DAGScheduler]ResultStage 155 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.002 s
[INFO][2018-05-25 11:27:30,024][org.apache.spark.scheduler.DAGScheduler]Job 155 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.005832 s
[INFO][2018-05-25 11:27:30,027][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:73
[INFO][2018-05-25 11:27:30,028][org.apache.spark.scheduler.DAGScheduler]Got job 156 (foreachPartition at ReceiveKafkaData.scala:73) with 1 output partitions
[INFO][2018-05-25 11:27:30,028][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 156 (foreachPartition at ReceiveKafkaData.scala:73)
[INFO][2018-05-25 11:27:30,028][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:27:30,028][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:27:30,028][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 156 (MapPartitionsRDD[161] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:27:30,029][org.apache.spark.storage.memory.MemoryStore]Block broadcast_156 stored as values in memory (estimated size 3.4 KB, free 912.2 MB)
[INFO][2018-05-25 11:27:30,029][org.apache.spark.storage.memory.MemoryStore]Block broadcast_156_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.2 MB)
[INFO][2018-05-25 11:27:30,029][org.apache.spark.storage.BlockManagerInfo]Added broadcast_156_piece0 in memory on 10.194.32.157:53453 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:27:30,030][org.apache.spark.SparkContext]Created broadcast 156 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:27:30,030][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 156 (MapPartitionsRDD[161] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:27:30,030][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 156.0 with 1 tasks
[INFO][2018-05-25 11:27:30,030][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 156.0 (TID 156, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:27:30,030][org.apache.spark.executor.Executor]Running task 0.0 in stage 156.0 (TID 156)
[INFO][2018-05-25 11:27:30,031][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 13040 -> 13050
[INFO][2018-05-25 11:27:30,031][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 13040
[INFO][2018-05-25 11:27:30,079][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-25 11:27:30,079][org.apache.spark.executor.Executor]Finished task 0.0 in stage 156.0 (TID 156). 665 bytes result sent to driver
[INFO][2018-05-25 11:27:30,080][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 156.0 (TID 156) in 50 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:27:30,080][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 156.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:27:30,080][org.apache.spark.scheduler.DAGScheduler]ResultStage 156 (foreachPartition at ReceiveKafkaData.scala:73) finished in 0.050 s
[INFO][2018-05-25 11:27:30,080][org.apache.spark.scheduler.DAGScheduler]Job 156 finished: foreachPartition at ReceiveKafkaData.scala:73, took 0.052730 s
[INFO][2018-05-25 11:27:30,080][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218850000 ms.0 from job set of time 1527218850000 ms
[INFO][2018-05-25 11:27:30,081][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.080 s for time 1527218850000 ms (execution: 0.066 s)
[INFO][2018-05-25 11:27:30,081][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 159 from persistence list
[INFO][2018-05-25 11:27:30,081][org.apache.spark.storage.BlockManager]Removing RDD 159
[INFO][2018-05-25 11:27:30,081][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 158 from persistence list
[INFO][2018-05-25 11:27:30,081][org.apache.spark.storage.BlockManager]Removing RDD 158
[INFO][2018-05-25 11:27:30,081][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:27:30,081][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218840000 ms
[INFO][2018-05-25 11:27:35,016][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218855000 ms
[INFO][2018-05-25 11:27:35,016][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218855000 ms.0 from job set of time 1527218855000 ms
[INFO][2018-05-25 11:27:35,020][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:27:35,020][org.apache.spark.scheduler.DAGScheduler]Got job 157 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:27:35,020][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 157 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:27:35,020][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:27:35,021][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:27:35,021][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 157 (MapPartitionsRDD[163] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:27:35,022][org.apache.spark.storage.memory.MemoryStore]Block broadcast_157 stored as values in memory (estimated size 3.1 KB, free 912.2 MB)
[INFO][2018-05-25 11:27:35,022][org.apache.spark.storage.memory.MemoryStore]Block broadcast_157_piece0 stored as bytes in memory (estimated size 1972.0 B, free 912.2 MB)
[INFO][2018-05-25 11:27:35,022][org.apache.spark.storage.BlockManagerInfo]Added broadcast_157_piece0 in memory on 10.194.32.157:53453 (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:27:35,023][org.apache.spark.SparkContext]Created broadcast 157 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:27:35,023][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 157 (MapPartitionsRDD[163] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:27:35,023][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 157.0 with 1 tasks
[INFO][2018-05-25 11:27:35,023][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 157.0 (TID 157, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:27:35,024][org.apache.spark.executor.Executor]Running task 0.0 in stage 157.0 (TID 157)
[INFO][2018-05-25 11:27:35,024][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 13050 -> 13060
[INFO][2018-05-25 11:27:35,025][org.apache.spark.executor.Executor]Finished task 0.0 in stage 157.0 (TID 157). 919 bytes result sent to driver
[INFO][2018-05-25 11:27:35,025][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 157.0 (TID 157) in 2 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:27:35,025][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 157.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:27:35,026][org.apache.spark.scheduler.DAGScheduler]ResultStage 157 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.003 s
[INFO][2018-05-25 11:27:35,026][org.apache.spark.scheduler.DAGScheduler]Job 157 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.005655 s
[INFO][2018-05-25 11:27:35,028][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:73
[INFO][2018-05-25 11:27:35,029][org.apache.spark.scheduler.DAGScheduler]Got job 158 (foreachPartition at ReceiveKafkaData.scala:73) with 1 output partitions
[INFO][2018-05-25 11:27:35,029][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 158 (foreachPartition at ReceiveKafkaData.scala:73)
[INFO][2018-05-25 11:27:35,029][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:27:35,029][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:27:35,029][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 158 (MapPartitionsRDD[163] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:27:35,029][org.apache.spark.storage.memory.MemoryStore]Block broadcast_158 stored as values in memory (estimated size 3.4 KB, free 912.2 MB)
[INFO][2018-05-25 11:27:35,030][org.apache.spark.storage.memory.MemoryStore]Block broadcast_158_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.2 MB)
[INFO][2018-05-25 11:27:35,030][org.apache.spark.storage.BlockManagerInfo]Added broadcast_158_piece0 in memory on 10.194.32.157:53453 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:27:35,030][org.apache.spark.SparkContext]Created broadcast 158 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:27:35,031][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 158 (MapPartitionsRDD[163] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:27:35,031][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 158.0 with 1 tasks
[INFO][2018-05-25 11:27:35,031][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 158.0 (TID 158, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:27:35,031][org.apache.spark.executor.Executor]Running task 0.0 in stage 158.0 (TID 158)
[INFO][2018-05-25 11:27:35,032][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 13050 -> 13060
[INFO][2018-05-25 11:27:35,032][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 13050
[INFO][2018-05-25 11:27:35,067][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-25 11:27:35,068][org.apache.spark.executor.Executor]Finished task 0.0 in stage 158.0 (TID 158). 708 bytes result sent to driver
[INFO][2018-05-25 11:27:35,068][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 158.0 (TID 158) in 37 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:27:35,068][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 158.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:27:35,069][org.apache.spark.scheduler.DAGScheduler]ResultStage 158 (foreachPartition at ReceiveKafkaData.scala:73) finished in 0.037 s
[INFO][2018-05-25 11:27:35,069][org.apache.spark.scheduler.DAGScheduler]Job 158 finished: foreachPartition at ReceiveKafkaData.scala:73, took 0.040323 s
[INFO][2018-05-25 11:27:35,069][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218855000 ms.0 from job set of time 1527218855000 ms
[INFO][2018-05-25 11:27:35,069][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.069 s for time 1527218855000 ms (execution: 0.053 s)
[INFO][2018-05-25 11:27:35,069][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 161 from persistence list
[INFO][2018-05-25 11:27:35,069][org.apache.spark.storage.BlockManager]Removing RDD 161
[INFO][2018-05-25 11:27:35,069][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 160 from persistence list
[INFO][2018-05-25 11:27:35,070][org.apache.spark.storage.BlockManager]Removing RDD 160
[INFO][2018-05-25 11:27:35,070][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:27:35,070][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218845000 ms
[INFO][2018-05-25 11:27:40,016][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218860000 ms
[INFO][2018-05-25 11:27:40,016][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218860000 ms.0 from job set of time 1527218860000 ms
[INFO][2018-05-25 11:27:40,020][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:27:40,020][org.apache.spark.scheduler.DAGScheduler]Got job 159 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:27:40,020][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 159 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:27:40,021][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:27:40,021][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:27:40,021][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 159 (MapPartitionsRDD[165] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:27:40,022][org.apache.spark.storage.memory.MemoryStore]Block broadcast_159 stored as values in memory (estimated size 3.1 KB, free 912.2 MB)
[INFO][2018-05-25 11:27:40,023][org.apache.spark.storage.memory.MemoryStore]Block broadcast_159_piece0 stored as bytes in memory (estimated size 1972.0 B, free 912.2 MB)
[INFO][2018-05-25 11:27:40,023][org.apache.spark.storage.BlockManagerInfo]Added broadcast_159_piece0 in memory on 10.194.32.157:53453 (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:27:40,023][org.apache.spark.SparkContext]Created broadcast 159 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:27:40,024][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 159 (MapPartitionsRDD[165] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:27:40,024][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 159.0 with 1 tasks
[INFO][2018-05-25 11:27:40,024][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 159.0 (TID 159, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:27:40,024][org.apache.spark.executor.Executor]Running task 0.0 in stage 159.0 (TID 159)
[INFO][2018-05-25 11:27:40,026][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 13060 -> 13070
[INFO][2018-05-25 11:27:40,027][org.apache.spark.executor.Executor]Finished task 0.0 in stage 159.0 (TID 159). 930 bytes result sent to driver
[INFO][2018-05-25 11:27:40,027][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 159.0 (TID 159) in 3 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:27:40,027][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 159.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:27:40,027][org.apache.spark.scheduler.DAGScheduler]ResultStage 159 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.003 s
[INFO][2018-05-25 11:27:40,027][org.apache.spark.scheduler.DAGScheduler]Job 159 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.007118 s
[INFO][2018-05-25 11:27:40,031][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:73
[INFO][2018-05-25 11:27:40,032][org.apache.spark.scheduler.DAGScheduler]Got job 160 (foreachPartition at ReceiveKafkaData.scala:73) with 1 output partitions
[INFO][2018-05-25 11:27:40,032][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 160 (foreachPartition at ReceiveKafkaData.scala:73)
[INFO][2018-05-25 11:27:40,032][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:27:40,032][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:27:40,032][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 160 (MapPartitionsRDD[165] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:27:40,033][org.apache.spark.storage.memory.MemoryStore]Block broadcast_160 stored as values in memory (estimated size 3.4 KB, free 912.2 MB)
[INFO][2018-05-25 11:27:40,033][org.apache.spark.storage.memory.MemoryStore]Block broadcast_160_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.2 MB)
[INFO][2018-05-25 11:27:40,034][org.apache.spark.storage.BlockManagerInfo]Added broadcast_160_piece0 in memory on 10.194.32.157:53453 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:27:40,034][org.apache.spark.SparkContext]Created broadcast 160 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:27:40,034][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 160 (MapPartitionsRDD[165] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:27:40,034][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 160.0 with 1 tasks
[INFO][2018-05-25 11:27:40,034][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 160.0 (TID 160, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:27:40,035][org.apache.spark.executor.Executor]Running task 0.0 in stage 160.0 (TID 160)
[INFO][2018-05-25 11:27:40,035][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 13060 -> 13070
[INFO][2018-05-25 11:27:40,035][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 13060
[INFO][2018-05-25 11:27:41,075][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-25 11:27:41,078][org.apache.spark.executor.Executor]Finished task 0.0 in stage 160.0 (TID 160). 665 bytes result sent to driver
[INFO][2018-05-25 11:27:41,079][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 160.0 (TID 160) in 1045 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:27:41,079][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 160.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:27:41,079][org.apache.spark.scheduler.DAGScheduler]ResultStage 160 (foreachPartition at ReceiveKafkaData.scala:73) finished in 1.045 s
[INFO][2018-05-25 11:27:41,079][org.apache.spark.scheduler.DAGScheduler]Job 160 finished: foreachPartition at ReceiveKafkaData.scala:73, took 1.048222 s
[INFO][2018-05-25 11:27:41,080][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218860000 ms.0 from job set of time 1527218860000 ms
[INFO][2018-05-25 11:27:41,080][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 1.080 s for time 1527218860000 ms (execution: 1.064 s)
[INFO][2018-05-25 11:27:41,080][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 163 from persistence list
[INFO][2018-05-25 11:27:41,080][org.apache.spark.storage.BlockManager]Removing RDD 163
[INFO][2018-05-25 11:27:41,080][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 162 from persistence list
[INFO][2018-05-25 11:27:41,080][org.apache.spark.storage.BlockManager]Removing RDD 162
[INFO][2018-05-25 11:27:41,080][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:27:41,081][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218850000 ms
[INFO][2018-05-25 11:27:45,016][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218865000 ms
[INFO][2018-05-25 11:27:45,016][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218865000 ms.0 from job set of time 1527218865000 ms
[INFO][2018-05-25 11:27:45,020][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:27:45,020][org.apache.spark.scheduler.DAGScheduler]Got job 161 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:27:45,020][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 161 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:27:45,020][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:27:45,020][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:27:45,021][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 161 (MapPartitionsRDD[167] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:27:45,021][org.apache.spark.storage.memory.MemoryStore]Block broadcast_161 stored as values in memory (estimated size 3.1 KB, free 912.2 MB)
[INFO][2018-05-25 11:27:45,022][org.apache.spark.storage.memory.MemoryStore]Block broadcast_161_piece0 stored as bytes in memory (estimated size 1972.0 B, free 912.2 MB)
[INFO][2018-05-25 11:27:45,022][org.apache.spark.storage.BlockManagerInfo]Added broadcast_161_piece0 in memory on 10.194.32.157:53453 (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:27:45,023][org.apache.spark.SparkContext]Created broadcast 161 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:27:45,023][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 161 (MapPartitionsRDD[167] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:27:45,023][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 161.0 with 1 tasks
[INFO][2018-05-25 11:27:45,023][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 161.0 (TID 161, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:27:45,023][org.apache.spark.executor.Executor]Running task 0.0 in stage 161.0 (TID 161)
[INFO][2018-05-25 11:27:45,024][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 13070 -> 13080
[INFO][2018-05-25 11:27:45,025][org.apache.spark.executor.Executor]Finished task 0.0 in stage 161.0 (TID 161). 975 bytes result sent to driver
[INFO][2018-05-25 11:27:45,026][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 161.0 (TID 161) in 3 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:27:45,026][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 161.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:27:45,026][org.apache.spark.scheduler.DAGScheduler]ResultStage 161 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.003 s
[INFO][2018-05-25 11:27:45,026][org.apache.spark.scheduler.DAGScheduler]Job 161 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.006082 s
[INFO][2018-05-25 11:27:45,029][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:73
[INFO][2018-05-25 11:27:45,029][org.apache.spark.scheduler.DAGScheduler]Got job 162 (foreachPartition at ReceiveKafkaData.scala:73) with 1 output partitions
[INFO][2018-05-25 11:27:45,029][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 162 (foreachPartition at ReceiveKafkaData.scala:73)
[INFO][2018-05-25 11:27:45,029][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:27:45,029][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:27:45,029][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 162 (MapPartitionsRDD[167] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:27:45,030][org.apache.spark.storage.memory.MemoryStore]Block broadcast_162 stored as values in memory (estimated size 3.4 KB, free 912.2 MB)
[INFO][2018-05-25 11:27:45,030][org.apache.spark.storage.memory.MemoryStore]Block broadcast_162_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.2 MB)
[INFO][2018-05-25 11:27:45,031][org.apache.spark.storage.BlockManagerInfo]Added broadcast_162_piece0 in memory on 10.194.32.157:53453 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:27:45,031][org.apache.spark.SparkContext]Created broadcast 162 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:27:45,031][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 162 (MapPartitionsRDD[167] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:27:45,031][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 162.0 with 1 tasks
[INFO][2018-05-25 11:27:45,031][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 162.0 (TID 162, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:27:45,031][org.apache.spark.executor.Executor]Running task 0.0 in stage 162.0 (TID 162)
[INFO][2018-05-25 11:27:45,032][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 13070 -> 13080
[INFO][2018-05-25 11:27:45,032][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 13070
[INFO][2018-05-25 11:27:45,066][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-25 11:27:45,066][org.apache.spark.executor.Executor]Finished task 0.0 in stage 162.0 (TID 162). 708 bytes result sent to driver
[INFO][2018-05-25 11:27:45,067][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 162.0 (TID 162) in 36 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:27:45,067][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 162.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:27:45,067][org.apache.spark.scheduler.DAGScheduler]ResultStage 162 (foreachPartition at ReceiveKafkaData.scala:73) finished in 0.036 s
[INFO][2018-05-25 11:27:45,067][org.apache.spark.scheduler.DAGScheduler]Job 162 finished: foreachPartition at ReceiveKafkaData.scala:73, took 0.038402 s
[INFO][2018-05-25 11:27:45,067][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218865000 ms.0 from job set of time 1527218865000 ms
[INFO][2018-05-25 11:27:45,067][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 165 from persistence list
[INFO][2018-05-25 11:27:45,067][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.067 s for time 1527218865000 ms (execution: 0.051 s)
[INFO][2018-05-25 11:27:45,068][org.apache.spark.storage.BlockManager]Removing RDD 165
[INFO][2018-05-25 11:27:45,068][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 164 from persistence list
[INFO][2018-05-25 11:27:45,068][org.apache.spark.storage.BlockManager]Removing RDD 164
[INFO][2018-05-25 11:27:45,068][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:27:45,068][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218855000 ms
[INFO][2018-05-25 11:27:50,017][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218870000 ms
[INFO][2018-05-25 11:27:50,018][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218870000 ms.0 from job set of time 1527218870000 ms
[INFO][2018-05-25 11:27:50,022][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:27:50,022][org.apache.spark.scheduler.DAGScheduler]Got job 163 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:27:50,022][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 163 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:27:50,022][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:27:50,022][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:27:50,022][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 163 (MapPartitionsRDD[169] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:27:50,023][org.apache.spark.storage.memory.MemoryStore]Block broadcast_163 stored as values in memory (estimated size 3.1 KB, free 912.2 MB)
[INFO][2018-05-25 11:27:50,024][org.apache.spark.storage.memory.MemoryStore]Block broadcast_163_piece0 stored as bytes in memory (estimated size 1972.0 B, free 912.2 MB)
[INFO][2018-05-25 11:27:50,024][org.apache.spark.storage.BlockManagerInfo]Added broadcast_163_piece0 in memory on 10.194.32.157:53453 (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:27:50,024][org.apache.spark.SparkContext]Created broadcast 163 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:27:50,025][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 163 (MapPartitionsRDD[169] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:27:50,025][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 163.0 with 1 tasks
[INFO][2018-05-25 11:27:50,025][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 163.0 (TID 163, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:27:50,025][org.apache.spark.executor.Executor]Running task 0.0 in stage 163.0 (TID 163)
[INFO][2018-05-25 11:27:50,026][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 13080 -> 13090
[INFO][2018-05-25 11:27:50,027][org.apache.spark.executor.Executor]Finished task 0.0 in stage 163.0 (TID 163). 919 bytes result sent to driver
[INFO][2018-05-25 11:27:50,027][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 163.0 (TID 163) in 2 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:27:50,027][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 163.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:27:50,027][org.apache.spark.scheduler.DAGScheduler]ResultStage 163 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.002 s
[INFO][2018-05-25 11:27:50,028][org.apache.spark.scheduler.DAGScheduler]Job 163 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.005725 s
[INFO][2018-05-25 11:27:50,031][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:73
[INFO][2018-05-25 11:27:50,032][org.apache.spark.scheduler.DAGScheduler]Got job 164 (foreachPartition at ReceiveKafkaData.scala:73) with 1 output partitions
[INFO][2018-05-25 11:27:50,032][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 164 (foreachPartition at ReceiveKafkaData.scala:73)
[INFO][2018-05-25 11:27:50,032][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:27:50,032][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:27:50,032][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 164 (MapPartitionsRDD[169] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:27:50,033][org.apache.spark.storage.memory.MemoryStore]Block broadcast_164 stored as values in memory (estimated size 3.4 KB, free 912.2 MB)
[INFO][2018-05-25 11:27:50,034][org.apache.spark.storage.memory.MemoryStore]Block broadcast_164_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.2 MB)
[INFO][2018-05-25 11:27:50,034][org.apache.spark.storage.BlockManagerInfo]Added broadcast_164_piece0 in memory on 10.194.32.157:53453 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:27:50,034][org.apache.spark.SparkContext]Created broadcast 164 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:27:50,034][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 164 (MapPartitionsRDD[169] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:27:50,034][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 164.0 with 1 tasks
[INFO][2018-05-25 11:27:50,035][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 164.0 (TID 164, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:27:50,035][org.apache.spark.executor.Executor]Running task 0.0 in stage 164.0 (TID 164)
[INFO][2018-05-25 11:27:50,036][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 13080 -> 13090
[INFO][2018-05-25 11:27:50,036][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 13080
[INFO][2018-05-25 11:27:50,072][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-25 11:27:50,073][org.apache.spark.executor.Executor]Finished task 0.0 in stage 164.0 (TID 164). 708 bytes result sent to driver
[INFO][2018-05-25 11:27:50,073][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 164.0 (TID 164) in 39 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:27:50,073][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 164.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:27:50,074][org.apache.spark.scheduler.DAGScheduler]ResultStage 164 (foreachPartition at ReceiveKafkaData.scala:73) finished in 0.040 s
[INFO][2018-05-25 11:27:50,074][org.apache.spark.scheduler.DAGScheduler]Job 164 finished: foreachPartition at ReceiveKafkaData.scala:73, took 0.042477 s
[INFO][2018-05-25 11:27:50,074][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218870000 ms.0 from job set of time 1527218870000 ms
[INFO][2018-05-25 11:27:50,074][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.074 s for time 1527218870000 ms (execution: 0.056 s)
[INFO][2018-05-25 11:27:50,074][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 167 from persistence list
[INFO][2018-05-25 11:27:50,075][org.apache.spark.storage.BlockManager]Removing RDD 167
[INFO][2018-05-25 11:27:50,075][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 166 from persistence list
[INFO][2018-05-25 11:27:50,075][org.apache.spark.storage.BlockManager]Removing RDD 166
[INFO][2018-05-25 11:27:50,075][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:27:50,075][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218860000 ms
[INFO][2018-05-25 11:27:55,017][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218875000 ms
[INFO][2018-05-25 11:27:55,017][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218875000 ms.0 from job set of time 1527218875000 ms
[INFO][2018-05-25 11:27:55,022][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:27:55,023][org.apache.spark.scheduler.DAGScheduler]Got job 165 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:27:55,023][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 165 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:27:55,023][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:27:55,024][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:27:55,024][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 165 (MapPartitionsRDD[171] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:27:55,026][org.apache.spark.storage.memory.MemoryStore]Block broadcast_165 stored as values in memory (estimated size 3.1 KB, free 912.2 MB)
[INFO][2018-05-25 11:27:55,029][org.apache.spark.storage.memory.MemoryStore]Block broadcast_165_piece0 stored as bytes in memory (estimated size 1972.0 B, free 912.2 MB)
[INFO][2018-05-25 11:27:55,030][org.apache.spark.storage.BlockManagerInfo]Added broadcast_165_piece0 in memory on 10.194.32.157:53453 (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:27:55,031][org.apache.spark.SparkContext]Created broadcast 165 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:27:55,031][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 165 (MapPartitionsRDD[171] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:27:55,031][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 165.0 with 1 tasks
[INFO][2018-05-25 11:27:55,032][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 165.0 (TID 165, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:27:55,033][org.apache.spark.executor.Executor]Running task 0.0 in stage 165.0 (TID 165)
[INFO][2018-05-25 11:27:55,033][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 13090 -> 13100
[INFO][2018-05-25 11:27:55,035][org.apache.spark.executor.Executor]Finished task 0.0 in stage 165.0 (TID 165). 962 bytes result sent to driver
[INFO][2018-05-25 11:27:55,036][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 165.0 (TID 165) in 4 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:27:55,036][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 165.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:27:55,036][org.apache.spark.scheduler.DAGScheduler]ResultStage 165 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.004 s
[INFO][2018-05-25 11:27:55,037][org.apache.spark.scheduler.DAGScheduler]Job 165 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.014342 s
[INFO][2018-05-25 11:27:55,041][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:73
[INFO][2018-05-25 11:27:55,041][org.apache.spark.scheduler.DAGScheduler]Got job 166 (foreachPartition at ReceiveKafkaData.scala:73) with 1 output partitions
[INFO][2018-05-25 11:27:55,041][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 166 (foreachPartition at ReceiveKafkaData.scala:73)
[INFO][2018-05-25 11:27:55,041][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:27:55,041][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:27:55,042][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 166 (MapPartitionsRDD[171] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:27:55,042][org.apache.spark.storage.memory.MemoryStore]Block broadcast_166 stored as values in memory (estimated size 3.4 KB, free 912.2 MB)
[INFO][2018-05-25 11:27:55,043][org.apache.spark.storage.memory.MemoryStore]Block broadcast_166_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.2 MB)
[INFO][2018-05-25 11:27:55,043][org.apache.spark.storage.BlockManagerInfo]Added broadcast_166_piece0 in memory on 10.194.32.157:53453 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:27:55,043][org.apache.spark.SparkContext]Created broadcast 166 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:27:55,044][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 166 (MapPartitionsRDD[171] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:27:55,044][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 166.0 with 1 tasks
[INFO][2018-05-25 11:27:55,044][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 166.0 (TID 166, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:27:55,044][org.apache.spark.executor.Executor]Running task 0.0 in stage 166.0 (TID 166)
[INFO][2018-05-25 11:27:55,045][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 13090 -> 13100
[INFO][2018-05-25 11:27:55,045][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 13090
[INFO][2018-05-25 11:27:55,080][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-25 11:27:55,081][org.apache.spark.executor.Executor]Finished task 0.0 in stage 166.0 (TID 166). 708 bytes result sent to driver
[INFO][2018-05-25 11:27:55,081][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 166.0 (TID 166) in 37 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:27:55,082][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 166.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:27:55,082][org.apache.spark.scheduler.DAGScheduler]ResultStage 166 (foreachPartition at ReceiveKafkaData.scala:73) finished in 0.038 s
[INFO][2018-05-25 11:27:55,082][org.apache.spark.scheduler.DAGScheduler]Job 166 finished: foreachPartition at ReceiveKafkaData.scala:73, took 0.040726 s
[INFO][2018-05-25 11:27:55,082][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218875000 ms.0 from job set of time 1527218875000 ms
[INFO][2018-05-25 11:27:55,082][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.082 s for time 1527218875000 ms (execution: 0.065 s)
[INFO][2018-05-25 11:27:55,082][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 169 from persistence list
[INFO][2018-05-25 11:27:55,083][org.apache.spark.storage.BlockManager]Removing RDD 169
[INFO][2018-05-25 11:27:55,083][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 168 from persistence list
[INFO][2018-05-25 11:27:55,083][org.apache.spark.storage.BlockManager]Removing RDD 168
[INFO][2018-05-25 11:27:55,083][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:27:55,083][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218865000 ms
[INFO][2018-05-25 11:28:00,016][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218880000 ms
[INFO][2018-05-25 11:28:00,017][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218880000 ms.0 from job set of time 1527218880000 ms
[INFO][2018-05-25 11:28:00,028][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:28:00,029][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_150_piece0 on 10.194.32.157:53453 in memory (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:28:00,029][org.apache.spark.scheduler.DAGScheduler]Got job 167 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:28:00,029][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 167 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:28:00,029][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:28:00,029][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:28:00,029][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 167 (MapPartitionsRDD[173] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:28:00,029][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_148_piece0 on 10.194.32.157:53453 in memory (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:28:00,030][org.apache.spark.storage.memory.MemoryStore]Block broadcast_167 stored as values in memory (estimated size 3.1 KB, free 912.2 MB)
[INFO][2018-05-25 11:28:00,031][org.apache.spark.storage.memory.MemoryStore]Block broadcast_167_piece0 stored as bytes in memory (estimated size 1975.0 B, free 912.2 MB)
[INFO][2018-05-25 11:28:00,031][org.apache.spark.storage.BlockManagerInfo]Added broadcast_167_piece0 in memory on 10.194.32.157:53453 (size: 1975.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:28:00,031][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_166_piece0 on 10.194.32.157:53453 in memory (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:28:00,031][org.apache.spark.SparkContext]Created broadcast 167 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:28:00,031][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 167 (MapPartitionsRDD[173] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:28:00,031][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 167.0 with 1 tasks
[INFO][2018-05-25 11:28:00,032][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 167.0 (TID 167, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:28:00,032][org.apache.spark.executor.Executor]Running task 0.0 in stage 167.0 (TID 167)
[INFO][2018-05-25 11:28:00,033][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_147_piece0 on 10.194.32.157:53453 in memory (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:28:00,033][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 13100 -> 13110
[INFO][2018-05-25 11:28:00,033][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_162_piece0 on 10.194.32.157:53453 in memory (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:28:00,034][org.apache.spark.executor.Executor]Finished task 0.0 in stage 167.0 (TID 167). 928 bytes result sent to driver
[INFO][2018-05-25 11:28:00,034][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_159_piece0 on 10.194.32.157:53453 in memory (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:28:00,035][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 167.0 (TID 167) in 2 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:28:00,035][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 167.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:28:00,035][org.apache.spark.scheduler.DAGScheduler]ResultStage 167 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.003 s
[INFO][2018-05-25 11:28:00,035][org.apache.spark.scheduler.DAGScheduler]Job 167 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.006628 s
[INFO][2018-05-25 11:28:00,035][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_153_piece0 on 10.194.32.157:53453 in memory (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:28:00,036][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_154_piece0 on 10.194.32.157:53453 in memory (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:28:00,039][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:73
[INFO][2018-05-25 11:28:00,039][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_155_piece0 on 10.194.32.157:53453 in memory (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:28:00,039][org.apache.spark.scheduler.DAGScheduler]Got job 168 (foreachPartition at ReceiveKafkaData.scala:73) with 1 output partitions
[INFO][2018-05-25 11:28:00,039][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 168 (foreachPartition at ReceiveKafkaData.scala:73)
[INFO][2018-05-25 11:28:00,039][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:28:00,039][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:28:00,039][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 168 (MapPartitionsRDD[173] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:28:00,039][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_152_piece0 on 10.194.32.157:53453 in memory (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:28:00,040][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_160_piece0 on 10.194.32.157:53453 in memory (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:28:00,040][org.apache.spark.storage.memory.MemoryStore]Block broadcast_168 stored as values in memory (estimated size 3.4 KB, free 912.2 MB)
[INFO][2018-05-25 11:28:00,040][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_156_piece0 on 10.194.32.157:53453 in memory (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:28:00,041][org.apache.spark.storage.memory.MemoryStore]Block broadcast_168_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.2 MB)
[INFO][2018-05-25 11:28:00,041][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_149_piece0 on 10.194.32.157:53453 in memory (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:28:00,041][org.apache.spark.storage.BlockManagerInfo]Added broadcast_168_piece0 in memory on 10.194.32.157:53453 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:28:00,041][org.apache.spark.SparkContext]Created broadcast 168 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:28:00,042][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_158_piece0 on 10.194.32.157:53453 in memory (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:28:00,042][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 168 (MapPartitionsRDD[173] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:28:00,042][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 168.0 with 1 tasks
[INFO][2018-05-25 11:28:00,042][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 168.0 (TID 168, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:28:00,042][org.apache.spark.executor.Executor]Running task 0.0 in stage 168.0 (TID 168)
[INFO][2018-05-25 11:28:00,042][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_161_piece0 on 10.194.32.157:53453 in memory (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:28:00,043][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_163_piece0 on 10.194.32.157:53453 in memory (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:28:00,043][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 13100 -> 13110
[INFO][2018-05-25 11:28:00,043][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 13100
[INFO][2018-05-25 11:28:00,043][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_165_piece0 on 10.194.32.157:53453 in memory (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:28:00,044][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_164_piece0 on 10.194.32.157:53453 in memory (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:28:00,044][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_151_piece0 on 10.194.32.157:53453 in memory (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:28:00,045][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_157_piece0 on 10.194.32.157:53453 in memory (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:28:00,083][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-25 11:28:00,084][org.apache.spark.executor.Executor]Finished task 0.0 in stage 168.0 (TID 168). 665 bytes result sent to driver
[INFO][2018-05-25 11:28:00,084][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 168.0 (TID 168) in 42 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:28:00,085][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 168.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:28:00,085][org.apache.spark.scheduler.DAGScheduler]ResultStage 168 (foreachPartition at ReceiveKafkaData.scala:73) finished in 0.043 s
[INFO][2018-05-25 11:28:00,086][org.apache.spark.scheduler.DAGScheduler]Job 168 finished: foreachPartition at ReceiveKafkaData.scala:73, took 0.047013 s
[INFO][2018-05-25 11:28:00,087][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218880000 ms.0 from job set of time 1527218880000 ms
[INFO][2018-05-25 11:28:00,087][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.087 s for time 1527218880000 ms (execution: 0.070 s)
[INFO][2018-05-25 11:28:00,088][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 171 from persistence list
[INFO][2018-05-25 11:28:00,088][org.apache.spark.storage.BlockManager]Removing RDD 171
[INFO][2018-05-25 11:28:00,089][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 170 from persistence list
[INFO][2018-05-25 11:28:00,090][org.apache.spark.storage.BlockManager]Removing RDD 170
[INFO][2018-05-25 11:28:00,090][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:28:00,090][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218870000 ms
[INFO][2018-05-25 11:28:05,027][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218885000 ms
[INFO][2018-05-25 11:28:05,027][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218885000 ms.0 from job set of time 1527218885000 ms
[INFO][2018-05-25 11:28:05,031][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:28:05,031][org.apache.spark.scheduler.DAGScheduler]Got job 169 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:28:05,031][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 169 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:28:05,031][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:28:05,031][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:28:05,031][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 169 (MapPartitionsRDD[175] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:28:05,032][org.apache.spark.storage.memory.MemoryStore]Block broadcast_169 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-25 11:28:05,033][org.apache.spark.storage.memory.MemoryStore]Block broadcast_169_piece0 stored as bytes in memory (estimated size 1972.0 B, free 912.3 MB)
[INFO][2018-05-25 11:28:05,033][org.apache.spark.storage.BlockManagerInfo]Added broadcast_169_piece0 in memory on 10.194.32.157:53453 (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:28:05,033][org.apache.spark.SparkContext]Created broadcast 169 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:28:05,034][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 169 (MapPartitionsRDD[175] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:28:05,034][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 169.0 with 1 tasks
[INFO][2018-05-25 11:28:05,034][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 169.0 (TID 169, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:28:05,034][org.apache.spark.executor.Executor]Running task 0.0 in stage 169.0 (TID 169)
[INFO][2018-05-25 11:28:05,035][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 13110 -> 13120
[INFO][2018-05-25 11:28:05,036][org.apache.spark.executor.Executor]Finished task 0.0 in stage 169.0 (TID 169). 919 bytes result sent to driver
[INFO][2018-05-25 11:28:05,036][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 169.0 (TID 169) in 2 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:28:05,037][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 169.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:28:05,037][org.apache.spark.scheduler.DAGScheduler]ResultStage 169 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.003 s
[INFO][2018-05-25 11:28:05,037][org.apache.spark.scheduler.DAGScheduler]Job 169 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.005908 s
[INFO][2018-05-25 11:28:05,039][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:73
[INFO][2018-05-25 11:28:05,040][org.apache.spark.scheduler.DAGScheduler]Got job 170 (foreachPartition at ReceiveKafkaData.scala:73) with 1 output partitions
[INFO][2018-05-25 11:28:05,040][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 170 (foreachPartition at ReceiveKafkaData.scala:73)
[INFO][2018-05-25 11:28:05,040][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:28:05,040][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:28:05,040][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 170 (MapPartitionsRDD[175] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:28:05,041][org.apache.spark.storage.memory.MemoryStore]Block broadcast_170 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-25 11:28:05,042][org.apache.spark.storage.memory.MemoryStore]Block broadcast_170_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.3 MB)
[INFO][2018-05-25 11:28:05,042][org.apache.spark.storage.BlockManagerInfo]Added broadcast_170_piece0 in memory on 10.194.32.157:53453 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:28:05,042][org.apache.spark.SparkContext]Created broadcast 170 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:28:05,043][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 170 (MapPartitionsRDD[175] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:28:05,043][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 170.0 with 1 tasks
[INFO][2018-05-25 11:28:05,043][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 170.0 (TID 170, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:28:05,043][org.apache.spark.executor.Executor]Running task 0.0 in stage 170.0 (TID 170)
[INFO][2018-05-25 11:28:05,044][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 13110 -> 13120
[INFO][2018-05-25 11:28:05,044][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 13110
[INFO][2018-05-25 11:28:05,087][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-25 11:28:05,088][org.apache.spark.executor.Executor]Finished task 0.0 in stage 170.0 (TID 170). 665 bytes result sent to driver
[INFO][2018-05-25 11:28:05,088][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 170.0 (TID 170) in 45 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:28:05,088][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 170.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:28:05,089][org.apache.spark.scheduler.DAGScheduler]ResultStage 170 (foreachPartition at ReceiveKafkaData.scala:73) finished in 0.046 s
[INFO][2018-05-25 11:28:05,089][org.apache.spark.scheduler.DAGScheduler]Job 170 finished: foreachPartition at ReceiveKafkaData.scala:73, took 0.049464 s
[INFO][2018-05-25 11:28:05,089][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218885000 ms.0 from job set of time 1527218885000 ms
[INFO][2018-05-25 11:28:05,089][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.089 s for time 1527218885000 ms (execution: 0.062 s)
[INFO][2018-05-25 11:28:05,089][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 173 from persistence list
[INFO][2018-05-25 11:28:05,090][org.apache.spark.storage.BlockManager]Removing RDD 173
[INFO][2018-05-25 11:28:05,090][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 172 from persistence list
[INFO][2018-05-25 11:28:05,090][org.apache.spark.storage.BlockManager]Removing RDD 172
[INFO][2018-05-25 11:28:05,090][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:28:05,090][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218875000 ms
[INFO][2018-05-25 11:28:10,013][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218890000 ms
[INFO][2018-05-25 11:28:10,013][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218890000 ms.0 from job set of time 1527218890000 ms
[INFO][2018-05-25 11:28:10,017][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:28:10,017][org.apache.spark.scheduler.DAGScheduler]Got job 171 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:28:10,017][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 171 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:28:10,017][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:28:10,017][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:28:10,017][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 171 (MapPartitionsRDD[177] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:28:10,018][org.apache.spark.storage.memory.MemoryStore]Block broadcast_171 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-25 11:28:10,019][org.apache.spark.storage.memory.MemoryStore]Block broadcast_171_piece0 stored as bytes in memory (estimated size 1971.0 B, free 912.3 MB)
[INFO][2018-05-25 11:28:10,019][org.apache.spark.storage.BlockManagerInfo]Added broadcast_171_piece0 in memory on 10.194.32.157:53453 (size: 1971.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:28:10,020][org.apache.spark.SparkContext]Created broadcast 171 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:28:10,020][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 171 (MapPartitionsRDD[177] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:28:10,020][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 171.0 with 1 tasks
[INFO][2018-05-25 11:28:10,020][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 171.0 (TID 171, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:28:10,020][org.apache.spark.executor.Executor]Running task 0.0 in stage 171.0 (TID 171)
[INFO][2018-05-25 11:28:10,021][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 13120 -> 13130
[INFO][2018-05-25 11:28:10,022][org.apache.spark.executor.Executor]Finished task 0.0 in stage 171.0 (TID 171). 973 bytes result sent to driver
[INFO][2018-05-25 11:28:10,022][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 171.0 (TID 171) in 2 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:28:10,022][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 171.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:28:10,022][org.apache.spark.scheduler.DAGScheduler]ResultStage 171 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.002 s
[INFO][2018-05-25 11:28:10,023][org.apache.spark.scheduler.DAGScheduler]Job 171 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.005727 s
[INFO][2018-05-25 11:28:10,025][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:73
[INFO][2018-05-25 11:28:10,026][org.apache.spark.scheduler.DAGScheduler]Got job 172 (foreachPartition at ReceiveKafkaData.scala:73) with 1 output partitions
[INFO][2018-05-25 11:28:10,026][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 172 (foreachPartition at ReceiveKafkaData.scala:73)
[INFO][2018-05-25 11:28:10,026][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:28:10,026][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:28:10,026][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 172 (MapPartitionsRDD[177] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:28:10,026][org.apache.spark.storage.memory.MemoryStore]Block broadcast_172 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-25 11:28:10,027][org.apache.spark.storage.memory.MemoryStore]Block broadcast_172_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.3 MB)
[INFO][2018-05-25 11:28:10,027][org.apache.spark.storage.BlockManagerInfo]Added broadcast_172_piece0 in memory on 10.194.32.157:53453 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:28:10,027][org.apache.spark.SparkContext]Created broadcast 172 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:28:10,027][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 172 (MapPartitionsRDD[177] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:28:10,028][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 172.0 with 1 tasks
[INFO][2018-05-25 11:28:10,028][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 172.0 (TID 172, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:28:10,028][org.apache.spark.executor.Executor]Running task 0.0 in stage 172.0 (TID 172)
[INFO][2018-05-25 11:28:10,029][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 13120 -> 13130
[INFO][2018-05-25 11:28:10,029][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 13120
[INFO][2018-05-25 11:28:10,072][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-25 11:28:10,073][org.apache.spark.executor.Executor]Finished task 0.0 in stage 172.0 (TID 172). 708 bytes result sent to driver
[INFO][2018-05-25 11:28:10,073][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 172.0 (TID 172) in 45 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:28:10,073][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 172.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:28:10,073][org.apache.spark.scheduler.DAGScheduler]ResultStage 172 (foreachPartition at ReceiveKafkaData.scala:73) finished in 0.045 s
[INFO][2018-05-25 11:28:10,074][org.apache.spark.scheduler.DAGScheduler]Job 172 finished: foreachPartition at ReceiveKafkaData.scala:73, took 0.048251 s
[INFO][2018-05-25 11:28:10,074][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218890000 ms.0 from job set of time 1527218890000 ms
[INFO][2018-05-25 11:28:10,074][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.074 s for time 1527218890000 ms (execution: 0.061 s)
[INFO][2018-05-25 11:28:10,074][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 175 from persistence list
[INFO][2018-05-25 11:28:10,074][org.apache.spark.storage.BlockManager]Removing RDD 175
[INFO][2018-05-25 11:28:10,074][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 174 from persistence list
[INFO][2018-05-25 11:28:10,075][org.apache.spark.storage.BlockManager]Removing RDD 174
[INFO][2018-05-25 11:28:10,075][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:28:10,075][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218880000 ms
[INFO][2018-05-25 11:28:15,018][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218895000 ms
[INFO][2018-05-25 11:28:15,018][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218895000 ms.0 from job set of time 1527218895000 ms
[INFO][2018-05-25 11:28:15,021][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:28:15,022][org.apache.spark.scheduler.DAGScheduler]Got job 173 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:28:15,022][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 173 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:28:15,022][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:28:15,022][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:28:15,022][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 173 (MapPartitionsRDD[179] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:28:15,023][org.apache.spark.storage.memory.MemoryStore]Block broadcast_173 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-25 11:28:15,023][org.apache.spark.storage.memory.MemoryStore]Block broadcast_173_piece0 stored as bytes in memory (estimated size 1973.0 B, free 912.3 MB)
[INFO][2018-05-25 11:28:15,024][org.apache.spark.storage.BlockManagerInfo]Added broadcast_173_piece0 in memory on 10.194.32.157:53453 (size: 1973.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:28:15,024][org.apache.spark.SparkContext]Created broadcast 173 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:28:15,024][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 173 (MapPartitionsRDD[179] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:28:15,024][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 173.0 with 1 tasks
[INFO][2018-05-25 11:28:15,024][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 173.0 (TID 173, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:28:15,025][org.apache.spark.executor.Executor]Running task 0.0 in stage 173.0 (TID 173)
[INFO][2018-05-25 11:28:15,025][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 13130 -> 13140
[INFO][2018-05-25 11:28:15,026][org.apache.spark.executor.Executor]Finished task 0.0 in stage 173.0 (TID 173). 935 bytes result sent to driver
[INFO][2018-05-25 11:28:15,027][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 173.0 (TID 173) in 2 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:28:15,027][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 173.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:28:15,027][org.apache.spark.scheduler.DAGScheduler]ResultStage 173 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.003 s
[INFO][2018-05-25 11:28:15,027][org.apache.spark.scheduler.DAGScheduler]Job 173 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.005674 s
[INFO][2018-05-25 11:28:15,029][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:73
[INFO][2018-05-25 11:28:15,030][org.apache.spark.scheduler.DAGScheduler]Got job 174 (foreachPartition at ReceiveKafkaData.scala:73) with 1 output partitions
[INFO][2018-05-25 11:28:15,030][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 174 (foreachPartition at ReceiveKafkaData.scala:73)
[INFO][2018-05-25 11:28:15,030][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:28:15,030][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:28:15,030][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 174 (MapPartitionsRDD[179] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:28:15,030][org.apache.spark.storage.memory.MemoryStore]Block broadcast_174 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-25 11:28:15,031][org.apache.spark.storage.memory.MemoryStore]Block broadcast_174_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.3 MB)
[INFO][2018-05-25 11:28:15,031][org.apache.spark.storage.BlockManagerInfo]Added broadcast_174_piece0 in memory on 10.194.32.157:53453 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:28:15,031][org.apache.spark.SparkContext]Created broadcast 174 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:28:15,032][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 174 (MapPartitionsRDD[179] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:28:15,032][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 174.0 with 1 tasks
[INFO][2018-05-25 11:28:15,032][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 174.0 (TID 174, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:28:15,032][org.apache.spark.executor.Executor]Running task 0.0 in stage 174.0 (TID 174)
[INFO][2018-05-25 11:28:15,033][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 13130 -> 13140
[INFO][2018-05-25 11:28:15,033][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 13130
[INFO][2018-05-25 11:28:15,076][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-25 11:28:15,077][org.apache.spark.executor.Executor]Finished task 0.0 in stage 174.0 (TID 174). 708 bytes result sent to driver
[INFO][2018-05-25 11:28:15,077][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 174.0 (TID 174) in 45 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:28:15,077][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 174.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:28:15,078][org.apache.spark.scheduler.DAGScheduler]ResultStage 174 (foreachPartition at ReceiveKafkaData.scala:73) finished in 0.046 s
[INFO][2018-05-25 11:28:15,078][org.apache.spark.scheduler.DAGScheduler]Job 174 finished: foreachPartition at ReceiveKafkaData.scala:73, took 0.048255 s
[INFO][2018-05-25 11:28:15,078][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218895000 ms.0 from job set of time 1527218895000 ms
[INFO][2018-05-25 11:28:15,078][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.078 s for time 1527218895000 ms (execution: 0.060 s)
[INFO][2018-05-25 11:28:15,078][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 177 from persistence list
[INFO][2018-05-25 11:28:15,078][org.apache.spark.storage.BlockManager]Removing RDD 177
[INFO][2018-05-25 11:28:15,078][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 176 from persistence list
[INFO][2018-05-25 11:28:15,079][org.apache.spark.storage.BlockManager]Removing RDD 176
[INFO][2018-05-25 11:28:15,079][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:28:15,079][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218885000 ms
[INFO][2018-05-25 11:28:20,014][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218900000 ms
[INFO][2018-05-25 11:28:20,015][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218900000 ms.0 from job set of time 1527218900000 ms
[INFO][2018-05-25 11:28:20,018][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:28:20,019][org.apache.spark.scheduler.DAGScheduler]Got job 175 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:28:20,019][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 175 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:28:20,019][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:28:20,019][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:28:20,019][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 175 (MapPartitionsRDD[181] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:28:20,020][org.apache.spark.storage.memory.MemoryStore]Block broadcast_175 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-25 11:28:20,021][org.apache.spark.storage.memory.MemoryStore]Block broadcast_175_piece0 stored as bytes in memory (estimated size 1972.0 B, free 912.2 MB)
[INFO][2018-05-25 11:28:20,021][org.apache.spark.storage.BlockManagerInfo]Added broadcast_175_piece0 in memory on 10.194.32.157:53453 (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:28:20,021][org.apache.spark.SparkContext]Created broadcast 175 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:28:20,021][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 175 (MapPartitionsRDD[181] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:28:20,021][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 175.0 with 1 tasks
[INFO][2018-05-25 11:28:20,022][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 175.0 (TID 175, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:28:20,022][org.apache.spark.executor.Executor]Running task 0.0 in stage 175.0 (TID 175)
[INFO][2018-05-25 11:28:20,023][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 13140 -> 13150
[INFO][2018-05-25 11:28:20,024][org.apache.spark.executor.Executor]Finished task 0.0 in stage 175.0 (TID 175). 919 bytes result sent to driver
[INFO][2018-05-25 11:28:20,024][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 175.0 (TID 175) in 2 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:28:20,024][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 175.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:28:20,024][org.apache.spark.scheduler.DAGScheduler]ResultStage 175 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.002 s
[INFO][2018-05-25 11:28:20,024][org.apache.spark.scheduler.DAGScheduler]Job 175 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.005781 s
[INFO][2018-05-25 11:28:20,027][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:73
[INFO][2018-05-25 11:28:20,027][org.apache.spark.scheduler.DAGScheduler]Got job 176 (foreachPartition at ReceiveKafkaData.scala:73) with 1 output partitions
[INFO][2018-05-25 11:28:20,027][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 176 (foreachPartition at ReceiveKafkaData.scala:73)
[INFO][2018-05-25 11:28:20,027][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:28:20,027][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:28:20,028][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 176 (MapPartitionsRDD[181] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:28:20,028][org.apache.spark.storage.memory.MemoryStore]Block broadcast_176 stored as values in memory (estimated size 3.4 KB, free 912.2 MB)
[INFO][2018-05-25 11:28:20,029][org.apache.spark.storage.memory.MemoryStore]Block broadcast_176_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.2 MB)
[INFO][2018-05-25 11:28:20,029][org.apache.spark.storage.BlockManagerInfo]Added broadcast_176_piece0 in memory on 10.194.32.157:53453 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:28:20,029][org.apache.spark.SparkContext]Created broadcast 176 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:28:20,029][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 176 (MapPartitionsRDD[181] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:28:20,029][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 176.0 with 1 tasks
[INFO][2018-05-25 11:28:20,030][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 176.0 (TID 176, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:28:20,030][org.apache.spark.executor.Executor]Running task 0.0 in stage 176.0 (TID 176)
[INFO][2018-05-25 11:28:20,030][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 13140 -> 13150
[INFO][2018-05-25 11:28:20,031][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 13140
[INFO][2018-05-25 11:28:20,062][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-25 11:28:20,063][org.apache.spark.executor.Executor]Finished task 0.0 in stage 176.0 (TID 176). 665 bytes result sent to driver
[INFO][2018-05-25 11:28:20,063][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 176.0 (TID 176) in 33 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:28:20,063][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 176.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:28:20,063][org.apache.spark.scheduler.DAGScheduler]ResultStage 176 (foreachPartition at ReceiveKafkaData.scala:73) finished in 0.034 s
[INFO][2018-05-25 11:28:20,063][org.apache.spark.scheduler.DAGScheduler]Job 176 finished: foreachPartition at ReceiveKafkaData.scala:73, took 0.036149 s
[INFO][2018-05-25 11:28:20,064][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218900000 ms.0 from job set of time 1527218900000 ms
[INFO][2018-05-25 11:28:20,064][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.063 s for time 1527218900000 ms (execution: 0.048 s)
[INFO][2018-05-25 11:28:20,064][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 179 from persistence list
[INFO][2018-05-25 11:28:20,064][org.apache.spark.storage.BlockManager]Removing RDD 179
[INFO][2018-05-25 11:28:20,064][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 178 from persistence list
[INFO][2018-05-25 11:28:20,064][org.apache.spark.storage.BlockManager]Removing RDD 178
[INFO][2018-05-25 11:28:20,064][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:28:20,064][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218890000 ms
[INFO][2018-05-25 11:28:25,012][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218905000 ms
[INFO][2018-05-25 11:28:25,012][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218905000 ms.0 from job set of time 1527218905000 ms
[INFO][2018-05-25 11:28:25,016][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:28:25,017][org.apache.spark.scheduler.DAGScheduler]Got job 177 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:28:25,017][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 177 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:28:25,017][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:28:25,017][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:28:25,017][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 177 (MapPartitionsRDD[183] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:28:25,018][org.apache.spark.storage.memory.MemoryStore]Block broadcast_177 stored as values in memory (estimated size 3.1 KB, free 912.2 MB)
[INFO][2018-05-25 11:28:25,019][org.apache.spark.storage.memory.MemoryStore]Block broadcast_177_piece0 stored as bytes in memory (estimated size 1972.0 B, free 912.2 MB)
[INFO][2018-05-25 11:28:25,019][org.apache.spark.storage.BlockManagerInfo]Added broadcast_177_piece0 in memory on 10.194.32.157:53453 (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:28:25,019][org.apache.spark.SparkContext]Created broadcast 177 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:28:25,019][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 177 (MapPartitionsRDD[183] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:28:25,019][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 177.0 with 1 tasks
[INFO][2018-05-25 11:28:25,020][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 177.0 (TID 177, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:28:25,020][org.apache.spark.executor.Executor]Running task 0.0 in stage 177.0 (TID 177)
[INFO][2018-05-25 11:28:25,021][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 13150 -> 13160
[INFO][2018-05-25 11:28:25,022][org.apache.spark.executor.Executor]Finished task 0.0 in stage 177.0 (TID 177). 971 bytes result sent to driver
[INFO][2018-05-25 11:28:25,022][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 177.0 (TID 177) in 2 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:28:25,022][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 177.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:28:25,022][org.apache.spark.scheduler.DAGScheduler]ResultStage 177 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.003 s
[INFO][2018-05-25 11:28:25,022][org.apache.spark.scheduler.DAGScheduler]Job 177 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.006585 s
[INFO][2018-05-25 11:28:25,025][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:73
[INFO][2018-05-25 11:28:25,026][org.apache.spark.scheduler.DAGScheduler]Got job 178 (foreachPartition at ReceiveKafkaData.scala:73) with 1 output partitions
[INFO][2018-05-25 11:28:25,026][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 178 (foreachPartition at ReceiveKafkaData.scala:73)
[INFO][2018-05-25 11:28:25,026][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:28:25,026][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:28:25,026][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 178 (MapPartitionsRDD[183] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:28:25,026][org.apache.spark.storage.memory.MemoryStore]Block broadcast_178 stored as values in memory (estimated size 3.4 KB, free 912.2 MB)
[INFO][2018-05-25 11:28:25,027][org.apache.spark.storage.memory.MemoryStore]Block broadcast_178_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.2 MB)
[INFO][2018-05-25 11:28:25,027][org.apache.spark.storage.BlockManagerInfo]Added broadcast_178_piece0 in memory on 10.194.32.157:53453 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:28:25,027][org.apache.spark.SparkContext]Created broadcast 178 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:28:25,028][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 178 (MapPartitionsRDD[183] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:28:25,028][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 178.0 with 1 tasks
[INFO][2018-05-25 11:28:25,028][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 178.0 (TID 178, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:28:25,028][org.apache.spark.executor.Executor]Running task 0.0 in stage 178.0 (TID 178)
[INFO][2018-05-25 11:28:25,029][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 13150 -> 13160
[INFO][2018-05-25 11:28:25,029][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 13150
[INFO][2018-05-25 11:28:25,066][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-25 11:28:25,067][org.apache.spark.executor.Executor]Finished task 0.0 in stage 178.0 (TID 178). 665 bytes result sent to driver
[INFO][2018-05-25 11:28:25,067][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 178.0 (TID 178) in 39 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:28:25,067][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 178.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:28:25,068][org.apache.spark.scheduler.DAGScheduler]ResultStage 178 (foreachPartition at ReceiveKafkaData.scala:73) finished in 0.040 s
[INFO][2018-05-25 11:28:25,068][org.apache.spark.scheduler.DAGScheduler]Job 178 finished: foreachPartition at ReceiveKafkaData.scala:73, took 0.042528 s
[INFO][2018-05-25 11:28:25,068][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218905000 ms.0 from job set of time 1527218905000 ms
[INFO][2018-05-25 11:28:25,068][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.068 s for time 1527218905000 ms (execution: 0.056 s)
[INFO][2018-05-25 11:28:25,068][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 181 from persistence list
[INFO][2018-05-25 11:28:25,069][org.apache.spark.storage.BlockManager]Removing RDD 181
[INFO][2018-05-25 11:28:25,069][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 180 from persistence list
[INFO][2018-05-25 11:28:25,069][org.apache.spark.storage.BlockManager]Removing RDD 180
[INFO][2018-05-25 11:28:25,069][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:28:25,069][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218895000 ms
[INFO][2018-05-25 11:28:30,014][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218910000 ms
[INFO][2018-05-25 11:28:30,014][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218910000 ms.0 from job set of time 1527218910000 ms
[INFO][2018-05-25 11:28:30,019][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:28:30,020][org.apache.spark.scheduler.DAGScheduler]Got job 179 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:28:30,020][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 179 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:28:30,020][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:28:30,020][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:28:30,020][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 179 (MapPartitionsRDD[185] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:28:30,021][org.apache.spark.storage.memory.MemoryStore]Block broadcast_179 stored as values in memory (estimated size 3.1 KB, free 912.2 MB)
[INFO][2018-05-25 11:28:30,022][org.apache.spark.storage.memory.MemoryStore]Block broadcast_179_piece0 stored as bytes in memory (estimated size 1972.0 B, free 912.2 MB)
[INFO][2018-05-25 11:28:30,022][org.apache.spark.storage.BlockManagerInfo]Added broadcast_179_piece0 in memory on 10.194.32.157:53453 (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:28:30,022][org.apache.spark.SparkContext]Created broadcast 179 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:28:30,023][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 179 (MapPartitionsRDD[185] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:28:30,023][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 179.0 with 1 tasks
[INFO][2018-05-25 11:28:30,023][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 179.0 (TID 179, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:28:30,023][org.apache.spark.executor.Executor]Running task 0.0 in stage 179.0 (TID 179)
[INFO][2018-05-25 11:28:30,024][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 13160 -> 13169
[INFO][2018-05-25 11:28:30,025][org.apache.spark.executor.Executor]Finished task 0.0 in stage 179.0 (TID 179). 931 bytes result sent to driver
[INFO][2018-05-25 11:28:30,025][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 179.0 (TID 179) in 2 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:28:30,025][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 179.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:28:30,026][org.apache.spark.scheduler.DAGScheduler]ResultStage 179 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.002 s
[INFO][2018-05-25 11:28:30,026][org.apache.spark.scheduler.DAGScheduler]Job 179 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.006102 s
[INFO][2018-05-25 11:28:30,029][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:73
[INFO][2018-05-25 11:28:30,029][org.apache.spark.scheduler.DAGScheduler]Got job 180 (foreachPartition at ReceiveKafkaData.scala:73) with 1 output partitions
[INFO][2018-05-25 11:28:30,029][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 180 (foreachPartition at ReceiveKafkaData.scala:73)
[INFO][2018-05-25 11:28:30,029][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:28:30,029][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:28:30,030][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 180 (MapPartitionsRDD[185] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:28:30,030][org.apache.spark.storage.memory.MemoryStore]Block broadcast_180 stored as values in memory (estimated size 3.4 KB, free 912.2 MB)
[INFO][2018-05-25 11:28:30,031][org.apache.spark.storage.memory.MemoryStore]Block broadcast_180_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.2 MB)
[INFO][2018-05-25 11:28:30,031][org.apache.spark.storage.BlockManagerInfo]Added broadcast_180_piece0 in memory on 10.194.32.157:53453 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:28:30,031][org.apache.spark.SparkContext]Created broadcast 180 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:28:30,031][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 180 (MapPartitionsRDD[185] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:28:30,031][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 180.0 with 1 tasks
[INFO][2018-05-25 11:28:30,032][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 180.0 (TID 180, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:28:30,032][org.apache.spark.executor.Executor]Running task 0.0 in stage 180.0 (TID 180)
[INFO][2018-05-25 11:28:30,032][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 13160 -> 13169
[INFO][2018-05-25 11:28:30,032][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 13160
[INFO][2018-05-25 11:28:30,068][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-25 11:28:30,069][org.apache.spark.executor.Executor]Finished task 0.0 in stage 180.0 (TID 180). 665 bytes result sent to driver
[INFO][2018-05-25 11:28:30,069][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 180.0 (TID 180) in 37 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:28:30,069][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 180.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:28:30,070][org.apache.spark.scheduler.DAGScheduler]ResultStage 180 (foreachPartition at ReceiveKafkaData.scala:73) finished in 0.039 s
[INFO][2018-05-25 11:28:30,070][org.apache.spark.scheduler.DAGScheduler]Job 180 finished: foreachPartition at ReceiveKafkaData.scala:73, took 0.040837 s
[INFO][2018-05-25 11:28:30,070][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218910000 ms.0 from job set of time 1527218910000 ms
[INFO][2018-05-25 11:28:30,070][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.070 s for time 1527218910000 ms (execution: 0.056 s)
[INFO][2018-05-25 11:28:30,070][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 183 from persistence list
[INFO][2018-05-25 11:28:30,071][org.apache.spark.storage.BlockManager]Removing RDD 183
[INFO][2018-05-25 11:28:30,071][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 182 from persistence list
[INFO][2018-05-25 11:28:30,071][org.apache.spark.storage.BlockManager]Removing RDD 182
[INFO][2018-05-25 11:28:30,071][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:28:30,071][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218900000 ms
[INFO][2018-05-25 11:28:35,016][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218915000 ms
[INFO][2018-05-25 11:28:35,016][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218915000 ms.0 from job set of time 1527218915000 ms
[INFO][2018-05-25 11:28:35,019][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:28:35,020][org.apache.spark.scheduler.DAGScheduler]Got job 181 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:28:35,020][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 181 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:28:35,020][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:28:35,020][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:28:35,020][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 181 (MapPartitionsRDD[187] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:28:35,021][org.apache.spark.storage.memory.MemoryStore]Block broadcast_181 stored as values in memory (estimated size 3.1 KB, free 912.2 MB)
[INFO][2018-05-25 11:28:35,022][org.apache.spark.storage.memory.MemoryStore]Block broadcast_181_piece0 stored as bytes in memory (estimated size 1972.0 B, free 912.2 MB)
[INFO][2018-05-25 11:28:35,022][org.apache.spark.storage.BlockManagerInfo]Added broadcast_181_piece0 in memory on 10.194.32.157:53453 (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:28:35,022][org.apache.spark.SparkContext]Created broadcast 181 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:28:35,022][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 181 (MapPartitionsRDD[187] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:28:35,022][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 181.0 with 1 tasks
[INFO][2018-05-25 11:28:35,023][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 181.0 (TID 181, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:28:35,023][org.apache.spark.executor.Executor]Running task 0.0 in stage 181.0 (TID 181)
[INFO][2018-05-25 11:28:35,023][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 13169 -> 13179
[INFO][2018-05-25 11:28:35,024][org.apache.spark.executor.Executor]Finished task 0.0 in stage 181.0 (TID 181). 919 bytes result sent to driver
[INFO][2018-05-25 11:28:35,025][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 181.0 (TID 181) in 1 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:28:35,025][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 181.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:28:35,025][org.apache.spark.scheduler.DAGScheduler]ResultStage 181 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.003 s
[INFO][2018-05-25 11:28:35,025][org.apache.spark.scheduler.DAGScheduler]Job 181 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.005567 s
[INFO][2018-05-25 11:28:35,028][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:73
[INFO][2018-05-25 11:28:35,029][org.apache.spark.scheduler.DAGScheduler]Got job 182 (foreachPartition at ReceiveKafkaData.scala:73) with 1 output partitions
[INFO][2018-05-25 11:28:35,029][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 182 (foreachPartition at ReceiveKafkaData.scala:73)
[INFO][2018-05-25 11:28:35,029][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:28:35,029][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:28:35,029][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 182 (MapPartitionsRDD[187] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:28:35,030][org.apache.spark.storage.memory.MemoryStore]Block broadcast_182 stored as values in memory (estimated size 3.4 KB, free 912.2 MB)
[INFO][2018-05-25 11:28:35,031][org.apache.spark.storage.memory.MemoryStore]Block broadcast_182_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.2 MB)
[INFO][2018-05-25 11:28:35,032][org.apache.spark.storage.BlockManagerInfo]Added broadcast_182_piece0 in memory on 10.194.32.157:53453 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:28:35,032][org.apache.spark.SparkContext]Created broadcast 182 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:28:35,032][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 182 (MapPartitionsRDD[187] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:28:35,032][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 182.0 with 1 tasks
[INFO][2018-05-25 11:28:35,033][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 182.0 (TID 182, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:28:35,033][org.apache.spark.executor.Executor]Running task 0.0 in stage 182.0 (TID 182)
[INFO][2018-05-25 11:28:35,034][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 13169 -> 13179
[INFO][2018-05-25 11:28:35,034][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 13169
[INFO][2018-05-25 11:28:35,070][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-25 11:28:35,071][org.apache.spark.executor.Executor]Finished task 0.0 in stage 182.0 (TID 182). 708 bytes result sent to driver
[INFO][2018-05-25 11:28:35,071][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 182.0 (TID 182) in 38 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:28:35,071][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 182.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:28:35,071][org.apache.spark.scheduler.DAGScheduler]ResultStage 182 (foreachPartition at ReceiveKafkaData.scala:73) finished in 0.039 s
[INFO][2018-05-25 11:28:35,072][org.apache.spark.scheduler.DAGScheduler]Job 182 finished: foreachPartition at ReceiveKafkaData.scala:73, took 0.043007 s
[INFO][2018-05-25 11:28:35,072][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218915000 ms.0 from job set of time 1527218915000 ms
[INFO][2018-05-25 11:28:35,072][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.072 s for time 1527218915000 ms (execution: 0.056 s)
[INFO][2018-05-25 11:28:35,072][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 185 from persistence list
[INFO][2018-05-25 11:28:35,072][org.apache.spark.storage.BlockManager]Removing RDD 185
[INFO][2018-05-25 11:28:35,072][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 184 from persistence list
[INFO][2018-05-25 11:28:35,072][org.apache.spark.storage.BlockManager]Removing RDD 184
[INFO][2018-05-25 11:28:35,072][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:28:35,072][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218905000 ms
[INFO][2018-05-25 11:28:40,018][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218920000 ms
[INFO][2018-05-25 11:28:40,018][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218920000 ms.0 from job set of time 1527218920000 ms
[INFO][2018-05-25 11:28:40,021][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:28:40,022][org.apache.spark.scheduler.DAGScheduler]Got job 183 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:28:40,022][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 183 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:28:40,022][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:28:40,022][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:28:40,022][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 183 (MapPartitionsRDD[189] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:28:40,023][org.apache.spark.storage.memory.MemoryStore]Block broadcast_183 stored as values in memory (estimated size 3.1 KB, free 912.2 MB)
[INFO][2018-05-25 11:28:40,024][org.apache.spark.storage.memory.MemoryStore]Block broadcast_183_piece0 stored as bytes in memory (estimated size 1972.0 B, free 912.2 MB)
[INFO][2018-05-25 11:28:40,024][org.apache.spark.storage.BlockManagerInfo]Added broadcast_183_piece0 in memory on 10.194.32.157:53453 (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:28:40,024][org.apache.spark.SparkContext]Created broadcast 183 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:28:40,025][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 183 (MapPartitionsRDD[189] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:28:40,025][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 183.0 with 1 tasks
[INFO][2018-05-25 11:28:40,025][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 183.0 (TID 183, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:28:40,025][org.apache.spark.executor.Executor]Running task 0.0 in stage 183.0 (TID 183)
[INFO][2018-05-25 11:28:40,026][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 13179 -> 13189
[INFO][2018-05-25 11:28:40,027][org.apache.spark.executor.Executor]Finished task 0.0 in stage 183.0 (TID 183). 930 bytes result sent to driver
[INFO][2018-05-25 11:28:40,027][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 183.0 (TID 183) in 2 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:28:40,027][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 183.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:28:40,027][org.apache.spark.scheduler.DAGScheduler]ResultStage 183 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.002 s
[INFO][2018-05-25 11:28:40,027][org.apache.spark.scheduler.DAGScheduler]Job 183 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.005862 s
[INFO][2018-05-25 11:28:40,030][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:73
[INFO][2018-05-25 11:28:40,030][org.apache.spark.scheduler.DAGScheduler]Got job 184 (foreachPartition at ReceiveKafkaData.scala:73) with 1 output partitions
[INFO][2018-05-25 11:28:40,030][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 184 (foreachPartition at ReceiveKafkaData.scala:73)
[INFO][2018-05-25 11:28:40,030][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:28:40,030][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:28:40,030][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 184 (MapPartitionsRDD[189] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:28:40,031][org.apache.spark.storage.memory.MemoryStore]Block broadcast_184 stored as values in memory (estimated size 3.4 KB, free 912.2 MB)
[INFO][2018-05-25 11:28:40,032][org.apache.spark.storage.memory.MemoryStore]Block broadcast_184_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.2 MB)
[INFO][2018-05-25 11:28:40,032][org.apache.spark.storage.BlockManagerInfo]Added broadcast_184_piece0 in memory on 10.194.32.157:53453 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:28:40,032][org.apache.spark.SparkContext]Created broadcast 184 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:28:40,032][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 184 (MapPartitionsRDD[189] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:28:40,032][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 184.0 with 1 tasks
[INFO][2018-05-25 11:28:40,033][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 184.0 (TID 184, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:28:40,033][org.apache.spark.executor.Executor]Running task 0.0 in stage 184.0 (TID 184)
[INFO][2018-05-25 11:28:40,034][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 13179 -> 13189
[INFO][2018-05-25 11:28:40,034][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 13179
[INFO][2018-05-25 11:28:40,072][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-25 11:28:40,072][org.apache.spark.executor.Executor]Finished task 0.0 in stage 184.0 (TID 184). 708 bytes result sent to driver
[INFO][2018-05-25 11:28:40,073][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 184.0 (TID 184) in 40 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:28:40,073][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 184.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:28:40,073][org.apache.spark.scheduler.DAGScheduler]ResultStage 184 (foreachPartition at ReceiveKafkaData.scala:73) finished in 0.040 s
[INFO][2018-05-25 11:28:40,073][org.apache.spark.scheduler.DAGScheduler]Job 184 finished: foreachPartition at ReceiveKafkaData.scala:73, took 0.043321 s
[INFO][2018-05-25 11:28:40,073][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218920000 ms.0 from job set of time 1527218920000 ms
[INFO][2018-05-25 11:28:40,074][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.073 s for time 1527218920000 ms (execution: 0.055 s)
[INFO][2018-05-25 11:28:40,074][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 187 from persistence list
[INFO][2018-05-25 11:28:40,074][org.apache.spark.storage.BlockManager]Removing RDD 187
[INFO][2018-05-25 11:28:40,074][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 186 from persistence list
[INFO][2018-05-25 11:28:40,074][org.apache.spark.storage.BlockManager]Removing RDD 186
[INFO][2018-05-25 11:28:40,074][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:28:40,074][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218910000 ms
[INFO][2018-05-25 11:28:45,022][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218925000 ms
[INFO][2018-05-25 11:28:45,023][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218925000 ms.0 from job set of time 1527218925000 ms
[INFO][2018-05-25 11:28:45,026][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:28:45,027][org.apache.spark.scheduler.DAGScheduler]Got job 185 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:28:45,027][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 185 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:28:45,027][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:28:45,027][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:28:45,027][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 185 (MapPartitionsRDD[191] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:28:45,028][org.apache.spark.storage.memory.MemoryStore]Block broadcast_185 stored as values in memory (estimated size 3.1 KB, free 912.2 MB)
[INFO][2018-05-25 11:28:45,037][org.apache.spark.storage.memory.MemoryStore]Block broadcast_185_piece0 stored as bytes in memory (estimated size 1972.0 B, free 912.2 MB)
[INFO][2018-05-25 11:28:45,037][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_173_piece0 on 10.194.32.157:53453 in memory (size: 1973.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:28:45,037][org.apache.spark.storage.BlockManagerInfo]Added broadcast_185_piece0 in memory on 10.194.32.157:53453 (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:28:45,038][org.apache.spark.SparkContext]Created broadcast 185 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:28:45,038][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 185 (MapPartitionsRDD[191] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:28:45,038][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_183_piece0 on 10.194.32.157:53453 in memory (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:28:45,038][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 185.0 with 1 tasks
[INFO][2018-05-25 11:28:45,038][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 185.0 (TID 185, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:28:45,038][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_168_piece0 on 10.194.32.157:53453 in memory (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:28:45,039][org.apache.spark.executor.Executor]Running task 0.0 in stage 185.0 (TID 185)
[INFO][2018-05-25 11:28:45,039][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_174_piece0 on 10.194.32.157:53453 in memory (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:28:45,039][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 13189 -> 13199
[INFO][2018-05-25 11:28:45,040][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_180_piece0 on 10.194.32.157:53453 in memory (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:28:45,040][org.apache.spark.executor.Executor]Finished task 0.0 in stage 185.0 (TID 185). 918 bytes result sent to driver
[INFO][2018-05-25 11:28:45,040][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_176_piece0 on 10.194.32.157:53453 in memory (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:28:45,041][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 185.0 (TID 185) in 3 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:28:45,041][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 185.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:28:45,041][org.apache.spark.scheduler.DAGScheduler]ResultStage 185 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.003 s
[INFO][2018-05-25 11:28:45,041][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_182_piece0 on 10.194.32.157:53453 in memory (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:28:45,041][org.apache.spark.scheduler.DAGScheduler]Job 185 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.014581 s
[INFO][2018-05-25 11:28:45,042][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_181_piece0 on 10.194.32.157:53453 in memory (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:28:45,045][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:73
[INFO][2018-05-25 11:28:45,045][org.apache.spark.scheduler.DAGScheduler]Got job 186 (foreachPartition at ReceiveKafkaData.scala:73) with 1 output partitions
[INFO][2018-05-25 11:28:45,045][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 186 (foreachPartition at ReceiveKafkaData.scala:73)
[INFO][2018-05-25 11:28:45,045][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:28:45,046][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:28:45,046][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 186 (MapPartitionsRDD[191] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:28:45,047][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_167_piece0 on 10.194.32.157:53453 in memory (size: 1975.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:28:45,048][org.apache.spark.storage.memory.MemoryStore]Block broadcast_186 stored as values in memory (estimated size 3.4 KB, free 912.2 MB)
[INFO][2018-05-25 11:28:45,048][org.apache.spark.storage.memory.MemoryStore]Block broadcast_186_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.2 MB)
[INFO][2018-05-25 11:28:45,049][org.apache.spark.storage.BlockManagerInfo]Added broadcast_186_piece0 in memory on 10.194.32.157:53453 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:28:45,049][org.apache.spark.SparkContext]Created broadcast 186 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:28:45,049][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 186 (MapPartitionsRDD[191] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:28:45,049][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 186.0 with 1 tasks
[INFO][2018-05-25 11:28:45,050][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 186.0 (TID 186, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:28:45,051][org.apache.spark.executor.Executor]Running task 0.0 in stage 186.0 (TID 186)
[INFO][2018-05-25 11:28:45,052][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 13189 -> 13199
[INFO][2018-05-25 11:28:45,052][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 13189
[INFO][2018-05-25 11:28:45,053][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_184_piece0 on 10.194.32.157:53453 in memory (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:28:45,054][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_172_piece0 on 10.194.32.157:53453 in memory (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:28:45,055][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_171_piece0 on 10.194.32.157:53453 in memory (size: 1971.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:28:45,056][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_170_piece0 on 10.194.32.157:53453 in memory (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:28:45,056][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_177_piece0 on 10.194.32.157:53453 in memory (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:28:45,057][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_179_piece0 on 10.194.32.157:53453 in memory (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:28:45,057][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_175_piece0 on 10.194.32.157:53453 in memory (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:28:45,057][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_178_piece0 on 10.194.32.157:53453 in memory (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:28:45,058][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_169_piece0 on 10.194.32.157:53453 in memory (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:28:46,101][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-25 11:28:46,102][org.apache.spark.executor.Executor]Finished task 0.0 in stage 186.0 (TID 186). 708 bytes result sent to driver
[INFO][2018-05-25 11:28:46,103][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 186.0 (TID 186) in 1053 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:28:46,103][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 186.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:28:46,103][org.apache.spark.scheduler.DAGScheduler]ResultStage 186 (foreachPartition at ReceiveKafkaData.scala:73) finished in 1.053 s
[INFO][2018-05-25 11:28:46,104][org.apache.spark.scheduler.DAGScheduler]Job 186 finished: foreachPartition at ReceiveKafkaData.scala:73, took 1.059168 s
[INFO][2018-05-25 11:28:46,105][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218925000 ms.0 from job set of time 1527218925000 ms
[INFO][2018-05-25 11:28:46,105][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 1.105 s for time 1527218925000 ms (execution: 1.082 s)
[INFO][2018-05-25 11:28:46,106][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 189 from persistence list
[INFO][2018-05-25 11:28:46,107][org.apache.spark.storage.BlockManager]Removing RDD 189
[INFO][2018-05-25 11:28:46,107][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 188 from persistence list
[INFO][2018-05-25 11:28:46,107][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:28:46,107][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218915000 ms
[INFO][2018-05-25 11:28:46,107][org.apache.spark.storage.BlockManager]Removing RDD 188
[INFO][2018-05-25 11:28:50,018][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218930000 ms
[INFO][2018-05-25 11:28:50,019][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218930000 ms.0 from job set of time 1527218930000 ms
[INFO][2018-05-25 11:28:50,022][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:28:50,023][org.apache.spark.scheduler.DAGScheduler]Got job 187 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:28:50,023][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 187 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:28:50,023][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:28:50,023][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:28:50,023][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 187 (MapPartitionsRDD[193] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:28:50,024][org.apache.spark.storage.memory.MemoryStore]Block broadcast_187 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-25 11:28:50,024][org.apache.spark.storage.memory.MemoryStore]Block broadcast_187_piece0 stored as bytes in memory (estimated size 1972.0 B, free 912.3 MB)
[INFO][2018-05-25 11:28:50,025][org.apache.spark.storage.BlockManagerInfo]Added broadcast_187_piece0 in memory on 10.194.32.157:53453 (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:28:50,025][org.apache.spark.SparkContext]Created broadcast 187 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:28:50,025][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 187 (MapPartitionsRDD[193] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:28:50,025][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 187.0 with 1 tasks
[INFO][2018-05-25 11:28:50,025][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 187.0 (TID 187, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:28:50,026][org.apache.spark.executor.Executor]Running task 0.0 in stage 187.0 (TID 187)
[INFO][2018-05-25 11:28:50,026][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 13199 -> 13209
[INFO][2018-05-25 11:28:50,027][org.apache.spark.executor.Executor]Finished task 0.0 in stage 187.0 (TID 187). 932 bytes result sent to driver
[INFO][2018-05-25 11:28:50,027][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 187.0 (TID 187) in 2 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:28:50,027][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 187.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:28:50,028][org.apache.spark.scheduler.DAGScheduler]ResultStage 187 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.003 s
[INFO][2018-05-25 11:28:50,028][org.apache.spark.scheduler.DAGScheduler]Job 187 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.005396 s
[INFO][2018-05-25 11:28:50,031][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:73
[INFO][2018-05-25 11:28:50,032][org.apache.spark.scheduler.DAGScheduler]Got job 188 (foreachPartition at ReceiveKafkaData.scala:73) with 1 output partitions
[INFO][2018-05-25 11:28:50,032][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 188 (foreachPartition at ReceiveKafkaData.scala:73)
[INFO][2018-05-25 11:28:50,032][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:28:50,032][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:28:50,032][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 188 (MapPartitionsRDD[193] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:28:50,034][org.apache.spark.storage.memory.MemoryStore]Block broadcast_188 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-25 11:28:50,034][org.apache.spark.storage.memory.MemoryStore]Block broadcast_188_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.3 MB)
[INFO][2018-05-25 11:28:50,034][org.apache.spark.storage.BlockManagerInfo]Added broadcast_188_piece0 in memory on 10.194.32.157:53453 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:28:50,035][org.apache.spark.SparkContext]Created broadcast 188 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:28:50,035][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 188 (MapPartitionsRDD[193] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:28:50,035][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 188.0 with 1 tasks
[INFO][2018-05-25 11:28:50,035][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 188.0 (TID 188, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:28:50,036][org.apache.spark.executor.Executor]Running task 0.0 in stage 188.0 (TID 188)
[INFO][2018-05-25 11:28:50,036][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 13199 -> 13209
[INFO][2018-05-25 11:28:50,036][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 13199
[INFO][2018-05-25 11:28:50,075][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-25 11:28:50,076][org.apache.spark.executor.Executor]Finished task 0.0 in stage 188.0 (TID 188). 665 bytes result sent to driver
[INFO][2018-05-25 11:28:50,076][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 188.0 (TID 188) in 41 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:28:50,076][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 188.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:28:50,076][org.apache.spark.scheduler.DAGScheduler]ResultStage 188 (foreachPartition at ReceiveKafkaData.scala:73) finished in 0.041 s
[INFO][2018-05-25 11:28:50,076][org.apache.spark.scheduler.DAGScheduler]Job 188 finished: foreachPartition at ReceiveKafkaData.scala:73, took 0.044847 s
[INFO][2018-05-25 11:28:50,077][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218930000 ms.0 from job set of time 1527218930000 ms
[INFO][2018-05-25 11:28:50,077][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.077 s for time 1527218930000 ms (execution: 0.058 s)
[INFO][2018-05-25 11:28:50,077][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 191 from persistence list
[INFO][2018-05-25 11:28:50,077][org.apache.spark.storage.BlockManager]Removing RDD 191
[INFO][2018-05-25 11:28:50,077][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 190 from persistence list
[INFO][2018-05-25 11:28:50,077][org.apache.spark.storage.BlockManager]Removing RDD 190
[INFO][2018-05-25 11:28:50,077][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:28:50,077][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218920000 ms
[INFO][2018-05-25 11:28:55,016][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218935000 ms
[INFO][2018-05-25 11:28:55,017][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218935000 ms.0 from job set of time 1527218935000 ms
[INFO][2018-05-25 11:28:55,020][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:28:55,021][org.apache.spark.scheduler.DAGScheduler]Got job 189 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:28:55,021][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 189 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:28:55,021][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:28:55,021][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:28:55,021][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 189 (MapPartitionsRDD[195] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:28:55,022][org.apache.spark.storage.memory.MemoryStore]Block broadcast_189 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-25 11:28:55,023][org.apache.spark.storage.memory.MemoryStore]Block broadcast_189_piece0 stored as bytes in memory (estimated size 1972.0 B, free 912.3 MB)
[INFO][2018-05-25 11:28:55,023][org.apache.spark.storage.BlockManagerInfo]Added broadcast_189_piece0 in memory on 10.194.32.157:53453 (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:28:55,023][org.apache.spark.SparkContext]Created broadcast 189 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:28:55,024][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 189 (MapPartitionsRDD[195] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:28:55,024][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 189.0 with 1 tasks
[INFO][2018-05-25 11:28:55,024][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 189.0 (TID 189, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:28:55,024][org.apache.spark.executor.Executor]Running task 0.0 in stage 189.0 (TID 189)
[INFO][2018-05-25 11:28:55,025][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 13209 -> 13219
[INFO][2018-05-25 11:28:55,026][org.apache.spark.executor.Executor]Finished task 0.0 in stage 189.0 (TID 189). 958 bytes result sent to driver
[INFO][2018-05-25 11:28:55,026][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 189.0 (TID 189) in 2 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:28:55,026][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 189.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:28:55,026][org.apache.spark.scheduler.DAGScheduler]ResultStage 189 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.002 s
[INFO][2018-05-25 11:28:55,027][org.apache.spark.scheduler.DAGScheduler]Job 189 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.006168 s
[INFO][2018-05-25 11:28:55,030][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:73
[INFO][2018-05-25 11:28:55,030][org.apache.spark.scheduler.DAGScheduler]Got job 190 (foreachPartition at ReceiveKafkaData.scala:73) with 1 output partitions
[INFO][2018-05-25 11:28:55,030][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 190 (foreachPartition at ReceiveKafkaData.scala:73)
[INFO][2018-05-25 11:28:55,030][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:28:55,030][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:28:55,030][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 190 (MapPartitionsRDD[195] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:28:55,031][org.apache.spark.storage.memory.MemoryStore]Block broadcast_190 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-25 11:28:55,032][org.apache.spark.storage.memory.MemoryStore]Block broadcast_190_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.3 MB)
[INFO][2018-05-25 11:28:55,032][org.apache.spark.storage.BlockManagerInfo]Added broadcast_190_piece0 in memory on 10.194.32.157:53453 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:28:55,032][org.apache.spark.SparkContext]Created broadcast 190 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:28:55,033][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 190 (MapPartitionsRDD[195] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:28:55,033][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 190.0 with 1 tasks
[INFO][2018-05-25 11:28:55,033][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 190.0 (TID 190, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:28:55,033][org.apache.spark.executor.Executor]Running task 0.0 in stage 190.0 (TID 190)
[INFO][2018-05-25 11:28:55,034][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 13209 -> 13219
[INFO][2018-05-25 11:28:55,034][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 13209
[INFO][2018-05-25 11:28:55,066][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-25 11:28:55,067][org.apache.spark.executor.Executor]Finished task 0.0 in stage 190.0 (TID 190). 708 bytes result sent to driver
[INFO][2018-05-25 11:28:55,067][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 190.0 (TID 190) in 34 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:28:55,067][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 190.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:28:55,067][org.apache.spark.scheduler.DAGScheduler]ResultStage 190 (foreachPartition at ReceiveKafkaData.scala:73) finished in 0.034 s
[INFO][2018-05-25 11:28:55,067][org.apache.spark.scheduler.DAGScheduler]Job 190 finished: foreachPartition at ReceiveKafkaData.scala:73, took 0.037645 s
[INFO][2018-05-25 11:28:55,068][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218935000 ms.0 from job set of time 1527218935000 ms
[INFO][2018-05-25 11:28:55,068][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.068 s for time 1527218935000 ms (execution: 0.051 s)
[INFO][2018-05-25 11:28:55,068][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 193 from persistence list
[INFO][2018-05-25 11:28:55,068][org.apache.spark.storage.BlockManager]Removing RDD 193
[INFO][2018-05-25 11:28:55,068][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 192 from persistence list
[INFO][2018-05-25 11:28:55,068][org.apache.spark.storage.BlockManager]Removing RDD 192
[INFO][2018-05-25 11:28:55,068][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:28:55,068][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218925000 ms
[INFO][2018-05-25 11:29:00,016][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218940000 ms
[INFO][2018-05-25 11:29:00,016][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218940000 ms.0 from job set of time 1527218940000 ms
[INFO][2018-05-25 11:29:00,019][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:29:00,020][org.apache.spark.scheduler.DAGScheduler]Got job 191 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:29:00,020][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 191 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:29:00,020][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:29:00,020][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:29:00,020][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 191 (MapPartitionsRDD[197] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:29:00,022][org.apache.spark.storage.memory.MemoryStore]Block broadcast_191 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-25 11:29:00,022][org.apache.spark.storage.memory.MemoryStore]Block broadcast_191_piece0 stored as bytes in memory (estimated size 1972.0 B, free 912.3 MB)
[INFO][2018-05-25 11:29:00,023][org.apache.spark.storage.BlockManagerInfo]Added broadcast_191_piece0 in memory on 10.194.32.157:53453 (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:29:00,023][org.apache.spark.SparkContext]Created broadcast 191 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:29:00,023][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 191 (MapPartitionsRDD[197] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:29:00,023][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 191.0 with 1 tasks
[INFO][2018-05-25 11:29:00,024][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 191.0 (TID 191, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:29:00,024][org.apache.spark.executor.Executor]Running task 0.0 in stage 191.0 (TID 191)
[INFO][2018-05-25 11:29:00,027][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 13219 -> 13229
[INFO][2018-05-25 11:29:00,028][org.apache.spark.executor.Executor]Finished task 0.0 in stage 191.0 (TID 191). 969 bytes result sent to driver
[INFO][2018-05-25 11:29:00,029][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 191.0 (TID 191) in 5 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:29:00,030][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 191.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:29:00,030][org.apache.spark.scheduler.DAGScheduler]ResultStage 191 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.006 s
[INFO][2018-05-25 11:29:00,031][org.apache.spark.scheduler.DAGScheduler]Job 191 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.011470 s
[INFO][2018-05-25 11:29:00,038][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:73
[INFO][2018-05-25 11:29:00,039][org.apache.spark.scheduler.DAGScheduler]Got job 192 (foreachPartition at ReceiveKafkaData.scala:73) with 1 output partitions
[INFO][2018-05-25 11:29:00,039][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 192 (foreachPartition at ReceiveKafkaData.scala:73)
[INFO][2018-05-25 11:29:00,039][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:29:00,040][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:29:00,040][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 192 (MapPartitionsRDD[197] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:29:00,042][org.apache.spark.storage.memory.MemoryStore]Block broadcast_192 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-25 11:29:00,044][org.apache.spark.storage.memory.MemoryStore]Block broadcast_192_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.3 MB)
[INFO][2018-05-25 11:29:00,044][org.apache.spark.storage.BlockManagerInfo]Added broadcast_192_piece0 in memory on 10.194.32.157:53453 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:29:00,044][org.apache.spark.SparkContext]Created broadcast 192 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:29:00,044][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 192 (MapPartitionsRDD[197] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:29:00,044][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 192.0 with 1 tasks
[INFO][2018-05-25 11:29:00,044][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 192.0 (TID 192, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:29:00,045][org.apache.spark.executor.Executor]Running task 0.0 in stage 192.0 (TID 192)
[INFO][2018-05-25 11:29:00,045][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 13219 -> 13229
[INFO][2018-05-25 11:29:00,045][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 13219
[INFO][2018-05-25 11:29:00,087][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-25 11:29:00,087][org.apache.spark.executor.Executor]Finished task 0.0 in stage 192.0 (TID 192). 665 bytes result sent to driver
[INFO][2018-05-25 11:29:00,088][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 192.0 (TID 192) in 44 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:29:00,088][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 192.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:29:00,088][org.apache.spark.scheduler.DAGScheduler]ResultStage 192 (foreachPartition at ReceiveKafkaData.scala:73) finished in 0.044 s
[INFO][2018-05-25 11:29:00,088][org.apache.spark.scheduler.DAGScheduler]Job 192 finished: foreachPartition at ReceiveKafkaData.scala:73, took 0.050164 s
[INFO][2018-05-25 11:29:00,088][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218940000 ms.0 from job set of time 1527218940000 ms
[INFO][2018-05-25 11:29:00,088][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.088 s for time 1527218940000 ms (execution: 0.072 s)
[INFO][2018-05-25 11:29:00,089][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 195 from persistence list
[INFO][2018-05-25 11:29:00,089][org.apache.spark.storage.BlockManager]Removing RDD 195
[INFO][2018-05-25 11:29:00,089][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 194 from persistence list
[INFO][2018-05-25 11:29:00,089][org.apache.spark.storage.BlockManager]Removing RDD 194
[INFO][2018-05-25 11:29:00,089][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:29:00,089][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218930000 ms
[INFO][2018-05-25 11:29:05,025][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218945000 ms
[INFO][2018-05-25 11:29:05,025][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218945000 ms.0 from job set of time 1527218945000 ms
[INFO][2018-05-25 11:29:05,029][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:29:05,030][org.apache.spark.scheduler.DAGScheduler]Got job 193 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:29:05,030][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 193 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:29:05,030][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:29:05,030][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:29:05,030][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 193 (MapPartitionsRDD[199] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:29:05,031][org.apache.spark.storage.memory.MemoryStore]Block broadcast_193 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-25 11:29:05,032][org.apache.spark.storage.memory.MemoryStore]Block broadcast_193_piece0 stored as bytes in memory (estimated size 1972.0 B, free 912.2 MB)
[INFO][2018-05-25 11:29:05,032][org.apache.spark.storage.BlockManagerInfo]Added broadcast_193_piece0 in memory on 10.194.32.157:53453 (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:29:05,032][org.apache.spark.SparkContext]Created broadcast 193 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:29:05,032][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 193 (MapPartitionsRDD[199] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:29:05,032][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 193.0 with 1 tasks
[INFO][2018-05-25 11:29:05,033][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 193.0 (TID 193, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:29:05,033][org.apache.spark.executor.Executor]Running task 0.0 in stage 193.0 (TID 193)
[INFO][2018-05-25 11:29:05,034][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 13229 -> 13239
[INFO][2018-05-25 11:29:05,035][org.apache.spark.executor.Executor]Finished task 0.0 in stage 193.0 (TID 193). 916 bytes result sent to driver
[INFO][2018-05-25 11:29:05,035][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 193.0 (TID 193) in 2 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:29:05,035][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 193.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:29:05,035][org.apache.spark.scheduler.DAGScheduler]ResultStage 193 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.002 s
[INFO][2018-05-25 11:29:05,035][org.apache.spark.scheduler.DAGScheduler]Job 193 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.006202 s
[INFO][2018-05-25 11:29:05,039][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:73
[INFO][2018-05-25 11:29:05,040][org.apache.spark.scheduler.DAGScheduler]Got job 194 (foreachPartition at ReceiveKafkaData.scala:73) with 1 output partitions
[INFO][2018-05-25 11:29:05,040][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 194 (foreachPartition at ReceiveKafkaData.scala:73)
[INFO][2018-05-25 11:29:05,040][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:29:05,040][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:29:05,040][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 194 (MapPartitionsRDD[199] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:29:05,041][org.apache.spark.storage.memory.MemoryStore]Block broadcast_194 stored as values in memory (estimated size 3.4 KB, free 912.2 MB)
[INFO][2018-05-25 11:29:05,042][org.apache.spark.storage.memory.MemoryStore]Block broadcast_194_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.2 MB)
[INFO][2018-05-25 11:29:05,042][org.apache.spark.storage.BlockManagerInfo]Added broadcast_194_piece0 in memory on 10.194.32.157:53453 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:29:05,042][org.apache.spark.SparkContext]Created broadcast 194 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:29:05,042][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 194 (MapPartitionsRDD[199] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:29:05,042][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 194.0 with 1 tasks
[INFO][2018-05-25 11:29:05,042][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 194.0 (TID 194, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:29:05,043][org.apache.spark.executor.Executor]Running task 0.0 in stage 194.0 (TID 194)
[INFO][2018-05-25 11:29:05,044][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 13229 -> 13239
[INFO][2018-05-25 11:29:05,044][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 13229
[INFO][2018-05-25 11:29:05,080][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-25 11:29:05,080][org.apache.spark.executor.Executor]Finished task 0.0 in stage 194.0 (TID 194). 665 bytes result sent to driver
[INFO][2018-05-25 11:29:05,080][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 194.0 (TID 194) in 38 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:29:05,080][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 194.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:29:05,081][org.apache.spark.scheduler.DAGScheduler]ResultStage 194 (foreachPartition at ReceiveKafkaData.scala:73) finished in 0.039 s
[INFO][2018-05-25 11:29:05,081][org.apache.spark.scheduler.DAGScheduler]Job 194 finished: foreachPartition at ReceiveKafkaData.scala:73, took 0.041377 s
[INFO][2018-05-25 11:29:05,081][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218945000 ms.0 from job set of time 1527218945000 ms
[INFO][2018-05-25 11:29:05,081][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.081 s for time 1527218945000 ms (execution: 0.056 s)
[INFO][2018-05-25 11:29:05,081][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 197 from persistence list
[INFO][2018-05-25 11:29:05,081][org.apache.spark.storage.BlockManager]Removing RDD 197
[INFO][2018-05-25 11:29:05,081][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 196 from persistence list
[INFO][2018-05-25 11:29:05,082][org.apache.spark.storage.BlockManager]Removing RDD 196
[INFO][2018-05-25 11:29:05,082][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:29:05,082][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218935000 ms
[INFO][2018-05-25 11:29:10,018][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218950000 ms
[INFO][2018-05-25 11:29:10,019][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218950000 ms.0 from job set of time 1527218950000 ms
[INFO][2018-05-25 11:29:10,022][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:29:10,023][org.apache.spark.scheduler.DAGScheduler]Got job 195 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:29:10,023][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 195 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:29:10,023][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:29:10,023][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:29:10,023][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 195 (MapPartitionsRDD[201] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:29:10,024][org.apache.spark.storage.memory.MemoryStore]Block broadcast_195 stored as values in memory (estimated size 3.1 KB, free 912.2 MB)
[INFO][2018-05-25 11:29:10,025][org.apache.spark.storage.memory.MemoryStore]Block broadcast_195_piece0 stored as bytes in memory (estimated size 1972.0 B, free 912.2 MB)
[INFO][2018-05-25 11:29:10,025][org.apache.spark.storage.BlockManagerInfo]Added broadcast_195_piece0 in memory on 10.194.32.157:53453 (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:29:10,025][org.apache.spark.SparkContext]Created broadcast 195 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:29:10,026][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 195 (MapPartitionsRDD[201] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:29:10,026][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 195.0 with 1 tasks
[INFO][2018-05-25 11:29:10,027][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 195.0 (TID 195, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:29:10,027][org.apache.spark.executor.Executor]Running task 0.0 in stage 195.0 (TID 195)
[INFO][2018-05-25 11:29:10,028][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 13239 -> 13249
[INFO][2018-05-25 11:29:10,029][org.apache.spark.executor.Executor]Finished task 0.0 in stage 195.0 (TID 195). 932 bytes result sent to driver
[INFO][2018-05-25 11:29:10,029][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 195.0 (TID 195) in 3 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:29:10,029][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 195.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:29:10,030][org.apache.spark.scheduler.DAGScheduler]ResultStage 195 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.003 s
[INFO][2018-05-25 11:29:10,030][org.apache.spark.scheduler.DAGScheduler]Job 195 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.007432 s
[INFO][2018-05-25 11:29:10,033][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:73
[INFO][2018-05-25 11:29:10,033][org.apache.spark.scheduler.DAGScheduler]Got job 196 (foreachPartition at ReceiveKafkaData.scala:73) with 1 output partitions
[INFO][2018-05-25 11:29:10,034][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 196 (foreachPartition at ReceiveKafkaData.scala:73)
[INFO][2018-05-25 11:29:10,034][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:29:10,034][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:29:10,034][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 196 (MapPartitionsRDD[201] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:29:10,035][org.apache.spark.storage.memory.MemoryStore]Block broadcast_196 stored as values in memory (estimated size 3.4 KB, free 912.2 MB)
[INFO][2018-05-25 11:29:10,036][org.apache.spark.storage.memory.MemoryStore]Block broadcast_196_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.2 MB)
[INFO][2018-05-25 11:29:10,036][org.apache.spark.storage.BlockManagerInfo]Added broadcast_196_piece0 in memory on 10.194.32.157:53453 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:29:10,036][org.apache.spark.SparkContext]Created broadcast 196 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:29:10,036][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 196 (MapPartitionsRDD[201] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:29:10,036][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 196.0 with 1 tasks
[INFO][2018-05-25 11:29:10,037][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 196.0 (TID 196, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:29:10,037][org.apache.spark.executor.Executor]Running task 0.0 in stage 196.0 (TID 196)
[INFO][2018-05-25 11:29:10,038][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 13239 -> 13249
[INFO][2018-05-25 11:29:10,038][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 13239
[INFO][2018-05-25 11:29:10,072][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-25 11:29:10,073][org.apache.spark.executor.Executor]Finished task 0.0 in stage 196.0 (TID 196). 708 bytes result sent to driver
[INFO][2018-05-25 11:29:10,073][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 196.0 (TID 196) in 36 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:29:10,073][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 196.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:29:10,074][org.apache.spark.scheduler.DAGScheduler]ResultStage 196 (foreachPartition at ReceiveKafkaData.scala:73) finished in 0.037 s
[INFO][2018-05-25 11:29:10,074][org.apache.spark.scheduler.DAGScheduler]Job 196 finished: foreachPartition at ReceiveKafkaData.scala:73, took 0.040611 s
[INFO][2018-05-25 11:29:10,074][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218950000 ms.0 from job set of time 1527218950000 ms
[INFO][2018-05-25 11:29:10,074][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.074 s for time 1527218950000 ms (execution: 0.055 s)
[INFO][2018-05-25 11:29:10,074][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 199 from persistence list
[INFO][2018-05-25 11:29:10,074][org.apache.spark.storage.BlockManager]Removing RDD 199
[INFO][2018-05-25 11:29:10,075][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 198 from persistence list
[INFO][2018-05-25 11:29:10,075][org.apache.spark.storage.BlockManager]Removing RDD 198
[INFO][2018-05-25 11:29:10,075][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:29:10,075][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218940000 ms
[INFO][2018-05-25 11:29:15,017][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218955000 ms
[INFO][2018-05-25 11:29:15,017][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218955000 ms.0 from job set of time 1527218955000 ms
[INFO][2018-05-25 11:29:15,020][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:29:15,021][org.apache.spark.scheduler.DAGScheduler]Got job 197 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:29:15,021][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 197 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:29:15,021][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:29:15,021][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:29:15,021][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 197 (MapPartitionsRDD[203] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:29:15,022][org.apache.spark.storage.memory.MemoryStore]Block broadcast_197 stored as values in memory (estimated size 3.1 KB, free 912.2 MB)
[INFO][2018-05-25 11:29:15,022][org.apache.spark.storage.memory.MemoryStore]Block broadcast_197_piece0 stored as bytes in memory (estimated size 1972.0 B, free 912.2 MB)
[INFO][2018-05-25 11:29:15,022][org.apache.spark.storage.BlockManagerInfo]Added broadcast_197_piece0 in memory on 10.194.32.157:53453 (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:29:15,023][org.apache.spark.SparkContext]Created broadcast 197 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:29:15,023][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 197 (MapPartitionsRDD[203] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:29:15,023][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 197.0 with 1 tasks
[INFO][2018-05-25 11:29:15,023][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 197.0 (TID 197, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:29:15,024][org.apache.spark.executor.Executor]Running task 0.0 in stage 197.0 (TID 197)
[INFO][2018-05-25 11:29:15,024][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 13249 -> 13259
[INFO][2018-05-25 11:29:15,025][org.apache.spark.executor.Executor]Finished task 0.0 in stage 197.0 (TID 197). 927 bytes result sent to driver
[INFO][2018-05-25 11:29:15,026][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 197.0 (TID 197) in 3 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:29:15,026][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 197.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:29:15,026][org.apache.spark.scheduler.DAGScheduler]ResultStage 197 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.003 s
[INFO][2018-05-25 11:29:15,026][org.apache.spark.scheduler.DAGScheduler]Job 197 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.005495 s
[INFO][2018-05-25 11:29:15,028][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:73
[INFO][2018-05-25 11:29:15,029][org.apache.spark.scheduler.DAGScheduler]Got job 198 (foreachPartition at ReceiveKafkaData.scala:73) with 1 output partitions
[INFO][2018-05-25 11:29:15,029][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 198 (foreachPartition at ReceiveKafkaData.scala:73)
[INFO][2018-05-25 11:29:15,029][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:29:15,029][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:29:15,029][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 198 (MapPartitionsRDD[203] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:29:15,030][org.apache.spark.storage.memory.MemoryStore]Block broadcast_198 stored as values in memory (estimated size 3.4 KB, free 912.2 MB)
[INFO][2018-05-25 11:29:15,030][org.apache.spark.storage.memory.MemoryStore]Block broadcast_198_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.2 MB)
[INFO][2018-05-25 11:29:15,030][org.apache.spark.storage.BlockManagerInfo]Added broadcast_198_piece0 in memory on 10.194.32.157:53453 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:29:15,030][org.apache.spark.SparkContext]Created broadcast 198 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:29:15,031][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 198 (MapPartitionsRDD[203] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:29:15,031][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 198.0 with 1 tasks
[INFO][2018-05-25 11:29:15,031][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 198.0 (TID 198, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:29:15,031][org.apache.spark.executor.Executor]Running task 0.0 in stage 198.0 (TID 198)
[INFO][2018-05-25 11:29:15,032][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 13249 -> 13259
[INFO][2018-05-25 11:29:15,032][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 13249
[INFO][2018-05-25 11:29:15,066][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-25 11:29:15,067][org.apache.spark.executor.Executor]Finished task 0.0 in stage 198.0 (TID 198). 708 bytes result sent to driver
[INFO][2018-05-25 11:29:15,067][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 198.0 (TID 198) in 36 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:29:15,067][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 198.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:29:15,068][org.apache.spark.scheduler.DAGScheduler]ResultStage 198 (foreachPartition at ReceiveKafkaData.scala:73) finished in 0.036 s
[INFO][2018-05-25 11:29:15,068][org.apache.spark.scheduler.DAGScheduler]Job 198 finished: foreachPartition at ReceiveKafkaData.scala:73, took 0.039160 s
[INFO][2018-05-25 11:29:15,068][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218955000 ms.0 from job set of time 1527218955000 ms
[INFO][2018-05-25 11:29:15,068][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.068 s for time 1527218955000 ms (execution: 0.051 s)
[INFO][2018-05-25 11:29:15,068][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 201 from persistence list
[INFO][2018-05-25 11:29:15,068][org.apache.spark.storage.BlockManager]Removing RDD 201
[INFO][2018-05-25 11:29:15,069][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 200 from persistence list
[INFO][2018-05-25 11:29:15,069][org.apache.spark.storage.BlockManager]Removing RDD 200
[INFO][2018-05-25 11:29:15,069][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:29:15,069][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218945000 ms
[INFO][2018-05-25 11:29:20,016][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218960000 ms
[INFO][2018-05-25 11:29:20,016][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218960000 ms.0 from job set of time 1527218960000 ms
[INFO][2018-05-25 11:29:20,020][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:29:20,021][org.apache.spark.scheduler.DAGScheduler]Got job 199 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:29:20,021][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 199 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:29:20,021][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:29:20,021][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:29:20,021][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 199 (MapPartitionsRDD[205] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:29:20,022][org.apache.spark.storage.memory.MemoryStore]Block broadcast_199 stored as values in memory (estimated size 3.1 KB, free 912.2 MB)
[INFO][2018-05-25 11:29:20,023][org.apache.spark.storage.memory.MemoryStore]Block broadcast_199_piece0 stored as bytes in memory (estimated size 1972.0 B, free 912.2 MB)
[INFO][2018-05-25 11:29:20,023][org.apache.spark.storage.BlockManagerInfo]Added broadcast_199_piece0 in memory on 10.194.32.157:53453 (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:29:20,023][org.apache.spark.SparkContext]Created broadcast 199 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:29:20,023][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 199 (MapPartitionsRDD[205] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:29:20,023][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 199.0 with 1 tasks
[INFO][2018-05-25 11:29:20,024][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 199.0 (TID 199, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:29:20,024][org.apache.spark.executor.Executor]Running task 0.0 in stage 199.0 (TID 199)
[INFO][2018-05-25 11:29:20,025][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 13259 -> 13269
[INFO][2018-05-25 11:29:20,026][org.apache.spark.executor.Executor]Finished task 0.0 in stage 199.0 (TID 199). 955 bytes result sent to driver
[INFO][2018-05-25 11:29:20,026][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 199.0 (TID 199) in 2 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:29:20,026][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 199.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:29:20,027][org.apache.spark.scheduler.DAGScheduler]ResultStage 199 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.003 s
[INFO][2018-05-25 11:29:20,027][org.apache.spark.scheduler.DAGScheduler]Job 199 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.006297 s
[INFO][2018-05-25 11:29:20,029][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:73
[INFO][2018-05-25 11:29:20,029][org.apache.spark.scheduler.DAGScheduler]Got job 200 (foreachPartition at ReceiveKafkaData.scala:73) with 1 output partitions
[INFO][2018-05-25 11:29:20,029][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 200 (foreachPartition at ReceiveKafkaData.scala:73)
[INFO][2018-05-25 11:29:20,030][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:29:20,030][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:29:20,030][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 200 (MapPartitionsRDD[205] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:29:20,030][org.apache.spark.storage.memory.MemoryStore]Block broadcast_200 stored as values in memory (estimated size 3.4 KB, free 912.2 MB)
[INFO][2018-05-25 11:29:20,031][org.apache.spark.storage.memory.MemoryStore]Block broadcast_200_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.2 MB)
[INFO][2018-05-25 11:29:20,031][org.apache.spark.storage.BlockManagerInfo]Added broadcast_200_piece0 in memory on 10.194.32.157:53453 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:29:20,031][org.apache.spark.SparkContext]Created broadcast 200 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:29:20,032][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 200 (MapPartitionsRDD[205] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:29:20,032][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 200.0 with 1 tasks
[INFO][2018-05-25 11:29:20,032][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 200.0 (TID 200, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:29:20,032][org.apache.spark.executor.Executor]Running task 0.0 in stage 200.0 (TID 200)
[INFO][2018-05-25 11:29:20,033][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 13259 -> 13269
[INFO][2018-05-25 11:29:20,033][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 13259
[INFO][2018-05-25 11:29:20,067][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-25 11:29:20,068][org.apache.spark.executor.Executor]Finished task 0.0 in stage 200.0 (TID 200). 665 bytes result sent to driver
[INFO][2018-05-25 11:29:20,068][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 200.0 (TID 200) in 36 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:29:20,068][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 200.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:29:20,068][org.apache.spark.scheduler.DAGScheduler]ResultStage 200 (foreachPartition at ReceiveKafkaData.scala:73) finished in 0.036 s
[INFO][2018-05-25 11:29:20,069][org.apache.spark.scheduler.DAGScheduler]Job 200 finished: foreachPartition at ReceiveKafkaData.scala:73, took 0.039383 s
[INFO][2018-05-25 11:29:20,069][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218960000 ms.0 from job set of time 1527218960000 ms
[INFO][2018-05-25 11:29:20,069][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.069 s for time 1527218960000 ms (execution: 0.053 s)
[INFO][2018-05-25 11:29:20,069][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 203 from persistence list
[INFO][2018-05-25 11:29:20,069][org.apache.spark.storage.BlockManager]Removing RDD 203
[INFO][2018-05-25 11:29:20,069][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 202 from persistence list
[INFO][2018-05-25 11:29:20,069][org.apache.spark.storage.BlockManager]Removing RDD 202
[INFO][2018-05-25 11:29:20,070][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:29:20,070][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218950000 ms
[INFO][2018-05-25 11:29:25,014][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218965000 ms
[INFO][2018-05-25 11:29:25,014][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218965000 ms.0 from job set of time 1527218965000 ms
[INFO][2018-05-25 11:29:25,018][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:29:25,019][org.apache.spark.scheduler.DAGScheduler]Got job 201 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:29:25,019][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 201 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:29:25,019][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:29:25,019][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:29:25,019][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 201 (MapPartitionsRDD[207] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:29:25,020][org.apache.spark.storage.memory.MemoryStore]Block broadcast_201 stored as values in memory (estimated size 3.1 KB, free 912.2 MB)
[INFO][2018-05-25 11:29:25,020][org.apache.spark.storage.memory.MemoryStore]Block broadcast_201_piece0 stored as bytes in memory (estimated size 1972.0 B, free 912.2 MB)
[INFO][2018-05-25 11:29:25,021][org.apache.spark.storage.BlockManagerInfo]Added broadcast_201_piece0 in memory on 10.194.32.157:53453 (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:29:25,021][org.apache.spark.SparkContext]Created broadcast 201 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:29:25,021][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 201 (MapPartitionsRDD[207] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:29:25,021][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 201.0 with 1 tasks
[INFO][2018-05-25 11:29:25,021][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 201.0 (TID 201, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:29:25,022][org.apache.spark.executor.Executor]Running task 0.0 in stage 201.0 (TID 201)
[INFO][2018-05-25 11:29:25,022][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 13269 -> 13279
[INFO][2018-05-25 11:29:25,023][org.apache.spark.executor.Executor]Finished task 0.0 in stage 201.0 (TID 201). 931 bytes result sent to driver
[INFO][2018-05-25 11:29:25,023][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 201.0 (TID 201) in 2 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:29:25,023][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 201.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:29:25,024][org.apache.spark.scheduler.DAGScheduler]ResultStage 201 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.003 s
[INFO][2018-05-25 11:29:25,024][org.apache.spark.scheduler.DAGScheduler]Job 201 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.005319 s
[INFO][2018-05-25 11:29:25,026][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:73
[INFO][2018-05-25 11:29:25,027][org.apache.spark.scheduler.DAGScheduler]Got job 202 (foreachPartition at ReceiveKafkaData.scala:73) with 1 output partitions
[INFO][2018-05-25 11:29:25,027][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 202 (foreachPartition at ReceiveKafkaData.scala:73)
[INFO][2018-05-25 11:29:25,027][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:29:25,027][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:29:25,027][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 202 (MapPartitionsRDD[207] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:29:25,027][org.apache.spark.storage.memory.MemoryStore]Block broadcast_202 stored as values in memory (estimated size 3.4 KB, free 912.2 MB)
[INFO][2018-05-25 11:29:25,028][org.apache.spark.storage.memory.MemoryStore]Block broadcast_202_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.2 MB)
[INFO][2018-05-25 11:29:25,028][org.apache.spark.storage.BlockManagerInfo]Added broadcast_202_piece0 in memory on 10.194.32.157:53453 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:29:25,029][org.apache.spark.SparkContext]Created broadcast 202 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:29:25,029][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 202 (MapPartitionsRDD[207] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:29:25,029][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 202.0 with 1 tasks
[INFO][2018-05-25 11:29:25,029][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 202.0 (TID 202, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:29:25,030][org.apache.spark.executor.Executor]Running task 0.0 in stage 202.0 (TID 202)
[INFO][2018-05-25 11:29:25,030][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 13269 -> 13279
[INFO][2018-05-25 11:29:25,030][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 13269
[INFO][2018-05-25 11:29:25,061][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-25 11:29:25,061][org.apache.spark.executor.Executor]Finished task 0.0 in stage 202.0 (TID 202). 708 bytes result sent to driver
[INFO][2018-05-25 11:29:25,061][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 202.0 (TID 202) in 32 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:29:25,061][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 202.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:29:25,062][org.apache.spark.scheduler.DAGScheduler]ResultStage 202 (foreachPartition at ReceiveKafkaData.scala:73) finished in 0.032 s
[INFO][2018-05-25 11:29:25,062][org.apache.spark.scheduler.DAGScheduler]Job 202 finished: foreachPartition at ReceiveKafkaData.scala:73, took 0.035332 s
[INFO][2018-05-25 11:29:25,062][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218965000 ms.0 from job set of time 1527218965000 ms
[INFO][2018-05-25 11:29:25,062][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.062 s for time 1527218965000 ms (execution: 0.048 s)
[INFO][2018-05-25 11:29:25,062][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 205 from persistence list
[INFO][2018-05-25 11:29:25,062][org.apache.spark.storage.BlockManager]Removing RDD 205
[INFO][2018-05-25 11:29:25,062][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 204 from persistence list
[INFO][2018-05-25 11:29:25,062][org.apache.spark.storage.BlockManager]Removing RDD 204
[INFO][2018-05-25 11:29:25,063][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:29:25,063][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218955000 ms
[INFO][2018-05-25 11:29:30,017][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218970000 ms
[INFO][2018-05-25 11:29:30,017][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218970000 ms.0 from job set of time 1527218970000 ms
[INFO][2018-05-25 11:29:30,021][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:29:30,021][org.apache.spark.scheduler.DAGScheduler]Got job 203 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:29:30,021][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 203 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:29:30,021][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:29:30,021][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:29:30,021][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 203 (MapPartitionsRDD[209] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:29:30,022][org.apache.spark.storage.memory.MemoryStore]Block broadcast_203 stored as values in memory (estimated size 3.1 KB, free 912.2 MB)
[INFO][2018-05-25 11:29:30,030][org.apache.spark.storage.memory.MemoryStore]Block broadcast_203_piece0 stored as bytes in memory (estimated size 1972.0 B, free 912.2 MB)
[INFO][2018-05-25 11:29:30,030][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_196_piece0 on 10.194.32.157:53453 in memory (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:29:30,030][org.apache.spark.storage.BlockManagerInfo]Added broadcast_203_piece0 in memory on 10.194.32.157:53453 (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:29:30,030][org.apache.spark.SparkContext]Created broadcast 203 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:29:30,030][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_188_piece0 on 10.194.32.157:53453 in memory (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:29:30,030][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 203 (MapPartitionsRDD[209] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:29:30,030][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 203.0 with 1 tasks
[INFO][2018-05-25 11:29:30,031][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_189_piece0 on 10.194.32.157:53453 in memory (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:29:30,031][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 203.0 (TID 203, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:29:30,031][org.apache.spark.executor.Executor]Running task 0.0 in stage 203.0 (TID 203)
[INFO][2018-05-25 11:29:30,031][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_192_piece0 on 10.194.32.157:53453 in memory (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:29:30,032][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 13279 -> 13289
[INFO][2018-05-25 11:29:30,032][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_200_piece0 on 10.194.32.157:53453 in memory (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:29:30,033][org.apache.spark.executor.Executor]Finished task 0.0 in stage 203.0 (TID 203). 962 bytes result sent to driver
[INFO][2018-05-25 11:29:30,033][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_198_piece0 on 10.194.32.157:53453 in memory (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:29:30,033][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 203.0 (TID 203) in 2 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:29:30,033][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 203.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:29:30,033][org.apache.spark.scheduler.DAGScheduler]ResultStage 203 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.003 s
[INFO][2018-05-25 11:29:30,033][org.apache.spark.scheduler.DAGScheduler]Job 203 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.012586 s
[INFO][2018-05-25 11:29:30,034][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_197_piece0 on 10.194.32.157:53453 in memory (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:29:30,034][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_185_piece0 on 10.194.32.157:53453 in memory (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:29:30,035][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_202_piece0 on 10.194.32.157:53453 in memory (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:29:30,035][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_190_piece0 on 10.194.32.157:53453 in memory (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:29:30,036][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_199_piece0 on 10.194.32.157:53453 in memory (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:29:30,036][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_187_piece0 on 10.194.32.157:53453 in memory (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:29:30,037][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_186_piece0 on 10.194.32.157:53453 in memory (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:29:30,037][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:73
[INFO][2018-05-25 11:29:30,037][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_194_piece0 on 10.194.32.157:53453 in memory (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:29:30,037][org.apache.spark.scheduler.DAGScheduler]Got job 204 (foreachPartition at ReceiveKafkaData.scala:73) with 1 output partitions
[INFO][2018-05-25 11:29:30,037][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 204 (foreachPartition at ReceiveKafkaData.scala:73)
[INFO][2018-05-25 11:29:30,037][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:29:30,037][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:29:30,037][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 204 (MapPartitionsRDD[209] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:29:30,037][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_191_piece0 on 10.194.32.157:53453 in memory (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:29:30,038][org.apache.spark.storage.memory.MemoryStore]Block broadcast_204 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-25 11:29:30,038][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_195_piece0 on 10.194.32.157:53453 in memory (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:29:30,039][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_201_piece0 on 10.194.32.157:53453 in memory (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:29:30,039][org.apache.spark.storage.memory.MemoryStore]Block broadcast_204_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.3 MB)
[INFO][2018-05-25 11:29:30,039][org.apache.spark.storage.BlockManagerInfo]Added broadcast_204_piece0 in memory on 10.194.32.157:53453 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:29:30,039][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_193_piece0 on 10.194.32.157:53453 in memory (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:29:30,039][org.apache.spark.SparkContext]Created broadcast 204 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:29:30,039][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 204 (MapPartitionsRDD[209] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:29:30,040][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 204.0 with 1 tasks
[INFO][2018-05-25 11:29:30,040][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 204.0 (TID 204, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:29:30,040][org.apache.spark.executor.Executor]Running task 0.0 in stage 204.0 (TID 204)
[INFO][2018-05-25 11:29:30,041][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 13279 -> 13289
[INFO][2018-05-25 11:29:30,041][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 13279
[INFO][2018-05-25 11:29:30,078][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-25 11:29:30,078][org.apache.spark.executor.Executor]Finished task 0.0 in stage 204.0 (TID 204). 708 bytes result sent to driver
[INFO][2018-05-25 11:29:30,079][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 204.0 (TID 204) in 39 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:29:30,079][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 204.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:29:30,079][org.apache.spark.scheduler.DAGScheduler]ResultStage 204 (foreachPartition at ReceiveKafkaData.scala:73) finished in 0.039 s
[INFO][2018-05-25 11:29:30,079][org.apache.spark.scheduler.DAGScheduler]Job 204 finished: foreachPartition at ReceiveKafkaData.scala:73, took 0.042187 s
[INFO][2018-05-25 11:29:30,079][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218970000 ms.0 from job set of time 1527218970000 ms
[INFO][2018-05-25 11:29:30,079][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.079 s for time 1527218970000 ms (execution: 0.062 s)
[INFO][2018-05-25 11:29:30,079][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 207 from persistence list
[INFO][2018-05-25 11:29:30,080][org.apache.spark.storage.BlockManager]Removing RDD 207
[INFO][2018-05-25 11:29:30,080][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 206 from persistence list
[INFO][2018-05-25 11:29:30,080][org.apache.spark.storage.BlockManager]Removing RDD 206
[INFO][2018-05-25 11:29:30,080][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:29:30,080][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218960000 ms
[INFO][2018-05-25 11:29:35,013][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218975000 ms
[INFO][2018-05-25 11:29:35,013][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218975000 ms.0 from job set of time 1527218975000 ms
[INFO][2018-05-25 11:29:35,017][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:29:35,017][org.apache.spark.scheduler.DAGScheduler]Got job 205 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:29:35,017][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 205 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:29:35,017][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:29:35,017][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:29:35,017][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 205 (MapPartitionsRDD[211] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:29:35,018][org.apache.spark.storage.memory.MemoryStore]Block broadcast_205 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-25 11:29:35,019][org.apache.spark.storage.memory.MemoryStore]Block broadcast_205_piece0 stored as bytes in memory (estimated size 1972.0 B, free 912.3 MB)
[INFO][2018-05-25 11:29:35,019][org.apache.spark.storage.BlockManagerInfo]Added broadcast_205_piece0 in memory on 10.194.32.157:53453 (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:29:35,019][org.apache.spark.SparkContext]Created broadcast 205 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:29:35,020][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 205 (MapPartitionsRDD[211] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:29:35,020][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 205.0 with 1 tasks
[INFO][2018-05-25 11:29:35,020][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 205.0 (TID 205, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:29:35,020][org.apache.spark.executor.Executor]Running task 0.0 in stage 205.0 (TID 205)
[INFO][2018-05-25 11:29:35,021][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 13289 -> 13298
[INFO][2018-05-25 11:29:35,022][org.apache.spark.executor.Executor]Finished task 0.0 in stage 205.0 (TID 205). 972 bytes result sent to driver
[INFO][2018-05-25 11:29:35,023][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 205.0 (TID 205) in 3 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:29:35,023][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 205.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:29:35,023][org.apache.spark.scheduler.DAGScheduler]ResultStage 205 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.003 s
[INFO][2018-05-25 11:29:35,023][org.apache.spark.scheduler.DAGScheduler]Job 205 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.006198 s
[INFO][2018-05-25 11:29:35,026][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:73
[INFO][2018-05-25 11:29:35,026][org.apache.spark.scheduler.DAGScheduler]Got job 206 (foreachPartition at ReceiveKafkaData.scala:73) with 1 output partitions
[INFO][2018-05-25 11:29:35,026][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 206 (foreachPartition at ReceiveKafkaData.scala:73)
[INFO][2018-05-25 11:29:35,026][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:29:35,026][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:29:35,026][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 206 (MapPartitionsRDD[211] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:29:35,027][org.apache.spark.storage.memory.MemoryStore]Block broadcast_206 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-25 11:29:35,027][org.apache.spark.storage.memory.MemoryStore]Block broadcast_206_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.3 MB)
[INFO][2018-05-25 11:29:35,027][org.apache.spark.storage.BlockManagerInfo]Added broadcast_206_piece0 in memory on 10.194.32.157:53453 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:29:35,027][org.apache.spark.SparkContext]Created broadcast 206 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:29:35,028][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 206 (MapPartitionsRDD[211] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:29:35,028][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 206.0 with 1 tasks
[INFO][2018-05-25 11:29:35,028][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 206.0 (TID 206, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:29:35,028][org.apache.spark.executor.Executor]Running task 0.0 in stage 206.0 (TID 206)
[INFO][2018-05-25 11:29:35,029][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 13289 -> 13298
[INFO][2018-05-25 11:29:35,029][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 13289
[INFO][2018-05-25 11:29:35,063][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-25 11:29:35,063][org.apache.spark.executor.Executor]Finished task 0.0 in stage 206.0 (TID 206). 708 bytes result sent to driver
[INFO][2018-05-25 11:29:35,063][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 206.0 (TID 206) in 35 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:29:35,064][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 206.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:29:35,064][org.apache.spark.scheduler.DAGScheduler]ResultStage 206 (foreachPartition at ReceiveKafkaData.scala:73) finished in 0.036 s
[INFO][2018-05-25 11:29:35,064][org.apache.spark.scheduler.DAGScheduler]Job 206 finished: foreachPartition at ReceiveKafkaData.scala:73, took 0.038265 s
[INFO][2018-05-25 11:29:35,064][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218975000 ms.0 from job set of time 1527218975000 ms
[INFO][2018-05-25 11:29:35,064][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.064 s for time 1527218975000 ms (execution: 0.051 s)
[INFO][2018-05-25 11:29:35,064][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 209 from persistence list
[INFO][2018-05-25 11:29:35,064][org.apache.spark.storage.BlockManager]Removing RDD 209
[INFO][2018-05-25 11:29:35,064][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 208 from persistence list
[INFO][2018-05-25 11:29:35,065][org.apache.spark.storage.BlockManager]Removing RDD 208
[INFO][2018-05-25 11:29:35,065][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:29:35,065][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218965000 ms
[INFO][2018-05-25 11:29:38,052][org.apache.kafka.clients.producer.KafkaProducer]Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
[INFO][2018-05-25 11:29:38,058][org.apache.spark.util.ShutdownHookManager]Shutdown hook called
[INFO][2018-05-25 11:29:38,059][org.apache.spark.util.ShutdownHookManager]Deleting directory /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/spark-608b2229-6852-41c4-8a87-6bfd466bb643
[INFO][2018-05-25 11:29:40,014][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218980000 ms
[INFO][2018-05-25 11:29:40,020][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218980000 ms.0 from job set of time 1527218980000 ms
[INFO][2018-05-25 11:29:40,024][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:29:40,025][org.apache.spark.scheduler.DAGScheduler]Got job 207 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:29:40,025][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 207 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:29:40,025][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:29:40,025][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:29:40,025][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 207 (MapPartitionsRDD[213] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:29:40,026][org.apache.spark.storage.memory.MemoryStore]Block broadcast_207 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-25 11:29:40,027][org.apache.spark.storage.memory.MemoryStore]Block broadcast_207_piece0 stored as bytes in memory (estimated size 1972.0 B, free 912.3 MB)
[INFO][2018-05-25 11:29:40,027][org.apache.spark.storage.BlockManagerInfo]Added broadcast_207_piece0 in memory on 10.194.32.157:53453 (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:29:40,027][org.apache.spark.SparkContext]Created broadcast 207 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:29:40,028][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 207 (MapPartitionsRDD[213] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:29:40,028][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 207.0 with 1 tasks
[INFO][2018-05-25 11:29:40,028][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 207.0 (TID 207, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:29:40,028][org.apache.spark.executor.Executor]Running task 0.0 in stage 207.0 (TID 207)
[INFO][2018-05-25 11:29:40,029][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 13298 -> 13304
[INFO][2018-05-25 11:29:40,030][org.apache.spark.executor.Executor]Finished task 0.0 in stage 207.0 (TID 207). 966 bytes result sent to driver
[INFO][2018-05-25 11:29:40,031][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 207.0 (TID 207) in 3 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:29:40,031][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 207.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:29:40,031][org.apache.spark.scheduler.DAGScheduler]ResultStage 207 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.003 s
[INFO][2018-05-25 11:29:40,031][org.apache.spark.scheduler.DAGScheduler]Job 207 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.006910 s
[INFO][2018-05-25 11:29:40,039][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:73
[INFO][2018-05-25 11:29:40,040][org.apache.spark.scheduler.DAGScheduler]Got job 208 (foreachPartition at ReceiveKafkaData.scala:73) with 1 output partitions
[INFO][2018-05-25 11:29:40,040][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 208 (foreachPartition at ReceiveKafkaData.scala:73)
[INFO][2018-05-25 11:29:40,040][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:29:40,040][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:29:40,041][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 208 (MapPartitionsRDD[213] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:29:40,042][org.apache.spark.storage.memory.MemoryStore]Block broadcast_208 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-25 11:29:40,044][org.apache.spark.storage.memory.MemoryStore]Block broadcast_208_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.3 MB)
[INFO][2018-05-25 11:29:40,044][org.apache.spark.storage.BlockManagerInfo]Added broadcast_208_piece0 in memory on 10.194.32.157:53453 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-25 11:29:40,044][org.apache.spark.SparkContext]Created broadcast 208 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:29:40,044][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 208 (MapPartitionsRDD[213] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:29:40,045][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 208.0 with 1 tasks
[INFO][2018-05-25 11:29:40,047][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 208.0 (TID 208, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:29:40,048][org.apache.spark.executor.Executor]Running task 0.0 in stage 208.0 (TID 208)
[INFO][2018-05-25 11:29:40,050][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 13298 -> 13304
[INFO][2018-05-25 11:29:40,050][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 13298
[INFO][2018-05-25 11:29:40,080][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-25 11:29:40,080][org.apache.spark.executor.Executor]Finished task 0.0 in stage 208.0 (TID 208). 708 bytes result sent to driver
[INFO][2018-05-25 11:29:40,081][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 208.0 (TID 208) in 34 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:29:40,081][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 208.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:29:40,081][org.apache.spark.scheduler.DAGScheduler]ResultStage 208 (foreachPartition at ReceiveKafkaData.scala:73) finished in 0.036 s
[INFO][2018-05-25 11:29:40,081][org.apache.spark.scheduler.DAGScheduler]Job 208 finished: foreachPartition at ReceiveKafkaData.scala:73, took 0.041760 s
[INFO][2018-05-25 11:29:40,082][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218980000 ms.0 from job set of time 1527218980000 ms
[INFO][2018-05-25 11:29:40,082][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.082 s for time 1527218980000 ms (execution: 0.065 s)
[INFO][2018-05-25 11:29:40,082][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 211 from persistence list
[INFO][2018-05-25 11:29:40,082][org.apache.spark.storage.BlockManager]Removing RDD 211
[INFO][2018-05-25 11:29:40,082][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 210 from persistence list
[INFO][2018-05-25 11:29:40,082][org.apache.spark.storage.BlockManager]Removing RDD 210
[INFO][2018-05-25 11:29:40,082][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:29:40,082][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218970000 ms
[INFO][2018-05-25 11:29:45,013][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218985000 ms
[INFO][2018-05-25 11:29:45,013][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218985000 ms.0 from job set of time 1527218985000 ms
[INFO][2018-05-25 11:29:45,017][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:29:45,017][org.apache.spark.scheduler.DAGScheduler]Got job 209 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:29:45,017][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 209 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:29:45,017][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:29:45,017][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:29:45,018][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 209 (MapPartitionsRDD[215] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:29:45,018][org.apache.spark.storage.memory.MemoryStore]Block broadcast_209 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-25 11:29:45,019][org.apache.spark.storage.memory.MemoryStore]Block broadcast_209_piece0 stored as bytes in memory (estimated size 1972.0 B, free 912.3 MB)
[INFO][2018-05-25 11:29:45,019][org.apache.spark.storage.BlockManagerInfo]Added broadcast_209_piece0 in memory on 10.194.32.157:53453 (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:29:45,019][org.apache.spark.SparkContext]Created broadcast 209 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:29:45,020][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 209 (MapPartitionsRDD[215] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:29:45,020][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 209.0 with 1 tasks
[INFO][2018-05-25 11:29:45,020][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 209.0 (TID 209, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:29:45,020][org.apache.spark.executor.Executor]Running task 0.0 in stage 209.0 (TID 209)
[INFO][2018-05-25 11:29:45,021][org.apache.spark.streaming.kafka010.KafkaRDD]Beginning offset 13304 is the same as ending offset skipping seven 0
[INFO][2018-05-25 11:29:45,021][org.apache.spark.executor.Executor]Finished task 0.0 in stage 209.0 (TID 209). 665 bytes result sent to driver
[INFO][2018-05-25 11:29:45,021][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 209.0 (TID 209) in 1 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:29:45,021][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 209.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:29:45,022][org.apache.spark.scheduler.DAGScheduler]ResultStage 209 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.002 s
[INFO][2018-05-25 11:29:45,022][org.apache.spark.scheduler.DAGScheduler]Job 209 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.004872 s
[INFO][2018-05-25 11:29:45,022][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218985000 ms.0 from job set of time 1527218985000 ms
[INFO][2018-05-25 11:29:45,022][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.022 s for time 1527218985000 ms (execution: 0.009 s)
[INFO][2018-05-25 11:29:45,022][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 213 from persistence list
[INFO][2018-05-25 11:29:45,022][org.apache.spark.storage.BlockManager]Removing RDD 213
[INFO][2018-05-25 11:29:45,022][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 212 from persistence list
[INFO][2018-05-25 11:29:45,022][org.apache.spark.storage.BlockManager]Removing RDD 212
[INFO][2018-05-25 11:29:45,022][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:29:45,022][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218975000 ms
[INFO][2018-05-25 11:29:50,015][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218990000 ms
[INFO][2018-05-25 11:29:50,015][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218990000 ms.0 from job set of time 1527218990000 ms
[INFO][2018-05-25 11:29:50,019][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:29:50,019][org.apache.spark.scheduler.DAGScheduler]Got job 210 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:29:50,019][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 210 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:29:50,019][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:29:50,019][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:29:50,019][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 210 (MapPartitionsRDD[217] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:29:50,020][org.apache.spark.storage.memory.MemoryStore]Block broadcast_210 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-25 11:29:50,021][org.apache.spark.storage.memory.MemoryStore]Block broadcast_210_piece0 stored as bytes in memory (estimated size 1972.0 B, free 912.3 MB)
[INFO][2018-05-25 11:29:50,021][org.apache.spark.storage.BlockManagerInfo]Added broadcast_210_piece0 in memory on 10.194.32.157:53453 (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:29:50,021][org.apache.spark.SparkContext]Created broadcast 210 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:29:50,022][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 210 (MapPartitionsRDD[217] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:29:50,022][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 210.0 with 1 tasks
[INFO][2018-05-25 11:29:50,022][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 210.0 (TID 210, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:29:50,022][org.apache.spark.executor.Executor]Running task 0.0 in stage 210.0 (TID 210)
[INFO][2018-05-25 11:29:50,023][org.apache.spark.streaming.kafka010.KafkaRDD]Beginning offset 13304 is the same as ending offset skipping seven 0
[INFO][2018-05-25 11:29:50,023][org.apache.spark.executor.Executor]Finished task 0.0 in stage 210.0 (TID 210). 665 bytes result sent to driver
[INFO][2018-05-25 11:29:50,024][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 210.0 (TID 210) in 1 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:29:50,024][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 210.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:29:50,024][org.apache.spark.scheduler.DAGScheduler]ResultStage 210 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.002 s
[INFO][2018-05-25 11:29:50,024][org.apache.spark.scheduler.DAGScheduler]Job 210 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.004973 s
[INFO][2018-05-25 11:29:50,024][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218990000 ms.0 from job set of time 1527218990000 ms
[INFO][2018-05-25 11:29:50,024][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.024 s for time 1527218990000 ms (execution: 0.009 s)
[INFO][2018-05-25 11:29:50,024][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 215 from persistence list
[INFO][2018-05-25 11:29:50,024][org.apache.spark.storage.BlockManager]Removing RDD 215
[INFO][2018-05-25 11:29:50,024][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 214 from persistence list
[INFO][2018-05-25 11:29:50,025][org.apache.spark.storage.BlockManager]Removing RDD 214
[INFO][2018-05-25 11:29:50,025][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:29:50,025][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218980000 ms
[INFO][2018-05-25 11:29:55,015][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527218995000 ms
[INFO][2018-05-25 11:29:55,015][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527218995000 ms.0 from job set of time 1527218995000 ms
[INFO][2018-05-25 11:29:55,019][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:29:55,019][org.apache.spark.scheduler.DAGScheduler]Got job 211 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:29:55,019][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 211 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:29:55,019][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:29:55,019][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:29:55,019][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 211 (MapPartitionsRDD[219] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:29:55,020][org.apache.spark.storage.memory.MemoryStore]Block broadcast_211 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-25 11:29:55,020][org.apache.spark.storage.memory.MemoryStore]Block broadcast_211_piece0 stored as bytes in memory (estimated size 1972.0 B, free 912.2 MB)
[INFO][2018-05-25 11:29:55,021][org.apache.spark.storage.BlockManagerInfo]Added broadcast_211_piece0 in memory on 10.194.32.157:53453 (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:29:55,021][org.apache.spark.SparkContext]Created broadcast 211 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:29:55,021][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 211 (MapPartitionsRDD[219] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:29:55,021][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 211.0 with 1 tasks
[INFO][2018-05-25 11:29:55,021][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 211.0 (TID 211, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:29:55,022][org.apache.spark.executor.Executor]Running task 0.0 in stage 211.0 (TID 211)
[INFO][2018-05-25 11:29:55,022][org.apache.spark.streaming.kafka010.KafkaRDD]Beginning offset 13304 is the same as ending offset skipping seven 0
[INFO][2018-05-25 11:29:55,023][org.apache.spark.executor.Executor]Finished task 0.0 in stage 211.0 (TID 211). 665 bytes result sent to driver
[INFO][2018-05-25 11:29:55,023][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 211.0 (TID 211) in 2 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:29:55,023][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 211.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:29:55,023][org.apache.spark.scheduler.DAGScheduler]ResultStage 211 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.002 s
[INFO][2018-05-25 11:29:55,023][org.apache.spark.scheduler.DAGScheduler]Job 211 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.004641 s
[INFO][2018-05-25 11:29:55,023][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527218995000 ms.0 from job set of time 1527218995000 ms
[INFO][2018-05-25 11:29:55,023][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.023 s for time 1527218995000 ms (execution: 0.008 s)
[INFO][2018-05-25 11:29:55,024][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 217 from persistence list
[INFO][2018-05-25 11:29:55,024][org.apache.spark.storage.BlockManager]Removing RDD 217
[INFO][2018-05-25 11:29:55,024][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 216 from persistence list
[INFO][2018-05-25 11:29:55,024][org.apache.spark.storage.BlockManager]Removing RDD 216
[INFO][2018-05-25 11:29:55,024][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:29:55,024][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218985000 ms
[INFO][2018-05-25 11:30:00,016][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527219000000 ms
[INFO][2018-05-25 11:30:00,017][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527219000000 ms.0 from job set of time 1527219000000 ms
[INFO][2018-05-25 11:30:00,020][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:72
[INFO][2018-05-25 11:30:00,021][org.apache.spark.scheduler.DAGScheduler]Got job 212 (isEmpty at ReceiveKafkaData.scala:72) with 1 output partitions
[INFO][2018-05-25 11:30:00,021][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 212 (isEmpty at ReceiveKafkaData.scala:72)
[INFO][2018-05-25 11:30:00,021][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-25 11:30:00,021][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-25 11:30:00,021][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 212 (MapPartitionsRDD[221] at map at ReceiveKafkaData.scala:71), which has no missing parents
[INFO][2018-05-25 11:30:00,022][org.apache.spark.storage.memory.MemoryStore]Block broadcast_212 stored as values in memory (estimated size 3.1 KB, free 912.2 MB)
[INFO][2018-05-25 11:30:00,022][org.apache.spark.storage.memory.MemoryStore]Block broadcast_212_piece0 stored as bytes in memory (estimated size 1972.0 B, free 912.2 MB)
[INFO][2018-05-25 11:30:00,023][org.apache.spark.storage.BlockManagerInfo]Added broadcast_212_piece0 in memory on 10.194.32.157:53453 (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-25 11:30:00,023][org.apache.spark.SparkContext]Created broadcast 212 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-25 11:30:00,023][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 212 (MapPartitionsRDD[221] at map at ReceiveKafkaData.scala:71) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-25 11:30:00,023][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 212.0 with 1 tasks
[INFO][2018-05-25 11:30:00,024][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 212.0 (TID 212, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-25 11:30:00,024][org.apache.spark.executor.Executor]Running task 0.0 in stage 212.0 (TID 212)
[INFO][2018-05-25 11:30:00,025][org.apache.spark.streaming.kafka010.KafkaRDD]Beginning offset 13304 is the same as ending offset skipping seven 0
[INFO][2018-05-25 11:30:00,025][org.apache.spark.executor.Executor]Finished task 0.0 in stage 212.0 (TID 212). 665 bytes result sent to driver
[INFO][2018-05-25 11:30:00,025][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 212.0 (TID 212) in 2 ms on localhost (executor driver) (1/1)
[INFO][2018-05-25 11:30:00,025][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 212.0, whose tasks have all completed, from pool 
[INFO][2018-05-25 11:30:00,025][org.apache.spark.scheduler.DAGScheduler]ResultStage 212 (isEmpty at ReceiveKafkaData.scala:72) finished in 0.002 s
[INFO][2018-05-25 11:30:00,026][org.apache.spark.scheduler.DAGScheduler]Job 212 finished: isEmpty at ReceiveKafkaData.scala:72, took 0.005403 s
[INFO][2018-05-25 11:30:00,026][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527219000000 ms.0 from job set of time 1527219000000 ms
[INFO][2018-05-25 11:30:00,026][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.026 s for time 1527219000000 ms (execution: 0.009 s)
[INFO][2018-05-25 11:30:00,026][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 219 from persistence list
[INFO][2018-05-25 11:30:00,026][org.apache.spark.storage.BlockManager]Removing RDD 219
[INFO][2018-05-25 11:30:00,026][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 218 from persistence list
[INFO][2018-05-25 11:30:00,027][org.apache.spark.storage.BlockManager]Removing RDD 218
[INFO][2018-05-25 11:30:00,027][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-25 11:30:00,027][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527218990000 ms
[INFO][2018-05-25 11:30:03,757][org.apache.spark.streaming.StreamingContext]Invoking stop(stopGracefully=false) from shutdown hook
[INFO][2018-05-25 11:30:03,760][org.apache.spark.streaming.scheduler.ReceiverTracker]ReceiverTracker stopped
[INFO][2018-05-25 11:30:03,760][org.apache.spark.streaming.scheduler.JobGenerator]Stopping JobGenerator immediately
[INFO][2018-05-25 11:30:03,761][org.apache.spark.streaming.util.RecurringTimer]Stopped timer for JobGenerator after time 1527219000000
[INFO][2018-05-25 11:30:03,769][org.apache.spark.streaming.scheduler.JobGenerator]Stopped JobGenerator
[INFO][2018-05-25 11:30:03,771][org.apache.spark.streaming.scheduler.JobScheduler]Stopped JobScheduler
[INFO][2018-05-25 11:30:03,776][org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@5c645b43{/streaming,null,UNAVAILABLE,@Spark}
[INFO][2018-05-25 11:30:03,776][org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@17814b1c{/streaming/batch,null,UNAVAILABLE,@Spark}
[INFO][2018-05-25 11:30:03,778][org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@29c5ee1d{/static/streaming,null,UNAVAILABLE,@Spark}
[INFO][2018-05-25 11:30:03,779][org.apache.spark.streaming.StreamingContext]StreamingContext stopped successfully
[INFO][2018-05-25 11:30:03,779][org.apache.spark.SparkContext]Invoking stop() from shutdown hook
[INFO][2018-05-25 11:30:03,783][org.spark_project.jetty.server.AbstractConnector]Stopped Spark@4de00ad1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-05-25 11:30:03,785][org.apache.spark.ui.SparkUI]Stopped Spark web UI at http://10.194.32.157:4040
[INFO][2018-05-25 11:30:03,791][org.apache.spark.MapOutputTrackerMasterEndpoint]MapOutputTrackerMasterEndpoint stopped!
[INFO][2018-05-25 11:30:03,821][org.apache.spark.storage.memory.MemoryStore]MemoryStore cleared
[INFO][2018-05-25 11:30:03,821][org.apache.spark.storage.BlockManager]BlockManager stopped
[INFO][2018-05-25 11:30:03,822][org.apache.spark.storage.BlockManagerMaster]BlockManagerMaster stopped
[INFO][2018-05-25 11:30:03,825][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint]OutputCommitCoordinator stopped!
[INFO][2018-05-25 11:30:03,826][org.apache.spark.SparkContext]Successfully stopped SparkContext
[INFO][2018-05-25 11:30:03,826][org.apache.spark.util.ShutdownHookManager]Shutdown hook called
[INFO][2018-05-25 11:30:03,827][org.apache.spark.util.ShutdownHookManager]Deleting directory /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/spark-fe29db88-c163-4b96-8992-6964534f6ca1
