[INFO][2018-05-29 16:49:02,616][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 10.194.32.157, 63514, None)
[INFO][2018-05-29 16:49:02,620][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 10.194.32.157:63514 with 912.3 MB RAM, BlockManagerId(driver, 10.194.32.157, 63514, None)
[INFO][2018-05-29 16:49:02,622][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 10.194.32.157, 63514, None)
[INFO][2018-05-29 16:49:02,623][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 10.194.32.157, 63514, None)
[INFO][2018-05-29 16:49:02,943][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@55b62629{/metrics/json,null,AVAILABLE,@Spark}
[WARN][2018-05-29 16:49:03,346][org.apache.spark.streaming.kafka010.KafkaUtils]overriding enable.auto.commit to false for executor
[WARN][2018-05-29 16:49:03,347][org.apache.spark.streaming.kafka010.KafkaUtils]overriding auto.offset.reset to none for executor
[WARN][2018-05-29 16:49:03,348][org.apache.spark.streaming.kafka010.KafkaUtils]overriding executor group.id to spark-executor-use_a_separate_group_id_for_each_stream
[WARN][2018-05-29 16:49:03,349][org.apache.spark.streaming.kafka010.KafkaUtils]overriding receive.buffer.bytes to 65536 see KAFKA-3135
[INFO][2018-05-29 16:49:03,432][org.apache.spark.streaming.kafka010.DirectKafkaInputDStream]Slide time = 5000 ms
[INFO][2018-05-29 16:49:03,432][org.apache.spark.streaming.kafka010.DirectKafkaInputDStream]Storage level = Serialized 1x Replicated
[INFO][2018-05-29 16:49:03,433][org.apache.spark.streaming.kafka010.DirectKafkaInputDStream]Checkpoint interval = null
[INFO][2018-05-29 16:49:03,433][org.apache.spark.streaming.kafka010.DirectKafkaInputDStream]Remember interval = 5000 ms
[INFO][2018-05-29 16:49:03,434][org.apache.spark.streaming.kafka010.DirectKafkaInputDStream]Initialized and validated org.apache.spark.streaming.kafka010.DirectKafkaInputDStream@3092bcc5
[INFO][2018-05-29 16:49:03,434][org.apache.spark.streaming.dstream.MappedDStream]Slide time = 5000 ms
[INFO][2018-05-29 16:49:03,434][org.apache.spark.streaming.dstream.MappedDStream]Storage level = Serialized 1x Replicated
[INFO][2018-05-29 16:49:03,434][org.apache.spark.streaming.dstream.MappedDStream]Checkpoint interval = null
[INFO][2018-05-29 16:49:03,434][org.apache.spark.streaming.dstream.MappedDStream]Remember interval = 5000 ms
[INFO][2018-05-29 16:49:03,434][org.apache.spark.streaming.dstream.MappedDStream]Initialized and validated org.apache.spark.streaming.dstream.MappedDStream@48f46e48
[INFO][2018-05-29 16:49:03,435][org.apache.spark.streaming.dstream.ForEachDStream]Slide time = 5000 ms
[INFO][2018-05-29 16:49:03,435][org.apache.spark.streaming.dstream.ForEachDStream]Storage level = Serialized 1x Replicated
[INFO][2018-05-29 16:49:03,435][org.apache.spark.streaming.dstream.ForEachDStream]Checkpoint interval = null
[INFO][2018-05-29 16:49:03,435][org.apache.spark.streaming.dstream.ForEachDStream]Remember interval = 5000 ms
[INFO][2018-05-29 16:49:03,435][org.apache.spark.streaming.dstream.ForEachDStream]Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@2a4365d4
[INFO][2018-05-29 16:49:03,495][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	metric.reporters = []
	metadata.max.age.ms = 300000
	partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor]
	reconnect.backoff.ms = 50
	sasl.kerberos.ticket.renew.window.factor = 0.8
	max.partition.fetch.bytes = 1048576
	bootstrap.servers = [vm-xaj-bigdata-da-d01:9092, vm-xaj-bigdata-da-d02:9092, vm-xaj-bigdata-da-d03:9092, vm-xaj-bigdata-da-d04:9092, vm-xaj-bigdata-da-d05:9092, vm-xaj-bigdata-da-d06:9092, vm-xaj-bigdata-da-d07:9092]
	ssl.keystore.type = JKS
	enable.auto.commit = false
	sasl.mechanism = GSSAPI
	interceptor.classes = null
	exclude.internal.topics = true
	ssl.truststore.password = null
	client.id = 
	ssl.endpoint.identification.algorithm = null
	max.poll.records = 2147483647
	check.crcs = true
	request.timeout.ms = 40000
	heartbeat.interval.ms = 3000
	auto.commit.interval.ms = 5000
	receive.buffer.bytes = 65536
	ssl.truststore.type = JKS
	ssl.truststore.location = null
	ssl.keystore.password = null
	fetch.min.bytes = 1
	send.buffer.bytes = 131072
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	group.id = use_a_separate_group_id_for_each_stream
	retry.backoff.ms = 100
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	ssl.trustmanager.algorithm = PKIX
	ssl.key.password = null
	fetch.max.wait.ms = 500
	sasl.kerberos.min.time.before.relogin = 60000
	connections.max.idle.ms = 540000
	session.timeout.ms = 30000
	metrics.num.samples = 2
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	ssl.protocol = TLS
	ssl.provider = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.keystore.location = null
	ssl.cipher.suites = null
	security.protocol = PLAINTEXT
	ssl.keymanager.algorithm = SunX509
	metrics.sample.window.ms = 30000
	auto.offset.reset = latest

[INFO][2018-05-29 16:49:10,642][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	metric.reporters = []
	metadata.max.age.ms = 300000
	partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor]
	reconnect.backoff.ms = 50
	sasl.kerberos.ticket.renew.window.factor = 0.8
	max.partition.fetch.bytes = 1048576
	bootstrap.servers = [vm-xaj-bigdata-da-d01:9092, vm-xaj-bigdata-da-d02:9092, vm-xaj-bigdata-da-d03:9092, vm-xaj-bigdata-da-d04:9092, vm-xaj-bigdata-da-d05:9092, vm-xaj-bigdata-da-d06:9092, vm-xaj-bigdata-da-d07:9092]
	ssl.keystore.type = JKS
	enable.auto.commit = false
	sasl.mechanism = GSSAPI
	interceptor.classes = null
	exclude.internal.topics = true
	ssl.truststore.password = null
	client.id = consumer-1
	ssl.endpoint.identification.algorithm = null
	max.poll.records = 2147483647
	check.crcs = true
	request.timeout.ms = 40000
	heartbeat.interval.ms = 3000
	auto.commit.interval.ms = 5000
	receive.buffer.bytes = 65536
	ssl.truststore.type = JKS
	ssl.truststore.location = null
	ssl.keystore.password = null
	fetch.min.bytes = 1
	send.buffer.bytes = 131072
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	group.id = use_a_separate_group_id_for_each_stream
	retry.backoff.ms = 100
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	ssl.trustmanager.algorithm = PKIX
	ssl.key.password = null
	fetch.max.wait.ms = 500
	sasl.kerberos.min.time.before.relogin = 60000
	connections.max.idle.ms = 540000
	session.timeout.ms = 30000
	metrics.num.samples = 2
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	ssl.protocol = TLS
	ssl.provider = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.keystore.location = null
	ssl.cipher.suites = null
	security.protocol = PLAINTEXT
	ssl.keymanager.algorithm = SunX509
	metrics.sample.window.ms = 30000
	auto.offset.reset = latest

[INFO][2018-05-29 16:49:10,665][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 0.10.0.1
[INFO][2018-05-29 16:49:10,665][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a7a17cdec9eaa6c5
[INFO][2018-05-29 16:49:10,822][org.apache.kafka.clients.consumer.internals.AbstractCoordinator]Discovered coordinator vm-xaj-bigdata-da-d03:9092 (id: 2147483531 rack: null) for group use_a_separate_group_id_for_each_stream.
[INFO][2018-05-29 16:49:10,823][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]Revoking previously assigned partitions [] for group use_a_separate_group_id_for_each_stream
[INFO][2018-05-29 16:49:10,823][org.apache.kafka.clients.consumer.internals.AbstractCoordinator](Re-)joining group use_a_separate_group_id_for_each_stream
[INFO][2018-05-29 16:49:13,906][org.apache.kafka.clients.consumer.internals.AbstractCoordinator]Successfully joined group use_a_separate_group_id_for_each_stream with generation 5
[INFO][2018-05-29 16:49:13,907][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]Setting newly assigned partitions [seven-0] for group use_a_separate_group_id_for_each_stream
[INFO][2018-05-29 16:49:13,940][org.apache.spark.streaming.util.RecurringTimer]Started timer for JobGenerator at time 1527583745000
[INFO][2018-05-29 16:49:13,941][org.apache.spark.streaming.scheduler.JobGenerator]Started JobGenerator at 1527583745000 ms
[INFO][2018-05-29 16:49:13,942][org.apache.spark.streaming.scheduler.JobScheduler]Started JobScheduler
[INFO][2018-05-29 16:49:13,948][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@57adfab0{/streaming,null,AVAILABLE,@Spark}
[INFO][2018-05-29 16:49:13,949][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5af97169{/streaming/json,null,AVAILABLE,@Spark}
[INFO][2018-05-29 16:49:13,949][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2a22ad2b{/streaming/batch,null,AVAILABLE,@Spark}
[INFO][2018-05-29 16:49:13,950][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@121c54fa{/streaming/batch/json,null,AVAILABLE,@Spark}
[INFO][2018-05-29 16:49:13,952][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5dbf5634{/static/streaming,null,AVAILABLE,@Spark}
[INFO][2018-05-29 16:49:13,954][org.apache.spark.streaming.StreamingContext]StreamingContext started
[INFO][2018-05-29 16:49:14,537][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527583745000 ms
[INFO][2018-05-29 16:49:14,539][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527583745000 ms.0 from job set of time 1527583745000 ms
[INFO][2018-05-29 16:49:14,549][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527583750000 ms
[INFO][2018-05-29 16:49:14,608][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:75
[INFO][2018-05-29 16:49:14,622][org.apache.spark.scheduler.DAGScheduler]Got job 0 (isEmpty at ReceiveKafkaData.scala:75) with 1 output partitions
[INFO][2018-05-29 16:49:14,623][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 0 (isEmpty at ReceiveKafkaData.scala:75)
[INFO][2018-05-29 16:49:14,623][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-29 16:49:14,625][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-29 16:49:14,634][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 0 (MapPartitionsRDD[1] at map at ReceiveKafkaData.scala:74), which has no missing parents
[INFO][2018-05-29 16:49:14,805][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-29 16:49:14,851][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0_piece0 stored as bytes in memory (estimated size 1972.0 B, free 912.3 MB)
[INFO][2018-05-29 16:49:14,852][org.apache.spark.storage.BlockManagerInfo]Added broadcast_0_piece0 in memory on 10.194.32.157:63514 (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-29 16:49:14,856][org.apache.spark.SparkContext]Created broadcast 0 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-29 16:49:14,889][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at map at ReceiveKafkaData.scala:74) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-29 16:49:14,890][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 0.0 with 1 tasks
[INFO][2018-05-29 16:49:14,942][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-29 16:49:14,958][org.apache.spark.executor.Executor]Running task 0.0 in stage 0.0 (TID 0)
[INFO][2018-05-29 16:49:14,995][org.apache.spark.streaming.kafka010.KafkaRDD]Beginning offset 188742 is the same as ending offset skipping seven 0
[INFO][2018-05-29 16:49:15,012][org.apache.spark.executor.Executor]Finished task 0.0 in stage 0.0 (TID 0). 751 bytes result sent to driver
[INFO][2018-05-29 16:49:15,022][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527583755000 ms
[INFO][2018-05-29 16:49:15,026][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 0.0 (TID 0) in 95 ms on localhost (executor driver) (1/1)
[INFO][2018-05-29 16:49:15,028][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO][2018-05-29 16:49:15,043][org.apache.spark.scheduler.DAGScheduler]ResultStage 0 (isEmpty at ReceiveKafkaData.scala:75) finished in 0.129 s
[INFO][2018-05-29 16:49:15,071][org.apache.spark.scheduler.DAGScheduler]Job 0 finished: isEmpty at ReceiveKafkaData.scala:75, took 0.462561 s
[INFO][2018-05-29 16:49:15,074][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527583745000 ms.0 from job set of time 1527583745000 ms
[INFO][2018-05-29 16:49:15,075][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 10.074 s for time 1527583745000 ms (execution: 0.536 s)
[INFO][2018-05-29 16:49:15,076][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527583750000 ms.0 from job set of time 1527583750000 ms
[INFO][2018-05-29 16:49:15,081][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-29 16:49:15,082][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:75
[INFO][2018-05-29 16:49:15,083][org.apache.spark.scheduler.DAGScheduler]Got job 1 (isEmpty at ReceiveKafkaData.scala:75) with 1 output partitions
[INFO][2018-05-29 16:49:15,083][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 1 (isEmpty at ReceiveKafkaData.scala:75)
[INFO][2018-05-29 16:49:15,084][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-29 16:49:15,084][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-29 16:49:15,085][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 1 (MapPartitionsRDD[3] at map at ReceiveKafkaData.scala:74), which has no missing parents
[INFO][2018-05-29 16:49:15,085][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 
[INFO][2018-05-29 16:49:15,089][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-29 16:49:15,092][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1_piece0 stored as bytes in memory (estimated size 1973.0 B, free 912.3 MB)
[INFO][2018-05-29 16:49:15,093][org.apache.spark.storage.BlockManagerInfo]Added broadcast_1_piece0 in memory on 10.194.32.157:63514 (size: 1973.0 B, free: 912.3 MB)
[INFO][2018-05-29 16:49:15,095][org.apache.spark.SparkContext]Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-29 16:49:15,097][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at map at ReceiveKafkaData.scala:74) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-29 16:49:15,097][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 1.0 with 1 tasks
[INFO][2018-05-29 16:49:15,098][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-29 16:49:15,098][org.apache.spark.executor.Executor]Running task 0.0 in stage 1.0 (TID 1)
[INFO][2018-05-29 16:49:15,103][org.apache.spark.streaming.kafka010.KafkaRDD]Beginning offset 188742 is the same as ending offset skipping seven 0
[INFO][2018-05-29 16:49:15,104][org.apache.spark.executor.Executor]Finished task 0.0 in stage 1.0 (TID 1). 708 bytes result sent to driver
[INFO][2018-05-29 16:49:15,105][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 1.0 (TID 1) in 7 ms on localhost (executor driver) (1/1)
[INFO][2018-05-29 16:49:15,105][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO][2018-05-29 16:49:15,107][org.apache.spark.scheduler.DAGScheduler]ResultStage 1 (isEmpty at ReceiveKafkaData.scala:75) finished in 0.009 s
[INFO][2018-05-29 16:49:15,107][org.apache.spark.scheduler.DAGScheduler]Job 1 finished: isEmpty at ReceiveKafkaData.scala:75, took 0.025078 s
[INFO][2018-05-29 16:49:15,108][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527583750000 ms.0 from job set of time 1527583750000 ms
[INFO][2018-05-29 16:49:15,108][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 5.108 s for time 1527583750000 ms (execution: 0.032 s)
[INFO][2018-05-29 16:49:15,108][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527583755000 ms.0 from job set of time 1527583755000 ms
[INFO][2018-05-29 16:49:15,109][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 1 from persistence list
[INFO][2018-05-29 16:49:15,115][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:75
[INFO][2018-05-29 16:49:15,117][org.apache.spark.storage.BlockManager]Removing RDD 1
[INFO][2018-05-29 16:49:15,118][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 0 from persistence list
[INFO][2018-05-29 16:49:15,118][org.apache.spark.storage.BlockManager]Removing RDD 0
[INFO][2018-05-29 16:49:15,118][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-29 16:49:15,120][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 
[INFO][2018-05-29 16:49:15,120][org.apache.spark.scheduler.DAGScheduler]Got job 2 (isEmpty at ReceiveKafkaData.scala:75) with 1 output partitions
[INFO][2018-05-29 16:49:15,122][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 2 (isEmpty at ReceiveKafkaData.scala:75)
[INFO][2018-05-29 16:49:15,122][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-29 16:49:15,124][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-29 16:49:15,124][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 2 (MapPartitionsRDD[5] at map at ReceiveKafkaData.scala:74), which has no missing parents
[INFO][2018-05-29 16:49:15,132][org.apache.spark.storage.memory.MemoryStore]Block broadcast_2 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-29 16:49:15,134][org.apache.spark.storage.memory.MemoryStore]Block broadcast_2_piece0 stored as bytes in memory (estimated size 1973.0 B, free 912.3 MB)
[INFO][2018-05-29 16:49:15,139][org.apache.spark.storage.BlockManagerInfo]Added broadcast_2_piece0 in memory on 10.194.32.157:63514 (size: 1973.0 B, free: 912.3 MB)
[INFO][2018-05-29 16:49:15,140][org.apache.spark.SparkContext]Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-29 16:49:15,142][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[5] at map at ReceiveKafkaData.scala:74) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-29 16:49:15,142][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 2.0 with 1 tasks
[INFO][2018-05-29 16:49:15,143][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-29 16:49:15,144][org.apache.spark.executor.Executor]Running task 0.0 in stage 2.0 (TID 2)
[INFO][2018-05-29 16:49:15,149][org.apache.spark.streaming.kafka010.KafkaRDD]Beginning offset 188742 is the same as ending offset skipping seven 0
[INFO][2018-05-29 16:49:15,150][org.apache.spark.executor.Executor]Finished task 0.0 in stage 2.0 (TID 2). 708 bytes result sent to driver
[INFO][2018-05-29 16:49:15,151][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 2.0 (TID 2) in 8 ms on localhost (executor driver) (1/1)
[INFO][2018-05-29 16:49:15,152][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO][2018-05-29 16:49:15,152][org.apache.spark.scheduler.DAGScheduler]ResultStage 2 (isEmpty at ReceiveKafkaData.scala:75) finished in 0.010 s
[INFO][2018-05-29 16:49:15,153][org.apache.spark.scheduler.DAGScheduler]Job 2 finished: isEmpty at ReceiveKafkaData.scala:75, took 0.037417 s
[INFO][2018-05-29 16:49:15,154][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527583755000 ms.0 from job set of time 1527583755000 ms
[INFO][2018-05-29 16:49:15,154][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.154 s for time 1527583755000 ms (execution: 0.046 s)
[INFO][2018-05-29 16:49:15,154][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 3 from persistence list
[INFO][2018-05-29 16:49:15,155][org.apache.spark.storage.BlockManager]Removing RDD 3
[INFO][2018-05-29 16:49:15,155][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 2 from persistence list
[INFO][2018-05-29 16:49:15,155][org.apache.spark.storage.BlockManager]Removing RDD 2
[INFO][2018-05-29 16:49:15,155][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-29 16:49:15,155][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527583745000 ms
[INFO][2018-05-29 16:49:20,017][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527583760000 ms
[INFO][2018-05-29 16:49:20,018][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527583760000 ms.0 from job set of time 1527583760000 ms
[INFO][2018-05-29 16:49:20,024][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:75
[INFO][2018-05-29 16:49:20,025][org.apache.spark.scheduler.DAGScheduler]Got job 3 (isEmpty at ReceiveKafkaData.scala:75) with 1 output partitions
[INFO][2018-05-29 16:49:20,025][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 3 (isEmpty at ReceiveKafkaData.scala:75)
[INFO][2018-05-29 16:49:20,025][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-29 16:49:20,025][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-29 16:49:20,026][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 3 (MapPartitionsRDD[7] at map at ReceiveKafkaData.scala:74), which has no missing parents
[INFO][2018-05-29 16:49:20,028][org.apache.spark.storage.memory.MemoryStore]Block broadcast_3 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-29 16:49:20,030][org.apache.spark.storage.memory.MemoryStore]Block broadcast_3_piece0 stored as bytes in memory (estimated size 1971.0 B, free 912.3 MB)
[INFO][2018-05-29 16:49:20,030][org.apache.spark.storage.BlockManagerInfo]Added broadcast_3_piece0 in memory on 10.194.32.157:63514 (size: 1971.0 B, free: 912.3 MB)
[INFO][2018-05-29 16:49:20,031][org.apache.spark.SparkContext]Created broadcast 3 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-29 16:49:20,032][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[7] at map at ReceiveKafkaData.scala:74) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-29 16:49:20,032][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 3.0 with 1 tasks
[INFO][2018-05-29 16:49:20,032][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 3.0 (TID 3, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-29 16:49:20,033][org.apache.spark.executor.Executor]Running task 0.0 in stage 3.0 (TID 3)
[INFO][2018-05-29 16:49:20,036][org.apache.spark.streaming.kafka010.KafkaRDD]Beginning offset 188742 is the same as ending offset skipping seven 0
[INFO][2018-05-29 16:49:20,037][org.apache.spark.executor.Executor]Finished task 0.0 in stage 3.0 (TID 3). 708 bytes result sent to driver
[INFO][2018-05-29 16:49:20,038][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 3.0 (TID 3) in 6 ms on localhost (executor driver) (1/1)
[INFO][2018-05-29 16:49:20,038][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 3.0, whose tasks have all completed, from pool 
[INFO][2018-05-29 16:49:20,039][org.apache.spark.scheduler.DAGScheduler]ResultStage 3 (isEmpty at ReceiveKafkaData.scala:75) finished in 0.006 s
[INFO][2018-05-29 16:49:20,039][org.apache.spark.scheduler.DAGScheduler]Job 3 finished: isEmpty at ReceiveKafkaData.scala:75, took 0.014652 s
[INFO][2018-05-29 16:49:20,039][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527583760000 ms.0 from job set of time 1527583760000 ms
[INFO][2018-05-29 16:49:20,040][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 5 from persistence list
[INFO][2018-05-29 16:49:20,040][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.039 s for time 1527583760000 ms (execution: 0.022 s)
[INFO][2018-05-29 16:49:20,041][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 4 from persistence list
[INFO][2018-05-29 16:49:20,041][org.apache.spark.storage.BlockManager]Removing RDD 5
[INFO][2018-05-29 16:49:20,042][org.apache.spark.storage.BlockManager]Removing RDD 4
[INFO][2018-05-29 16:49:20,042][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-29 16:49:20,042][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527583750000 ms
[INFO][2018-05-29 16:49:25,017][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527583765000 ms
[INFO][2018-05-29 16:49:25,017][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527583765000 ms.0 from job set of time 1527583765000 ms
[INFO][2018-05-29 16:49:25,024][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:75
[INFO][2018-05-29 16:49:25,025][org.apache.spark.scheduler.DAGScheduler]Got job 4 (isEmpty at ReceiveKafkaData.scala:75) with 1 output partitions
[INFO][2018-05-29 16:49:25,025][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 4 (isEmpty at ReceiveKafkaData.scala:75)
[INFO][2018-05-29 16:49:25,025][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-29 16:49:25,026][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-29 16:49:25,026][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 4 (MapPartitionsRDD[9] at map at ReceiveKafkaData.scala:74), which has no missing parents
[INFO][2018-05-29 16:49:25,028][org.apache.spark.storage.memory.MemoryStore]Block broadcast_4 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-29 16:49:25,032][org.apache.spark.storage.memory.MemoryStore]Block broadcast_4_piece0 stored as bytes in memory (estimated size 1973.0 B, free 912.3 MB)
[INFO][2018-05-29 16:49:25,032][org.apache.spark.storage.BlockManagerInfo]Added broadcast_4_piece0 in memory on 10.194.32.157:63514 (size: 1973.0 B, free: 912.3 MB)
[INFO][2018-05-29 16:49:25,033][org.apache.spark.SparkContext]Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-29 16:49:25,034][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[9] at map at ReceiveKafkaData.scala:74) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-29 16:49:25,034][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 4.0 with 1 tasks
[INFO][2018-05-29 16:49:25,035][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 4.0 (TID 4, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-29 16:49:25,036][org.apache.spark.executor.Executor]Running task 0.0 in stage 4.0 (TID 4)
[INFO][2018-05-29 16:49:25,040][org.apache.spark.streaming.kafka010.KafkaRDD]Beginning offset 188742 is the same as ending offset skipping seven 0
[INFO][2018-05-29 16:49:25,041][org.apache.spark.executor.Executor]Finished task 0.0 in stage 4.0 (TID 4). 708 bytes result sent to driver
[INFO][2018-05-29 16:49:25,042][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 4.0 (TID 4) in 7 ms on localhost (executor driver) (1/1)
[INFO][2018-05-29 16:49:25,042][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 4.0, whose tasks have all completed, from pool 
[INFO][2018-05-29 16:49:25,042][org.apache.spark.scheduler.DAGScheduler]ResultStage 4 (isEmpty at ReceiveKafkaData.scala:75) finished in 0.007 s
[INFO][2018-05-29 16:49:25,043][org.apache.spark.scheduler.DAGScheduler]Job 4 finished: isEmpty at ReceiveKafkaData.scala:75, took 0.018698 s
[INFO][2018-05-29 16:49:25,044][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527583765000 ms.0 from job set of time 1527583765000 ms
[INFO][2018-05-29 16:49:25,044][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 7 from persistence list
[INFO][2018-05-29 16:49:25,044][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.044 s for time 1527583765000 ms (execution: 0.027 s)
[INFO][2018-05-29 16:49:25,045][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 6 from persistence list
[INFO][2018-05-29 16:49:25,045][org.apache.spark.storage.BlockManager]Removing RDD 7
[INFO][2018-05-29 16:49:25,046][org.apache.spark.storage.BlockManager]Removing RDD 6
[INFO][2018-05-29 16:49:25,047][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-29 16:49:25,047][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527583755000 ms
[INFO][2018-05-29 16:49:30,017][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527583770000 ms
[INFO][2018-05-29 16:49:30,017][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527583770000 ms.0 from job set of time 1527583770000 ms
[INFO][2018-05-29 16:49:30,027][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:75
[INFO][2018-05-29 16:49:30,027][org.apache.spark.scheduler.DAGScheduler]Got job 5 (isEmpty at ReceiveKafkaData.scala:75) with 1 output partitions
[INFO][2018-05-29 16:49:30,028][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 5 (isEmpty at ReceiveKafkaData.scala:75)
[INFO][2018-05-29 16:49:30,028][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-29 16:49:30,028][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-29 16:49:30,028][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 5 (MapPartitionsRDD[11] at map at ReceiveKafkaData.scala:74), which has no missing parents
[INFO][2018-05-29 16:49:30,030][org.apache.spark.storage.memory.MemoryStore]Block broadcast_5 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-29 16:49:30,039][org.apache.spark.storage.memory.MemoryStore]Block broadcast_5_piece0 stored as bytes in memory (estimated size 1973.0 B, free 912.3 MB)
[INFO][2018-05-29 16:49:30,039][org.apache.spark.storage.BlockManagerInfo]Added broadcast_5_piece0 in memory on 10.194.32.157:63514 (size: 1973.0 B, free: 912.3 MB)
[INFO][2018-05-29 16:49:30,040][org.apache.spark.SparkContext]Created broadcast 5 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-29 16:49:30,041][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[11] at map at ReceiveKafkaData.scala:74) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-29 16:49:30,041][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 5.0 with 1 tasks
[INFO][2018-05-29 16:49:30,042][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 5.0 (TID 5, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-29 16:49:30,042][org.apache.spark.executor.Executor]Running task 0.0 in stage 5.0 (TID 5)
[INFO][2018-05-29 16:49:30,049][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 188742 -> 188770
[INFO][2018-05-29 16:49:30,052][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initializing cache 16 64 0.75
[INFO][2018-05-29 16:49:30,054][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Cache miss for CacheKey(spark-executor-use_a_separate_group_id_for_each_stream,seven,0)
[INFO][2018-05-29 16:49:30,056][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	metric.reporters = []
	metadata.max.age.ms = 300000
	partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor]
	reconnect.backoff.ms = 50
	sasl.kerberos.ticket.renew.window.factor = 0.8
	max.partition.fetch.bytes = 1048576
	bootstrap.servers = [vm-xaj-bigdata-da-d01:9092, vm-xaj-bigdata-da-d02:9092, vm-xaj-bigdata-da-d03:9092, vm-xaj-bigdata-da-d04:9092, vm-xaj-bigdata-da-d05:9092, vm-xaj-bigdata-da-d06:9092, vm-xaj-bigdata-da-d07:9092]
	ssl.keystore.type = JKS
	enable.auto.commit = false
	sasl.mechanism = GSSAPI
	interceptor.classes = null
	exclude.internal.topics = true
	ssl.truststore.password = null
	client.id = 
	ssl.endpoint.identification.algorithm = null
	max.poll.records = 2147483647
	check.crcs = true
	request.timeout.ms = 40000
	heartbeat.interval.ms = 3000
	auto.commit.interval.ms = 5000
	receive.buffer.bytes = 65536
	ssl.truststore.type = JKS
	ssl.truststore.location = null
	ssl.keystore.password = null
	fetch.min.bytes = 1
	send.buffer.bytes = 131072
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	group.id = spark-executor-use_a_separate_group_id_for_each_stream
	retry.backoff.ms = 100
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	ssl.trustmanager.algorithm = PKIX
	ssl.key.password = null
	fetch.max.wait.ms = 500
	sasl.kerberos.min.time.before.relogin = 60000
	connections.max.idle.ms = 540000
	session.timeout.ms = 30000
	metrics.num.samples = 2
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	ssl.protocol = TLS
	ssl.provider = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.keystore.location = null
	ssl.cipher.suites = null
	security.protocol = PLAINTEXT
	ssl.keymanager.algorithm = SunX509
	metrics.sample.window.ms = 30000
	auto.offset.reset = none

[INFO][2018-05-29 16:49:30,057][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	metric.reporters = []
	metadata.max.age.ms = 300000
	partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor]
	reconnect.backoff.ms = 50
	sasl.kerberos.ticket.renew.window.factor = 0.8
	max.partition.fetch.bytes = 1048576
	bootstrap.servers = [vm-xaj-bigdata-da-d01:9092, vm-xaj-bigdata-da-d02:9092, vm-xaj-bigdata-da-d03:9092, vm-xaj-bigdata-da-d04:9092, vm-xaj-bigdata-da-d05:9092, vm-xaj-bigdata-da-d06:9092, vm-xaj-bigdata-da-d07:9092]
	ssl.keystore.type = JKS
	enable.auto.commit = false
	sasl.mechanism = GSSAPI
	interceptor.classes = null
	exclude.internal.topics = true
	ssl.truststore.password = null
	client.id = consumer-2
	ssl.endpoint.identification.algorithm = null
	max.poll.records = 2147483647
	check.crcs = true
	request.timeout.ms = 40000
	heartbeat.interval.ms = 3000
	auto.commit.interval.ms = 5000
	receive.buffer.bytes = 65536
	ssl.truststore.type = JKS
	ssl.truststore.location = null
	ssl.keystore.password = null
	fetch.min.bytes = 1
	send.buffer.bytes = 131072
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	group.id = spark-executor-use_a_separate_group_id_for_each_stream
	retry.backoff.ms = 100
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	ssl.trustmanager.algorithm = PKIX
	ssl.key.password = null
	fetch.max.wait.ms = 500
	sasl.kerberos.min.time.before.relogin = 60000
	connections.max.idle.ms = 540000
	session.timeout.ms = 30000
	metrics.num.samples = 2
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	ssl.protocol = TLS
	ssl.provider = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.keystore.location = null
	ssl.cipher.suites = null
	security.protocol = PLAINTEXT
	ssl.keymanager.algorithm = SunX509
	metrics.sample.window.ms = 30000
	auto.offset.reset = none

[INFO][2018-05-29 16:49:30,059][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 0.10.0.1
[INFO][2018-05-29 16:49:30,059][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a7a17cdec9eaa6c5
[INFO][2018-05-29 16:49:30,061][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 188742
[INFO][2018-05-29 16:49:30,179][org.apache.kafka.clients.consumer.internals.AbstractCoordinator]Discovered coordinator vm-xaj-bigdata-da-d06:9092 (id: 2147483530 rack: null) for group spark-executor-use_a_separate_group_id_for_each_stream.
[INFO][2018-05-29 16:49:30,217][org.apache.spark.executor.Executor]Finished task 0.0 in stage 5.0 (TID 5). 969 bytes result sent to driver
[INFO][2018-05-29 16:49:30,218][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 5.0 (TID 5) in 176 ms on localhost (executor driver) (1/1)
[INFO][2018-05-29 16:49:30,219][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 5.0, whose tasks have all completed, from pool 
[INFO][2018-05-29 16:49:30,219][org.apache.spark.scheduler.DAGScheduler]ResultStage 5 (isEmpty at ReceiveKafkaData.scala:75) finished in 0.177 s
[INFO][2018-05-29 16:49:30,220][org.apache.spark.scheduler.DAGScheduler]Job 5 finished: isEmpty at ReceiveKafkaData.scala:75, took 0.192789 s
[INFO][2018-05-29 16:49:30,232][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:76
[INFO][2018-05-29 16:49:30,233][org.apache.spark.scheduler.DAGScheduler]Got job 6 (foreachPartition at ReceiveKafkaData.scala:76) with 1 output partitions
[INFO][2018-05-29 16:49:30,233][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 6 (foreachPartition at ReceiveKafkaData.scala:76)
[INFO][2018-05-29 16:49:30,233][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-29 16:49:30,233][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-29 16:49:30,233][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 6 (MapPartitionsRDD[11] at map at ReceiveKafkaData.scala:74), which has no missing parents
[INFO][2018-05-29 16:49:30,236][org.apache.spark.storage.memory.MemoryStore]Block broadcast_6 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-29 16:49:30,241][org.apache.spark.storage.memory.MemoryStore]Block broadcast_6_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.3 MB)
[INFO][2018-05-29 16:49:30,242][org.apache.spark.storage.BlockManagerInfo]Added broadcast_6_piece0 in memory on 10.194.32.157:63514 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-29 16:49:30,242][org.apache.spark.SparkContext]Created broadcast 6 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-29 16:49:30,243][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[11] at map at ReceiveKafkaData.scala:74) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-29 16:49:30,243][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 6.0 with 1 tasks
[INFO][2018-05-29 16:49:30,244][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 6.0 (TID 6, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-29 16:49:30,244][org.apache.spark.executor.Executor]Running task 0.0 in stage 6.0 (TID 6)
[INFO][2018-05-29 16:49:30,247][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 188742 -> 188770
[INFO][2018-05-29 16:49:30,248][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 188742
[INFO][2018-05-29 16:49:30,524][org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper]Process identifier=hconnection-0x538ba4d9 connecting to ZooKeeper ensemble=vm-xaj-bigdata-da-d01:2181,vm-xaj-bigdata-da-d02:2181,vm-xaj-bigdata-da-d03:2181
[INFO][2018-05-29 16:49:30,533][org.apache.zookeeper.ZooKeeper]Client environment:zookeeper.version=3.4.5-cdh5.7.0--1, built on 04/06/2016 14:24 GMT
[INFO][2018-05-29 16:49:30,534][org.apache.zookeeper.ZooKeeper]Client environment:host.name=10.194.32.157
[INFO][2018-05-29 16:49:30,534][org.apache.zookeeper.ZooKeeper]Client environment:java.version=1.8.0_161
[INFO][2018-05-29 16:49:30,534][org.apache.zookeeper.ZooKeeper]Client environment:java.vendor=Oracle Corporation
[INFO][2018-05-29 16:49:30,534][org.apache.zookeeper.ZooKeeper]Client environment:java.home=/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre
[INFO][2018-05-29 16:49:30,534][org.apache.zookeeper.ZooKeeper]Client environment:java.class.path=/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/charsets.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/deploy.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/cldrdata.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/dnsns.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/jaccess.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/jfxrt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/localedata.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/nashorn.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/sunec.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/sunjce_provider.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/sunpkcs11.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/zipfs.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/javaws.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/jce.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/jfr.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/jfxswt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/jsse.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/management-agent.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/plugin.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/resources.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/rt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/ant-javafx.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/dt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/javafx-mx.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/jconsole.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/packager.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/sa-jdi.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/tools.jar:/Users/seven/project/github/dataMining/target/classes:/Users/seven/software/maven/repository/org/scala-lang/scala-library/2.11.12/scala-library-2.11.12.jar:/Users/seven/software/maven/repository/org/scala-lang/scala-compiler/2.11.12/scala-compiler-2.11.12.jar:/Users/seven/software/maven/repository/org/scala-lang/modules/scala-xml_2.11/1.0.5/scala-xml_2.11-1.0.5.jar:/Users/seven/software/maven/repository/org/scala-lang/modules/scala-parser-combinators_2.11/1.0.4/scala-parser-combinators_2.11-1.0.4.jar:/Users/seven/software/maven/repository/org/scala-lang/scala-reflect/2.11.12/scala-reflect-2.11.12.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-client/2.6.0-cdh5.7.0/hadoop-client-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-common/2.6.0-cdh5.7.0/hadoop-common-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/xmlenc/xmlenc/0.52/xmlenc-0.52.jar:/Users/seven/software/maven/repository/commons-net/commons-net/3.1/commons-net-3.1.jar:/Users/seven/software/maven/repository/commons-configuration/commons-configuration/1.6/commons-configuration-1.6.jar:/Users/seven/software/maven/repository/commons-digester/commons-digester/1.8/commons-digester-1.8.jar:/Users/seven/software/maven/repository/commons-beanutils/commons-beanutils/1.7.0/commons-beanutils-1.7.0.jar:/Users/seven/software/maven/repository/org/slf4j/slf4j-api/1.7.5/slf4j-api-1.7.5.jar:/Users/seven/software/maven/repository/org/slf4j/slf4j-log4j12/1.7.5/slf4j-log4j12-1.7.5.jar:/Users/seven/software/maven/repository/org/apache/avro/avro/1.7.6-cdh5.7.0/avro-1.7.6-cdh5.7.0.jar:/Users/seven/software/maven/repository/com/google/code/gson/gson/2.2.4/gson-2.2.4.jar:/Users/seven/software/maven/repository/org/apache/curator/curator-client/2.7.1/curator-client-2.7.1.jar:/Users/seven/software/maven/repository/org/apache/curator/curator-recipes/2.7.1/curator-recipes-2.7.1.jar:/Users/seven/software/maven/repository/com/google/code/findbugs/jsr305/3.0.0/jsr305-3.0.0.jar:/Users/seven/software/maven/repository/org/apache/htrace/htrace-core4/4.0.1-incubating/htrace-core4-4.0.1-incubating.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-compress/1.4.1/commons-compress-1.4.1.jar:/Users/seven/software/maven/repository/org/tukaani/xz/1.0/xz-1.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-hdfs/2.6.0-cdh5.7.0/hadoop-hdfs-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/io/netty/netty/3.6.2.Final/netty-3.6.2.Final.jar:/Users/seven/software/maven/repository/xerces/xercesImpl/2.9.1/xercesImpl-2.9.1.jar:/Users/seven/software/maven/repository/xml-apis/xml-apis/1.3.04/xml-apis-1.3.04.jar:/Users/seven/software/maven/repository/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-app/2.6.0-cdh5.7.0/hadoop-mapreduce-client-app-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-common/2.6.0-cdh5.7.0/hadoop-mapreduce-client-common-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-yarn-client/2.6.0-cdh5.7.0/hadoop-yarn-client-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-yarn-server-common/2.6.0-cdh5.7.0/hadoop-yarn-server-common-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-shuffle/2.6.0-cdh5.7.0/hadoop-mapreduce-client-shuffle-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-yarn-api/2.6.0-cdh5.7.0/hadoop-yarn-api-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-core/2.6.0-cdh5.7.0/hadoop-mapreduce-client-core-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-yarn-common/2.6.0-cdh5.7.0/hadoop-yarn-common-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/javax/xml/bind/jaxb-api/2.2.2/jaxb-api-2.2.2.jar:/Users/seven/software/maven/repository/javax/xml/stream/stax-api/1.0-2/stax-api-1.0-2.jar:/Users/seven/software/maven/repository/javax/activation/activation/1.1/activation-1.1.jar:/Users/seven/software/maven/repository/com/sun/jersey/jersey-client/1.9/jersey-client-1.9.jar:/Users/seven/software/maven/repository/org/codehaus/jackson/jackson-xc/1.8.8/jackson-xc-1.8.8.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.6.0-cdh5.7.0/hadoop-mapreduce-client-jobclient-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-aws/2.6.0-cdh5.7.0/hadoop-aws-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/com/amazonaws/aws-java-sdk-s3/1.10.6/aws-java-sdk-s3-1.10.6.jar:/Users/seven/software/maven/repository/com/amazonaws/aws-java-sdk-kms/1.10.6/aws-java-sdk-kms-1.10.6.jar:/Users/seven/software/maven/repository/com/amazonaws/aws-java-sdk-core/1.10.6/aws-java-sdk-core-1.10.6.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-annotations/2.6.0-cdh5.7.0/hadoop-annotations-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-client/1.2.0-cdh5.7.0/hbase-client-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-annotations/1.2.0-cdh5.7.0/hbase-annotations-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-common/1.2.0-cdh5.7.0/hbase-common-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-protocol/1.2.0-cdh5.7.0/hbase-protocol-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/commons-codec/commons-codec/1.9/commons-codec-1.9.jar:/Users/seven/software/maven/repository/commons-io/commons-io/2.4/commons-io-2.4.jar:/Users/seven/software/maven/repository/commons-lang/commons-lang/2.6/commons-lang-2.6.jar:/Users/seven/software/maven/repository/commons-logging/commons-logging/1.2/commons-logging-1.2.jar:/Users/seven/software/maven/repository/com/google/guava/guava/12.0.1/guava-12.0.1.jar:/Users/seven/software/maven/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/Users/seven/software/maven/repository/io/netty/netty-all/4.0.23.Final/netty-all-4.0.23.Final.jar:/Users/seven/software/maven/repository/org/apache/zookeeper/zookeeper/3.4.5-cdh5.7.0/zookeeper-3.4.5-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/htrace/htrace-core/3.2.0-incubating/htrace-core-3.2.0-incubating.jar:/Users/seven/software/maven/repository/org/codehaus/jackson/jackson-mapper-asl/1.8.8/jackson-mapper-asl-1.8.8.jar:/Users/seven/software/maven/repository/org/jruby/jcodings/jcodings/1.0.8/jcodings-1.0.8.jar:/Users/seven/software/maven/repository/org/jruby/joni/joni/2.1.2/joni-2.1.2.jar:/Users/seven/software/maven/repository/com/yammer/metrics/metrics-core/2.2.0/metrics-core-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-auth/2.6.0-cdh5.7.0/hadoop-auth-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/httpcomponents/httpclient/4.2.5/httpclient-4.2.5.jar:/Users/seven/software/maven/repository/org/apache/httpcomponents/httpcore/4.2.4/httpcore-4.2.4.jar:/Users/seven/software/maven/repository/org/apache/directory/server/apacheds-kerberos-codec/2.0.0-M15/apacheds-kerberos-codec-2.0.0-M15.jar:/Users/seven/software/maven/repository/org/apache/directory/server/apacheds-i18n/2.0.0-M15/apacheds-i18n-2.0.0-M15.jar:/Users/seven/software/maven/repository/org/apache/directory/api/api-asn1-api/1.0.0-M20/api-asn1-api-1.0.0-M20.jar:/Users/seven/software/maven/repository/org/apache/directory/api/api-util/1.0.0-M20/api-util-1.0.0-M20.jar:/Users/seven/software/maven/repository/org/apache/curator/curator-framework/2.7.1/curator-framework-2.7.1.jar:/Users/seven/software/maven/repository/com/github/stephenc/findbugs/findbugs-annotations/1.3.9-1/findbugs-annotations-1.3.9-1.jar:/Users/seven/software/maven/repository/junit/junit/4.12/junit-4.12.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-server/1.2.0-cdh5.7.0/hbase-server-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-procedure/1.2.0-cdh5.7.0/hbase-procedure-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-common/1.2.0-cdh5.7.0/hbase-common-1.2.0-cdh5.7.0-tests.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-prefix-tree/1.2.0-cdh5.7.0/hbase-prefix-tree-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar:/Users/seven/software/maven/repository/commons-collections/commons-collections/3.2.2/commons-collections-3.2.2.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-hadoop-compat/1.2.0-cdh5.7.0/hbase-hadoop-compat-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-hadoop2-compat/1.2.0-cdh5.7.0/hbase-hadoop2-compat-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/com/sun/jersey/jersey-core/1.9/jersey-core-1.9.jar:/Users/seven/software/maven/repository/com/sun/jersey/jersey-server/1.9/jersey-server-1.9.jar:/Users/seven/software/maven/repository/asm/asm/3.1/asm-3.1.jar:/Users/seven/software/maven/repository/commons-cli/commons-cli/1.2/commons-cli-1.2.jar:/Users/seven/software/maven/repository/com/github/stephenc/high-scale-lib/high-scale-lib/1.1.1/high-scale-lib-1.1.1.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-math/2.1/commons-math-2.1.jar:/Users/seven/software/maven/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jetty/6.1.26.cloudera.4/jetty-6.1.26.cloudera.4.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jetty-util/6.1.26.cloudera.4/jetty-util-6.1.26.cloudera.4.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jetty-sslengine/6.1.26.cloudera.4/jetty-sslengine-6.1.26.cloudera.4.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jsp-2.1/6.1.14/jsp-2.1-6.1.14.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jsp-api-2.1/6.1.14/jsp-api-2.1-6.1.14.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/servlet-api-2.5/6.1.14/servlet-api-2.5-6.1.14.jar:/Users/seven/software/maven/repository/org/codehaus/jackson/jackson-core-asl/1.8.8/jackson-core-asl-1.8.8.jar:/Users/seven/software/maven/repository/org/codehaus/jackson/jackson-jaxrs/1.8.8/jackson-jaxrs-1.8.8.jar:/Users/seven/software/maven/repository/tomcat/jasper-compiler/5.5.23/jasper-compiler-5.5.23.jar:/Users/seven/software/maven/repository/tomcat/jasper-runtime/5.5.23/jasper-runtime-5.5.23.jar:/Users/seven/software/maven/repository/commons-el/commons-el/1.0/commons-el-1.0.jar:/Users/seven/software/maven/repository/org/jamon/jamon-runtime/2.4.1/jamon-runtime-2.4.1.jar:/Users/seven/software/maven/repository/com/lmax/disruptor/3.3.0/disruptor-3.3.0.jar:/Users/seven/software/maven/repository/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar:/Users/seven/software/maven/repository/org/owasp/esapi/esapi/2.1.0/esapi-2.1.0.jar:/Users/seven/software/maven/repository/commons-beanutils/commons-beanutils-core/1.7.0/commons-beanutils-core-1.7.0.jar:/Users/seven/software/maven/repository/commons-fileupload/commons-fileupload/1.2/commons-fileupload-1.2.jar:/Users/seven/software/maven/repository/xom/xom/1.2.5/xom-1.2.5.jar:/Users/seven/software/maven/repository/xalan/xalan/2.7.0/xalan-2.7.0.jar:/Users/seven/software/maven/repository/org/beanshell/bsh-core/2.0b4/bsh-core-2.0b4.jar:/Users/seven/software/maven/repository/org/owasp/antisamy/antisamy/1.4.3/antisamy-1.4.3.jar:/Users/seven/software/maven/repository/org/apache/xmlgraphics/batik-css/1.7/batik-css-1.7.jar:/Users/seven/software/maven/repository/org/apache/xmlgraphics/batik-ext/1.7/batik-ext-1.7.jar:/Users/seven/software/maven/repository/org/apache/xmlgraphics/batik-util/1.7/batik-util-1.7.jar:/Users/seven/software/maven/repository/xml-apis/xml-apis-ext/1.3.04/xml-apis-ext-1.3.04.jar:/Users/seven/software/maven/repository/net/sourceforge/nekohtml/nekohtml/1.9.12/nekohtml-1.9.12.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-core/2.6.0-mr1-cdh5.7.0/hadoop-core-2.6.0-mr1-cdh5.7.0.jar:/Users/seven/software/maven/repository/javax/servlet/jsp/jsp-api/2.1/jsp-api-2.1.jar:/Users/seven/software/maven/repository/net/java/dev/jets3t/jets3t/0.6.1/jets3t-0.6.1.jar:/Users/seven/software/maven/repository/javax/servlet/servlet-api/2.5/servlet-api-2.5.jar:/Users/seven/software/maven/repository/hsqldb/hsqldb/1.8.0.10/hsqldb-1.8.0.10.jar:/Users/seven/software/maven/repository/org/eclipse/jdt/core/3.1.1/core-3.1.1.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-hdfs/2.6.0-cdh5.7.0/hadoop-hdfs-2.6.0-cdh5.7.0-tests.jar:/Users/seven/software/maven/repository/commons-daemon/commons-daemon/1.0.13/commons-daemon-1.0.13.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-lang3/3.5/commons-lang3-3.5.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-pool2/2.4.2/commons-pool2-2.4.2.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-mllib_2.11/2.2.0/spark-mllib_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-core_2.11/2.2.0/spark-core_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/avro/avro-mapred/1.7.7/avro-mapred-1.7.7-hadoop2.jar:/Users/seven/software/maven/repository/org/apache/avro/avro-ipc/1.7.7/avro-ipc-1.7.7-tests.jar:/Users/seven/software/maven/repository/com/twitter/chill_2.11/0.8.0/chill_2.11-0.8.0.jar:/Users/seven/software/maven/repository/com/esotericsoftware/kryo-shaded/3.0.3/kryo-shaded-3.0.3.jar:/Users/seven/software/maven/repository/com/esotericsoftware/minlog/1.3.0/minlog-1.3.0.jar:/Users/seven/software/maven/repository/org/objenesis/objenesis/2.1/objenesis-2.1.jar:/Users/seven/software/maven/repository/com/twitter/chill-java/0.8.0/chill-java-0.8.0.jar:/Users/seven/software/maven/repository/org/apache/xbean/xbean-asm5-shaded/4.4/xbean-asm5-shaded-4.4.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-launcher_2.11/2.2.0/spark-launcher_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-network-common_2.11/2.2.0/spark-network-common_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-network-shuffle_2.11/2.2.0/spark-network-shuffle_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-unsafe_2.11/2.2.0/spark-unsafe_2.11-2.2.0.jar:/Users/seven/software/maven/repository/javax/servlet/javax.servlet-api/3.1.0/javax.servlet-api-3.1.0.jar:/Users/seven/software/maven/repository/org/slf4j/jul-to-slf4j/1.7.16/jul-to-slf4j-1.7.16.jar:/Users/seven/software/maven/repository/org/slf4j/jcl-over-slf4j/1.7.16/jcl-over-slf4j-1.7.16.jar:/Users/seven/software/maven/repository/com/ning/compress-lzf/1.0.3/compress-lzf-1.0.3.jar:/Users/seven/software/maven/repository/org/xerial/snappy/snappy-java/1.1.2.6/snappy-java-1.1.2.6.jar:/Users/seven/software/maven/repository/org/roaringbitmap/RoaringBitmap/0.5.11/RoaringBitmap-0.5.11.jar:/Users/seven/software/maven/repository/org/json4s/json4s-jackson_2.11/3.2.11/json4s-jackson_2.11-3.2.11.jar:/Users/seven/software/maven/repository/org/json4s/json4s-core_2.11/3.2.11/json4s-core_2.11-3.2.11.jar:/Users/seven/software/maven/repository/org/json4s/json4s-ast_2.11/3.2.11/json4s-ast_2.11-3.2.11.jar:/Users/seven/software/maven/repository/org/scala-lang/scalap/2.11.0/scalap-2.11.0.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/core/jersey-client/2.22.2/jersey-client-2.22.2.jar:/Users/seven/software/maven/repository/javax/ws/rs/javax.ws.rs-api/2.0.1/javax.ws.rs-api-2.0.1.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/hk2-api/2.4.0-b34/hk2-api-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/hk2-utils/2.4.0-b34/hk2-utils-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/external/aopalliance-repackaged/2.4.0-b34/aopalliance-repackaged-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/external/javax.inject/2.4.0-b34/javax.inject-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/hk2-locator/2.4.0-b34/hk2-locator-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/javassist/javassist/3.18.1-GA/javassist-3.18.1-GA.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/core/jersey-common/2.22.2/jersey-common-2.22.2.jar:/Users/seven/software/maven/repository/javax/annotation/javax.annotation-api/1.2/javax.annotation-api-1.2.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/bundles/repackaged/jersey-guava/2.22.2/jersey-guava-2.22.2.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/osgi-resource-locator/1.0.1/osgi-resource-locator-1.0.1.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/core/jersey-server/2.22.2/jersey-server-2.22.2.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/media/jersey-media-jaxb/2.22.2/jersey-media-jaxb-2.22.2.jar:/Users/seven/software/maven/repository/javax/validation/validation-api/1.1.0.Final/validation-api-1.1.0.Final.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/containers/jersey-container-servlet/2.22.2/jersey-container-servlet-2.22.2.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/containers/jersey-container-servlet-core/2.22.2/jersey-container-servlet-core-2.22.2.jar:/Users/seven/software/maven/repository/com/clearspring/analytics/stream/2.7.0/stream-2.7.0.jar:/Users/seven/software/maven/repository/io/dropwizard/metrics/metrics-core/3.1.2/metrics-core-3.1.2.jar:/Users/seven/software/maven/repository/io/dropwizard/metrics/metrics-jvm/3.1.2/metrics-jvm-3.1.2.jar:/Users/seven/software/maven/repository/io/dropwizard/metrics/metrics-json/3.1.2/metrics-json-3.1.2.jar:/Users/seven/software/maven/repository/io/dropwizard/metrics/metrics-graphite/3.1.2/metrics-graphite-3.1.2.jar:/Users/seven/software/maven/repository/org/apache/ivy/ivy/2.4.0/ivy-2.4.0.jar:/Users/seven/software/maven/repository/oro/oro/2.0.8/oro-2.0.8.jar:/Users/seven/software/maven/repository/net/razorvine/pyrolite/4.13/pyrolite-4.13.jar:/Users/seven/software/maven/repository/net/sf/py4j/py4j/0.10.4/py4j-0.10.4.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-crypto/1.0.0/commons-crypto-1.0.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-streaming_2.11/2.2.0/spark-streaming_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-sql_2.11/2.2.0/spark-sql_2.11-2.2.0.jar:/Users/seven/software/maven/repository/com/univocity/univocity-parsers/2.2.1/univocity-parsers-2.2.1.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-sketch_2.11/2.2.0/spark-sketch_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-catalyst_2.11/2.2.0/spark-catalyst_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/codehaus/janino/janino/3.0.0/janino-3.0.0.jar:/Users/seven/software/maven/repository/org/codehaus/janino/commons-compiler/3.0.0/commons-compiler-3.0.0.jar:/Users/seven/software/maven/repository/org/antlr/antlr4-runtime/4.5.3/antlr4-runtime-4.5.3.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-column/1.8.2/parquet-column-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-common/1.8.2/parquet-common-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-encoding/1.8.2/parquet-encoding-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-hadoop/1.8.2/parquet-hadoop-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-format/2.3.1/parquet-format-2.3.1.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-jackson/1.8.2/parquet-jackson-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-graphx_2.11/2.2.0/spark-graphx_2.11-2.2.0.jar:/Users/seven/software/maven/repository/com/github/fommil/netlib/core/1.1.2/core-1.1.2.jar:/Users/seven/software/maven/repository/net/sourceforge/f2j/arpack_combined_all/0.1/arpack_combined_all-0.1.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-mllib-local_2.11/2.2.0/spark-mllib-local_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/scalanlp/breeze_2.11/0.13.1/breeze_2.11-0.13.1.jar:/Users/seven/software/maven/repository/org/scalanlp/breeze-macros_2.11/0.13.1/breeze-macros_2.11-0.13.1.jar:/Users/seven/software/maven/repository/net/sf/opencsv/opencsv/2.3/opencsv-2.3.jar:/Users/seven/software/maven/repository/com/github/rwl/jtransforms/2.4.0/jtransforms-2.4.0.jar:/Users/seven/software/maven/repository/org/spire-math/spire_2.11/0.13.0/spire_2.11-0.13.0.jar:/Users/seven/software/maven/repository/org/spire-math/spire-macros_2.11/0.13.0/spire-macros_2.11-0.13.0.jar:/Users/seven/software/maven/repository/org/typelevel/machinist_2.11/0.6.1/machinist_2.11-0.6.1.jar:/Users/seven/software/maven/repository/com/chuusai/shapeless_2.11/2.3.2/shapeless_2.11-2.3.2.jar:/Users/seven/software/maven/repository/org/typelevel/macro-compat_2.11/1.1.1/macro-compat_2.11-1.1.1.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-math3/3.4.1/commons-math3-3.4.1.jar:/Users/seven/software/maven/repository/org/jpmml/pmml-model/1.2.15/pmml-model-1.2.15.jar:/Users/seven/software/maven/repository/org/jpmml/pmml-schema/1.2.15/pmml-schema-1.2.15.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-tags_2.11/2.2.0/spark-tags_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-streaming-kafka-0-10_2.11/2.2.0/spark-streaming-kafka-0-10_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/kafka/kafka_2.11/0.10.0.1/kafka_2.11-0.10.0.1.jar:/Users/seven/software/maven/repository/com/101tec/zkclient/0.8/zkclient-0.8.jar:/Users/seven/software/maven/repository/org/apache/kafka/kafka-clients/0.10.0.1/kafka-clients-0.10.0.1.jar:/Users/seven/software/maven/repository/mysql/mysql-connector-java/5.1.31/mysql-connector-java-5.1.31.jar:/Users/seven/software/maven/repository/org/elasticsearch/client/transport/5.1.2/transport-5.1.2.jar:/Users/seven/software/maven/repository/org/elasticsearch/plugin/transport-netty3-client/5.1.2/transport-netty3-client-5.1.2.jar:/Users/seven/software/maven/repository/org/elasticsearch/plugin/transport-netty4-client/5.1.2/transport-netty4-client-5.1.2.jar:/Users/seven/software/maven/repository/io/netty/netty-buffer/4.1.6.Final/netty-buffer-4.1.6.Final.jar:/Users/seven/software/maven/repository/io/netty/netty-codec/4.1.6.Final/netty-codec-4.1.6.Final.jar:/Users/seven/software/maven/repository/io/netty/netty-codec-http/4.1.6.Final/netty-codec-http-4.1.6.Final.jar:/Users/seven/software/maven/repository/io/netty/netty-common/4.1.6.Final/netty-common-4.1.6.Final.jar:/Users/seven/software/maven/repository/io/netty/netty-handler/4.1.6.Final/netty-handler-4.1.6.Final.jar:/Users/seven/software/maven/repository/io/netty/netty-resolver/4.1.6.Final/netty-resolver-4.1.6.Final.jar:/Users/seven/software/maven/repository/io/netty/netty-transport/4.1.6.Final/netty-transport-4.1.6.Final.jar:/Users/seven/software/maven/repository/org/elasticsearch/plugin/reindex-client/5.1.2/reindex-client-5.1.2.jar:/Users/seven/software/maven/repository/org/elasticsearch/client/rest/5.1.2/rest-5.1.2.jar:/Users/seven/software/maven/repository/org/apache/httpcomponents/httpasyncclient/4.1.2/httpasyncclient-4.1.2.jar:/Users/seven/software/maven/repository/org/apache/httpcomponents/httpcore-nio/4.4.5/httpcore-nio-4.4.5.jar:/Users/seven/software/maven/repository/org/elasticsearch/plugin/lang-mustache-client/5.1.2/lang-mustache-client-5.1.2.jar:/Users/seven/software/maven/repository/com/github/spullara/mustache/java/compiler/0.9.3/compiler-0.9.3.jar:/Users/seven/software/maven/repository/org/elasticsearch/plugin/percolator-client/5.1.2/percolator-client-5.1.2.jar:/Users/seven/software/maven/repository/org/elasticsearch/elasticsearch/5.1.2/elasticsearch-5.1.2.jar:/Users/seven/software/maven/repository/org/apache/lucene/lucene-core/6.3.0/lucene-core-6.3.0.jar:/Users/seven/software/maven/repository/org/apache/lucene/lucene-analyzers-common/6.3.0/lucene-analyzers-common-6.3.0.jar:/Users/seven/software/maven/repository/org/apache/lucene/lucene-backward-codecs/6.3.0/lucene-backward-codecs-6.3.0.jar:/Users/seven/software/maven/repository/org/apache/lucene/lucene-grouping/6.3.0/lucene-grouping-6.3.0.jar:/Users/seven/software/maven/repository/org/apache/lucene/lucene-highlighter/6.3.0/lucene-highlighter-6.3.0.jar:/Users/seven/software/maven/repository/org/apache/lucene/lucene-join/6.3.0/lucene-join-6.3.0.jar:/Users/seven/software/maven/repository/org/apache/lucene/lucene-memory/6.3.0/lucene-memory-6.3.0.jar:/Users/seven/software/maven/repository/org/apache/lucene/lucene-misc/6.3.0/lucene-misc-6.3.0.jar:/Users/seven/software/maven/repository/org/apache/lucene/lucene-queries/6.3.0/lucene-queries-6.3.0.jar:/Users/seven/software/maven/repository/org/apache/lucene/lucene-queryparser/6.3.0/lucene-queryparser-6.3.0.jar:/Users/seven/software/maven/repository/org/apache/lucene/lucene-sandbox/6.3.0/lucene-sandbox-6.3.0.jar:/Users/seven/software/maven/repository/org/apache/lucene/lucene-spatial/6.3.0/lucene-spatial-6.3.0.jar:/Users/seven/software/maven/repository/org/apache/lucene/lucene-spatial-extras/6.3.0/lucene-spatial-extras-6.3.0.jar:/Users/seven/software/maven/repository/org/apache/lucene/lucene-spatial3d/6.3.0/lucene-spatial3d-6.3.0.jar:/Users/seven/software/maven/repository/org/apache/lucene/lucene-suggest/6.3.0/lucene-suggest-6.3.0.jar:/Users/seven/software/maven/repository/org/elasticsearch/securesm/1.1/securesm-1.1.jar:/Users/seven/software/maven/repository/net/sf/jopt-simple/jopt-simple/5.0.2/jopt-simple-5.0.2.jar:/Users/seven/software/maven/repository/com/carrotsearch/hppc/0.7.1/hppc-0.7.1.jar:/Users/seven/software/maven/repository/joda-time/joda-time/2.9.5/joda-time-2.9.5.jar:/Users/seven/software/maven/repository/org/yaml/snakeyaml/1.15/snakeyaml-1.15.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/core/jackson-core/2.8.1/jackson-core-2.8.1.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/dataformat/jackson-dataformat-smile/2.8.1/jackson-dataformat-smile-2.8.1.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/dataformat/jackson-dataformat-yaml/2.8.1/jackson-dataformat-yaml-2.8.1.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/dataformat/jackson-dataformat-cbor/2.8.1/jackson-dataformat-cbor-2.8.1.jar:/Users/seven/software/maven/repository/com/tdunning/t-digest/3.0/t-digest-3.0.jar:/Users/seven/software/maven/repository/org/hdrhistogram/HdrHistogram/2.1.6/HdrHistogram-2.1.6.jar:/Users/seven/software/maven/repository/net/java/dev/jna/jna/4.2.2/jna-4.2.2.jar:/Users/seven/software/maven/repository/org/apache/logging/log4j/log4j-api/2.6.2/log4j-api-2.6.2.jar:/Users/seven/software/maven/repository/org/apache/logging/log4j/log4j-core/2.6.2/log4j-core-2.6.2.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/module/jackson-module-scala_2.11/2.6.5/jackson-module-scala_2.11-2.6.5.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/core/jackson-annotations/2.6.5/jackson-annotations-2.6.5.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/core/jackson-databind/2.6.5/jackson-databind-2.6.5.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/module/jackson-module-paranamer/2.6.5/jackson-module-paranamer-2.6.5.jar:/Users/seven/software/maven/repository/com/thoughtworks/paranamer/paranamer/2.6/paranamer-2.6.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-streaming-flume_2.11/2.2.0/spark-streaming-flume_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/flume/flume-ng-core/1.6.0/flume-ng-core-1.6.0.jar:/Users/seven/software/maven/repository/org/apache/flume/flume-ng-configuration/1.6.0/flume-ng-configuration-1.6.0.jar:/Users/seven/software/maven/repository/org/apache/avro/avro-ipc/1.7.4/avro-ipc-1.7.4.jar:/Users/seven/software/maven/repository/org/apache/velocity/velocity/1.7/velocity-1.7.jar:/Users/seven/software/maven/repository/org/apache/mina/mina-core/2.0.4/mina-core-2.0.4.jar:/Users/seven/software/maven/repository/org/apache/flume/flume-ng-sdk/1.6.0/flume-ng-sdk-1.6.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-streaming-flume-sink_2.11/2.2.0/spark-streaming-flume-sink_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/flume/flume-ng-clients/flume-ng-log4jappender/1.6.0/flume-ng-log4jappender-1.6.0.jar:/Users/seven/software/maven/repository/net/jpountz/lz4/lz4/1.3.0/lz4-1.3.0.jar:/Applications/IntelliJ IDEA.app/Contents/lib/idea_rt.jar
[INFO][2018-05-29 16:49:30,536][org.apache.zookeeper.ZooKeeper]Client environment:java.library.path=/Users/seven/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:.
[INFO][2018-05-29 16:49:30,536][org.apache.zookeeper.ZooKeeper]Client environment:java.io.tmpdir=/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/
[INFO][2018-05-29 16:49:30,536][org.apache.zookeeper.ZooKeeper]Client environment:java.compiler=<NA>
[INFO][2018-05-29 16:49:30,537][org.apache.zookeeper.ZooKeeper]Client environment:os.name=Mac OS X
[INFO][2018-05-29 16:49:30,537][org.apache.zookeeper.ZooKeeper]Client environment:os.arch=x86_64
[INFO][2018-05-29 16:49:30,537][org.apache.zookeeper.ZooKeeper]Client environment:os.version=10.13.4
[INFO][2018-05-29 16:49:30,537][org.apache.zookeeper.ZooKeeper]Client environment:user.name=seven
[INFO][2018-05-29 16:49:30,537][org.apache.zookeeper.ZooKeeper]Client environment:user.home=/Users/seven
[INFO][2018-05-29 16:49:30,537][org.apache.zookeeper.ZooKeeper]Client environment:user.dir=/Users/seven/project/github/dataMining
[INFO][2018-05-29 16:49:30,538][org.apache.zookeeper.ZooKeeper]Initiating client connection, connectString=vm-xaj-bigdata-da-d01:2181,vm-xaj-bigdata-da-d02:2181,vm-xaj-bigdata-da-d03:2181 sessionTimeout=60000 watcher=hconnection-0x538ba4d90x0, quorum=vm-xaj-bigdata-da-d01:2181,vm-xaj-bigdata-da-d02:2181,vm-xaj-bigdata-da-d03:2181, baseZNode=/hbase
[INFO][2018-05-29 16:49:30,573][org.apache.zookeeper.ClientCnxn]Opening socket connection to server vm-xaj-bigdata-da-d02/10.213.4.26:2181. Will not attempt to authenticate using SASL (unknown error)
[INFO][2018-05-29 16:49:30,582][org.apache.zookeeper.ClientCnxn]Socket connection established, initiating session, client: /10.194.32.157:63540, server: vm-xaj-bigdata-da-d02/10.213.4.26:2181
[INFO][2018-05-29 16:49:30,599][org.apache.zookeeper.ClientCnxn]Session establishment complete on server vm-xaj-bigdata-da-d02/10.213.4.26:2181, sessionid = 0x262b4dc569b8718, negotiated timeout = 60000
[INFO][2018-05-29 16:49:31,006][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_0_piece0 on 10.194.32.157:63514 in memory (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-29 16:49:31,011][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_1_piece0 on 10.194.32.157:63514 in memory (size: 1973.0 B, free: 912.3 MB)
[INFO][2018-05-29 16:49:31,012][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_5_piece0 on 10.194.32.157:63514 in memory (size: 1973.0 B, free: 912.3 MB)
[INFO][2018-05-29 16:49:31,013][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_4_piece0 on 10.194.32.157:63514 in memory (size: 1973.0 B, free: 912.3 MB)
[INFO][2018-05-29 16:49:31,014][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_2_piece0 on 10.194.32.157:63514 in memory (size: 1973.0 B, free: 912.3 MB)
[INFO][2018-05-29 16:49:31,015][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_3_piece0 on 10.194.32.157:63514 in memory (size: 1971.0 B, free: 912.3 MB)
[WARN][2018-05-29 16:49:31,117][org.apache.hadoop.util.NativeCodeLoader]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO][2018-05-29 16:49:31,611][com.seven.spark.streaming.ReceiveKafkaData$]Inserting 28 lines of data to HBase is success . . .
[INFO][2018-05-29 16:49:33,867][com.seven.spark.streaming.ReceiveKafkaData$]Inserting 28 lines of data to ElasticSearch is success . . .
[INFO][2018-05-29 16:49:33,868][org.apache.spark.executor.Executor]Finished task 0.0 in stage 6.0 (TID 6). 751 bytes result sent to driver
[INFO][2018-05-29 16:49:33,870][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 6.0 (TID 6) in 3626 ms on localhost (executor driver) (1/1)
[INFO][2018-05-29 16:49:33,871][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 6.0, whose tasks have all completed, from pool 
[INFO][2018-05-29 16:49:33,872][org.apache.spark.scheduler.DAGScheduler]ResultStage 6 (foreachPartition at ReceiveKafkaData.scala:76) finished in 3.628 s
[INFO][2018-05-29 16:49:33,873][org.apache.spark.scheduler.DAGScheduler]Job 6 finished: foreachPartition at ReceiveKafkaData.scala:76, took 3.640608 s
[INFO][2018-05-29 16:49:33,874][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527583770000 ms.0 from job set of time 1527583770000 ms
[INFO][2018-05-29 16:49:33,875][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 3.874 s for time 1527583770000 ms (execution: 3.857 s)
[INFO][2018-05-29 16:49:33,875][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 9 from persistence list
[INFO][2018-05-29 16:49:33,878][org.apache.spark.storage.BlockManager]Removing RDD 9
[INFO][2018-05-29 16:49:33,878][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 8 from persistence list
[INFO][2018-05-29 16:49:33,879][org.apache.spark.storage.BlockManager]Removing RDD 8
[INFO][2018-05-29 16:49:33,879][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-29 16:49:33,879][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527583760000 ms
[INFO][2018-05-29 16:49:35,017][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527583775000 ms
[INFO][2018-05-29 16:49:35,017][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527583775000 ms.0 from job set of time 1527583775000 ms
[INFO][2018-05-29 16:49:35,029][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:75
[INFO][2018-05-29 16:49:35,030][org.apache.spark.scheduler.DAGScheduler]Got job 7 (isEmpty at ReceiveKafkaData.scala:75) with 1 output partitions
[INFO][2018-05-29 16:49:35,030][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 7 (isEmpty at ReceiveKafkaData.scala:75)
[INFO][2018-05-29 16:49:35,030][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-29 16:49:35,031][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-29 16:49:35,031][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 7 (MapPartitionsRDD[13] at map at ReceiveKafkaData.scala:74), which has no missing parents
[INFO][2018-05-29 16:49:35,035][org.apache.spark.storage.memory.MemoryStore]Block broadcast_7 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-29 16:49:35,038][org.apache.spark.storage.memory.MemoryStore]Block broadcast_7_piece0 stored as bytes in memory (estimated size 1973.0 B, free 912.3 MB)
[INFO][2018-05-29 16:49:35,039][org.apache.spark.storage.BlockManagerInfo]Added broadcast_7_piece0 in memory on 10.194.32.157:63514 (size: 1973.0 B, free: 912.3 MB)
[INFO][2018-05-29 16:49:35,045][org.apache.spark.SparkContext]Created broadcast 7 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-29 16:49:35,046][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[13] at map at ReceiveKafkaData.scala:74) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-29 16:49:35,046][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 7.0 with 1 tasks
[INFO][2018-05-29 16:49:35,047][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 7.0 (TID 7, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-29 16:49:35,048][org.apache.spark.executor.Executor]Running task 0.0 in stage 7.0 (TID 7)
[INFO][2018-05-29 16:49:35,078][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 188770 -> 188818
[INFO][2018-05-29 16:49:35,079][org.apache.spark.executor.Executor]Finished task 0.0 in stage 7.0 (TID 7). 974 bytes result sent to driver
[INFO][2018-05-29 16:49:35,080][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 7.0 (TID 7) in 33 ms on localhost (executor driver) (1/1)
[INFO][2018-05-29 16:49:35,080][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 7.0, whose tasks have all completed, from pool 
[INFO][2018-05-29 16:49:35,080][org.apache.spark.scheduler.DAGScheduler]ResultStage 7 (isEmpty at ReceiveKafkaData.scala:75) finished in 0.033 s
[INFO][2018-05-29 16:49:35,081][org.apache.spark.scheduler.DAGScheduler]Job 7 finished: isEmpty at ReceiveKafkaData.scala:75, took 0.051494 s
[INFO][2018-05-29 16:49:35,088][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:76
[INFO][2018-05-29 16:49:35,088][org.apache.spark.scheduler.DAGScheduler]Got job 8 (foreachPartition at ReceiveKafkaData.scala:76) with 1 output partitions
[INFO][2018-05-29 16:49:35,088][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 8 (foreachPartition at ReceiveKafkaData.scala:76)
[INFO][2018-05-29 16:49:35,089][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-29 16:49:35,089][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-29 16:49:35,089][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 8 (MapPartitionsRDD[13] at map at ReceiveKafkaData.scala:74), which has no missing parents
[INFO][2018-05-29 16:49:35,091][org.apache.spark.storage.memory.MemoryStore]Block broadcast_8 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-29 16:49:35,107][org.apache.spark.storage.memory.MemoryStore]Block broadcast_8_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.3 MB)
[INFO][2018-05-29 16:49:35,107][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_7_piece0 on 10.194.32.157:63514 in memory (size: 1973.0 B, free: 912.3 MB)
[INFO][2018-05-29 16:49:35,108][org.apache.spark.storage.BlockManagerInfo]Added broadcast_8_piece0 in memory on 10.194.32.157:63514 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-29 16:49:35,108][org.apache.spark.SparkContext]Created broadcast 8 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-29 16:49:35,108][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[13] at map at ReceiveKafkaData.scala:74) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-29 16:49:35,109][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 8.0 with 1 tasks
[INFO][2018-05-29 16:49:35,109][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 8.0 (TID 8, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-29 16:49:35,109][org.apache.spark.executor.Executor]Running task 0.0 in stage 8.0 (TID 8)
[INFO][2018-05-29 16:49:35,114][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 188770 -> 188818
[INFO][2018-05-29 16:49:35,114][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 188770
[INFO][2018-05-29 16:49:35,177][com.seven.spark.streaming.ReceiveKafkaData$]Inserting 48 lines of data to HBase is success . . .
[INFO][2018-05-29 16:49:35,244][com.seven.spark.streaming.ReceiveKafkaData$]Inserting 48 lines of data to ElasticSearch is success . . .
[INFO][2018-05-29 16:49:35,245][org.apache.spark.executor.Executor]Finished task 0.0 in stage 8.0 (TID 8). 708 bytes result sent to driver
[INFO][2018-05-29 16:49:35,246][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 8.0 (TID 8) in 137 ms on localhost (executor driver) (1/1)
[INFO][2018-05-29 16:49:35,246][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 8.0, whose tasks have all completed, from pool 
[INFO][2018-05-29 16:49:35,247][org.apache.spark.scheduler.DAGScheduler]ResultStage 8 (foreachPartition at ReceiveKafkaData.scala:76) finished in 0.138 s
[INFO][2018-05-29 16:49:35,247][org.apache.spark.scheduler.DAGScheduler]Job 8 finished: foreachPartition at ReceiveKafkaData.scala:76, took 0.159103 s
[INFO][2018-05-29 16:49:35,247][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527583775000 ms.0 from job set of time 1527583775000 ms
[INFO][2018-05-29 16:49:35,248][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.247 s for time 1527583775000 ms (execution: 0.230 s)
[INFO][2018-05-29 16:49:35,248][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 11 from persistence list
[INFO][2018-05-29 16:49:35,248][org.apache.spark.storage.BlockManager]Removing RDD 11
[INFO][2018-05-29 16:49:35,249][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 10 from persistence list
[INFO][2018-05-29 16:49:35,249][org.apache.spark.storage.BlockManager]Removing RDD 10
[INFO][2018-05-29 16:49:35,250][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-29 16:49:35,250][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527583765000 ms
[INFO][2018-05-29 16:49:40,015][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527583780000 ms
[INFO][2018-05-29 16:49:40,015][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527583780000 ms.0 from job set of time 1527583780000 ms
[INFO][2018-05-29 16:49:40,020][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:75
[INFO][2018-05-29 16:49:40,021][org.apache.spark.scheduler.DAGScheduler]Got job 9 (isEmpty at ReceiveKafkaData.scala:75) with 1 output partitions
[INFO][2018-05-29 16:49:40,021][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 9 (isEmpty at ReceiveKafkaData.scala:75)
[INFO][2018-05-29 16:49:40,021][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-29 16:49:40,021][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-29 16:49:40,022][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 9 (MapPartitionsRDD[15] at map at ReceiveKafkaData.scala:74), which has no missing parents
[INFO][2018-05-29 16:49:40,027][org.apache.spark.storage.memory.MemoryStore]Block broadcast_9 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-29 16:49:40,029][org.apache.spark.storage.memory.MemoryStore]Block broadcast_9_piece0 stored as bytes in memory (estimated size 1973.0 B, free 912.3 MB)
[INFO][2018-05-29 16:49:40,029][org.apache.spark.storage.BlockManagerInfo]Added broadcast_9_piece0 in memory on 10.194.32.157:63514 (size: 1973.0 B, free: 912.3 MB)
[INFO][2018-05-29 16:49:40,030][org.apache.spark.SparkContext]Created broadcast 9 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-29 16:49:40,030][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[15] at map at ReceiveKafkaData.scala:74) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-29 16:49:40,030][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 9.0 with 1 tasks
[INFO][2018-05-29 16:49:40,031][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 9.0 (TID 9, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-29 16:49:40,032][org.apache.spark.executor.Executor]Running task 0.0 in stage 9.0 (TID 9)
[INFO][2018-05-29 16:49:40,033][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 188818 -> 188842
[INFO][2018-05-29 16:49:40,035][org.apache.spark.executor.Executor]Finished task 0.0 in stage 9.0 (TID 9). 972 bytes result sent to driver
[INFO][2018-05-29 16:49:40,036][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 9.0 (TID 9) in 5 ms on localhost (executor driver) (1/1)
[INFO][2018-05-29 16:49:40,036][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 9.0, whose tasks have all completed, from pool 
[INFO][2018-05-29 16:49:40,036][org.apache.spark.scheduler.DAGScheduler]ResultStage 9 (isEmpty at ReceiveKafkaData.scala:75) finished in 0.005 s
[INFO][2018-05-29 16:49:40,037][org.apache.spark.scheduler.DAGScheduler]Job 9 finished: isEmpty at ReceiveKafkaData.scala:75, took 0.016536 s
[INFO][2018-05-29 16:49:40,044][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:76
[INFO][2018-05-29 16:49:40,044][org.apache.spark.scheduler.DAGScheduler]Got job 10 (foreachPartition at ReceiveKafkaData.scala:76) with 1 output partitions
[INFO][2018-05-29 16:49:40,044][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 10 (foreachPartition at ReceiveKafkaData.scala:76)
[INFO][2018-05-29 16:49:40,044][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-29 16:49:40,044][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-29 16:49:40,045][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 10 (MapPartitionsRDD[15] at map at ReceiveKafkaData.scala:74), which has no missing parents
[INFO][2018-05-29 16:49:40,049][org.apache.spark.storage.memory.MemoryStore]Block broadcast_10 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-29 16:49:40,051][org.apache.spark.storage.memory.MemoryStore]Block broadcast_10_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.3 MB)
[INFO][2018-05-29 16:49:40,051][org.apache.spark.storage.BlockManagerInfo]Added broadcast_10_piece0 in memory on 10.194.32.157:63514 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-29 16:49:40,052][org.apache.spark.SparkContext]Created broadcast 10 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-29 16:49:40,053][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[15] at map at ReceiveKafkaData.scala:74) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-29 16:49:40,053][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 10.0 with 1 tasks
[INFO][2018-05-29 16:49:40,054][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 10.0 (TID 10, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-29 16:49:40,054][org.apache.spark.executor.Executor]Running task 0.0 in stage 10.0 (TID 10)
[INFO][2018-05-29 16:49:40,056][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 188818 -> 188842
[INFO][2018-05-29 16:49:40,056][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 188818
[INFO][2018-05-29 16:49:40,103][com.seven.spark.streaming.ReceiveKafkaData$]Inserting 24 lines of data to HBase is success . . .
[INFO][2018-05-29 16:49:40,149][com.seven.spark.streaming.ReceiveKafkaData$]Inserting 24 lines of data to ElasticSearch is success . . .
[INFO][2018-05-29 16:49:40,151][org.apache.spark.executor.Executor]Finished task 0.0 in stage 10.0 (TID 10). 665 bytes result sent to driver
[INFO][2018-05-29 16:49:40,151][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 10.0 (TID 10) in 98 ms on localhost (executor driver) (1/1)
[INFO][2018-05-29 16:49:40,152][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 10.0, whose tasks have all completed, from pool 
[INFO][2018-05-29 16:49:40,152][org.apache.spark.scheduler.DAGScheduler]ResultStage 10 (foreachPartition at ReceiveKafkaData.scala:76) finished in 0.099 s
[INFO][2018-05-29 16:49:40,152][org.apache.spark.scheduler.DAGScheduler]Job 10 finished: foreachPartition at ReceiveKafkaData.scala:76, took 0.108538 s
[INFO][2018-05-29 16:49:40,153][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527583780000 ms.0 from job set of time 1527583780000 ms
[INFO][2018-05-29 16:49:40,153][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.153 s for time 1527583780000 ms (execution: 0.138 s)
[INFO][2018-05-29 16:49:40,153][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 13 from persistence list
[INFO][2018-05-29 16:49:40,153][org.apache.spark.storage.BlockManager]Removing RDD 13
[INFO][2018-05-29 16:49:40,171][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 12 from persistence list
[INFO][2018-05-29 16:49:40,171][org.apache.spark.storage.BlockManager]Removing RDD 12
[INFO][2018-05-29 16:49:40,171][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-29 16:49:40,171][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527583770000 ms
[INFO][2018-05-29 16:49:45,014][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527583785000 ms
[INFO][2018-05-29 16:49:45,014][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527583785000 ms.0 from job set of time 1527583785000 ms
[INFO][2018-05-29 16:49:45,020][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:75
[INFO][2018-05-29 16:49:45,021][org.apache.spark.scheduler.DAGScheduler]Got job 11 (isEmpty at ReceiveKafkaData.scala:75) with 1 output partitions
[INFO][2018-05-29 16:49:45,021][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 11 (isEmpty at ReceiveKafkaData.scala:75)
[INFO][2018-05-29 16:49:45,021][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-29 16:49:45,021][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-29 16:49:45,022][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 11 (MapPartitionsRDD[17] at map at ReceiveKafkaData.scala:74), which has no missing parents
[INFO][2018-05-29 16:49:45,024][org.apache.spark.storage.memory.MemoryStore]Block broadcast_11 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-29 16:49:45,025][org.apache.spark.storage.memory.MemoryStore]Block broadcast_11_piece0 stored as bytes in memory (estimated size 1970.0 B, free 912.3 MB)
[INFO][2018-05-29 16:49:45,026][org.apache.spark.storage.BlockManagerInfo]Added broadcast_11_piece0 in memory on 10.194.32.157:63514 (size: 1970.0 B, free: 912.3 MB)
[INFO][2018-05-29 16:49:45,026][org.apache.spark.SparkContext]Created broadcast 11 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-29 16:49:45,027][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[17] at map at ReceiveKafkaData.scala:74) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-29 16:49:45,027][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 11.0 with 1 tasks
[INFO][2018-05-29 16:49:45,028][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 11.0 (TID 11, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-29 16:49:45,029][org.apache.spark.executor.Executor]Running task 0.0 in stage 11.0 (TID 11)
[INFO][2018-05-29 16:49:45,030][org.apache.spark.streaming.kafka010.KafkaRDD]Beginning offset 188842 is the same as ending offset skipping seven 0
[INFO][2018-05-29 16:49:45,030][org.apache.spark.executor.Executor]Finished task 0.0 in stage 11.0 (TID 11). 665 bytes result sent to driver
[INFO][2018-05-29 16:49:45,031][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 11.0 (TID 11) in 3 ms on localhost (executor driver) (1/1)
[INFO][2018-05-29 16:49:45,031][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 11.0, whose tasks have all completed, from pool 
[INFO][2018-05-29 16:49:45,031][org.apache.spark.scheduler.DAGScheduler]ResultStage 11 (isEmpty at ReceiveKafkaData.scala:75) finished in 0.003 s
[INFO][2018-05-29 16:49:45,032][org.apache.spark.scheduler.DAGScheduler]Job 11 finished: isEmpty at ReceiveKafkaData.scala:75, took 0.011174 s
[INFO][2018-05-29 16:49:45,032][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527583785000 ms.0 from job set of time 1527583785000 ms
[INFO][2018-05-29 16:49:45,032][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 15 from persistence list
[INFO][2018-05-29 16:49:45,032][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.032 s for time 1527583785000 ms (execution: 0.018 s)
[INFO][2018-05-29 16:49:45,032][org.apache.spark.storage.BlockManager]Removing RDD 15
[INFO][2018-05-29 16:49:45,033][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 14 from persistence list
[INFO][2018-05-29 16:49:45,033][org.apache.spark.storage.BlockManager]Removing RDD 14
[INFO][2018-05-29 16:49:45,033][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-29 16:49:45,033][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527583775000 ms
[INFO][2018-05-29 16:49:50,017][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527583790000 ms
[INFO][2018-05-29 16:49:50,017][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527583790000 ms.0 from job set of time 1527583790000 ms
[INFO][2018-05-29 16:49:50,022][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:75
[INFO][2018-05-29 16:49:50,023][org.apache.spark.scheduler.DAGScheduler]Got job 12 (isEmpty at ReceiveKafkaData.scala:75) with 1 output partitions
[INFO][2018-05-29 16:49:50,023][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 12 (isEmpty at ReceiveKafkaData.scala:75)
[INFO][2018-05-29 16:49:50,023][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-29 16:49:50,023][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-29 16:49:50,024][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 12 (MapPartitionsRDD[19] at map at ReceiveKafkaData.scala:74), which has no missing parents
[INFO][2018-05-29 16:49:50,026][org.apache.spark.storage.memory.MemoryStore]Block broadcast_12 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-29 16:49:50,027][org.apache.spark.storage.memory.MemoryStore]Block broadcast_12_piece0 stored as bytes in memory (estimated size 1973.0 B, free 912.3 MB)
[INFO][2018-05-29 16:49:50,027][org.apache.spark.storage.BlockManagerInfo]Added broadcast_12_piece0 in memory on 10.194.32.157:63514 (size: 1973.0 B, free: 912.3 MB)
[INFO][2018-05-29 16:49:50,028][org.apache.spark.SparkContext]Created broadcast 12 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-29 16:49:50,028][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[19] at map at ReceiveKafkaData.scala:74) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-29 16:49:50,028][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 12.0 with 1 tasks
[INFO][2018-05-29 16:49:50,029][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 12.0 (TID 12, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-29 16:49:50,029][org.apache.spark.executor.Executor]Running task 0.0 in stage 12.0 (TID 12)
[INFO][2018-05-29 16:49:50,030][org.apache.spark.streaming.kafka010.KafkaRDD]Beginning offset 188842 is the same as ending offset skipping seven 0
[INFO][2018-05-29 16:49:50,031][org.apache.spark.executor.Executor]Finished task 0.0 in stage 12.0 (TID 12). 665 bytes result sent to driver
[INFO][2018-05-29 16:49:50,031][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 12.0 (TID 12) in 3 ms on localhost (executor driver) (1/1)
[INFO][2018-05-29 16:49:50,031][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 12.0, whose tasks have all completed, from pool 
[INFO][2018-05-29 16:49:50,032][org.apache.spark.scheduler.DAGScheduler]ResultStage 12 (isEmpty at ReceiveKafkaData.scala:75) finished in 0.004 s
[INFO][2018-05-29 16:49:50,032][org.apache.spark.scheduler.DAGScheduler]Job 12 finished: isEmpty at ReceiveKafkaData.scala:75, took 0.009622 s
[INFO][2018-05-29 16:49:50,033][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527583790000 ms.0 from job set of time 1527583790000 ms
[INFO][2018-05-29 16:49:50,033][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.033 s for time 1527583790000 ms (execution: 0.016 s)
[INFO][2018-05-29 16:49:50,033][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 17 from persistence list
[INFO][2018-05-29 16:49:50,033][org.apache.spark.storage.BlockManager]Removing RDD 17
[INFO][2018-05-29 16:49:50,034][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 16 from persistence list
[INFO][2018-05-29 16:49:50,034][org.apache.spark.storage.BlockManager]Removing RDD 16
[INFO][2018-05-29 16:49:50,034][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-29 16:49:50,034][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527583780000 ms
[INFO][2018-05-29 16:49:52,581][org.apache.spark.streaming.StreamingContext]Invoking stop(stopGracefully=false) from shutdown hook
[INFO][2018-05-29 16:49:52,583][org.apache.spark.streaming.scheduler.ReceiverTracker]ReceiverTracker stopped
[INFO][2018-05-29 16:49:52,583][org.apache.spark.streaming.scheduler.JobGenerator]Stopping JobGenerator immediately
[INFO][2018-05-29 16:49:52,584][org.apache.spark.streaming.util.RecurringTimer]Stopped timer for JobGenerator after time 1527583790000
[INFO][2018-05-29 16:49:52,591][org.apache.spark.streaming.scheduler.JobGenerator]Stopped JobGenerator
[INFO][2018-05-29 16:49:52,593][org.apache.spark.streaming.scheduler.JobScheduler]Stopped JobScheduler
[INFO][2018-05-29 16:49:52,598][org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@57adfab0{/streaming,null,UNAVAILABLE,@Spark}
[INFO][2018-05-29 16:49:52,598][org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@2a22ad2b{/streaming/batch,null,UNAVAILABLE,@Spark}
[INFO][2018-05-29 16:49:52,600][org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@5dbf5634{/static/streaming,null,UNAVAILABLE,@Spark}
[INFO][2018-05-29 16:49:52,600][org.apache.spark.streaming.StreamingContext]StreamingContext stopped successfully
[INFO][2018-05-29 16:49:52,601][org.apache.spark.SparkContext]Invoking stop() from shutdown hook
[INFO][2018-05-29 16:49:52,609][org.spark_project.jetty.server.AbstractConnector]Stopped Spark@6ff8596d{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-05-29 16:49:52,611][org.apache.spark.ui.SparkUI]Stopped Spark web UI at http://10.194.32.157:4040
[INFO][2018-05-29 16:49:52,626][org.apache.spark.MapOutputTrackerMasterEndpoint]MapOutputTrackerMasterEndpoint stopped!
[INFO][2018-05-29 16:49:52,639][org.apache.spark.storage.memory.MemoryStore]MemoryStore cleared
[INFO][2018-05-29 16:49:52,639][org.apache.spark.storage.BlockManager]BlockManager stopped
[INFO][2018-05-29 16:49:52,641][org.apache.spark.storage.BlockManagerMaster]BlockManagerMaster stopped
[INFO][2018-05-29 16:49:52,645][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint]OutputCommitCoordinator stopped!
[INFO][2018-05-29 16:49:52,647][org.apache.spark.SparkContext]Successfully stopped SparkContext
[INFO][2018-05-29 16:49:52,647][org.apache.spark.util.ShutdownHookManager]Shutdown hook called
[INFO][2018-05-29 16:49:52,648][org.apache.spark.util.ShutdownHookManager]Deleting directory /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/spark-f7465409-bcf6-4810-bf17-f2761eff0b1a
[INFO][2018-05-29 17:01:54,390][org.apache.spark.SparkContext]Running Spark version 2.2.0
[INFO][2018-05-29 17:01:55,423][org.apache.spark.SparkContext]Submitted application: ReceiveKafkaData$
[INFO][2018-05-29 17:01:55,446][org.apache.spark.SecurityManager]Changing view acls to: seven
[INFO][2018-05-29 17:01:55,446][org.apache.spark.SecurityManager]Changing modify acls to: seven
[INFO][2018-05-29 17:01:55,447][org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO][2018-05-29 17:01:55,448][org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO][2018-05-29 17:01:55,448][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(seven); groups with view permissions: Set(); users  with modify permissions: Set(seven); groups with modify permissions: Set()
[INFO][2018-05-29 17:01:55,744][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 63704.
[INFO][2018-05-29 17:01:55,770][org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO][2018-05-29 17:01:55,801][org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO][2018-05-29 17:01:55,807][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2018-05-29 17:01:55,807][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
[INFO][2018-05-29 17:01:55,819][org.apache.spark.storage.DiskBlockManager]Created local directory at /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/blockmgr-8a6aa8cc-3898-4744-8865-ab06fb5c90df
[INFO][2018-05-29 17:01:55,836][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 912.3 MB
[INFO][2018-05-29 17:01:55,958][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO][2018-05-29 17:01:56,075][org.spark_project.jetty.util.log]Logging initialized @2678ms
[INFO][2018-05-29 17:01:56,127][org.spark_project.jetty.server.Server]jetty-9.3.z-SNAPSHOT
[INFO][2018-05-29 17:01:56,138][org.spark_project.jetty.server.Server]Started @2742ms
[INFO][2018-05-29 17:01:56,159][org.spark_project.jetty.server.AbstractConnector]Started ServerConnector@5fdff907{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-05-29 17:01:56,159][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4040.
[INFO][2018-05-29 17:01:56,190][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3e2822{/jobs,null,AVAILABLE,@Spark}
[INFO][2018-05-29 17:01:56,191][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@35fe2125{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2018-05-29 17:01:56,192][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@34645867{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2018-05-29 17:01:56,193][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1255b1d1{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2018-05-29 17:01:56,195][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7c22d4f{/stages,null,AVAILABLE,@Spark}
[INFO][2018-05-29 17:01:56,196][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@60bdf15d{/stages/json,null,AVAILABLE,@Spark}
[INFO][2018-05-29 17:01:56,197][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@51e4ccb3{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2018-05-29 17:01:56,198][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@28a0fd6c{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-29 17:01:56,199][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@66629f63{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2018-05-29 17:01:56,200][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@27a5328c{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2018-05-29 17:01:56,200][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6c345c5f{/storage,null,AVAILABLE,@Spark}
[INFO][2018-05-29 17:01:56,201][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@65e61854{/storage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-29 17:01:56,202][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4fcee388{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2018-05-29 17:01:56,203][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3af17be2{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2018-05-29 17:01:56,203][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@37f21974{/environment,null,AVAILABLE,@Spark}
[INFO][2018-05-29 17:01:56,204][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6e521c1e{/environment/json,null,AVAILABLE,@Spark}
[INFO][2018-05-29 17:01:56,205][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5d5d9e5{/executors,null,AVAILABLE,@Spark}
[INFO][2018-05-29 17:01:56,206][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4ef27d66{/executors/json,null,AVAILABLE,@Spark}
[INFO][2018-05-29 17:01:56,206][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1d9bec4d{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2018-05-29 17:01:56,207][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@10c8f62{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2018-05-29 17:01:56,215][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@25f7391e{/static,null,AVAILABLE,@Spark}
[INFO][2018-05-29 17:01:56,219][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@593e824f{/,null,AVAILABLE,@Spark}
[INFO][2018-05-29 17:01:56,221][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6d8792db{/api,null,AVAILABLE,@Spark}
[INFO][2018-05-29 17:01:56,221][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2f162cc0{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-29 17:01:56,222][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7c041b41{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-29 17:01:56,225][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://10.194.32.157:4040
[INFO][2018-05-29 17:01:56,313][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
[INFO][2018-05-29 17:01:56,361][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 63705.
[INFO][2018-05-29 17:01:56,362][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 10.194.32.157:63705
[INFO][2018-05-29 17:01:56,370][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2018-05-29 17:01:56,381][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 10.194.32.157, 63705, None)
[INFO][2018-05-29 17:01:56,385][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 10.194.32.157:63705 with 912.3 MB RAM, BlockManagerId(driver, 10.194.32.157, 63705, None)
[INFO][2018-05-29 17:01:56,391][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 10.194.32.157, 63705, None)
[INFO][2018-05-29 17:01:56,391][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 10.194.32.157, 63705, None)
[INFO][2018-05-29 17:01:56,706][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@55b62629{/metrics/json,null,AVAILABLE,@Spark}
[WARN][2018-05-29 17:01:57,053][org.apache.spark.streaming.kafka010.KafkaUtils]overriding enable.auto.commit to false for executor
[WARN][2018-05-29 17:01:57,055][org.apache.spark.streaming.kafka010.KafkaUtils]overriding auto.offset.reset to none for executor
[WARN][2018-05-29 17:01:57,055][org.apache.spark.streaming.kafka010.KafkaUtils]overriding executor group.id to spark-executor-use_a_separate_group_id_for_each_stream
[WARN][2018-05-29 17:01:57,056][org.apache.spark.streaming.kafka010.KafkaUtils]overriding receive.buffer.bytes to 65536 see KAFKA-3135
[INFO][2018-05-29 17:01:57,129][org.apache.spark.streaming.kafka010.DirectKafkaInputDStream]Slide time = 5000 ms
[INFO][2018-05-29 17:01:57,129][org.apache.spark.streaming.kafka010.DirectKafkaInputDStream]Storage level = Serialized 1x Replicated
[INFO][2018-05-29 17:01:57,130][org.apache.spark.streaming.kafka010.DirectKafkaInputDStream]Checkpoint interval = null
[INFO][2018-05-29 17:01:57,130][org.apache.spark.streaming.kafka010.DirectKafkaInputDStream]Remember interval = 5000 ms
[INFO][2018-05-29 17:01:57,131][org.apache.spark.streaming.kafka010.DirectKafkaInputDStream]Initialized and validated org.apache.spark.streaming.kafka010.DirectKafkaInputDStream@294274f0
[INFO][2018-05-29 17:01:57,131][org.apache.spark.streaming.dstream.MappedDStream]Slide time = 5000 ms
[INFO][2018-05-29 17:01:57,131][org.apache.spark.streaming.dstream.MappedDStream]Storage level = Serialized 1x Replicated
[INFO][2018-05-29 17:01:57,132][org.apache.spark.streaming.dstream.MappedDStream]Checkpoint interval = null
[INFO][2018-05-29 17:01:57,132][org.apache.spark.streaming.dstream.MappedDStream]Remember interval = 5000 ms
[INFO][2018-05-29 17:01:57,132][org.apache.spark.streaming.dstream.MappedDStream]Initialized and validated org.apache.spark.streaming.dstream.MappedDStream@73e3960a
[INFO][2018-05-29 17:01:57,132][org.apache.spark.streaming.dstream.ForEachDStream]Slide time = 5000 ms
[INFO][2018-05-29 17:01:57,132][org.apache.spark.streaming.dstream.ForEachDStream]Storage level = Serialized 1x Replicated
[INFO][2018-05-29 17:01:57,132][org.apache.spark.streaming.dstream.ForEachDStream]Checkpoint interval = null
[INFO][2018-05-29 17:01:57,132][org.apache.spark.streaming.dstream.ForEachDStream]Remember interval = 5000 ms
[INFO][2018-05-29 17:01:57,132][org.apache.spark.streaming.dstream.ForEachDStream]Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@1f5c5ab9
[INFO][2018-05-29 17:01:57,190][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	metric.reporters = []
	metadata.max.age.ms = 300000
	partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor]
	reconnect.backoff.ms = 50
	sasl.kerberos.ticket.renew.window.factor = 0.8
	max.partition.fetch.bytes = 1048576
	bootstrap.servers = [vm-xaj-bigdata-da-d01:9092, vm-xaj-bigdata-da-d02:9092, vm-xaj-bigdata-da-d03:9092, vm-xaj-bigdata-da-d04:9092, vm-xaj-bigdata-da-d05:9092, vm-xaj-bigdata-da-d06:9092, vm-xaj-bigdata-da-d07:9092]
	ssl.keystore.type = JKS
	enable.auto.commit = false
	sasl.mechanism = GSSAPI
	interceptor.classes = null
	exclude.internal.topics = true
	ssl.truststore.password = null
	client.id = 
	ssl.endpoint.identification.algorithm = null
	max.poll.records = 2147483647
	check.crcs = true
	request.timeout.ms = 40000
	heartbeat.interval.ms = 3000
	auto.commit.interval.ms = 5000
	receive.buffer.bytes = 65536
	ssl.truststore.type = JKS
	ssl.truststore.location = null
	ssl.keystore.password = null
	fetch.min.bytes = 1
	send.buffer.bytes = 131072
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	group.id = use_a_separate_group_id_for_each_stream
	retry.backoff.ms = 100
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	ssl.trustmanager.algorithm = PKIX
	ssl.key.password = null
	fetch.max.wait.ms = 500
	sasl.kerberos.min.time.before.relogin = 60000
	connections.max.idle.ms = 540000
	session.timeout.ms = 30000
	metrics.num.samples = 2
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	ssl.protocol = TLS
	ssl.provider = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.keystore.location = null
	ssl.cipher.suites = null
	security.protocol = PLAINTEXT
	ssl.keymanager.algorithm = SunX509
	metrics.sample.window.ms = 30000
	auto.offset.reset = latest

[INFO][2018-05-29 17:02:04,338][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	metric.reporters = []
	metadata.max.age.ms = 300000
	partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor]
	reconnect.backoff.ms = 50
	sasl.kerberos.ticket.renew.window.factor = 0.8
	max.partition.fetch.bytes = 1048576
	bootstrap.servers = [vm-xaj-bigdata-da-d01:9092, vm-xaj-bigdata-da-d02:9092, vm-xaj-bigdata-da-d03:9092, vm-xaj-bigdata-da-d04:9092, vm-xaj-bigdata-da-d05:9092, vm-xaj-bigdata-da-d06:9092, vm-xaj-bigdata-da-d07:9092]
	ssl.keystore.type = JKS
	enable.auto.commit = false
	sasl.mechanism = GSSAPI
	interceptor.classes = null
	exclude.internal.topics = true
	ssl.truststore.password = null
	client.id = consumer-1
	ssl.endpoint.identification.algorithm = null
	max.poll.records = 2147483647
	check.crcs = true
	request.timeout.ms = 40000
	heartbeat.interval.ms = 3000
	auto.commit.interval.ms = 5000
	receive.buffer.bytes = 65536
	ssl.truststore.type = JKS
	ssl.truststore.location = null
	ssl.keystore.password = null
	fetch.min.bytes = 1
	send.buffer.bytes = 131072
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	group.id = use_a_separate_group_id_for_each_stream
	retry.backoff.ms = 100
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	ssl.trustmanager.algorithm = PKIX
	ssl.key.password = null
	fetch.max.wait.ms = 500
	sasl.kerberos.min.time.before.relogin = 60000
	connections.max.idle.ms = 540000
	session.timeout.ms = 30000
	metrics.num.samples = 2
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	ssl.protocol = TLS
	ssl.provider = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.keystore.location = null
	ssl.cipher.suites = null
	security.protocol = PLAINTEXT
	ssl.keymanager.algorithm = SunX509
	metrics.sample.window.ms = 30000
	auto.offset.reset = latest

[INFO][2018-05-29 17:02:04,359][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 0.10.0.1
[INFO][2018-05-29 17:02:04,360][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a7a17cdec9eaa6c5
[INFO][2018-05-29 17:02:04,525][org.apache.kafka.clients.consumer.internals.AbstractCoordinator]Discovered coordinator vm-xaj-bigdata-da-d03:9092 (id: 2147483531 rack: null) for group use_a_separate_group_id_for_each_stream.
[INFO][2018-05-29 17:02:04,525][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]Revoking previously assigned partitions [] for group use_a_separate_group_id_for_each_stream
[INFO][2018-05-29 17:02:04,526][org.apache.kafka.clients.consumer.internals.AbstractCoordinator](Re-)joining group use_a_separate_group_id_for_each_stream
[INFO][2018-05-29 17:02:05,825][org.apache.spark.SparkContext]Running Spark version 2.2.0
[INFO][2018-05-29 17:02:06,718][org.apache.spark.SparkContext]Submitted application: SimulationKafkaSendOutData$
[INFO][2018-05-29 17:02:06,743][org.apache.spark.SecurityManager]Changing view acls to: seven
[INFO][2018-05-29 17:02:06,754][org.apache.spark.SecurityManager]Changing modify acls to: seven
[INFO][2018-05-29 17:02:06,755][org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO][2018-05-29 17:02:06,756][org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO][2018-05-29 17:02:06,757][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(seven); groups with view permissions: Set(); users  with modify permissions: Set(seven); groups with modify permissions: Set()
[INFO][2018-05-29 17:02:07,229][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 63717.
[INFO][2018-05-29 17:02:07,251][org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO][2018-05-29 17:02:07,266][org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO][2018-05-29 17:02:07,269][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2018-05-29 17:02:07,269][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
[INFO][2018-05-29 17:02:07,280][org.apache.spark.storage.DiskBlockManager]Created local directory at /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/blockmgr-8eab62ac-6786-4585-a833-04cd6c67472d
[INFO][2018-05-29 17:02:07,297][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 912.3 MB
[INFO][2018-05-29 17:02:07,382][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO][2018-05-29 17:02:07,466][org.spark_project.jetty.util.log]Logging initialized @2528ms
[INFO][2018-05-29 17:02:07,538][org.spark_project.jetty.server.Server]jetty-9.3.z-SNAPSHOT
[INFO][2018-05-29 17:02:07,551][org.spark_project.jetty.server.Server]Started @2614ms
[WARN][2018-05-29 17:02:07,564][org.apache.spark.util.Utils]Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[INFO][2018-05-29 17:02:07,569][org.spark_project.jetty.server.AbstractConnector]Started ServerConnector@208a9c91{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}
[INFO][2018-05-29 17:02:07,569][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4041.
[INFO][2018-05-29 17:02:07,589][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@35beb15e{/jobs,null,AVAILABLE,@Spark}
[INFO][2018-05-29 17:02:07,590][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@79e18e38{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2018-05-29 17:02:07,590][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1849db1a{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2018-05-29 17:02:07,591][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5fcacc0{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2018-05-29 17:02:07,592][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6d1d4d7{/stages,null,AVAILABLE,@Spark}
[INFO][2018-05-29 17:02:07,593][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6865c751{/stages/json,null,AVAILABLE,@Spark}
[INFO][2018-05-29 17:02:07,593][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6a988392{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2018-05-29 17:02:07,594][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2b58f754{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-29 17:02:07,595][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2552f2cb{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2018-05-29 17:02:07,596][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5f3b9c57{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2018-05-29 17:02:07,596][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2cf23c81{/storage,null,AVAILABLE,@Spark}
[INFO][2018-05-29 17:02:07,597][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@35fe2125{/storage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-29 17:02:07,598][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@34645867{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2018-05-29 17:02:07,599][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@60b71e8f{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2018-05-29 17:02:07,599][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@464649c{/environment,null,AVAILABLE,@Spark}
[INFO][2018-05-29 17:02:07,600][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5f59185e{/environment/json,null,AVAILABLE,@Spark}
[INFO][2018-05-29 17:02:07,601][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@47da3952{/executors,null,AVAILABLE,@Spark}
[INFO][2018-05-29 17:02:07,602][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@46e8a539{/executors/json,null,AVAILABLE,@Spark}
[INFO][2018-05-29 17:02:07,602][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5fd62371{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2018-05-29 17:02:07,603][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2b62442c{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2018-05-29 17:02:07,615][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@841e575{/static,null,AVAILABLE,@Spark}
[INFO][2018-05-29 17:02:07,619][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5c48c0c0{/,null,AVAILABLE,@Spark}
[INFO][2018-05-29 17:02:07,623][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@674c583e{/api,null,AVAILABLE,@Spark}
[INFO][2018-05-29 17:02:07,624][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@61861a29{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-29 17:02:07,625][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@25bcd0c7{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-29 17:02:07,627][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://10.194.32.157:4041
[INFO][2018-05-29 17:02:07,661][org.apache.kafka.clients.consumer.internals.AbstractCoordinator]Successfully joined group use_a_separate_group_id_for_each_stream with generation 1
[INFO][2018-05-29 17:02:07,662][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]Setting newly assigned partitions [seven-0] for group use_a_separate_group_id_for_each_stream
[INFO][2018-05-29 17:02:07,700][org.apache.spark.streaming.util.RecurringTimer]Started timer for JobGenerator at time 1527584520000
[INFO][2018-05-29 17:02:07,701][org.apache.spark.streaming.scheduler.JobGenerator]Started JobGenerator at 1527584520000 ms
[INFO][2018-05-29 17:02:07,701][org.apache.spark.streaming.scheduler.JobScheduler]Started JobScheduler
[INFO][2018-05-29 17:02:07,704][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@57adfab0{/streaming,null,AVAILABLE,@Spark}
[INFO][2018-05-29 17:02:07,705][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5af97169{/streaming/json,null,AVAILABLE,@Spark}
[INFO][2018-05-29 17:02:07,706][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2a22ad2b{/streaming/batch,null,AVAILABLE,@Spark}
[INFO][2018-05-29 17:02:07,708][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@121c54fa{/streaming/batch/json,null,AVAILABLE,@Spark}
[INFO][2018-05-29 17:02:07,714][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5dbf5634{/static/streaming,null,AVAILABLE,@Spark}
[INFO][2018-05-29 17:02:07,715][org.apache.spark.streaming.StreamingContext]StreamingContext started
[INFO][2018-05-29 17:02:07,775][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
[INFO][2018-05-29 17:02:07,805][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 63719.
[INFO][2018-05-29 17:02:07,806][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 10.194.32.157:63719
[INFO][2018-05-29 17:02:07,807][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2018-05-29 17:02:07,809][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 10.194.32.157, 63719, None)
[INFO][2018-05-29 17:02:07,812][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 10.194.32.157:63719 with 912.3 MB RAM, BlockManagerId(driver, 10.194.32.157, 63719, None)
[INFO][2018-05-29 17:02:07,815][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 10.194.32.157, 63719, None)
[INFO][2018-05-29 17:02:07,816][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 10.194.32.157, 63719, None)
[INFO][2018-05-29 17:02:08,131][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4fbb001b{/metrics/json,null,AVAILABLE,@Spark}
[INFO][2018-05-29 17:02:08,400][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527584520000 ms
[INFO][2018-05-29 17:02:08,402][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527584520000 ms.0 from job set of time 1527584520000 ms
[INFO][2018-05-29 17:02:08,413][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527584525000 ms
[INFO][2018-05-29 17:02:08,521][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:75
[INFO][2018-05-29 17:02:08,538][org.apache.spark.scheduler.DAGScheduler]Got job 0 (isEmpty at ReceiveKafkaData.scala:75) with 1 output partitions
[INFO][2018-05-29 17:02:08,539][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 0 (isEmpty at ReceiveKafkaData.scala:75)
[INFO][2018-05-29 17:02:08,539][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-29 17:02:08,541][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-29 17:02:08,551][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 0 (MapPartitionsRDD[1] at map at ReceiveKafkaData.scala:74), which has no missing parents
[INFO][2018-05-29 17:02:08,680][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-29 17:02:08,717][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0_piece0 stored as bytes in memory (estimated size 1972.0 B, free 912.3 MB)
[INFO][2018-05-29 17:02:08,719][org.apache.spark.storage.BlockManagerInfo]Added broadcast_0_piece0 in memory on 10.194.32.157:63705 (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-29 17:02:08,725][org.apache.spark.SparkContext]Created broadcast 0 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-29 17:02:08,747][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at map at ReceiveKafkaData.scala:74) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-29 17:02:08,748][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 0.0 with 1 tasks
[INFO][2018-05-29 17:02:08,790][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-29 17:02:08,801][org.apache.spark.executor.Executor]Running task 0.0 in stage 0.0 (TID 0)
[INFO][2018-05-29 17:02:08,831][org.apache.spark.streaming.kafka010.KafkaRDD]Beginning offset 188842 is the same as ending offset skipping seven 0
[INFO][2018-05-29 17:02:08,847][org.apache.spark.executor.Executor]Finished task 0.0 in stage 0.0 (TID 0). 708 bytes result sent to driver
[INFO][2018-05-29 17:02:08,854][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 0.0 (TID 0) in 73 ms on localhost (executor driver) (1/1)
[INFO][2018-05-29 17:02:08,856][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO][2018-05-29 17:02:08,862][org.apache.spark.scheduler.DAGScheduler]ResultStage 0 (isEmpty at ReceiveKafkaData.scala:75) finished in 0.092 s
[INFO][2018-05-29 17:02:08,876][org.apache.spark.scheduler.DAGScheduler]Job 0 finished: isEmpty at ReceiveKafkaData.scala:75, took 0.354086 s
[INFO][2018-05-29 17:02:08,879][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527584520000 ms.0 from job set of time 1527584520000 ms
[INFO][2018-05-29 17:02:08,880][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 8.878 s for time 1527584520000 ms (execution: 0.477 s)
[INFO][2018-05-29 17:02:08,880][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527584525000 ms.0 from job set of time 1527584525000 ms
[INFO][2018-05-29 17:02:08,886][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-29 17:02:08,887][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:75
[INFO][2018-05-29 17:02:08,889][org.apache.spark.scheduler.DAGScheduler]Got job 1 (isEmpty at ReceiveKafkaData.scala:75) with 1 output partitions
[INFO][2018-05-29 17:02:08,889][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 1 (isEmpty at ReceiveKafkaData.scala:75)
[INFO][2018-05-29 17:02:08,889][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-29 17:02:08,889][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-29 17:02:08,890][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 1 (MapPartitionsRDD[3] at map at ReceiveKafkaData.scala:74), which has no missing parents
[INFO][2018-05-29 17:02:08,893][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-29 17:02:08,897][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 
[INFO][2018-05-29 17:02:08,898][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1_piece0 stored as bytes in memory (estimated size 1973.0 B, free 912.3 MB)
[INFO][2018-05-29 17:02:08,900][org.apache.spark.storage.BlockManagerInfo]Added broadcast_1_piece0 in memory on 10.194.32.157:63705 (size: 1973.0 B, free: 912.3 MB)
[INFO][2018-05-29 17:02:08,901][org.apache.spark.SparkContext]Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-29 17:02:08,902][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at map at ReceiveKafkaData.scala:74) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-29 17:02:08,902][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 1.0 with 1 tasks
[INFO][2018-05-29 17:02:08,903][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-29 17:02:08,904][org.apache.spark.executor.Executor]Running task 0.0 in stage 1.0 (TID 1)
[INFO][2018-05-29 17:02:08,910][org.apache.spark.streaming.kafka010.KafkaRDD]Beginning offset 188842 is the same as ending offset skipping seven 0
[INFO][2018-05-29 17:02:08,911][org.apache.spark.executor.Executor]Finished task 0.0 in stage 1.0 (TID 1). 708 bytes result sent to driver
[INFO][2018-05-29 17:02:08,913][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 1.0 (TID 1) in 10 ms on localhost (executor driver) (1/1)
[INFO][2018-05-29 17:02:08,913][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO][2018-05-29 17:02:08,914][org.apache.spark.scheduler.DAGScheduler]ResultStage 1 (isEmpty at ReceiveKafkaData.scala:75) finished in 0.009 s
[INFO][2018-05-29 17:02:08,914][org.apache.spark.scheduler.DAGScheduler]Job 1 finished: isEmpty at ReceiveKafkaData.scala:75, took 0.026870 s
[INFO][2018-05-29 17:02:08,915][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527584525000 ms.0 from job set of time 1527584525000 ms
[INFO][2018-05-29 17:02:08,916][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 3.915 s for time 1527584525000 ms (execution: 0.035 s)
[INFO][2018-05-29 17:02:08,917][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 1 from persistence list
[INFO][2018-05-29 17:02:08,922][org.apache.spark.storage.BlockManager]Removing RDD 1
[INFO][2018-05-29 17:02:08,925][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 0 from persistence list
[INFO][2018-05-29 17:02:08,925][org.apache.spark.storage.BlockManager]Removing RDD 0
[INFO][2018-05-29 17:02:08,926][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-29 17:02:08,926][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 
[INFO][2018-05-29 17:02:09,022][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0 stored as values in memory (estimated size 228.1 KB, free 912.1 MB)
[INFO][2018-05-29 17:02:09,096][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0_piece0 stored as bytes in memory (estimated size 22.1 KB, free 912.1 MB)
[INFO][2018-05-29 17:02:09,099][org.apache.spark.storage.BlockManagerInfo]Added broadcast_0_piece0 in memory on 10.194.32.157:63719 (size: 22.1 KB, free: 912.3 MB)
[INFO][2018-05-29 17:02:09,105][org.apache.spark.SparkContext]Created broadcast 0 from textFile at SimulationKafkaSendOutData.scala:24
[WARN][2018-05-29 17:02:09,562][org.apache.hadoop.util.NativeCodeLoader]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO][2018-05-29 17:02:09,701][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 1
[INFO][2018-05-29 17:02:09,780][org.apache.spark.SparkContext]Starting job: collect at SimulationKafkaSendOutData.scala:24
[INFO][2018-05-29 17:02:09,796][org.apache.spark.scheduler.DAGScheduler]Got job 0 (collect at SimulationKafkaSendOutData.scala:24) with 2 output partitions
[INFO][2018-05-29 17:02:09,797][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 0 (collect at SimulationKafkaSendOutData.scala:24)
[INFO][2018-05-29 17:02:09,797][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-29 17:02:09,798][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-29 17:02:09,804][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 0 (hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/* MapPartitionsRDD[1] at textFile at SimulationKafkaSendOutData.scala:24), which has no missing parents
[INFO][2018-05-29 17:02:09,820][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1 stored as values in memory (estimated size 3.3 KB, free 912.1 MB)
[INFO][2018-05-29 17:02:09,822][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1_piece0 stored as bytes in memory (estimated size 1969.0 B, free 912.1 MB)
[INFO][2018-05-29 17:02:09,822][org.apache.spark.storage.BlockManagerInfo]Added broadcast_1_piece0 in memory on 10.194.32.157:63719 (size: 1969.0 B, free: 912.3 MB)
[INFO][2018-05-29 17:02:09,823][org.apache.spark.SparkContext]Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-29 17:02:09,838][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ResultStage 0 (hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/* MapPartitionsRDD[1] at textFile at SimulationKafkaSendOutData.scala:24) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2018-05-29 17:02:09,838][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 0.0 with 2 tasks
[INFO][2018-05-29 17:02:09,876][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 4883 bytes)
[INFO][2018-05-29 17:02:09,878][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 4883 bytes)
[INFO][2018-05-29 17:02:09,886][org.apache.spark.executor.Executor]Running task 1.0 in stage 0.0 (TID 1)
[INFO][2018-05-29 17:02:09,886][org.apache.spark.executor.Executor]Running task 0.0 in stage 0.0 (TID 0)
[INFO][2018-05-29 17:02:09,940][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/part-00000:11987589+11987590
[INFO][2018-05-29 17:02:09,940][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/part-00000:0+11987589
[INFO][2018-05-29 17:02:10,015][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527584530000 ms
[INFO][2018-05-29 17:02:10,015][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527584530000 ms.0 from job set of time 1527584530000 ms
[INFO][2018-05-29 17:02:10,023][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:75
[INFO][2018-05-29 17:02:10,024][org.apache.spark.scheduler.DAGScheduler]Got job 2 (isEmpty at ReceiveKafkaData.scala:75) with 1 output partitions
[INFO][2018-05-29 17:02:10,024][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 2 (isEmpty at ReceiveKafkaData.scala:75)
[INFO][2018-05-29 17:02:10,024][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-29 17:02:10,024][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-29 17:02:10,025][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 2 (MapPartitionsRDD[5] at map at ReceiveKafkaData.scala:74), which has no missing parents
[INFO][2018-05-29 17:02:10,035][org.apache.spark.storage.memory.MemoryStore]Block broadcast_2 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-29 17:02:10,039][org.apache.spark.storage.memory.MemoryStore]Block broadcast_2_piece0 stored as bytes in memory (estimated size 1973.0 B, free 912.3 MB)
[INFO][2018-05-29 17:02:10,044][org.apache.spark.storage.BlockManagerInfo]Added broadcast_2_piece0 in memory on 10.194.32.157:63705 (size: 1973.0 B, free: 912.3 MB)
[INFO][2018-05-29 17:02:10,046][org.apache.spark.SparkContext]Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-29 17:02:10,049][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[5] at map at ReceiveKafkaData.scala:74) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-29 17:02:10,050][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 2.0 with 1 tasks
[INFO][2018-05-29 17:02:10,053][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-29 17:02:10,053][org.apache.spark.executor.Executor]Running task 0.0 in stage 2.0 (TID 2)
[INFO][2018-05-29 17:02:10,058][org.apache.spark.streaming.kafka010.KafkaRDD]Beginning offset 188842 is the same as ending offset skipping seven 0
[INFO][2018-05-29 17:02:10,059][org.apache.spark.executor.Executor]Finished task 0.0 in stage 2.0 (TID 2). 751 bytes result sent to driver
[INFO][2018-05-29 17:02:10,060][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 2.0 (TID 2) in 8 ms on localhost (executor driver) (1/1)
[INFO][2018-05-29 17:02:10,060][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO][2018-05-29 17:02:10,061][org.apache.spark.scheduler.DAGScheduler]ResultStage 2 (isEmpty at ReceiveKafkaData.scala:75) finished in 0.010 s
[INFO][2018-05-29 17:02:10,062][org.apache.spark.scheduler.DAGScheduler]Job 2 finished: isEmpty at ReceiveKafkaData.scala:75, took 0.038568 s
[INFO][2018-05-29 17:02:10,062][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527584530000 ms.0 from job set of time 1527584530000 ms
[INFO][2018-05-29 17:02:10,062][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 3 from persistence list
[INFO][2018-05-29 17:02:10,063][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.062 s for time 1527584530000 ms (execution: 0.047 s)
[INFO][2018-05-29 17:02:10,064][org.apache.spark.storage.BlockManager]Removing RDD 3
[INFO][2018-05-29 17:02:10,064][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 2 from persistence list
[INFO][2018-05-29 17:02:10,068][org.apache.spark.storage.BlockManager]Removing RDD 2
[INFO][2018-05-29 17:02:10,068][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-29 17:02:10,068][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527584520000 ms
[INFO][2018-05-29 17:02:15,024][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527584535000 ms
[INFO][2018-05-29 17:02:15,024][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527584535000 ms.0 from job set of time 1527584535000 ms
[INFO][2018-05-29 17:02:15,034][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:75
[INFO][2018-05-29 17:02:15,036][org.apache.spark.scheduler.DAGScheduler]Got job 3 (isEmpty at ReceiveKafkaData.scala:75) with 1 output partitions
[INFO][2018-05-29 17:02:15,036][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 3 (isEmpty at ReceiveKafkaData.scala:75)
[INFO][2018-05-29 17:02:15,036][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-29 17:02:15,036][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-29 17:02:15,037][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 3 (MapPartitionsRDD[7] at map at ReceiveKafkaData.scala:74), which has no missing parents
[INFO][2018-05-29 17:02:15,042][org.apache.spark.storage.memory.MemoryStore]Block broadcast_3 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-29 17:02:15,044][org.apache.spark.storage.memory.MemoryStore]Block broadcast_3_piece0 stored as bytes in memory (estimated size 1971.0 B, free 912.3 MB)
[INFO][2018-05-29 17:02:15,046][org.apache.spark.storage.BlockManagerInfo]Added broadcast_3_piece0 in memory on 10.194.32.157:63705 (size: 1971.0 B, free: 912.3 MB)
[INFO][2018-05-29 17:02:15,047][org.apache.spark.SparkContext]Created broadcast 3 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-29 17:02:15,048][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[7] at map at ReceiveKafkaData.scala:74) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-29 17:02:15,049][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 3.0 with 1 tasks
[INFO][2018-05-29 17:02:15,051][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 3.0 (TID 3, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-29 17:02:15,051][org.apache.spark.executor.Executor]Running task 0.0 in stage 3.0 (TID 3)
[INFO][2018-05-29 17:02:15,057][org.apache.spark.streaming.kafka010.KafkaRDD]Beginning offset 188842 is the same as ending offset skipping seven 0
[INFO][2018-05-29 17:02:15,058][org.apache.spark.executor.Executor]Finished task 0.0 in stage 3.0 (TID 3). 708 bytes result sent to driver
[INFO][2018-05-29 17:02:15,060][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 3.0 (TID 3) in 9 ms on localhost (executor driver) (1/1)
[INFO][2018-05-29 17:02:15,060][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 3.0, whose tasks have all completed, from pool 
[INFO][2018-05-29 17:02:15,061][org.apache.spark.scheduler.DAGScheduler]ResultStage 3 (isEmpty at ReceiveKafkaData.scala:75) finished in 0.010 s
[INFO][2018-05-29 17:02:15,063][org.apache.spark.scheduler.DAGScheduler]Job 3 finished: isEmpty at ReceiveKafkaData.scala:75, took 0.028722 s
[INFO][2018-05-29 17:02:15,064][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527584535000 ms.0 from job set of time 1527584535000 ms
[INFO][2018-05-29 17:02:15,065][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.064 s for time 1527584535000 ms (execution: 0.040 s)
[INFO][2018-05-29 17:02:15,065][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 5 from persistence list
[INFO][2018-05-29 17:02:15,067][org.apache.spark.storage.BlockManager]Removing RDD 5
[INFO][2018-05-29 17:02:15,068][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 4 from persistence list
[INFO][2018-05-29 17:02:15,069][org.apache.spark.storage.BlockManager]Removing RDD 4
[INFO][2018-05-29 17:02:15,069][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-29 17:02:15,069][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527584525000 ms
[INFO][2018-05-29 17:02:19,965][org.apache.spark.storage.memory.MemoryStore]Block taskresult_1 stored as bytes in memory (estimated size 11.6 MB, free 900.5 MB)
[INFO][2018-05-29 17:02:19,966][org.apache.spark.storage.BlockManagerInfo]Added taskresult_1 in memory on 10.194.32.157:63719 (size: 11.6 MB, free: 900.7 MB)
[INFO][2018-05-29 17:02:19,967][org.apache.spark.executor.Executor]Finished task 1.0 in stage 0.0 (TID 1). 12142516 bytes result sent via BlockManager)
[INFO][2018-05-29 17:02:20,006][org.apache.spark.network.client.TransportClientFactory]Successfully created connection to /10.194.32.157:63719 after 21 ms (0 ms spent in bootstraps)
[INFO][2018-05-29 17:02:20,018][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527584540000 ms
[INFO][2018-05-29 17:02:20,024][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527584540000 ms.0 from job set of time 1527584540000 ms
[INFO][2018-05-29 17:02:20,033][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:75
[INFO][2018-05-29 17:02:20,035][org.apache.spark.scheduler.DAGScheduler]Got job 4 (isEmpty at ReceiveKafkaData.scala:75) with 1 output partitions
[INFO][2018-05-29 17:02:20,035][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 4 (isEmpty at ReceiveKafkaData.scala:75)
[INFO][2018-05-29 17:02:20,035][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-29 17:02:20,035][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-29 17:02:20,036][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 4 (MapPartitionsRDD[9] at map at ReceiveKafkaData.scala:74), which has no missing parents
[INFO][2018-05-29 17:02:20,038][org.apache.spark.storage.memory.MemoryStore]Block broadcast_4 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-29 17:02:20,040][org.apache.spark.storage.memory.MemoryStore]Block broadcast_4_piece0 stored as bytes in memory (estimated size 1973.0 B, free 912.3 MB)
[INFO][2018-05-29 17:02:20,041][org.apache.spark.storage.BlockManagerInfo]Added broadcast_4_piece0 in memory on 10.194.32.157:63705 (size: 1973.0 B, free: 912.3 MB)
[INFO][2018-05-29 17:02:20,041][org.apache.spark.SparkContext]Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-29 17:02:20,042][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[9] at map at ReceiveKafkaData.scala:74) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-29 17:02:20,042][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 4.0 with 1 tasks
[INFO][2018-05-29 17:02:20,043][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 4.0 (TID 4, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-29 17:02:20,044][org.apache.spark.executor.Executor]Running task 0.0 in stage 4.0 (TID 4)
[INFO][2018-05-29 17:02:20,047][org.apache.spark.streaming.kafka010.KafkaRDD]Beginning offset 188842 is the same as ending offset skipping seven 0
[INFO][2018-05-29 17:02:20,048][org.apache.spark.executor.Executor]Finished task 0.0 in stage 4.0 (TID 4). 708 bytes result sent to driver
[INFO][2018-05-29 17:02:20,049][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 4.0 (TID 4) in 6 ms on localhost (executor driver) (1/1)
[INFO][2018-05-29 17:02:20,049][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 4.0, whose tasks have all completed, from pool 
[INFO][2018-05-29 17:02:20,049][org.apache.spark.scheduler.DAGScheduler]ResultStage 4 (isEmpty at ReceiveKafkaData.scala:75) finished in 0.006 s
[INFO][2018-05-29 17:02:20,050][org.apache.spark.scheduler.DAGScheduler]Job 4 finished: isEmpty at ReceiveKafkaData.scala:75, took 0.016581 s
[INFO][2018-05-29 17:02:20,051][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527584540000 ms.0 from job set of time 1527584540000 ms
[INFO][2018-05-29 17:02:20,051][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.051 s for time 1527584540000 ms (execution: 0.027 s)
[INFO][2018-05-29 17:02:20,052][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 7 from persistence list
[INFO][2018-05-29 17:02:20,053][org.apache.spark.storage.BlockManager]Removing RDD 7
[INFO][2018-05-29 17:02:20,053][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 6 from persistence list
[INFO][2018-05-29 17:02:20,054][org.apache.spark.storage.BlockManager]Removing RDD 6
[INFO][2018-05-29 17:02:20,055][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-29 17:02:20,056][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527584530000 ms
[INFO][2018-05-29 17:02:20,435][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 0.0 (TID 1) in 10556 ms on localhost (executor driver) (1/2)
[INFO][2018-05-29 17:02:20,437][org.apache.spark.storage.BlockManagerInfo]Removed taskresult_1 on 10.194.32.157:63719 in memory (size: 11.6 MB, free: 912.3 MB)
[INFO][2018-05-29 17:02:20,733][org.apache.spark.storage.memory.MemoryStore]Block taskresult_0 stored as bytes in memory (estimated size 11.6 MB, free 900.5 MB)
[INFO][2018-05-29 17:02:20,734][org.apache.spark.storage.BlockManagerInfo]Added taskresult_0 in memory on 10.194.32.157:63719 (size: 11.6 MB, free: 900.7 MB)
[INFO][2018-05-29 17:02:20,734][org.apache.spark.executor.Executor]Finished task 0.0 in stage 0.0 (TID 0). 12142767 bytes result sent via BlockManager)
[INFO][2018-05-29 17:02:20,889][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 0.0 (TID 0) in 11025 ms on localhost (executor driver) (2/2)
[INFO][2018-05-29 17:02:20,890][org.apache.spark.storage.BlockManagerInfo]Removed taskresult_0 on 10.194.32.157:63719 in memory (size: 11.6 MB, free: 912.3 MB)
[INFO][2018-05-29 17:02:20,890][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO][2018-05-29 17:02:20,891][org.apache.spark.scheduler.DAGScheduler]ResultStage 0 (collect at SimulationKafkaSendOutData.scala:24) finished in 11.037 s
[INFO][2018-05-29 17:02:20,895][org.apache.spark.scheduler.DAGScheduler]Job 0 finished: collect at SimulationKafkaSendOutData.scala:24, took 11.114644 s
[INFO][2018-05-29 17:02:20,956][org.spark_project.jetty.server.AbstractConnector]Stopped Spark@208a9c91{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}
[INFO][2018-05-29 17:02:20,958][org.apache.spark.ui.SparkUI]Stopped Spark web UI at http://10.194.32.157:4041
[INFO][2018-05-29 17:02:20,968][org.apache.spark.MapOutputTrackerMasterEndpoint]MapOutputTrackerMasterEndpoint stopped!
[INFO][2018-05-29 17:02:20,980][org.apache.spark.storage.memory.MemoryStore]MemoryStore cleared
[INFO][2018-05-29 17:02:20,981][org.apache.spark.storage.BlockManager]BlockManager stopped
[INFO][2018-05-29 17:02:20,983][org.apache.spark.storage.BlockManagerMaster]BlockManagerMaster stopped
[INFO][2018-05-29 17:02:20,987][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint]OutputCommitCoordinator stopped!
[INFO][2018-05-29 17:02:20,989][org.apache.spark.SparkContext]Successfully stopped SparkContext
[INFO][2018-05-29 17:02:21,010][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	metric.reporters = []
	metadata.max.age.ms = 300000
	reconnect.backoff.ms = 50
	sasl.kerberos.ticket.renew.window.factor = 0.8
	bootstrap.servers = [vm-xaj-bigdata-da-d01:9092, vm-xaj-bigdata-da-d02:9092, vm-xaj-bigdata-da-d03:9092, vm-xaj-bigdata-da-d04:9092, vm-xaj-bigdata-da-d05:9092, vm-xaj-bigdata-da-d06:9092, vm-xaj-bigdata-da-d07:9092]
	ssl.keystore.type = JKS
	sasl.mechanism = GSSAPI
	max.block.ms = 60000
	interceptor.classes = null
	ssl.truststore.password = null
	client.id = 
	ssl.endpoint.identification.algorithm = null
	request.timeout.ms = 30000
	acks = all
	receive.buffer.bytes = 32768
	ssl.truststore.type = JKS
	retries = 0
	ssl.truststore.location = null
	ssl.keystore.password = null
	send.buffer.bytes = 131072
	compression.type = none
	metadata.fetch.timeout.ms = 60000
	retry.backoff.ms = 100
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	buffer.memory = 33554432
	timeout.ms = 30000
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	ssl.trustmanager.algorithm = PKIX
	block.on.buffer.full = false
	ssl.key.password = null
	sasl.kerberos.min.time.before.relogin = 60000
	connections.max.idle.ms = 540000
	max.in.flight.requests.per.connection = 5
	metrics.num.samples = 2
	ssl.protocol = TLS
	ssl.provider = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	batch.size = 16384
	ssl.keystore.location = null
	ssl.cipher.suites = null
	security.protocol = PLAINTEXT
	max.request.size = 1048576
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
	ssl.keymanager.algorithm = SunX509
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	linger.ms = 1

[INFO][2018-05-29 17:02:21,045][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	metric.reporters = []
	metadata.max.age.ms = 300000
	reconnect.backoff.ms = 50
	sasl.kerberos.ticket.renew.window.factor = 0.8
	bootstrap.servers = [vm-xaj-bigdata-da-d01:9092, vm-xaj-bigdata-da-d02:9092, vm-xaj-bigdata-da-d03:9092, vm-xaj-bigdata-da-d04:9092, vm-xaj-bigdata-da-d05:9092, vm-xaj-bigdata-da-d06:9092, vm-xaj-bigdata-da-d07:9092]
	ssl.keystore.type = JKS
	sasl.mechanism = GSSAPI
	max.block.ms = 60000
	interceptor.classes = null
	ssl.truststore.password = null
	client.id = producer-1
	ssl.endpoint.identification.algorithm = null
	request.timeout.ms = 30000
	acks = all
	receive.buffer.bytes = 32768
	ssl.truststore.type = JKS
	retries = 0
	ssl.truststore.location = null
	ssl.keystore.password = null
	send.buffer.bytes = 131072
	compression.type = none
	metadata.fetch.timeout.ms = 60000
	retry.backoff.ms = 100
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	buffer.memory = 33554432
	timeout.ms = 30000
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	ssl.trustmanager.algorithm = PKIX
	block.on.buffer.full = false
	ssl.key.password = null
	sasl.kerberos.min.time.before.relogin = 60000
	connections.max.idle.ms = 540000
	max.in.flight.requests.per.connection = 5
	metrics.num.samples = 2
	ssl.protocol = TLS
	ssl.provider = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	batch.size = 16384
	ssl.keystore.location = null
	ssl.cipher.suites = null
	security.protocol = PLAINTEXT
	max.request.size = 1048576
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
	ssl.keymanager.algorithm = SunX509
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	linger.ms = 1

[INFO][2018-05-29 17:02:21,051][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 0.10.0.1
[INFO][2018-05-29 17:02:21,052][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a7a17cdec9eaa6c5
[INFO][2018-05-29 17:02:25,020][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527584545000 ms
[INFO][2018-05-29 17:02:25,020][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527584545000 ms.0 from job set of time 1527584545000 ms
[INFO][2018-05-29 17:02:25,027][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:75
[INFO][2018-05-29 17:02:25,028][org.apache.spark.scheduler.DAGScheduler]Got job 5 (isEmpty at ReceiveKafkaData.scala:75) with 1 output partitions
[INFO][2018-05-29 17:02:25,028][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 5 (isEmpty at ReceiveKafkaData.scala:75)
[INFO][2018-05-29 17:02:25,028][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-29 17:02:25,028][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-29 17:02:25,029][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 5 (MapPartitionsRDD[11] at map at ReceiveKafkaData.scala:74), which has no missing parents
[INFO][2018-05-29 17:02:25,031][org.apache.spark.storage.memory.MemoryStore]Block broadcast_5 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-29 17:02:25,032][org.apache.spark.storage.memory.MemoryStore]Block broadcast_5_piece0 stored as bytes in memory (estimated size 1973.0 B, free 912.3 MB)
[INFO][2018-05-29 17:02:25,033][org.apache.spark.storage.BlockManagerInfo]Added broadcast_5_piece0 in memory on 10.194.32.157:63705 (size: 1973.0 B, free: 912.3 MB)
[INFO][2018-05-29 17:02:25,033][org.apache.spark.SparkContext]Created broadcast 5 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-29 17:02:25,035][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[11] at map at ReceiveKafkaData.scala:74) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-29 17:02:25,035][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 5.0 with 1 tasks
[INFO][2018-05-29 17:02:25,036][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 5.0 (TID 5, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-29 17:02:25,036][org.apache.spark.executor.Executor]Running task 0.0 in stage 5.0 (TID 5)
[INFO][2018-05-29 17:02:25,040][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 188842 -> 188846
[INFO][2018-05-29 17:02:25,042][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initializing cache 16 64 0.75
[INFO][2018-05-29 17:02:25,044][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Cache miss for CacheKey(spark-executor-use_a_separate_group_id_for_each_stream,seven,0)
[INFO][2018-05-29 17:02:25,046][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	metric.reporters = []
	metadata.max.age.ms = 300000
	partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor]
	reconnect.backoff.ms = 50
	sasl.kerberos.ticket.renew.window.factor = 0.8
	max.partition.fetch.bytes = 1048576
	bootstrap.servers = [vm-xaj-bigdata-da-d01:9092, vm-xaj-bigdata-da-d02:9092, vm-xaj-bigdata-da-d03:9092, vm-xaj-bigdata-da-d04:9092, vm-xaj-bigdata-da-d05:9092, vm-xaj-bigdata-da-d06:9092, vm-xaj-bigdata-da-d07:9092]
	ssl.keystore.type = JKS
	enable.auto.commit = false
	sasl.mechanism = GSSAPI
	interceptor.classes = null
	exclude.internal.topics = true
	ssl.truststore.password = null
	client.id = 
	ssl.endpoint.identification.algorithm = null
	max.poll.records = 2147483647
	check.crcs = true
	request.timeout.ms = 40000
	heartbeat.interval.ms = 3000
	auto.commit.interval.ms = 5000
	receive.buffer.bytes = 65536
	ssl.truststore.type = JKS
	ssl.truststore.location = null
	ssl.keystore.password = null
	fetch.min.bytes = 1
	send.buffer.bytes = 131072
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	group.id = spark-executor-use_a_separate_group_id_for_each_stream
	retry.backoff.ms = 100
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	ssl.trustmanager.algorithm = PKIX
	ssl.key.password = null
	fetch.max.wait.ms = 500
	sasl.kerberos.min.time.before.relogin = 60000
	connections.max.idle.ms = 540000
	session.timeout.ms = 30000
	metrics.num.samples = 2
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	ssl.protocol = TLS
	ssl.provider = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.keystore.location = null
	ssl.cipher.suites = null
	security.protocol = PLAINTEXT
	ssl.keymanager.algorithm = SunX509
	metrics.sample.window.ms = 30000
	auto.offset.reset = none

[INFO][2018-05-29 17:02:25,047][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	metric.reporters = []
	metadata.max.age.ms = 300000
	partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor]
	reconnect.backoff.ms = 50
	sasl.kerberos.ticket.renew.window.factor = 0.8
	max.partition.fetch.bytes = 1048576
	bootstrap.servers = [vm-xaj-bigdata-da-d01:9092, vm-xaj-bigdata-da-d02:9092, vm-xaj-bigdata-da-d03:9092, vm-xaj-bigdata-da-d04:9092, vm-xaj-bigdata-da-d05:9092, vm-xaj-bigdata-da-d06:9092, vm-xaj-bigdata-da-d07:9092]
	ssl.keystore.type = JKS
	enable.auto.commit = false
	sasl.mechanism = GSSAPI
	interceptor.classes = null
	exclude.internal.topics = true
	ssl.truststore.password = null
	client.id = consumer-2
	ssl.endpoint.identification.algorithm = null
	max.poll.records = 2147483647
	check.crcs = true
	request.timeout.ms = 40000
	heartbeat.interval.ms = 3000
	auto.commit.interval.ms = 5000
	receive.buffer.bytes = 65536
	ssl.truststore.type = JKS
	ssl.truststore.location = null
	ssl.keystore.password = null
	fetch.min.bytes = 1
	send.buffer.bytes = 131072
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	group.id = spark-executor-use_a_separate_group_id_for_each_stream
	retry.backoff.ms = 100
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	ssl.trustmanager.algorithm = PKIX
	ssl.key.password = null
	fetch.max.wait.ms = 500
	sasl.kerberos.min.time.before.relogin = 60000
	connections.max.idle.ms = 540000
	session.timeout.ms = 30000
	metrics.num.samples = 2
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	ssl.protocol = TLS
	ssl.provider = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.keystore.location = null
	ssl.cipher.suites = null
	security.protocol = PLAINTEXT
	ssl.keymanager.algorithm = SunX509
	metrics.sample.window.ms = 30000
	auto.offset.reset = none

[INFO][2018-05-29 17:02:25,048][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 0.10.0.1
[INFO][2018-05-29 17:02:25,048][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a7a17cdec9eaa6c5
[INFO][2018-05-29 17:02:25,050][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 188842
[INFO][2018-05-29 17:02:25,165][org.apache.kafka.clients.consumer.internals.AbstractCoordinator]Discovered coordinator vm-xaj-bigdata-da-d06:9092 (id: 2147483530 rack: null) for group spark-executor-use_a_separate_group_id_for_each_stream.
[INFO][2018-05-29 17:02:25,194][org.apache.spark.executor.Executor]Finished task 0.0 in stage 5.0 (TID 5). 969 bytes result sent to driver
[INFO][2018-05-29 17:02:25,195][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 5.0 (TID 5) in 159 ms on localhost (executor driver) (1/1)
[INFO][2018-05-29 17:02:25,195][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 5.0, whose tasks have all completed, from pool 
[INFO][2018-05-29 17:02:25,196][org.apache.spark.scheduler.DAGScheduler]ResultStage 5 (isEmpty at ReceiveKafkaData.scala:75) finished in 0.160 s
[INFO][2018-05-29 17:02:25,196][org.apache.spark.scheduler.DAGScheduler]Job 5 finished: isEmpty at ReceiveKafkaData.scala:75, took 0.168935 s
[INFO][2018-05-29 17:02:25,208][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:76
[INFO][2018-05-29 17:02:25,209][org.apache.spark.scheduler.DAGScheduler]Got job 6 (foreachPartition at ReceiveKafkaData.scala:76) with 1 output partitions
[INFO][2018-05-29 17:02:25,209][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 6 (foreachPartition at ReceiveKafkaData.scala:76)
[INFO][2018-05-29 17:02:25,209][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-29 17:02:25,209][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-29 17:02:25,210][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 6 (MapPartitionsRDD[11] at map at ReceiveKafkaData.scala:74), which has no missing parents
[INFO][2018-05-29 17:02:25,211][org.apache.spark.storage.memory.MemoryStore]Block broadcast_6 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-29 17:02:25,214][org.apache.spark.storage.memory.MemoryStore]Block broadcast_6_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.3 MB)
[INFO][2018-05-29 17:02:25,214][org.apache.spark.storage.BlockManagerInfo]Added broadcast_6_piece0 in memory on 10.194.32.157:63705 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-29 17:02:25,215][org.apache.spark.SparkContext]Created broadcast 6 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-29 17:02:25,216][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[11] at map at ReceiveKafkaData.scala:74) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-29 17:02:25,216][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 6.0 with 1 tasks
[INFO][2018-05-29 17:02:25,216][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 6.0 (TID 6, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-29 17:02:25,217][org.apache.spark.executor.Executor]Running task 0.0 in stage 6.0 (TID 6)
[INFO][2018-05-29 17:02:25,219][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 188842 -> 188846
[INFO][2018-05-29 17:02:25,220][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 188842
[INFO][2018-05-29 17:02:25,535][org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper]Process identifier=hconnection-0x8d117d3 connecting to ZooKeeper ensemble=vm-xaj-bigdata-da-d01:2181,vm-xaj-bigdata-da-d02:2181,vm-xaj-bigdata-da-d03:2181
[INFO][2018-05-29 17:02:25,540][org.apache.zookeeper.ZooKeeper]Client environment:zookeeper.version=3.4.5-cdh5.7.0--1, built on 04/06/2016 14:24 GMT
[INFO][2018-05-29 17:02:25,540][org.apache.zookeeper.ZooKeeper]Client environment:host.name=10.194.32.157
[INFO][2018-05-29 17:02:25,540][org.apache.zookeeper.ZooKeeper]Client environment:java.version=1.8.0_161
[INFO][2018-05-29 17:02:25,540][org.apache.zookeeper.ZooKeeper]Client environment:java.vendor=Oracle Corporation
[INFO][2018-05-29 17:02:25,541][org.apache.zookeeper.ZooKeeper]Client environment:java.home=/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre
[INFO][2018-05-29 17:02:25,541][org.apache.zookeeper.ZooKeeper]Client environment:java.class.path=/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/charsets.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/deploy.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/cldrdata.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/dnsns.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/jaccess.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/jfxrt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/localedata.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/nashorn.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/sunec.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/sunjce_provider.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/sunpkcs11.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/zipfs.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/javaws.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/jce.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/jfr.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/jfxswt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/jsse.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/management-agent.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/plugin.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/resources.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/rt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/ant-javafx.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/dt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/javafx-mx.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/jconsole.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/packager.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/sa-jdi.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/tools.jar:/Users/seven/project/github/dataMining/target/classes:/Users/seven/software/maven/repository/org/scala-lang/scala-library/2.11.12/scala-library-2.11.12.jar:/Users/seven/software/maven/repository/org/scala-lang/scala-compiler/2.11.12/scala-compiler-2.11.12.jar:/Users/seven/software/maven/repository/org/scala-lang/modules/scala-xml_2.11/1.0.5/scala-xml_2.11-1.0.5.jar:/Users/seven/software/maven/repository/org/scala-lang/modules/scala-parser-combinators_2.11/1.0.4/scala-parser-combinators_2.11-1.0.4.jar:/Users/seven/software/maven/repository/org/scala-lang/scala-reflect/2.11.12/scala-reflect-2.11.12.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-client/2.6.0-cdh5.7.0/hadoop-client-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-common/2.6.0-cdh5.7.0/hadoop-common-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/xmlenc/xmlenc/0.52/xmlenc-0.52.jar:/Users/seven/software/maven/repository/commons-net/commons-net/3.1/commons-net-3.1.jar:/Users/seven/software/maven/repository/commons-configuration/commons-configuration/1.6/commons-configuration-1.6.jar:/Users/seven/software/maven/repository/commons-digester/commons-digester/1.8/commons-digester-1.8.jar:/Users/seven/software/maven/repository/commons-beanutils/commons-beanutils/1.7.0/commons-beanutils-1.7.0.jar:/Users/seven/software/maven/repository/org/slf4j/slf4j-api/1.7.5/slf4j-api-1.7.5.jar:/Users/seven/software/maven/repository/org/slf4j/slf4j-log4j12/1.7.5/slf4j-log4j12-1.7.5.jar:/Users/seven/software/maven/repository/org/apache/avro/avro/1.7.6-cdh5.7.0/avro-1.7.6-cdh5.7.0.jar:/Users/seven/software/maven/repository/com/google/code/gson/gson/2.2.4/gson-2.2.4.jar:/Users/seven/software/maven/repository/org/apache/curator/curator-client/2.7.1/curator-client-2.7.1.jar:/Users/seven/software/maven/repository/org/apache/curator/curator-recipes/2.7.1/curator-recipes-2.7.1.jar:/Users/seven/software/maven/repository/com/google/code/findbugs/jsr305/3.0.0/jsr305-3.0.0.jar:/Users/seven/software/maven/repository/org/apache/htrace/htrace-core4/4.0.1-incubating/htrace-core4-4.0.1-incubating.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-compress/1.4.1/commons-compress-1.4.1.jar:/Users/seven/software/maven/repository/org/tukaani/xz/1.0/xz-1.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-hdfs/2.6.0-cdh5.7.0/hadoop-hdfs-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/io/netty/netty/3.6.2.Final/netty-3.6.2.Final.jar:/Users/seven/software/maven/repository/xerces/xercesImpl/2.9.1/xercesImpl-2.9.1.jar:/Users/seven/software/maven/repository/xml-apis/xml-apis/1.3.04/xml-apis-1.3.04.jar:/Users/seven/software/maven/repository/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-app/2.6.0-cdh5.7.0/hadoop-mapreduce-client-app-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-common/2.6.0-cdh5.7.0/hadoop-mapreduce-client-common-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-yarn-client/2.6.0-cdh5.7.0/hadoop-yarn-client-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-yarn-server-common/2.6.0-cdh5.7.0/hadoop-yarn-server-common-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-shuffle/2.6.0-cdh5.7.0/hadoop-mapreduce-client-shuffle-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-yarn-api/2.6.0-cdh5.7.0/hadoop-yarn-api-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-core/2.6.0-cdh5.7.0/hadoop-mapreduce-client-core-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-yarn-common/2.6.0-cdh5.7.0/hadoop-yarn-common-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/javax/xml/bind/jaxb-api/2.2.2/jaxb-api-2.2.2.jar:/Users/seven/software/maven/repository/javax/xml/stream/stax-api/1.0-2/stax-api-1.0-2.jar:/Users/seven/software/maven/repository/javax/activation/activation/1.1/activation-1.1.jar:/Users/seven/software/maven/repository/com/sun/jersey/jersey-client/1.9/jersey-client-1.9.jar:/Users/seven/software/maven/repository/org/codehaus/jackson/jackson-xc/1.8.8/jackson-xc-1.8.8.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.6.0-cdh5.7.0/hadoop-mapreduce-client-jobclient-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-aws/2.6.0-cdh5.7.0/hadoop-aws-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/com/amazonaws/aws-java-sdk-s3/1.10.6/aws-java-sdk-s3-1.10.6.jar:/Users/seven/software/maven/repository/com/amazonaws/aws-java-sdk-kms/1.10.6/aws-java-sdk-kms-1.10.6.jar:/Users/seven/software/maven/repository/com/amazonaws/aws-java-sdk-core/1.10.6/aws-java-sdk-core-1.10.6.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-annotations/2.6.0-cdh5.7.0/hadoop-annotations-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-client/1.2.0-cdh5.7.0/hbase-client-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-annotations/1.2.0-cdh5.7.0/hbase-annotations-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-common/1.2.0-cdh5.7.0/hbase-common-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-protocol/1.2.0-cdh5.7.0/hbase-protocol-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/commons-codec/commons-codec/1.9/commons-codec-1.9.jar:/Users/seven/software/maven/repository/commons-io/commons-io/2.4/commons-io-2.4.jar:/Users/seven/software/maven/repository/commons-lang/commons-lang/2.6/commons-lang-2.6.jar:/Users/seven/software/maven/repository/commons-logging/commons-logging/1.2/commons-logging-1.2.jar:/Users/seven/software/maven/repository/com/google/guava/guava/12.0.1/guava-12.0.1.jar:/Users/seven/software/maven/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/Users/seven/software/maven/repository/io/netty/netty-all/4.0.23.Final/netty-all-4.0.23.Final.jar:/Users/seven/software/maven/repository/org/apache/zookeeper/zookeeper/3.4.5-cdh5.7.0/zookeeper-3.4.5-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/htrace/htrace-core/3.2.0-incubating/htrace-core-3.2.0-incubating.jar:/Users/seven/software/maven/repository/org/codehaus/jackson/jackson-mapper-asl/1.8.8/jackson-mapper-asl-1.8.8.jar:/Users/seven/software/maven/repository/org/jruby/jcodings/jcodings/1.0.8/jcodings-1.0.8.jar:/Users/seven/software/maven/repository/org/jruby/joni/joni/2.1.2/joni-2.1.2.jar:/Users/seven/software/maven/repository/com/yammer/metrics/metrics-core/2.2.0/metrics-core-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-auth/2.6.0-cdh5.7.0/hadoop-auth-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/httpcomponents/httpclient/4.2.5/httpclient-4.2.5.jar:/Users/seven/software/maven/repository/org/apache/httpcomponents/httpcore/4.2.4/httpcore-4.2.4.jar:/Users/seven/software/maven/repository/org/apache/directory/server/apacheds-kerberos-codec/2.0.0-M15/apacheds-kerberos-codec-2.0.0-M15.jar:/Users/seven/software/maven/repository/org/apache/directory/server/apacheds-i18n/2.0.0-M15/apacheds-i18n-2.0.0-M15.jar:/Users/seven/software/maven/repository/org/apache/directory/api/api-asn1-api/1.0.0-M20/api-asn1-api-1.0.0-M20.jar:/Users/seven/software/maven/repository/org/apache/directory/api/api-util/1.0.0-M20/api-util-1.0.0-M20.jar:/Users/seven/software/maven/repository/org/apache/curator/curator-framework/2.7.1/curator-framework-2.7.1.jar:/Users/seven/software/maven/repository/com/github/stephenc/findbugs/findbugs-annotations/1.3.9-1/findbugs-annotations-1.3.9-1.jar:/Users/seven/software/maven/repository/junit/junit/4.12/junit-4.12.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-server/1.2.0-cdh5.7.0/hbase-server-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-procedure/1.2.0-cdh5.7.0/hbase-procedure-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-common/1.2.0-cdh5.7.0/hbase-common-1.2.0-cdh5.7.0-tests.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-prefix-tree/1.2.0-cdh5.7.0/hbase-prefix-tree-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar:/Users/seven/software/maven/repository/commons-collections/commons-collections/3.2.2/commons-collections-3.2.2.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-hadoop-compat/1.2.0-cdh5.7.0/hbase-hadoop-compat-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-hadoop2-compat/1.2.0-cdh5.7.0/hbase-hadoop2-compat-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/com/sun/jersey/jersey-core/1.9/jersey-core-1.9.jar:/Users/seven/software/maven/repository/com/sun/jersey/jersey-server/1.9/jersey-server-1.9.jar:/Users/seven/software/maven/repository/asm/asm/3.1/asm-3.1.jar:/Users/seven/software/maven/repository/commons-cli/commons-cli/1.2/commons-cli-1.2.jar:/Users/seven/software/maven/repository/com/github/stephenc/high-scale-lib/high-scale-lib/1.1.1/high-scale-lib-1.1.1.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-math/2.1/commons-math-2.1.jar:/Users/seven/software/maven/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jetty/6.1.26.cloudera.4/jetty-6.1.26.cloudera.4.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jetty-util/6.1.26.cloudera.4/jetty-util-6.1.26.cloudera.4.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jetty-sslengine/6.1.26.cloudera.4/jetty-sslengine-6.1.26.cloudera.4.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jsp-2.1/6.1.14/jsp-2.1-6.1.14.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jsp-api-2.1/6.1.14/jsp-api-2.1-6.1.14.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/servlet-api-2.5/6.1.14/servlet-api-2.5-6.1.14.jar:/Users/seven/software/maven/repository/org/codehaus/jackson/jackson-core-asl/1.8.8/jackson-core-asl-1.8.8.jar:/Users/seven/software/maven/repository/org/codehaus/jackson/jackson-jaxrs/1.8.8/jackson-jaxrs-1.8.8.jar:/Users/seven/software/maven/repository/tomcat/jasper-compiler/5.5.23/jasper-compiler-5.5.23.jar:/Users/seven/software/maven/repository/tomcat/jasper-runtime/5.5.23/jasper-runtime-5.5.23.jar:/Users/seven/software/maven/repository/commons-el/commons-el/1.0/commons-el-1.0.jar:/Users/seven/software/maven/repository/org/jamon/jamon-runtime/2.4.1/jamon-runtime-2.4.1.jar:/Users/seven/software/maven/repository/com/lmax/disruptor/3.3.0/disruptor-3.3.0.jar:/Users/seven/software/maven/repository/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar:/Users/seven/software/maven/repository/org/owasp/esapi/esapi/2.1.0/esapi-2.1.0.jar:/Users/seven/software/maven/repository/commons-beanutils/commons-beanutils-core/1.7.0/commons-beanutils-core-1.7.0.jar:/Users/seven/software/maven/repository/commons-fileupload/commons-fileupload/1.2/commons-fileupload-1.2.jar:/Users/seven/software/maven/repository/xom/xom/1.2.5/xom-1.2.5.jar:/Users/seven/software/maven/repository/xalan/xalan/2.7.0/xalan-2.7.0.jar:/Users/seven/software/maven/repository/org/beanshell/bsh-core/2.0b4/bsh-core-2.0b4.jar:/Users/seven/software/maven/repository/org/owasp/antisamy/antisamy/1.4.3/antisamy-1.4.3.jar:/Users/seven/software/maven/repository/org/apache/xmlgraphics/batik-css/1.7/batik-css-1.7.jar:/Users/seven/software/maven/repository/org/apache/xmlgraphics/batik-ext/1.7/batik-ext-1.7.jar:/Users/seven/software/maven/repository/org/apache/xmlgraphics/batik-util/1.7/batik-util-1.7.jar:/Users/seven/software/maven/repository/xml-apis/xml-apis-ext/1.3.04/xml-apis-ext-1.3.04.jar:/Users/seven/software/maven/repository/net/sourceforge/nekohtml/nekohtml/1.9.12/nekohtml-1.9.12.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-core/2.6.0-mr1-cdh5.7.0/hadoop-core-2.6.0-mr1-cdh5.7.0.jar:/Users/seven/software/maven/repository/javax/servlet/jsp/jsp-api/2.1/jsp-api-2.1.jar:/Users/seven/software/maven/repository/net/java/dev/jets3t/jets3t/0.6.1/jets3t-0.6.1.jar:/Users/seven/software/maven/repository/javax/servlet/servlet-api/2.5/servlet-api-2.5.jar:/Users/seven/software/maven/repository/hsqldb/hsqldb/1.8.0.10/hsqldb-1.8.0.10.jar:/Users/seven/software/maven/repository/org/eclipse/jdt/core/3.1.1/core-3.1.1.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-hdfs/2.6.0-cdh5.7.0/hadoop-hdfs-2.6.0-cdh5.7.0-tests.jar:/Users/seven/software/maven/repository/commons-daemon/commons-daemon/1.0.13/commons-daemon-1.0.13.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-lang3/3.5/commons-lang3-3.5.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-pool2/2.4.2/commons-pool2-2.4.2.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-mllib_2.11/2.2.0/spark-mllib_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-core_2.11/2.2.0/spark-core_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/avro/avro-mapred/1.7.7/avro-mapred-1.7.7-hadoop2.jar:/Users/seven/software/maven/repository/org/apache/avro/avro-ipc/1.7.7/avro-ipc-1.7.7-tests.jar:/Users/seven/software/maven/repository/com/twitter/chill_2.11/0.8.0/chill_2.11-0.8.0.jar:/Users/seven/software/maven/repository/com/esotericsoftware/kryo-shaded/3.0.3/kryo-shaded-3.0.3.jar:/Users/seven/software/maven/repository/com/esotericsoftware/minlog/1.3.0/minlog-1.3.0.jar:/Users/seven/software/maven/repository/org/objenesis/objenesis/2.1/objenesis-2.1.jar:/Users/seven/software/maven/repository/com/twitter/chill-java/0.8.0/chill-java-0.8.0.jar:/Users/seven/software/maven/repository/org/apache/xbean/xbean-asm5-shaded/4.4/xbean-asm5-shaded-4.4.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-launcher_2.11/2.2.0/spark-launcher_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-network-common_2.11/2.2.0/spark-network-common_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-network-shuffle_2.11/2.2.0/spark-network-shuffle_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-unsafe_2.11/2.2.0/spark-unsafe_2.11-2.2.0.jar:/Users/seven/software/maven/repository/javax/servlet/javax.servlet-api/3.1.0/javax.servlet-api-3.1.0.jar:/Users/seven/software/maven/repository/org/slf4j/jul-to-slf4j/1.7.16/jul-to-slf4j-1.7.16.jar:/Users/seven/software/maven/repository/org/slf4j/jcl-over-slf4j/1.7.16/jcl-over-slf4j-1.7.16.jar:/Users/seven/software/maven/repository/com/ning/compress-lzf/1.0.3/compress-lzf-1.0.3.jar:/Users/seven/software/maven/repository/org/xerial/snappy/snappy-java/1.1.2.6/snappy-java-1.1.2.6.jar:/Users/seven/software/maven/repository/org/roaringbitmap/RoaringBitmap/0.5.11/RoaringBitmap-0.5.11.jar:/Users/seven/software/maven/repository/org/json4s/json4s-jackson_2.11/3.2.11/json4s-jackson_2.11-3.2.11.jar:/Users/seven/software/maven/repository/org/json4s/json4s-core_2.11/3.2.11/json4s-core_2.11-3.2.11.jar:/Users/seven/software/maven/repository/org/json4s/json4s-ast_2.11/3.2.11/json4s-ast_2.11-3.2.11.jar:/Users/seven/software/maven/repository/org/scala-lang/scalap/2.11.0/scalap-2.11.0.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/core/jersey-client/2.22.2/jersey-client-2.22.2.jar:/Users/seven/software/maven/repository/javax/ws/rs/javax.ws.rs-api/2.0.1/javax.ws.rs-api-2.0.1.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/hk2-api/2.4.0-b34/hk2-api-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/hk2-utils/2.4.0-b34/hk2-utils-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/external/aopalliance-repackaged/2.4.0-b34/aopalliance-repackaged-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/external/javax.inject/2.4.0-b34/javax.inject-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/hk2-locator/2.4.0-b34/hk2-locator-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/javassist/javassist/3.18.1-GA/javassist-3.18.1-GA.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/core/jersey-common/2.22.2/jersey-common-2.22.2.jar:/Users/seven/software/maven/repository/javax/annotation/javax.annotation-api/1.2/javax.annotation-api-1.2.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/bundles/repackaged/jersey-guava/2.22.2/jersey-guava-2.22.2.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/osgi-resource-locator/1.0.1/osgi-resource-locator-1.0.1.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/core/jersey-server/2.22.2/jersey-server-2.22.2.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/media/jersey-media-jaxb/2.22.2/jersey-media-jaxb-2.22.2.jar:/Users/seven/software/maven/repository/javax/validation/validation-api/1.1.0.Final/validation-api-1.1.0.Final.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/containers/jersey-container-servlet/2.22.2/jersey-container-servlet-2.22.2.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/containers/jersey-container-servlet-core/2.22.2/jersey-container-servlet-core-2.22.2.jar:/Users/seven/software/maven/repository/com/clearspring/analytics/stream/2.7.0/stream-2.7.0.jar:/Users/seven/software/maven/repository/io/dropwizard/metrics/metrics-core/3.1.2/metrics-core-3.1.2.jar:/Users/seven/software/maven/repository/io/dropwizard/metrics/metrics-jvm/3.1.2/metrics-jvm-3.1.2.jar:/Users/seven/software/maven/repository/io/dropwizard/metrics/metrics-json/3.1.2/metrics-json-3.1.2.jar:/Users/seven/software/maven/repository/io/dropwizard/metrics/metrics-graphite/3.1.2/metrics-graphite-3.1.2.jar:/Users/seven/software/maven/repository/org/apache/ivy/ivy/2.4.0/ivy-2.4.0.jar:/Users/seven/software/maven/repository/oro/oro/2.0.8/oro-2.0.8.jar:/Users/seven/software/maven/repository/net/razorvine/pyrolite/4.13/pyrolite-4.13.jar:/Users/seven/software/maven/repository/net/sf/py4j/py4j/0.10.4/py4j-0.10.4.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-crypto/1.0.0/commons-crypto-1.0.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-streaming_2.11/2.2.0/spark-streaming_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-sql_2.11/2.2.0/spark-sql_2.11-2.2.0.jar:/Users/seven/software/maven/repository/com/univocity/univocity-parsers/2.2.1/univocity-parsers-2.2.1.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-sketch_2.11/2.2.0/spark-sketch_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-catalyst_2.11/2.2.0/spark-catalyst_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/codehaus/janino/janino/3.0.0/janino-3.0.0.jar:/Users/seven/software/maven/repository/org/codehaus/janino/commons-compiler/3.0.0/commons-compiler-3.0.0.jar:/Users/seven/software/maven/repository/org/antlr/antlr4-runtime/4.5.3/antlr4-runtime-4.5.3.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-column/1.8.2/parquet-column-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-common/1.8.2/parquet-common-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-encoding/1.8.2/parquet-encoding-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-hadoop/1.8.2/parquet-hadoop-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-format/2.3.1/parquet-format-2.3.1.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-jackson/1.8.2/parquet-jackson-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-graphx_2.11/2.2.0/spark-graphx_2.11-2.2.0.jar:/Users/seven/software/maven/repository/com/github/fommil/netlib/core/1.1.2/core-1.1.2.jar:/Users/seven/software/maven/repository/net/sourceforge/f2j/arpack_combined_all/0.1/arpack_combined_all-0.1.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-mllib-local_2.11/2.2.0/spark-mllib-local_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/scalanlp/breeze_2.11/0.13.1/breeze_2.11-0.13.1.jar:/Users/seven/software/maven/repository/org/scalanlp/breeze-macros_2.11/0.13.1/breeze-macros_2.11-0.13.1.jar:/Users/seven/software/maven/repository/net/sf/opencsv/opencsv/2.3/opencsv-2.3.jar:/Users/seven/software/maven/repository/com/github/rwl/jtransforms/2.4.0/jtransforms-2.4.0.jar:/Users/seven/software/maven/repository/org/spire-math/spire_2.11/0.13.0/spire_2.11-0.13.0.jar:/Users/seven/software/maven/repository/org/spire-math/spire-macros_2.11/0.13.0/spire-macros_2.11-0.13.0.jar:/Users/seven/software/maven/repository/org/typelevel/machinist_2.11/0.6.1/machinist_2.11-0.6.1.jar:/Users/seven/software/maven/repository/com/chuusai/shapeless_2.11/2.3.2/shapeless_2.11-2.3.2.jar:/Users/seven/software/maven/repository/org/typelevel/macro-compat_2.11/1.1.1/macro-compat_2.11-1.1.1.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-math3/3.4.1/commons-math3-3.4.1.jar:/Users/seven/software/maven/repository/org/jpmml/pmml-model/1.2.15/pmml-model-1.2.15.jar:/Users/seven/software/maven/repository/org/jpmml/pmml-schema/1.2.15/pmml-schema-1.2.15.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-tags_2.11/2.2.0/spark-tags_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-streaming-kafka-0-10_2.11/2.2.0/spark-streaming-kafka-0-10_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/kafka/kafka_2.11/0.10.0.1/kafka_2.11-0.10.0.1.jar:/Users/seven/software/maven/repository/com/101tec/zkclient/0.8/zkclient-0.8.jar:/Users/seven/software/maven/repository/org/apache/kafka/kafka-clients/0.10.0.1/kafka-clients-0.10.0.1.jar:/Users/seven/software/maven/repository/mysql/mysql-connector-java/5.1.31/mysql-connector-java-5.1.31.jar:/Users/seven/software/maven/repository/org/elasticsearch/client/transport/5.1.2/transport-5.1.2.jar:/Users/seven/software/maven/repository/org/elasticsearch/plugin/transport-netty3-client/5.1.2/transport-netty3-client-5.1.2.jar:/Users/seven/software/maven/repository/org/elasticsearch/plugin/transport-netty4-client/5.1.2/transport-netty4-client-5.1.2.jar:/Users/seven/software/maven/repository/io/netty/netty-buffer/4.1.6.Final/netty-buffer-4.1.6.Final.jar:/Users/seven/software/maven/repository/io/netty/netty-codec/4.1.6.Final/netty-codec-4.1.6.Final.jar:/Users/seven/software/maven/repository/io/netty/netty-codec-http/4.1.6.Final/netty-codec-http-4.1.6.Final.jar:/Users/seven/software/maven/repository/io/netty/netty-common/4.1.6.Final/netty-common-4.1.6.Final.jar:/Users/seven/software/maven/repository/io/netty/netty-handler/4.1.6.Final/netty-handler-4.1.6.Final.jar:/Users/seven/software/maven/repository/io/netty/netty-resolver/4.1.6.Final/netty-resolver-4.1.6.Final.jar:/Users/seven/software/maven/repository/io/netty/netty-transport/4.1.6.Final/netty-transport-4.1.6.Final.jar:/Users/seven/software/maven/repository/org/elasticsearch/plugin/reindex-client/5.1.2/reindex-client-5.1.2.jar:/Users/seven/software/maven/repository/org/elasticsearch/client/rest/5.1.2/rest-5.1.2.jar:/Users/seven/software/maven/repository/org/apache/httpcomponents/httpasyncclient/4.1.2/httpasyncclient-4.1.2.jar:/Users/seven/software/maven/repository/org/apache/httpcomponents/httpcore-nio/4.4.5/httpcore-nio-4.4.5.jar:/Users/seven/software/maven/repository/org/elasticsearch/plugin/lang-mustache-client/5.1.2/lang-mustache-client-5.1.2.jar:/Users/seven/software/maven/repository/com/github/spullara/mustache/java/compiler/0.9.3/compiler-0.9.3.jar:/Users/seven/software/maven/repository/org/elasticsearch/plugin/percolator-client/5.1.2/percolator-client-5.1.2.jar:/Users/seven/software/maven/repository/org/elasticsearch/elasticsearch/5.1.2/elasticsearch-5.1.2.jar:/Users/seven/software/maven/repository/org/apache/lucene/lucene-core/6.3.0/lucene-core-6.3.0.jar:/Users/seven/software/maven/repository/org/apache/lucene/lucene-analyzers-common/6.3.0/lucene-analyzers-common-6.3.0.jar:/Users/seven/software/maven/repository/org/apache/lucene/lucene-backward-codecs/6.3.0/lucene-backward-codecs-6.3.0.jar:/Users/seven/software/maven/repository/org/apache/lucene/lucene-grouping/6.3.0/lucene-grouping-6.3.0.jar:/Users/seven/software/maven/repository/org/apache/lucene/lucene-highlighter/6.3.0/lucene-highlighter-6.3.0.jar:/Users/seven/software/maven/repository/org/apache/lucene/lucene-join/6.3.0/lucene-join-6.3.0.jar:/Users/seven/software/maven/repository/org/apache/lucene/lucene-memory/6.3.0/lucene-memory-6.3.0.jar:/Users/seven/software/maven/repository/org/apache/lucene/lucene-misc/6.3.0/lucene-misc-6.3.0.jar:/Users/seven/software/maven/repository/org/apache/lucene/lucene-queries/6.3.0/lucene-queries-6.3.0.jar:/Users/seven/software/maven/repository/org/apache/lucene/lucene-queryparser/6.3.0/lucene-queryparser-6.3.0.jar:/Users/seven/software/maven/repository/org/apache/lucene/lucene-sandbox/6.3.0/lucene-sandbox-6.3.0.jar:/Users/seven/software/maven/repository/org/apache/lucene/lucene-spatial/6.3.0/lucene-spatial-6.3.0.jar:/Users/seven/software/maven/repository/org/apache/lucene/lucene-spatial-extras/6.3.0/lucene-spatial-extras-6.3.0.jar:/Users/seven/software/maven/repository/org/apache/lucene/lucene-spatial3d/6.3.0/lucene-spatial3d-6.3.0.jar:/Users/seven/software/maven/repository/org/apache/lucene/lucene-suggest/6.3.0/lucene-suggest-6.3.0.jar:/Users/seven/software/maven/repository/org/elasticsearch/securesm/1.1/securesm-1.1.jar:/Users/seven/software/maven/repository/net/sf/jopt-simple/jopt-simple/5.0.2/jopt-simple-5.0.2.jar:/Users/seven/software/maven/repository/com/carrotsearch/hppc/0.7.1/hppc-0.7.1.jar:/Users/seven/software/maven/repository/joda-time/joda-time/2.9.5/joda-time-2.9.5.jar:/Users/seven/software/maven/repository/org/yaml/snakeyaml/1.15/snakeyaml-1.15.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/core/jackson-core/2.8.1/jackson-core-2.8.1.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/dataformat/jackson-dataformat-smile/2.8.1/jackson-dataformat-smile-2.8.1.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/dataformat/jackson-dataformat-yaml/2.8.1/jackson-dataformat-yaml-2.8.1.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/dataformat/jackson-dataformat-cbor/2.8.1/jackson-dataformat-cbor-2.8.1.jar:/Users/seven/software/maven/repository/com/tdunning/t-digest/3.0/t-digest-3.0.jar:/Users/seven/software/maven/repository/org/hdrhistogram/HdrHistogram/2.1.6/HdrHistogram-2.1.6.jar:/Users/seven/software/maven/repository/net/java/dev/jna/jna/4.2.2/jna-4.2.2.jar:/Users/seven/software/maven/repository/org/apache/logging/log4j/log4j-api/2.6.2/log4j-api-2.6.2.jar:/Users/seven/software/maven/repository/org/apache/logging/log4j/log4j-core/2.6.2/log4j-core-2.6.2.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/module/jackson-module-scala_2.11/2.6.5/jackson-module-scala_2.11-2.6.5.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/core/jackson-annotations/2.6.5/jackson-annotations-2.6.5.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/core/jackson-databind/2.6.5/jackson-databind-2.6.5.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/module/jackson-module-paranamer/2.6.5/jackson-module-paranamer-2.6.5.jar:/Users/seven/software/maven/repository/com/thoughtworks/paranamer/paranamer/2.6/paranamer-2.6.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-streaming-flume_2.11/2.2.0/spark-streaming-flume_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/flume/flume-ng-core/1.6.0/flume-ng-core-1.6.0.jar:/Users/seven/software/maven/repository/org/apache/flume/flume-ng-configuration/1.6.0/flume-ng-configuration-1.6.0.jar:/Users/seven/software/maven/repository/org/apache/avro/avro-ipc/1.7.4/avro-ipc-1.7.4.jar:/Users/seven/software/maven/repository/org/apache/velocity/velocity/1.7/velocity-1.7.jar:/Users/seven/software/maven/repository/org/apache/mina/mina-core/2.0.4/mina-core-2.0.4.jar:/Users/seven/software/maven/repository/org/apache/flume/flume-ng-sdk/1.6.0/flume-ng-sdk-1.6.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-streaming-flume-sink_2.11/2.2.0/spark-streaming-flume-sink_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/flume/flume-ng-clients/flume-ng-log4jappender/1.6.0/flume-ng-log4jappender-1.6.0.jar:/Users/seven/software/maven/repository/net/jpountz/lz4/lz4/1.3.0/lz4-1.3.0.jar:/Applications/IntelliJ IDEA.app/Contents/lib/idea_rt.jar
[INFO][2018-05-29 17:02:25,544][org.apache.zookeeper.ZooKeeper]Client environment:java.library.path=/Users/seven/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:.
[INFO][2018-05-29 17:02:25,544][org.apache.zookeeper.ZooKeeper]Client environment:java.io.tmpdir=/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/
[INFO][2018-05-29 17:02:25,544][org.apache.zookeeper.ZooKeeper]Client environment:java.compiler=<NA>
[INFO][2018-05-29 17:02:25,544][org.apache.zookeeper.ZooKeeper]Client environment:os.name=Mac OS X
[INFO][2018-05-29 17:02:25,544][org.apache.zookeeper.ZooKeeper]Client environment:os.arch=x86_64
[INFO][2018-05-29 17:02:25,544][org.apache.zookeeper.ZooKeeper]Client environment:os.version=10.13.4
[INFO][2018-05-29 17:02:25,544][org.apache.zookeeper.ZooKeeper]Client environment:user.name=seven
[INFO][2018-05-29 17:02:25,544][org.apache.zookeeper.ZooKeeper]Client environment:user.home=/Users/seven
[INFO][2018-05-29 17:02:25,544][org.apache.zookeeper.ZooKeeper]Client environment:user.dir=/Users/seven/project/github/dataMining
[INFO][2018-05-29 17:02:25,545][org.apache.zookeeper.ZooKeeper]Initiating client connection, connectString=vm-xaj-bigdata-da-d01:2181,vm-xaj-bigdata-da-d02:2181,vm-xaj-bigdata-da-d03:2181 sessionTimeout=60000 watcher=hconnection-0x8d117d30x0, quorum=vm-xaj-bigdata-da-d01:2181,vm-xaj-bigdata-da-d02:2181,vm-xaj-bigdata-da-d03:2181, baseZNode=/hbase
[INFO][2018-05-29 17:02:25,560][org.apache.zookeeper.ClientCnxn]Opening socket connection to server vm-xaj-bigdata-da-d02/10.213.4.26:2181. Will not attempt to authenticate using SASL (unknown error)
[INFO][2018-05-29 17:02:25,569][org.apache.zookeeper.ClientCnxn]Socket connection established, initiating session, client: /10.194.32.157:63737, server: vm-xaj-bigdata-da-d02/10.213.4.26:2181
[INFO][2018-05-29 17:02:25,584][org.apache.zookeeper.ClientCnxn]Session establishment complete on server vm-xaj-bigdata-da-d02/10.213.4.26:2181, sessionid = 0x262b4dc569b872e, negotiated timeout = 60000
[WARN][2018-05-29 17:02:26,084][org.apache.hadoop.util.NativeCodeLoader]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO][2018-05-29 17:02:26,145][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_5_piece0 on 10.194.32.157:63705 in memory (size: 1973.0 B, free: 912.3 MB)
[INFO][2018-05-29 17:02:26,157][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_0_piece0 on 10.194.32.157:63705 in memory (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-29 17:02:26,163][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_4_piece0 on 10.194.32.157:63705 in memory (size: 1973.0 B, free: 912.3 MB)
[INFO][2018-05-29 17:02:26,168][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_2_piece0 on 10.194.32.157:63705 in memory (size: 1973.0 B, free: 912.3 MB)
[INFO][2018-05-29 17:02:26,176][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_3_piece0 on 10.194.32.157:63705 in memory (size: 1971.0 B, free: 912.3 MB)
[INFO][2018-05-29 17:02:26,183][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_1_piece0 on 10.194.32.157:63705 in memory (size: 1973.0 B, free: 912.3 MB)
[INFO][2018-05-29 17:02:26,554][com.seven.spark.streaming.ReceiveKafkaData$]Inserting 4 lines of data to HBase is success . . .
[INFO][2018-05-29 17:02:28,821][com.seven.spark.streaming.ReceiveKafkaData$]Inserting 4 lines of data to ElasticSearch is success . . .
[INFO][2018-05-29 17:02:28,823][org.apache.spark.executor.Executor]Finished task 0.0 in stage 6.0 (TID 6). 794 bytes result sent to driver
[INFO][2018-05-29 17:02:28,826][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 6.0 (TID 6) in 3610 ms on localhost (executor driver) (1/1)
[INFO][2018-05-29 17:02:28,827][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 6.0, whose tasks have all completed, from pool 
[INFO][2018-05-29 17:02:28,827][org.apache.spark.scheduler.DAGScheduler]ResultStage 6 (foreachPartition at ReceiveKafkaData.scala:76) finished in 3.611 s
[INFO][2018-05-29 17:02:28,828][org.apache.spark.scheduler.DAGScheduler]Job 6 finished: foreachPartition at ReceiveKafkaData.scala:76, took 3.619342 s
[INFO][2018-05-29 17:02:28,828][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527584545000 ms.0 from job set of time 1527584545000 ms
[INFO][2018-05-29 17:02:28,829][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 3.828 s for time 1527584545000 ms (execution: 3.808 s)
[INFO][2018-05-29 17:02:28,829][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 9 from persistence list
[INFO][2018-05-29 17:02:28,830][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 8 from persistence list
[INFO][2018-05-29 17:02:28,830][org.apache.spark.storage.BlockManager]Removing RDD 9
[INFO][2018-05-29 17:02:28,831][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-29 17:02:28,831][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527584535000 ms
[INFO][2018-05-29 17:02:28,831][org.apache.spark.storage.BlockManager]Removing RDD 8
[INFO][2018-05-29 17:02:30,019][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527584550000 ms
[INFO][2018-05-29 17:02:30,020][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527584550000 ms.0 from job set of time 1527584550000 ms
[INFO][2018-05-29 17:02:30,028][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:75
[INFO][2018-05-29 17:02:30,029][org.apache.spark.scheduler.DAGScheduler]Got job 7 (isEmpty at ReceiveKafkaData.scala:75) with 1 output partitions
[INFO][2018-05-29 17:02:30,029][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 7 (isEmpty at ReceiveKafkaData.scala:75)
[INFO][2018-05-29 17:02:30,029][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-29 17:02:30,029][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-29 17:02:30,030][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 7 (MapPartitionsRDD[13] at map at ReceiveKafkaData.scala:74), which has no missing parents
[INFO][2018-05-29 17:02:30,032][org.apache.spark.storage.memory.MemoryStore]Block broadcast_7 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-29 17:02:30,037][org.apache.spark.storage.memory.MemoryStore]Block broadcast_7_piece0 stored as bytes in memory (estimated size 1973.0 B, free 912.3 MB)
[INFO][2018-05-29 17:02:30,037][org.apache.spark.storage.BlockManagerInfo]Added broadcast_7_piece0 in memory on 10.194.32.157:63705 (size: 1973.0 B, free: 912.3 MB)
[INFO][2018-05-29 17:02:30,038][org.apache.spark.SparkContext]Created broadcast 7 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-29 17:02:30,039][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[13] at map at ReceiveKafkaData.scala:74) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-29 17:02:30,039][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 7.0 with 1 tasks
[INFO][2018-05-29 17:02:30,039][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 7.0 (TID 7, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-29 17:02:30,040][org.apache.spark.executor.Executor]Running task 0.0 in stage 7.0 (TID 7)
[INFO][2018-05-29 17:02:30,067][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 188846 -> 188851
[INFO][2018-05-29 17:02:30,069][org.apache.spark.executor.Executor]Finished task 0.0 in stage 7.0 (TID 7). 958 bytes result sent to driver
[INFO][2018-05-29 17:02:30,070][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 7.0 (TID 7) in 31 ms on localhost (executor driver) (1/1)
[INFO][2018-05-29 17:02:30,070][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 7.0, whose tasks have all completed, from pool 
[INFO][2018-05-29 17:02:30,071][org.apache.spark.scheduler.DAGScheduler]ResultStage 7 (isEmpty at ReceiveKafkaData.scala:75) finished in 0.032 s
[INFO][2018-05-29 17:02:30,071][org.apache.spark.scheduler.DAGScheduler]Job 7 finished: isEmpty at ReceiveKafkaData.scala:75, took 0.043061 s
[INFO][2018-05-29 17:02:30,078][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:76
[INFO][2018-05-29 17:02:30,078][org.apache.spark.scheduler.DAGScheduler]Got job 8 (foreachPartition at ReceiveKafkaData.scala:76) with 1 output partitions
[INFO][2018-05-29 17:02:30,078][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 8 (foreachPartition at ReceiveKafkaData.scala:76)
[INFO][2018-05-29 17:02:30,078][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-29 17:02:30,079][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-29 17:02:30,079][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 8 (MapPartitionsRDD[13] at map at ReceiveKafkaData.scala:74), which has no missing parents
[INFO][2018-05-29 17:02:30,081][org.apache.spark.storage.memory.MemoryStore]Block broadcast_8 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-29 17:02:30,085][org.apache.spark.storage.memory.MemoryStore]Block broadcast_8_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.3 MB)
[INFO][2018-05-29 17:02:30,086][org.apache.spark.storage.BlockManagerInfo]Added broadcast_8_piece0 in memory on 10.194.32.157:63705 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-29 17:02:30,087][org.apache.spark.SparkContext]Created broadcast 8 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-29 17:02:30,087][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[13] at map at ReceiveKafkaData.scala:74) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-29 17:02:30,088][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 8.0 with 1 tasks
[INFO][2018-05-29 17:02:30,088][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 8.0 (TID 8, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-29 17:02:30,089][org.apache.spark.executor.Executor]Running task 0.0 in stage 8.0 (TID 8)
[INFO][2018-05-29 17:02:30,091][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 188846 -> 188851
[INFO][2018-05-29 17:02:30,091][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 188846
[INFO][2018-05-29 17:02:30,122][com.seven.spark.streaming.ReceiveKafkaData$]Inserting 5 lines of data to HBase is success . . .
[INFO][2018-05-29 17:02:30,171][com.seven.spark.streaming.ReceiveKafkaData$]Inserting 5 lines of data to ElasticSearch is success . . .
[INFO][2018-05-29 17:02:30,173][org.apache.spark.executor.Executor]Finished task 0.0 in stage 8.0 (TID 8). 708 bytes result sent to driver
[INFO][2018-05-29 17:02:30,176][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 8.0 (TID 8) in 88 ms on localhost (executor driver) (1/1)
[INFO][2018-05-29 17:02:30,176][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 8.0, whose tasks have all completed, from pool 
[INFO][2018-05-29 17:02:30,177][org.apache.spark.scheduler.DAGScheduler]ResultStage 8 (foreachPartition at ReceiveKafkaData.scala:76) finished in 0.088 s
[INFO][2018-05-29 17:02:30,179][org.apache.spark.scheduler.DAGScheduler]Job 8 finished: foreachPartition at ReceiveKafkaData.scala:76, took 0.099758 s
[INFO][2018-05-29 17:02:30,179][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527584550000 ms.0 from job set of time 1527584550000 ms
[INFO][2018-05-29 17:02:30,179][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.179 s for time 1527584550000 ms (execution: 0.159 s)
[INFO][2018-05-29 17:02:30,180][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 11 from persistence list
[INFO][2018-05-29 17:02:30,181][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 10 from persistence list
[INFO][2018-05-29 17:02:30,181][org.apache.spark.storage.BlockManager]Removing RDD 11
[INFO][2018-05-29 17:02:30,183][org.apache.spark.storage.BlockManager]Removing RDD 10
[INFO][2018-05-29 17:02:30,183][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-29 17:02:30,183][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527584540000 ms
[INFO][2018-05-29 17:02:35,019][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527584555000 ms
[INFO][2018-05-29 17:02:35,020][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527584555000 ms.0 from job set of time 1527584555000 ms
[INFO][2018-05-29 17:02:35,027][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:75
[INFO][2018-05-29 17:02:35,027][org.apache.spark.scheduler.DAGScheduler]Got job 9 (isEmpty at ReceiveKafkaData.scala:75) with 1 output partitions
[INFO][2018-05-29 17:02:35,028][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 9 (isEmpty at ReceiveKafkaData.scala:75)
[INFO][2018-05-29 17:02:35,028][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-29 17:02:35,028][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-29 17:02:35,029][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 9 (MapPartitionsRDD[15] at map at ReceiveKafkaData.scala:74), which has no missing parents
[INFO][2018-05-29 17:02:35,031][org.apache.spark.storage.memory.MemoryStore]Block broadcast_9 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-29 17:02:35,035][org.apache.spark.storage.memory.MemoryStore]Block broadcast_9_piece0 stored as bytes in memory (estimated size 1973.0 B, free 912.3 MB)
[INFO][2018-05-29 17:02:35,036][org.apache.spark.storage.BlockManagerInfo]Added broadcast_9_piece0 in memory on 10.194.32.157:63705 (size: 1973.0 B, free: 912.3 MB)
[INFO][2018-05-29 17:02:35,036][org.apache.spark.SparkContext]Created broadcast 9 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-29 17:02:35,037][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[15] at map at ReceiveKafkaData.scala:74) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-29 17:02:35,038][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 9.0 with 1 tasks
[INFO][2018-05-29 17:02:35,038][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 9.0 (TID 9, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-29 17:02:35,039][org.apache.spark.executor.Executor]Running task 0.0 in stage 9.0 (TID 9)
[INFO][2018-05-29 17:02:35,041][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 188851 -> 188856
[INFO][2018-05-29 17:02:35,042][org.apache.spark.executor.Executor]Finished task 0.0 in stage 9.0 (TID 9). 971 bytes result sent to driver
[INFO][2018-05-29 17:02:35,043][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 9.0 (TID 9) in 5 ms on localhost (executor driver) (1/1)
[INFO][2018-05-29 17:02:35,043][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 9.0, whose tasks have all completed, from pool 
[INFO][2018-05-29 17:02:35,044][org.apache.spark.scheduler.DAGScheduler]ResultStage 9 (isEmpty at ReceiveKafkaData.scala:75) finished in 0.006 s
[INFO][2018-05-29 17:02:35,044][org.apache.spark.scheduler.DAGScheduler]Job 9 finished: isEmpty at ReceiveKafkaData.scala:75, took 0.017373 s
[INFO][2018-05-29 17:02:35,050][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:76
[INFO][2018-05-29 17:02:35,050][org.apache.spark.scheduler.DAGScheduler]Got job 10 (foreachPartition at ReceiveKafkaData.scala:76) with 1 output partitions
[INFO][2018-05-29 17:02:35,050][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 10 (foreachPartition at ReceiveKafkaData.scala:76)
[INFO][2018-05-29 17:02:35,050][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-29 17:02:35,051][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-29 17:02:35,051][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 10 (MapPartitionsRDD[15] at map at ReceiveKafkaData.scala:74), which has no missing parents
[INFO][2018-05-29 17:02:35,053][org.apache.spark.storage.memory.MemoryStore]Block broadcast_10 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-29 17:02:35,056][org.apache.spark.storage.memory.MemoryStore]Block broadcast_10_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.3 MB)
[INFO][2018-05-29 17:02:35,056][org.apache.spark.storage.BlockManagerInfo]Added broadcast_10_piece0 in memory on 10.194.32.157:63705 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-29 17:02:35,057][org.apache.spark.SparkContext]Created broadcast 10 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-29 17:02:35,058][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[15] at map at ReceiveKafkaData.scala:74) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-29 17:02:35,058][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 10.0 with 1 tasks
[INFO][2018-05-29 17:02:35,058][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 10.0 (TID 10, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-29 17:02:35,059][org.apache.spark.executor.Executor]Running task 0.0 in stage 10.0 (TID 10)
[INFO][2018-05-29 17:02:35,061][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 188851 -> 188856
[INFO][2018-05-29 17:02:35,061][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 188851
[INFO][2018-05-29 17:02:35,088][com.seven.spark.streaming.ReceiveKafkaData$]Inserting 5 lines of data to HBase is success . . .
[INFO][2018-05-29 17:02:35,112][com.seven.spark.streaming.ReceiveKafkaData$]Inserting 5 lines of data to ElasticSearch is success . . .
[INFO][2018-05-29 17:02:35,113][org.apache.spark.executor.Executor]Finished task 0.0 in stage 10.0 (TID 10). 708 bytes result sent to driver
[INFO][2018-05-29 17:02:35,113][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 10.0 (TID 10) in 55 ms on localhost (executor driver) (1/1)
[INFO][2018-05-29 17:02:35,114][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 10.0, whose tasks have all completed, from pool 
[INFO][2018-05-29 17:02:35,114][org.apache.spark.scheduler.DAGScheduler]ResultStage 10 (foreachPartition at ReceiveKafkaData.scala:76) finished in 0.056 s
[INFO][2018-05-29 17:02:35,114][org.apache.spark.scheduler.DAGScheduler]Job 10 finished: foreachPartition at ReceiveKafkaData.scala:76, took 0.064369 s
[INFO][2018-05-29 17:02:35,115][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527584555000 ms.0 from job set of time 1527584555000 ms
[INFO][2018-05-29 17:02:35,115][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.115 s for time 1527584555000 ms (execution: 0.095 s)
[INFO][2018-05-29 17:02:35,115][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 13 from persistence list
[INFO][2018-05-29 17:02:35,115][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 12 from persistence list
[INFO][2018-05-29 17:02:35,116][org.apache.spark.storage.BlockManager]Removing RDD 13
[INFO][2018-05-29 17:02:35,116][org.apache.spark.storage.BlockManager]Removing RDD 12
[INFO][2018-05-29 17:02:35,116][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-29 17:02:35,116][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527584545000 ms
[INFO][2018-05-29 17:02:40,015][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527584560000 ms
[INFO][2018-05-29 17:02:40,015][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527584560000 ms.0 from job set of time 1527584560000 ms
[INFO][2018-05-29 17:02:40,022][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:75
[INFO][2018-05-29 17:02:40,023][org.apache.spark.scheduler.DAGScheduler]Got job 11 (isEmpty at ReceiveKafkaData.scala:75) with 1 output partitions
[INFO][2018-05-29 17:02:40,023][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 11 (isEmpty at ReceiveKafkaData.scala:75)
[INFO][2018-05-29 17:02:40,023][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-29 17:02:40,023][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-29 17:02:40,024][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 11 (MapPartitionsRDD[17] at map at ReceiveKafkaData.scala:74), which has no missing parents
[INFO][2018-05-29 17:02:40,026][org.apache.spark.storage.memory.MemoryStore]Block broadcast_11 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-29 17:02:40,029][org.apache.spark.storage.memory.MemoryStore]Block broadcast_11_piece0 stored as bytes in memory (estimated size 1970.0 B, free 912.3 MB)
[INFO][2018-05-29 17:02:40,030][org.apache.spark.storage.BlockManagerInfo]Added broadcast_11_piece0 in memory on 10.194.32.157:63705 (size: 1970.0 B, free: 912.3 MB)
[INFO][2018-05-29 17:02:40,031][org.apache.spark.SparkContext]Created broadcast 11 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-29 17:02:40,031][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[17] at map at ReceiveKafkaData.scala:74) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-29 17:02:40,031][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 11.0 with 1 tasks
[INFO][2018-05-29 17:02:40,032][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 11.0 (TID 11, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-29 17:02:40,032][org.apache.spark.executor.Executor]Running task 0.0 in stage 11.0 (TID 11)
[INFO][2018-05-29 17:02:40,034][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 188856 -> 188861
[INFO][2018-05-29 17:02:40,035][org.apache.spark.executor.Executor]Finished task 0.0 in stage 11.0 (TID 11). 973 bytes result sent to driver
[INFO][2018-05-29 17:02:40,035][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 11.0 (TID 11) in 3 ms on localhost (executor driver) (1/1)
[INFO][2018-05-29 17:02:40,035][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 11.0, whose tasks have all completed, from pool 
[INFO][2018-05-29 17:02:40,036][org.apache.spark.scheduler.DAGScheduler]ResultStage 11 (isEmpty at ReceiveKafkaData.scala:75) finished in 0.005 s
[INFO][2018-05-29 17:02:40,036][org.apache.spark.scheduler.DAGScheduler]Job 11 finished: isEmpty at ReceiveKafkaData.scala:75, took 0.014420 s
[INFO][2018-05-29 17:02:40,042][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:76
[INFO][2018-05-29 17:02:40,042][org.apache.spark.scheduler.DAGScheduler]Got job 12 (foreachPartition at ReceiveKafkaData.scala:76) with 1 output partitions
[INFO][2018-05-29 17:02:40,042][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 12 (foreachPartition at ReceiveKafkaData.scala:76)
[INFO][2018-05-29 17:02:40,042][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-29 17:02:40,042][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-29 17:02:40,043][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 12 (MapPartitionsRDD[17] at map at ReceiveKafkaData.scala:74), which has no missing parents
[INFO][2018-05-29 17:02:40,044][org.apache.spark.storage.memory.MemoryStore]Block broadcast_12 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-29 17:02:40,047][org.apache.spark.storage.memory.MemoryStore]Block broadcast_12_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.3 MB)
[INFO][2018-05-29 17:02:40,048][org.apache.spark.storage.BlockManagerInfo]Added broadcast_12_piece0 in memory on 10.194.32.157:63705 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-29 17:02:40,048][org.apache.spark.SparkContext]Created broadcast 12 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-29 17:02:40,048][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[17] at map at ReceiveKafkaData.scala:74) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-29 17:02:40,048][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 12.0 with 1 tasks
[INFO][2018-05-29 17:02:40,049][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 12.0 (TID 12, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-29 17:02:40,049][org.apache.spark.executor.Executor]Running task 0.0 in stage 12.0 (TID 12)
[INFO][2018-05-29 17:02:40,051][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 188856 -> 188861
[INFO][2018-05-29 17:02:40,051][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 188856
[INFO][2018-05-29 17:02:40,078][com.seven.spark.streaming.ReceiveKafkaData$]Inserting 5 lines of data to HBase is success . . .
[INFO][2018-05-29 17:02:40,097][com.seven.spark.streaming.ReceiveKafkaData$]Inserting 5 lines of data to ElasticSearch is success . . .
[INFO][2018-05-29 17:02:40,099][org.apache.spark.executor.Executor]Finished task 0.0 in stage 12.0 (TID 12). 708 bytes result sent to driver
[INFO][2018-05-29 17:02:40,100][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 12.0 (TID 12) in 51 ms on localhost (executor driver) (1/1)
[INFO][2018-05-29 17:02:40,100][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 12.0, whose tasks have all completed, from pool 
[INFO][2018-05-29 17:02:40,101][org.apache.spark.scheduler.DAGScheduler]ResultStage 12 (foreachPartition at ReceiveKafkaData.scala:76) finished in 0.052 s
[INFO][2018-05-29 17:02:40,102][org.apache.spark.scheduler.DAGScheduler]Job 12 finished: foreachPartition at ReceiveKafkaData.scala:76, took 0.060177 s
[INFO][2018-05-29 17:02:40,103][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527584560000 ms.0 from job set of time 1527584560000 ms
[INFO][2018-05-29 17:02:40,103][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.103 s for time 1527584560000 ms (execution: 0.088 s)
[INFO][2018-05-29 17:02:40,103][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 15 from persistence list
[INFO][2018-05-29 17:02:40,104][org.apache.spark.storage.BlockManager]Removing RDD 15
[INFO][2018-05-29 17:02:40,104][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 14 from persistence list
[INFO][2018-05-29 17:02:40,105][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-29 17:02:40,105][org.apache.spark.storage.BlockManager]Removing RDD 14
[INFO][2018-05-29 17:02:40,105][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527584550000 ms
[INFO][2018-05-29 17:02:45,018][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527584565000 ms
[INFO][2018-05-29 17:02:45,018][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527584565000 ms.0 from job set of time 1527584565000 ms
[INFO][2018-05-29 17:02:45,025][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:75
[INFO][2018-05-29 17:02:45,026][org.apache.spark.scheduler.DAGScheduler]Got job 13 (isEmpty at ReceiveKafkaData.scala:75) with 1 output partitions
[INFO][2018-05-29 17:02:45,026][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 13 (isEmpty at ReceiveKafkaData.scala:75)
[INFO][2018-05-29 17:02:45,026][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-29 17:02:45,026][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-29 17:02:45,026][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 13 (MapPartitionsRDD[19] at map at ReceiveKafkaData.scala:74), which has no missing parents
[INFO][2018-05-29 17:02:45,028][org.apache.spark.storage.memory.MemoryStore]Block broadcast_13 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-29 17:02:45,031][org.apache.spark.storage.memory.MemoryStore]Block broadcast_13_piece0 stored as bytes in memory (estimated size 1973.0 B, free 912.3 MB)
[INFO][2018-05-29 17:02:45,032][org.apache.spark.storage.BlockManagerInfo]Added broadcast_13_piece0 in memory on 10.194.32.157:63705 (size: 1973.0 B, free: 912.3 MB)
[INFO][2018-05-29 17:02:45,032][org.apache.spark.SparkContext]Created broadcast 13 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-29 17:02:45,033][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[19] at map at ReceiveKafkaData.scala:74) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-29 17:02:45,033][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 13.0 with 1 tasks
[INFO][2018-05-29 17:02:45,034][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 13.0 (TID 13, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-29 17:02:45,034][org.apache.spark.executor.Executor]Running task 0.0 in stage 13.0 (TID 13)
[INFO][2018-05-29 17:02:45,036][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 188861 -> 188866
[INFO][2018-05-29 17:02:45,038][org.apache.spark.executor.Executor]Finished task 0.0 in stage 13.0 (TID 13). 971 bytes result sent to driver
[INFO][2018-05-29 17:02:45,038][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 13.0 (TID 13) in 5 ms on localhost (executor driver) (1/1)
[INFO][2018-05-29 17:02:45,038][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 13.0, whose tasks have all completed, from pool 
[INFO][2018-05-29 17:02:45,039][org.apache.spark.scheduler.DAGScheduler]ResultStage 13 (isEmpty at ReceiveKafkaData.scala:75) finished in 0.006 s
[INFO][2018-05-29 17:02:45,039][org.apache.spark.scheduler.DAGScheduler]Job 13 finished: isEmpty at ReceiveKafkaData.scala:75, took 0.014052 s
[INFO][2018-05-29 17:02:45,045][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:76
[INFO][2018-05-29 17:02:45,046][org.apache.spark.scheduler.DAGScheduler]Got job 14 (foreachPartition at ReceiveKafkaData.scala:76) with 1 output partitions
[INFO][2018-05-29 17:02:45,046][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 14 (foreachPartition at ReceiveKafkaData.scala:76)
[INFO][2018-05-29 17:02:45,046][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-29 17:02:45,046][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-29 17:02:45,047][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 14 (MapPartitionsRDD[19] at map at ReceiveKafkaData.scala:74), which has no missing parents
[INFO][2018-05-29 17:02:45,049][org.apache.spark.storage.memory.MemoryStore]Block broadcast_14 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-29 17:02:45,053][org.apache.spark.storage.memory.MemoryStore]Block broadcast_14_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.3 MB)
[INFO][2018-05-29 17:02:45,053][org.apache.spark.storage.BlockManagerInfo]Added broadcast_14_piece0 in memory on 10.194.32.157:63705 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-29 17:02:45,054][org.apache.spark.SparkContext]Created broadcast 14 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-29 17:02:45,054][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[19] at map at ReceiveKafkaData.scala:74) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-29 17:02:45,054][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 14.0 with 1 tasks
[INFO][2018-05-29 17:02:45,055][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 14.0 (TID 14, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-29 17:02:45,056][org.apache.spark.executor.Executor]Running task 0.0 in stage 14.0 (TID 14)
[INFO][2018-05-29 17:02:45,058][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 188861 -> 188866
[INFO][2018-05-29 17:02:45,058][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 188861
[INFO][2018-05-29 17:02:45,083][com.seven.spark.streaming.ReceiveKafkaData$]Inserting 5 lines of data to HBase is success . . .
[INFO][2018-05-29 17:02:45,106][com.seven.spark.streaming.ReceiveKafkaData$]Inserting 5 lines of data to ElasticSearch is success . . .
[INFO][2018-05-29 17:02:45,107][org.apache.spark.executor.Executor]Finished task 0.0 in stage 14.0 (TID 14). 751 bytes result sent to driver
[INFO][2018-05-29 17:02:45,107][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 14.0 (TID 14) in 52 ms on localhost (executor driver) (1/1)
[INFO][2018-05-29 17:02:45,107][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 14.0, whose tasks have all completed, from pool 
[INFO][2018-05-29 17:02:45,108][org.apache.spark.scheduler.DAGScheduler]ResultStage 14 (foreachPartition at ReceiveKafkaData.scala:76) finished in 0.053 s
[INFO][2018-05-29 17:02:45,108][org.apache.spark.scheduler.DAGScheduler]Job 14 finished: foreachPartition at ReceiveKafkaData.scala:76, took 0.062780 s
[INFO][2018-05-29 17:02:45,109][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527584565000 ms.0 from job set of time 1527584565000 ms
[INFO][2018-05-29 17:02:45,109][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.109 s for time 1527584565000 ms (execution: 0.091 s)
[INFO][2018-05-29 17:02:45,109][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 17 from persistence list
[INFO][2018-05-29 17:02:45,109][org.apache.spark.storage.BlockManager]Removing RDD 17
[INFO][2018-05-29 17:02:45,110][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 16 from persistence list
[INFO][2018-05-29 17:02:45,110][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-29 17:02:45,110][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527584555000 ms
[INFO][2018-05-29 17:02:45,110][org.apache.spark.storage.BlockManager]Removing RDD 16
[INFO][2018-05-29 17:02:50,019][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527584570000 ms
[INFO][2018-05-29 17:02:50,019][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527584570000 ms.0 from job set of time 1527584570000 ms
[INFO][2018-05-29 17:02:50,026][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:75
[INFO][2018-05-29 17:02:50,026][org.apache.spark.scheduler.DAGScheduler]Got job 15 (isEmpty at ReceiveKafkaData.scala:75) with 1 output partitions
[INFO][2018-05-29 17:02:50,026][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 15 (isEmpty at ReceiveKafkaData.scala:75)
[INFO][2018-05-29 17:02:50,026][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-29 17:02:50,027][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-29 17:02:50,027][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 15 (MapPartitionsRDD[21] at map at ReceiveKafkaData.scala:74), which has no missing parents
[INFO][2018-05-29 17:02:50,029][org.apache.spark.storage.memory.MemoryStore]Block broadcast_15 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-29 17:02:50,032][org.apache.spark.storage.memory.MemoryStore]Block broadcast_15_piece0 stored as bytes in memory (estimated size 1973.0 B, free 912.2 MB)
[INFO][2018-05-29 17:02:50,032][org.apache.spark.storage.BlockManagerInfo]Added broadcast_15_piece0 in memory on 10.194.32.157:63705 (size: 1973.0 B, free: 912.3 MB)
[INFO][2018-05-29 17:02:50,033][org.apache.spark.SparkContext]Created broadcast 15 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-29 17:02:50,034][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 15 (MapPartitionsRDD[21] at map at ReceiveKafkaData.scala:74) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-29 17:02:50,034][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 15.0 with 1 tasks
[INFO][2018-05-29 17:02:50,035][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 15.0 (TID 15, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-29 17:02:50,035][org.apache.spark.executor.Executor]Running task 0.0 in stage 15.0 (TID 15)
[INFO][2018-05-29 17:02:50,037][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 188866 -> 188871
[INFO][2018-05-29 17:02:50,038][org.apache.spark.executor.Executor]Finished task 0.0 in stage 15.0 (TID 15). 973 bytes result sent to driver
[INFO][2018-05-29 17:02:50,039][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 15.0 (TID 15) in 5 ms on localhost (executor driver) (1/1)
[INFO][2018-05-29 17:02:50,039][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 15.0, whose tasks have all completed, from pool 
[INFO][2018-05-29 17:02:50,040][org.apache.spark.scheduler.DAGScheduler]ResultStage 15 (isEmpty at ReceiveKafkaData.scala:75) finished in 0.005 s
[INFO][2018-05-29 17:02:50,040][org.apache.spark.scheduler.DAGScheduler]Job 15 finished: isEmpty at ReceiveKafkaData.scala:75, took 0.014108 s
[INFO][2018-05-29 17:02:50,046][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:76
[INFO][2018-05-29 17:02:50,046][org.apache.spark.scheduler.DAGScheduler]Got job 16 (foreachPartition at ReceiveKafkaData.scala:76) with 1 output partitions
[INFO][2018-05-29 17:02:50,046][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 16 (foreachPartition at ReceiveKafkaData.scala:76)
[INFO][2018-05-29 17:02:50,046][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-29 17:02:50,046][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-29 17:02:50,047][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 16 (MapPartitionsRDD[21] at map at ReceiveKafkaData.scala:74), which has no missing parents
[INFO][2018-05-29 17:02:50,049][org.apache.spark.storage.memory.MemoryStore]Block broadcast_16 stored as values in memory (estimated size 3.4 KB, free 912.2 MB)
[INFO][2018-05-29 17:02:50,052][org.apache.spark.storage.memory.MemoryStore]Block broadcast_16_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.2 MB)
[INFO][2018-05-29 17:02:50,052][org.apache.spark.storage.BlockManagerInfo]Added broadcast_16_piece0 in memory on 10.194.32.157:63705 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-29 17:02:50,053][org.apache.spark.SparkContext]Created broadcast 16 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-29 17:02:50,053][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 16 (MapPartitionsRDD[21] at map at ReceiveKafkaData.scala:74) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-29 17:02:50,054][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 16.0 with 1 tasks
[INFO][2018-05-29 17:02:50,054][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 16.0 (TID 16, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-29 17:02:50,054][org.apache.spark.executor.Executor]Running task 0.0 in stage 16.0 (TID 16)
[INFO][2018-05-29 17:02:50,056][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 188866 -> 188871
[INFO][2018-05-29 17:02:50,056][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 188866
[INFO][2018-05-29 17:02:50,081][com.seven.spark.streaming.ReceiveKafkaData$]Inserting 5 lines of data to HBase is success . . .
[INFO][2018-05-29 17:02:50,099][com.seven.spark.streaming.ReceiveKafkaData$]Inserting 5 lines of data to ElasticSearch is success . . .
[INFO][2018-05-29 17:02:50,100][org.apache.spark.executor.Executor]Finished task 0.0 in stage 16.0 (TID 16). 751 bytes result sent to driver
[INFO][2018-05-29 17:02:50,100][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 16.0 (TID 16) in 46 ms on localhost (executor driver) (1/1)
[INFO][2018-05-29 17:02:50,101][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 16.0, whose tasks have all completed, from pool 
[INFO][2018-05-29 17:02:50,101][org.apache.spark.scheduler.DAGScheduler]ResultStage 16 (foreachPartition at ReceiveKafkaData.scala:76) finished in 0.047 s
[INFO][2018-05-29 17:02:50,101][org.apache.spark.scheduler.DAGScheduler]Job 16 finished: foreachPartition at ReceiveKafkaData.scala:76, took 0.055731 s
[INFO][2018-05-29 17:02:50,102][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527584570000 ms.0 from job set of time 1527584570000 ms
[INFO][2018-05-29 17:02:50,102][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.102 s for time 1527584570000 ms (execution: 0.083 s)
[INFO][2018-05-29 17:02:50,102][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 19 from persistence list
[INFO][2018-05-29 17:02:50,103][org.apache.spark.storage.BlockManager]Removing RDD 19
[INFO][2018-05-29 17:02:50,103][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 18 from persistence list
[INFO][2018-05-29 17:02:50,103][org.apache.spark.storage.BlockManager]Removing RDD 18
[INFO][2018-05-29 17:02:50,103][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-29 17:02:50,103][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527584560000 ms
[INFO][2018-05-29 17:02:55,017][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527584575000 ms
[INFO][2018-05-29 17:02:55,018][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527584575000 ms.0 from job set of time 1527584575000 ms
[INFO][2018-05-29 17:02:55,024][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:75
[INFO][2018-05-29 17:02:55,024][org.apache.spark.scheduler.DAGScheduler]Got job 17 (isEmpty at ReceiveKafkaData.scala:75) with 1 output partitions
[INFO][2018-05-29 17:02:55,025][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 17 (isEmpty at ReceiveKafkaData.scala:75)
[INFO][2018-05-29 17:02:55,025][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-29 17:02:55,025][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-29 17:02:55,025][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 17 (MapPartitionsRDD[23] at map at ReceiveKafkaData.scala:74), which has no missing parents
[INFO][2018-05-29 17:02:55,026][org.apache.spark.storage.memory.MemoryStore]Block broadcast_17 stored as values in memory (estimated size 3.1 KB, free 912.2 MB)
[INFO][2018-05-29 17:02:55,030][org.apache.spark.storage.memory.MemoryStore]Block broadcast_17_piece0 stored as bytes in memory (estimated size 1973.0 B, free 912.2 MB)
[INFO][2018-05-29 17:02:55,030][org.apache.spark.storage.BlockManagerInfo]Added broadcast_17_piece0 in memory on 10.194.32.157:63705 (size: 1973.0 B, free: 912.3 MB)
[INFO][2018-05-29 17:02:55,031][org.apache.spark.SparkContext]Created broadcast 17 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-29 17:02:55,031][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[23] at map at ReceiveKafkaData.scala:74) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-29 17:02:55,031][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 17.0 with 1 tasks
[INFO][2018-05-29 17:02:55,032][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 17.0 (TID 17, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-29 17:02:55,032][org.apache.spark.executor.Executor]Running task 0.0 in stage 17.0 (TID 17)
[INFO][2018-05-29 17:02:55,033][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 188871 -> 188876
[INFO][2018-05-29 17:02:55,034][org.apache.spark.executor.Executor]Finished task 0.0 in stage 17.0 (TID 17). 931 bytes result sent to driver
[INFO][2018-05-29 17:02:55,035][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 17.0 (TID 17) in 3 ms on localhost (executor driver) (1/1)
[INFO][2018-05-29 17:02:55,035][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 17.0, whose tasks have all completed, from pool 
[INFO][2018-05-29 17:02:55,035][org.apache.spark.scheduler.DAGScheduler]ResultStage 17 (isEmpty at ReceiveKafkaData.scala:75) finished in 0.004 s
[INFO][2018-05-29 17:02:55,035][org.apache.spark.scheduler.DAGScheduler]Job 17 finished: isEmpty at ReceiveKafkaData.scala:75, took 0.011395 s
[INFO][2018-05-29 17:02:55,040][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:76
[INFO][2018-05-29 17:02:55,041][org.apache.spark.scheduler.DAGScheduler]Got job 18 (foreachPartition at ReceiveKafkaData.scala:76) with 1 output partitions
[INFO][2018-05-29 17:02:55,041][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 18 (foreachPartition at ReceiveKafkaData.scala:76)
[INFO][2018-05-29 17:02:55,041][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-29 17:02:55,041][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-29 17:02:55,041][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 18 (MapPartitionsRDD[23] at map at ReceiveKafkaData.scala:74), which has no missing parents
[INFO][2018-05-29 17:02:55,043][org.apache.spark.storage.memory.MemoryStore]Block broadcast_18 stored as values in memory (estimated size 3.4 KB, free 912.2 MB)
[INFO][2018-05-29 17:02:55,046][org.apache.spark.storage.memory.MemoryStore]Block broadcast_18_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.2 MB)
[INFO][2018-05-29 17:02:55,047][org.apache.spark.storage.BlockManagerInfo]Added broadcast_18_piece0 in memory on 10.194.32.157:63705 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-29 17:02:55,047][org.apache.spark.SparkContext]Created broadcast 18 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-29 17:02:55,048][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 18 (MapPartitionsRDD[23] at map at ReceiveKafkaData.scala:74) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-29 17:02:55,048][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 18.0 with 1 tasks
[INFO][2018-05-29 17:02:55,048][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 18.0 (TID 18, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-29 17:02:55,049][org.apache.spark.executor.Executor]Running task 0.0 in stage 18.0 (TID 18)
[INFO][2018-05-29 17:02:55,050][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 188871 -> 188876
[INFO][2018-05-29 17:02:55,050][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 188871
[INFO][2018-05-29 17:02:55,076][com.seven.spark.streaming.ReceiveKafkaData$]Inserting 5 lines of data to HBase is success . . .
[INFO][2018-05-29 17:02:55,095][com.seven.spark.streaming.ReceiveKafkaData$]Inserting 5 lines of data to ElasticSearch is success . . .
[INFO][2018-05-29 17:02:55,095][org.apache.spark.executor.Executor]Finished task 0.0 in stage 18.0 (TID 18). 708 bytes result sent to driver
[INFO][2018-05-29 17:02:55,095][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 18.0 (TID 18) in 47 ms on localhost (executor driver) (1/1)
[INFO][2018-05-29 17:02:55,096][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 18.0, whose tasks have all completed, from pool 
[INFO][2018-05-29 17:02:55,096][org.apache.spark.scheduler.DAGScheduler]ResultStage 18 (foreachPartition at ReceiveKafkaData.scala:76) finished in 0.048 s
[INFO][2018-05-29 17:02:55,096][org.apache.spark.scheduler.DAGScheduler]Job 18 finished: foreachPartition at ReceiveKafkaData.scala:76, took 0.055941 s
[INFO][2018-05-29 17:02:55,096][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527584575000 ms.0 from job set of time 1527584575000 ms
[INFO][2018-05-29 17:02:55,097][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.096 s for time 1527584575000 ms (execution: 0.078 s)
[INFO][2018-05-29 17:02:55,097][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 21 from persistence list
[INFO][2018-05-29 17:02:55,097][org.apache.spark.storage.BlockManager]Removing RDD 21
[INFO][2018-05-29 17:02:55,097][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 20 from persistence list
[INFO][2018-05-29 17:02:55,098][org.apache.spark.storage.BlockManager]Removing RDD 20
[INFO][2018-05-29 17:02:55,098][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-29 17:02:55,098][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527584565000 ms
[INFO][2018-05-29 17:03:00,015][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527584580000 ms
[INFO][2018-05-29 17:03:00,015][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527584580000 ms.0 from job set of time 1527584580000 ms
[INFO][2018-05-29 17:03:00,022][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:75
[INFO][2018-05-29 17:03:00,023][org.apache.spark.scheduler.DAGScheduler]Got job 19 (isEmpty at ReceiveKafkaData.scala:75) with 1 output partitions
[INFO][2018-05-29 17:03:00,023][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 19 (isEmpty at ReceiveKafkaData.scala:75)
[INFO][2018-05-29 17:03:00,023][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-29 17:03:00,023][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-29 17:03:00,023][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 19 (MapPartitionsRDD[25] at map at ReceiveKafkaData.scala:74), which has no missing parents
[INFO][2018-05-29 17:03:00,025][org.apache.spark.storage.memory.MemoryStore]Block broadcast_19 stored as values in memory (estimated size 3.1 KB, free 912.2 MB)
[INFO][2018-05-29 17:03:00,028][org.apache.spark.storage.memory.MemoryStore]Block broadcast_19_piece0 stored as bytes in memory (estimated size 1973.0 B, free 912.2 MB)
[INFO][2018-05-29 17:03:00,028][org.apache.spark.storage.BlockManagerInfo]Added broadcast_19_piece0 in memory on 10.194.32.157:63705 (size: 1973.0 B, free: 912.3 MB)
[INFO][2018-05-29 17:03:00,029][org.apache.spark.SparkContext]Created broadcast 19 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-29 17:03:00,030][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 19 (MapPartitionsRDD[25] at map at ReceiveKafkaData.scala:74) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-29 17:03:00,030][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 19.0 with 1 tasks
[INFO][2018-05-29 17:03:00,030][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 19.0 (TID 19, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-29 17:03:00,031][org.apache.spark.executor.Executor]Running task 0.0 in stage 19.0 (TID 19)
[INFO][2018-05-29 17:03:00,032][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 188876 -> 188881
[INFO][2018-05-29 17:03:00,033][org.apache.spark.executor.Executor]Finished task 0.0 in stage 19.0 (TID 19). 973 bytes result sent to driver
[INFO][2018-05-29 17:03:00,034][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 19.0 (TID 19) in 4 ms on localhost (executor driver) (1/1)
[INFO][2018-05-29 17:03:00,034][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 19.0, whose tasks have all completed, from pool 
[INFO][2018-05-29 17:03:00,034][org.apache.spark.scheduler.DAGScheduler]ResultStage 19 (isEmpty at ReceiveKafkaData.scala:75) finished in 0.004 s
[INFO][2018-05-29 17:03:00,035][org.apache.spark.scheduler.DAGScheduler]Job 19 finished: isEmpty at ReceiveKafkaData.scala:75, took 0.012619 s
[INFO][2018-05-29 17:03:00,040][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:76
[INFO][2018-05-29 17:03:00,040][org.apache.spark.scheduler.DAGScheduler]Got job 20 (foreachPartition at ReceiveKafkaData.scala:76) with 1 output partitions
[INFO][2018-05-29 17:03:00,040][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 20 (foreachPartition at ReceiveKafkaData.scala:76)
[INFO][2018-05-29 17:03:00,040][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-29 17:03:00,040][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-29 17:03:00,041][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 20 (MapPartitionsRDD[25] at map at ReceiveKafkaData.scala:74), which has no missing parents
[INFO][2018-05-29 17:03:00,042][org.apache.spark.storage.memory.MemoryStore]Block broadcast_20 stored as values in memory (estimated size 3.4 KB, free 912.2 MB)
[INFO][2018-05-29 17:03:00,056][org.apache.spark.storage.memory.MemoryStore]Block broadcast_20_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.2 MB)
[INFO][2018-05-29 17:03:00,057][org.apache.spark.storage.BlockManagerInfo]Added broadcast_20_piece0 in memory on 10.194.32.157:63705 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-29 17:03:00,057][org.apache.spark.SparkContext]Created broadcast 20 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-29 17:03:00,058][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 20 (MapPartitionsRDD[25] at map at ReceiveKafkaData.scala:74) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-29 17:03:00,058][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 20.0 with 1 tasks
[INFO][2018-05-29 17:03:00,060][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 20.0 (TID 20, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-29 17:03:00,060][org.apache.spark.executor.Executor]Running task 0.0 in stage 20.0 (TID 20)
[INFO][2018-05-29 17:03:00,060][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_11_piece0 on 10.194.32.157:63705 in memory (size: 1970.0 B, free: 912.3 MB)
[INFO][2018-05-29 17:03:00,061][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 188876 -> 188881
[INFO][2018-05-29 17:03:00,062][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 188876
[INFO][2018-05-29 17:03:00,062][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_18_piece0 on 10.194.32.157:63705 in memory (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-29 17:03:00,064][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_19_piece0 on 10.194.32.157:63705 in memory (size: 1973.0 B, free: 912.3 MB)
[INFO][2018-05-29 17:03:00,065][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_12_piece0 on 10.194.32.157:63705 in memory (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-29 17:03:00,065][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_13_piece0 on 10.194.32.157:63705 in memory (size: 1973.0 B, free: 912.3 MB)
[INFO][2018-05-29 17:03:00,066][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_7_piece0 on 10.194.32.157:63705 in memory (size: 1973.0 B, free: 912.3 MB)
[INFO][2018-05-29 17:03:00,068][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_17_piece0 on 10.194.32.157:63705 in memory (size: 1973.0 B, free: 912.3 MB)
[INFO][2018-05-29 17:03:00,071][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_9_piece0 on 10.194.32.157:63705 in memory (size: 1973.0 B, free: 912.3 MB)
[INFO][2018-05-29 17:03:00,071][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_8_piece0 on 10.194.32.157:63705 in memory (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-29 17:03:00,073][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_16_piece0 on 10.194.32.157:63705 in memory (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-29 17:03:00,075][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_15_piece0 on 10.194.32.157:63705 in memory (size: 1973.0 B, free: 912.3 MB)
[INFO][2018-05-29 17:03:00,078][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_10_piece0 on 10.194.32.157:63705 in memory (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-29 17:03:00,079][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_14_piece0 on 10.194.32.157:63705 in memory (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-29 17:03:00,105][com.seven.spark.streaming.ReceiveKafkaData$]Inserting 5 lines of data to HBase is success . . .
[INFO][2018-05-29 17:03:00,121][com.seven.spark.streaming.ReceiveKafkaData$]Inserting 5 lines of data to ElasticSearch is success . . .
[INFO][2018-05-29 17:03:00,121][org.apache.spark.executor.Executor]Finished task 0.0 in stage 20.0 (TID 20). 665 bytes result sent to driver
[INFO][2018-05-29 17:03:00,122][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 20.0 (TID 20) in 63 ms on localhost (executor driver) (1/1)
[INFO][2018-05-29 17:03:00,122][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 20.0, whose tasks have all completed, from pool 
[INFO][2018-05-29 17:03:00,123][org.apache.spark.scheduler.DAGScheduler]ResultStage 20 (foreachPartition at ReceiveKafkaData.scala:76) finished in 0.063 s
[INFO][2018-05-29 17:03:00,123][org.apache.spark.scheduler.DAGScheduler]Job 20 finished: foreachPartition at ReceiveKafkaData.scala:76, took 0.082959 s
[INFO][2018-05-29 17:03:00,123][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527584580000 ms.0 from job set of time 1527584580000 ms
[INFO][2018-05-29 17:03:00,124][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.123 s for time 1527584580000 ms (execution: 0.108 s)
[INFO][2018-05-29 17:03:00,124][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 23 from persistence list
[INFO][2018-05-29 17:03:00,124][org.apache.spark.storage.BlockManager]Removing RDD 23
[INFO][2018-05-29 17:03:00,124][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 22 from persistence list
[INFO][2018-05-29 17:03:00,124][org.apache.spark.storage.BlockManager]Removing RDD 22
[INFO][2018-05-29 17:03:00,124][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-29 17:03:00,124][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527584570000 ms
[INFO][2018-05-29 17:03:05,015][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527584585000 ms
[INFO][2018-05-29 17:03:05,016][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527584585000 ms.0 from job set of time 1527584585000 ms
[INFO][2018-05-29 17:03:05,020][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:75
[INFO][2018-05-29 17:03:05,021][org.apache.spark.scheduler.DAGScheduler]Got job 21 (isEmpty at ReceiveKafkaData.scala:75) with 1 output partitions
[INFO][2018-05-29 17:03:05,021][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 21 (isEmpty at ReceiveKafkaData.scala:75)
[INFO][2018-05-29 17:03:05,021][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-29 17:03:05,021][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-29 17:03:05,021][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 21 (MapPartitionsRDD[27] at map at ReceiveKafkaData.scala:74), which has no missing parents
[INFO][2018-05-29 17:03:05,023][org.apache.spark.storage.memory.MemoryStore]Block broadcast_21 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-29 17:03:05,024][org.apache.spark.storage.memory.MemoryStore]Block broadcast_21_piece0 stored as bytes in memory (estimated size 1973.0 B, free 912.3 MB)
[INFO][2018-05-29 17:03:05,025][org.apache.spark.storage.BlockManagerInfo]Added broadcast_21_piece0 in memory on 10.194.32.157:63705 (size: 1973.0 B, free: 912.3 MB)
[INFO][2018-05-29 17:03:05,025][org.apache.spark.SparkContext]Created broadcast 21 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-29 17:03:05,025][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 21 (MapPartitionsRDD[27] at map at ReceiveKafkaData.scala:74) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-29 17:03:05,026][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 21.0 with 1 tasks
[INFO][2018-05-29 17:03:05,027][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 21.0 (TID 21, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-29 17:03:05,028][org.apache.spark.executor.Executor]Running task 0.0 in stage 21.0 (TID 21)
[INFO][2018-05-29 17:03:05,029][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 188881 -> 188886
[INFO][2018-05-29 17:03:05,031][org.apache.spark.executor.Executor]Finished task 0.0 in stage 21.0 (TID 21). 927 bytes result sent to driver
[INFO][2018-05-29 17:03:05,031][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 21.0 (TID 21) in 5 ms on localhost (executor driver) (1/1)
[INFO][2018-05-29 17:03:05,031][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 21.0, whose tasks have all completed, from pool 
[INFO][2018-05-29 17:03:05,032][org.apache.spark.scheduler.DAGScheduler]ResultStage 21 (isEmpty at ReceiveKafkaData.scala:75) finished in 0.005 s
[INFO][2018-05-29 17:03:05,032][org.apache.spark.scheduler.DAGScheduler]Job 21 finished: isEmpty at ReceiveKafkaData.scala:75, took 0.011497 s
[INFO][2018-05-29 17:03:05,037][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:76
[INFO][2018-05-29 17:03:05,037][org.apache.spark.scheduler.DAGScheduler]Got job 22 (foreachPartition at ReceiveKafkaData.scala:76) with 1 output partitions
[INFO][2018-05-29 17:03:05,037][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 22 (foreachPartition at ReceiveKafkaData.scala:76)
[INFO][2018-05-29 17:03:05,037][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-29 17:03:05,038][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-29 17:03:05,038][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 22 (MapPartitionsRDD[27] at map at ReceiveKafkaData.scala:74), which has no missing parents
[INFO][2018-05-29 17:03:05,040][org.apache.spark.storage.memory.MemoryStore]Block broadcast_22 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-29 17:03:05,041][org.apache.spark.storage.memory.MemoryStore]Block broadcast_22_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.3 MB)
[INFO][2018-05-29 17:03:05,042][org.apache.spark.storage.BlockManagerInfo]Added broadcast_22_piece0 in memory on 10.194.32.157:63705 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-29 17:03:05,042][org.apache.spark.SparkContext]Created broadcast 22 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-29 17:03:05,042][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 22 (MapPartitionsRDD[27] at map at ReceiveKafkaData.scala:74) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-29 17:03:05,042][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 22.0 with 1 tasks
[INFO][2018-05-29 17:03:05,043][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 22.0 (TID 22, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-29 17:03:05,043][org.apache.spark.executor.Executor]Running task 0.0 in stage 22.0 (TID 22)
[INFO][2018-05-29 17:03:05,044][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 188881 -> 188886
[INFO][2018-05-29 17:03:05,044][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 188881
[INFO][2018-05-29 17:03:05,070][com.seven.spark.streaming.ReceiveKafkaData$]Inserting 5 lines of data to HBase is success . . .
[INFO][2018-05-29 17:03:05,088][com.seven.spark.streaming.ReceiveKafkaData$]Inserting 5 lines of data to ElasticSearch is success . . .
[INFO][2018-05-29 17:03:05,088][org.apache.spark.executor.Executor]Finished task 0.0 in stage 22.0 (TID 22). 708 bytes result sent to driver
[INFO][2018-05-29 17:03:05,089][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 22.0 (TID 22) in 46 ms on localhost (executor driver) (1/1)
[INFO][2018-05-29 17:03:05,089][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 22.0, whose tasks have all completed, from pool 
[INFO][2018-05-29 17:03:05,089][org.apache.spark.scheduler.DAGScheduler]ResultStage 22 (foreachPartition at ReceiveKafkaData.scala:76) finished in 0.046 s
[INFO][2018-05-29 17:03:05,090][org.apache.spark.scheduler.DAGScheduler]Job 22 finished: foreachPartition at ReceiveKafkaData.scala:76, took 0.052568 s
[INFO][2018-05-29 17:03:05,090][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527584585000 ms.0 from job set of time 1527584585000 ms
[INFO][2018-05-29 17:03:05,090][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.090 s for time 1527584585000 ms (execution: 0.074 s)
[INFO][2018-05-29 17:03:05,090][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 25 from persistence list
[INFO][2018-05-29 17:03:05,091][org.apache.spark.storage.BlockManager]Removing RDD 25
[INFO][2018-05-29 17:03:05,091][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 24 from persistence list
[INFO][2018-05-29 17:03:05,091][org.apache.spark.storage.BlockManager]Removing RDD 24
[INFO][2018-05-29 17:03:05,091][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-29 17:03:05,092][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527584575000 ms
[INFO][2018-05-29 17:03:10,015][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527584590000 ms
[INFO][2018-05-29 17:03:10,016][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527584590000 ms.0 from job set of time 1527584590000 ms
[INFO][2018-05-29 17:03:10,021][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:75
[INFO][2018-05-29 17:03:10,021][org.apache.spark.scheduler.DAGScheduler]Got job 23 (isEmpty at ReceiveKafkaData.scala:75) with 1 output partitions
[INFO][2018-05-29 17:03:10,021][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 23 (isEmpty at ReceiveKafkaData.scala:75)
[INFO][2018-05-29 17:03:10,021][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-29 17:03:10,022][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-29 17:03:10,022][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 23 (MapPartitionsRDD[29] at map at ReceiveKafkaData.scala:74), which has no missing parents
[INFO][2018-05-29 17:03:10,024][org.apache.spark.storage.memory.MemoryStore]Block broadcast_23 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-29 17:03:10,025][org.apache.spark.storage.memory.MemoryStore]Block broadcast_23_piece0 stored as bytes in memory (estimated size 1973.0 B, free 912.3 MB)
[INFO][2018-05-29 17:03:10,026][org.apache.spark.storage.BlockManagerInfo]Added broadcast_23_piece0 in memory on 10.194.32.157:63705 (size: 1973.0 B, free: 912.3 MB)
[INFO][2018-05-29 17:03:10,026][org.apache.spark.SparkContext]Created broadcast 23 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-29 17:03:10,026][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 23 (MapPartitionsRDD[29] at map at ReceiveKafkaData.scala:74) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-29 17:03:10,026][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 23.0 with 1 tasks
[INFO][2018-05-29 17:03:10,027][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 23.0 (TID 23, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-29 17:03:10,027][org.apache.spark.executor.Executor]Running task 0.0 in stage 23.0 (TID 23)
[INFO][2018-05-29 17:03:10,029][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 188886 -> 188891
[INFO][2018-05-29 17:03:10,030][org.apache.spark.executor.Executor]Finished task 0.0 in stage 23.0 (TID 23). 914 bytes result sent to driver
[INFO][2018-05-29 17:03:10,030][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 23.0 (TID 23) in 3 ms on localhost (executor driver) (1/1)
[INFO][2018-05-29 17:03:10,030][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 23.0, whose tasks have all completed, from pool 
[INFO][2018-05-29 17:03:10,031][org.apache.spark.scheduler.DAGScheduler]ResultStage 23 (isEmpty at ReceiveKafkaData.scala:75) finished in 0.004 s
[INFO][2018-05-29 17:03:10,031][org.apache.spark.scheduler.DAGScheduler]Job 23 finished: isEmpty at ReceiveKafkaData.scala:75, took 0.010362 s
[INFO][2018-05-29 17:03:10,035][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:76
[INFO][2018-05-29 17:03:10,035][org.apache.spark.scheduler.DAGScheduler]Got job 24 (foreachPartition at ReceiveKafkaData.scala:76) with 1 output partitions
[INFO][2018-05-29 17:03:10,035][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 24 (foreachPartition at ReceiveKafkaData.scala:76)
[INFO][2018-05-29 17:03:10,035][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-29 17:03:10,035][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-29 17:03:10,035][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 24 (MapPartitionsRDD[29] at map at ReceiveKafkaData.scala:74), which has no missing parents
[INFO][2018-05-29 17:03:10,036][org.apache.spark.storage.memory.MemoryStore]Block broadcast_24 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-29 17:03:10,038][org.apache.spark.storage.memory.MemoryStore]Block broadcast_24_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.3 MB)
[INFO][2018-05-29 17:03:10,038][org.apache.spark.storage.BlockManagerInfo]Added broadcast_24_piece0 in memory on 10.194.32.157:63705 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-29 17:03:10,038][org.apache.spark.SparkContext]Created broadcast 24 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-29 17:03:10,038][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 24 (MapPartitionsRDD[29] at map at ReceiveKafkaData.scala:74) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-29 17:03:10,038][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 24.0 with 1 tasks
[INFO][2018-05-29 17:03:10,039][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 24.0 (TID 24, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-29 17:03:10,039][org.apache.spark.executor.Executor]Running task 0.0 in stage 24.0 (TID 24)
[INFO][2018-05-29 17:03:10,040][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 188886 -> 188891
[INFO][2018-05-29 17:03:10,040][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 188886
[INFO][2018-05-29 17:03:11,077][com.seven.spark.streaming.ReceiveKafkaData$]Inserting 5 lines of data to HBase is success . . .
[INFO][2018-05-29 17:03:11,094][com.seven.spark.streaming.ReceiveKafkaData$]Inserting 5 lines of data to ElasticSearch is success . . .
[INFO][2018-05-29 17:03:11,094][org.apache.spark.executor.Executor]Finished task 0.0 in stage 24.0 (TID 24). 665 bytes result sent to driver
[INFO][2018-05-29 17:03:11,095][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 24.0 (TID 24) in 1056 ms on localhost (executor driver) (1/1)
[INFO][2018-05-29 17:03:11,095][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 24.0, whose tasks have all completed, from pool 
[INFO][2018-05-29 17:03:11,095][org.apache.spark.scheduler.DAGScheduler]ResultStage 24 (foreachPartition at ReceiveKafkaData.scala:76) finished in 1.056 s
[INFO][2018-05-29 17:03:11,095][org.apache.spark.scheduler.DAGScheduler]Job 24 finished: foreachPartition at ReceiveKafkaData.scala:76, took 1.060596 s
[INFO][2018-05-29 17:03:11,096][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527584590000 ms.0 from job set of time 1527584590000 ms
[INFO][2018-05-29 17:03:11,096][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 27 from persistence list
[INFO][2018-05-29 17:03:11,096][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 1.096 s for time 1527584590000 ms (execution: 1.080 s)
[INFO][2018-05-29 17:03:11,096][org.apache.spark.storage.BlockManager]Removing RDD 27
[INFO][2018-05-29 17:03:11,096][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 26 from persistence list
[INFO][2018-05-29 17:03:11,096][org.apache.spark.storage.BlockManager]Removing RDD 26
[INFO][2018-05-29 17:03:11,097][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-29 17:03:11,097][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527584580000 ms
[INFO][2018-05-29 17:03:15,016][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527584595000 ms
[INFO][2018-05-29 17:03:15,017][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527584595000 ms.0 from job set of time 1527584595000 ms
[INFO][2018-05-29 17:03:15,021][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:75
[INFO][2018-05-29 17:03:15,022][org.apache.spark.scheduler.DAGScheduler]Got job 25 (isEmpty at ReceiveKafkaData.scala:75) with 1 output partitions
[INFO][2018-05-29 17:03:15,022][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 25 (isEmpty at ReceiveKafkaData.scala:75)
[INFO][2018-05-29 17:03:15,022][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-29 17:03:15,022][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-29 17:03:15,022][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 25 (MapPartitionsRDD[31] at map at ReceiveKafkaData.scala:74), which has no missing parents
[INFO][2018-05-29 17:03:15,024][org.apache.spark.storage.memory.MemoryStore]Block broadcast_25 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-29 17:03:15,026][org.apache.spark.storage.memory.MemoryStore]Block broadcast_25_piece0 stored as bytes in memory (estimated size 1973.0 B, free 912.3 MB)
[INFO][2018-05-29 17:03:15,026][org.apache.spark.storage.BlockManagerInfo]Added broadcast_25_piece0 in memory on 10.194.32.157:63705 (size: 1973.0 B, free: 912.3 MB)
[INFO][2018-05-29 17:03:15,026][org.apache.spark.SparkContext]Created broadcast 25 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-29 17:03:15,027][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 25 (MapPartitionsRDD[31] at map at ReceiveKafkaData.scala:74) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-29 17:03:15,027][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 25.0 with 1 tasks
[INFO][2018-05-29 17:03:15,027][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 25.0 (TID 25, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-29 17:03:15,028][org.apache.spark.executor.Executor]Running task 0.0 in stage 25.0 (TID 25)
[INFO][2018-05-29 17:03:15,029][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 188891 -> 188896
[INFO][2018-05-29 17:03:15,030][org.apache.spark.executor.Executor]Finished task 0.0 in stage 25.0 (TID 25). 929 bytes result sent to driver
[INFO][2018-05-29 17:03:15,031][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 25.0 (TID 25) in 4 ms on localhost (executor driver) (1/1)
[INFO][2018-05-29 17:03:15,031][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 25.0, whose tasks have all completed, from pool 
[INFO][2018-05-29 17:03:15,031][org.apache.spark.scheduler.DAGScheduler]ResultStage 25 (isEmpty at ReceiveKafkaData.scala:75) finished in 0.004 s
[INFO][2018-05-29 17:03:15,031][org.apache.spark.scheduler.DAGScheduler]Job 25 finished: isEmpty at ReceiveKafkaData.scala:75, took 0.010126 s
[INFO][2018-05-29 17:03:15,036][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:76
[INFO][2018-05-29 17:03:15,036][org.apache.spark.scheduler.DAGScheduler]Got job 26 (foreachPartition at ReceiveKafkaData.scala:76) with 1 output partitions
[INFO][2018-05-29 17:03:15,037][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 26 (foreachPartition at ReceiveKafkaData.scala:76)
[INFO][2018-05-29 17:03:15,037][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-29 17:03:15,037][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-29 17:03:15,037][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 26 (MapPartitionsRDD[31] at map at ReceiveKafkaData.scala:74), which has no missing parents
[INFO][2018-05-29 17:03:15,038][org.apache.spark.storage.memory.MemoryStore]Block broadcast_26 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-29 17:03:15,039][org.apache.spark.storage.memory.MemoryStore]Block broadcast_26_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.3 MB)
[INFO][2018-05-29 17:03:15,039][org.apache.spark.storage.BlockManagerInfo]Added broadcast_26_piece0 in memory on 10.194.32.157:63705 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-29 17:03:15,040][org.apache.spark.SparkContext]Created broadcast 26 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-29 17:03:15,040][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 26 (MapPartitionsRDD[31] at map at ReceiveKafkaData.scala:74) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-29 17:03:15,040][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 26.0 with 1 tasks
[INFO][2018-05-29 17:03:15,041][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 26.0 (TID 26, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-29 17:03:15,041][org.apache.spark.executor.Executor]Running task 0.0 in stage 26.0 (TID 26)
[INFO][2018-05-29 17:03:15,042][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 188891 -> 188896
[INFO][2018-05-29 17:03:15,042][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 188891
[INFO][2018-05-29 17:03:15,066][com.seven.spark.streaming.ReceiveKafkaData$]Inserting 5 lines of data to HBase is success . . .
[INFO][2018-05-29 17:03:15,091][com.seven.spark.streaming.ReceiveKafkaData$]Inserting 5 lines of data to ElasticSearch is success . . .
[INFO][2018-05-29 17:03:15,092][org.apache.spark.executor.Executor]Finished task 0.0 in stage 26.0 (TID 26). 708 bytes result sent to driver
[INFO][2018-05-29 17:03:15,095][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 26.0 (TID 26) in 54 ms on localhost (executor driver) (1/1)
[INFO][2018-05-29 17:03:15,095][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 26.0, whose tasks have all completed, from pool 
[INFO][2018-05-29 17:03:15,095][org.apache.spark.scheduler.DAGScheduler]ResultStage 26 (foreachPartition at ReceiveKafkaData.scala:76) finished in 0.055 s
[INFO][2018-05-29 17:03:15,097][org.apache.spark.scheduler.DAGScheduler]Job 26 finished: foreachPartition at ReceiveKafkaData.scala:76, took 0.060307 s
[INFO][2018-05-29 17:03:15,098][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527584595000 ms.0 from job set of time 1527584595000 ms
[INFO][2018-05-29 17:03:15,098][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.098 s for time 1527584595000 ms (execution: 0.081 s)
[INFO][2018-05-29 17:03:15,099][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 29 from persistence list
[INFO][2018-05-29 17:03:15,099][org.apache.spark.storage.BlockManager]Removing RDD 29
[INFO][2018-05-29 17:03:15,100][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 28 from persistence list
[INFO][2018-05-29 17:03:15,100][org.apache.spark.storage.BlockManager]Removing RDD 28
[INFO][2018-05-29 17:03:15,100][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-29 17:03:15,100][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527584585000 ms
[INFO][2018-05-29 17:03:20,015][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527584600000 ms
[INFO][2018-05-29 17:03:20,016][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527584600000 ms.0 from job set of time 1527584600000 ms
[INFO][2018-05-29 17:03:20,020][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:75
[INFO][2018-05-29 17:03:20,021][org.apache.spark.scheduler.DAGScheduler]Got job 27 (isEmpty at ReceiveKafkaData.scala:75) with 1 output partitions
[INFO][2018-05-29 17:03:20,021][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 27 (isEmpty at ReceiveKafkaData.scala:75)
[INFO][2018-05-29 17:03:20,021][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-29 17:03:20,021][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-29 17:03:20,021][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 27 (MapPartitionsRDD[33] at map at ReceiveKafkaData.scala:74), which has no missing parents
[INFO][2018-05-29 17:03:20,023][org.apache.spark.storage.memory.MemoryStore]Block broadcast_27 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-29 17:03:20,024][org.apache.spark.storage.memory.MemoryStore]Block broadcast_27_piece0 stored as bytes in memory (estimated size 1973.0 B, free 912.3 MB)
[INFO][2018-05-29 17:03:20,025][org.apache.spark.storage.BlockManagerInfo]Added broadcast_27_piece0 in memory on 10.194.32.157:63705 (size: 1973.0 B, free: 912.3 MB)
[INFO][2018-05-29 17:03:20,025][org.apache.spark.SparkContext]Created broadcast 27 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-29 17:03:20,025][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 27 (MapPartitionsRDD[33] at map at ReceiveKafkaData.scala:74) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-29 17:03:20,025][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 27.0 with 1 tasks
[INFO][2018-05-29 17:03:20,026][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 27.0 (TID 27, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-29 17:03:20,026][org.apache.spark.executor.Executor]Running task 0.0 in stage 27.0 (TID 27)
[INFO][2018-05-29 17:03:20,027][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 188896 -> 188901
[INFO][2018-05-29 17:03:20,028][org.apache.spark.executor.Executor]Finished task 0.0 in stage 27.0 (TID 27). 969 bytes result sent to driver
[INFO][2018-05-29 17:03:20,029][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 27.0 (TID 27) in 3 ms on localhost (executor driver) (1/1)
[INFO][2018-05-29 17:03:20,029][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 27.0, whose tasks have all completed, from pool 
[INFO][2018-05-29 17:03:20,029][org.apache.spark.scheduler.DAGScheduler]ResultStage 27 (isEmpty at ReceiveKafkaData.scala:75) finished in 0.004 s
[INFO][2018-05-29 17:03:20,029][org.apache.spark.scheduler.DAGScheduler]Job 27 finished: isEmpty at ReceiveKafkaData.scala:75, took 0.008826 s
[INFO][2018-05-29 17:03:20,033][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:76
[INFO][2018-05-29 17:03:20,033][org.apache.spark.scheduler.DAGScheduler]Got job 28 (foreachPartition at ReceiveKafkaData.scala:76) with 1 output partitions
[INFO][2018-05-29 17:03:20,033][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 28 (foreachPartition at ReceiveKafkaData.scala:76)
[INFO][2018-05-29 17:03:20,033][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-29 17:03:20,033][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-29 17:03:20,033][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 28 (MapPartitionsRDD[33] at map at ReceiveKafkaData.scala:74), which has no missing parents
[INFO][2018-05-29 17:03:20,035][org.apache.spark.storage.memory.MemoryStore]Block broadcast_28 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-29 17:03:20,036][org.apache.spark.storage.memory.MemoryStore]Block broadcast_28_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.2 MB)
[INFO][2018-05-29 17:03:20,036][org.apache.spark.storage.BlockManagerInfo]Added broadcast_28_piece0 in memory on 10.194.32.157:63705 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-29 17:03:20,036][org.apache.spark.SparkContext]Created broadcast 28 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-29 17:03:20,037][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 28 (MapPartitionsRDD[33] at map at ReceiveKafkaData.scala:74) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-29 17:03:20,037][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 28.0 with 1 tasks
[INFO][2018-05-29 17:03:20,037][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 28.0 (TID 28, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-29 17:03:20,038][org.apache.spark.executor.Executor]Running task 0.0 in stage 28.0 (TID 28)
[INFO][2018-05-29 17:03:20,039][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 188896 -> 188901
[INFO][2018-05-29 17:03:20,039][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 188896
[INFO][2018-05-29 17:03:20,062][com.seven.spark.streaming.ReceiveKafkaData$]Inserting 5 lines of data to HBase is success . . .
[INFO][2018-05-29 17:03:20,077][com.seven.spark.streaming.ReceiveKafkaData$]Inserting 5 lines of data to ElasticSearch is success . . .
[INFO][2018-05-29 17:03:20,078][org.apache.spark.executor.Executor]Finished task 0.0 in stage 28.0 (TID 28). 708 bytes result sent to driver
[INFO][2018-05-29 17:03:20,078][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 28.0 (TID 28) in 41 ms on localhost (executor driver) (1/1)
[INFO][2018-05-29 17:03:20,078][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 28.0, whose tasks have all completed, from pool 
[INFO][2018-05-29 17:03:20,079][org.apache.spark.scheduler.DAGScheduler]ResultStage 28 (foreachPartition at ReceiveKafkaData.scala:76) finished in 0.041 s
[INFO][2018-05-29 17:03:20,079][org.apache.spark.scheduler.DAGScheduler]Job 28 finished: foreachPartition at ReceiveKafkaData.scala:76, took 0.045924 s
[INFO][2018-05-29 17:03:20,079][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527584600000 ms.0 from job set of time 1527584600000 ms
[INFO][2018-05-29 17:03:20,079][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.079 s for time 1527584600000 ms (execution: 0.063 s)
[INFO][2018-05-29 17:03:20,079][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 31 from persistence list
[INFO][2018-05-29 17:03:20,079][org.apache.spark.storage.BlockManager]Removing RDD 31
[INFO][2018-05-29 17:03:20,080][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 30 from persistence list
[INFO][2018-05-29 17:03:20,080][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-29 17:03:20,080][org.apache.spark.storage.BlockManager]Removing RDD 30
[INFO][2018-05-29 17:03:20,080][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527584590000 ms
[INFO][2018-05-29 17:03:25,015][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527584605000 ms
[INFO][2018-05-29 17:03:25,015][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527584605000 ms.0 from job set of time 1527584605000 ms
[INFO][2018-05-29 17:03:25,020][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:75
[INFO][2018-05-29 17:03:25,021][org.apache.spark.scheduler.DAGScheduler]Got job 29 (isEmpty at ReceiveKafkaData.scala:75) with 1 output partitions
[INFO][2018-05-29 17:03:25,021][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 29 (isEmpty at ReceiveKafkaData.scala:75)
[INFO][2018-05-29 17:03:25,021][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-29 17:03:25,021][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-29 17:03:25,021][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 29 (MapPartitionsRDD[35] at map at ReceiveKafkaData.scala:74), which has no missing parents
[INFO][2018-05-29 17:03:25,022][org.apache.spark.storage.memory.MemoryStore]Block broadcast_29 stored as values in memory (estimated size 3.1 KB, free 912.2 MB)
[INFO][2018-05-29 17:03:25,023][org.apache.spark.storage.memory.MemoryStore]Block broadcast_29_piece0 stored as bytes in memory (estimated size 1973.0 B, free 912.2 MB)
[INFO][2018-05-29 17:03:25,024][org.apache.spark.storage.BlockManagerInfo]Added broadcast_29_piece0 in memory on 10.194.32.157:63705 (size: 1973.0 B, free: 912.3 MB)
[INFO][2018-05-29 17:03:25,024][org.apache.spark.SparkContext]Created broadcast 29 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-29 17:03:25,024][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 29 (MapPartitionsRDD[35] at map at ReceiveKafkaData.scala:74) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-29 17:03:25,024][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 29.0 with 1 tasks
[INFO][2018-05-29 17:03:25,025][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 29.0 (TID 29, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-29 17:03:25,025][org.apache.spark.executor.Executor]Running task 0.0 in stage 29.0 (TID 29)
[INFO][2018-05-29 17:03:25,026][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 188901 -> 188906
[INFO][2018-05-29 17:03:25,028][org.apache.spark.executor.Executor]Finished task 0.0 in stage 29.0 (TID 29). 928 bytes result sent to driver
[INFO][2018-05-29 17:03:25,028][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 29.0 (TID 29) in 3 ms on localhost (executor driver) (1/1)
[INFO][2018-05-29 17:03:25,028][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 29.0, whose tasks have all completed, from pool 
[INFO][2018-05-29 17:03:25,028][org.apache.spark.scheduler.DAGScheduler]ResultStage 29 (isEmpty at ReceiveKafkaData.scala:75) finished in 0.003 s
[INFO][2018-05-29 17:03:25,029][org.apache.spark.scheduler.DAGScheduler]Job 29 finished: isEmpty at ReceiveKafkaData.scala:75, took 0.008831 s
[INFO][2018-05-29 17:03:25,032][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:76
[INFO][2018-05-29 17:03:25,033][org.apache.spark.scheduler.DAGScheduler]Got job 30 (foreachPartition at ReceiveKafkaData.scala:76) with 1 output partitions
[INFO][2018-05-29 17:03:25,033][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 30 (foreachPartition at ReceiveKafkaData.scala:76)
[INFO][2018-05-29 17:03:25,033][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-29 17:03:25,033][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-29 17:03:25,033][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 30 (MapPartitionsRDD[35] at map at ReceiveKafkaData.scala:74), which has no missing parents
[INFO][2018-05-29 17:03:25,034][org.apache.spark.storage.memory.MemoryStore]Block broadcast_30 stored as values in memory (estimated size 3.4 KB, free 912.2 MB)
[INFO][2018-05-29 17:03:25,035][org.apache.spark.storage.memory.MemoryStore]Block broadcast_30_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.2 MB)
[INFO][2018-05-29 17:03:25,035][org.apache.spark.storage.BlockManagerInfo]Added broadcast_30_piece0 in memory on 10.194.32.157:63705 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-29 17:03:25,035][org.apache.spark.SparkContext]Created broadcast 30 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-29 17:03:25,036][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 30 (MapPartitionsRDD[35] at map at ReceiveKafkaData.scala:74) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-29 17:03:25,036][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 30.0 with 1 tasks
[INFO][2018-05-29 17:03:25,036][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 30.0 (TID 30, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-29 17:03:25,036][org.apache.spark.executor.Executor]Running task 0.0 in stage 30.0 (TID 30)
[INFO][2018-05-29 17:03:25,037][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 188901 -> 188906
[INFO][2018-05-29 17:03:25,038][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 188901
[INFO][2018-05-29 17:03:25,063][com.seven.spark.streaming.ReceiveKafkaData$]Inserting 5 lines of data to HBase is success . . .
[INFO][2018-05-29 17:03:25,074][com.seven.spark.streaming.ReceiveKafkaData$]Inserting 5 lines of data to ElasticSearch is success . . .
[INFO][2018-05-29 17:03:25,074][org.apache.spark.executor.Executor]Finished task 0.0 in stage 30.0 (TID 30). 708 bytes result sent to driver
[INFO][2018-05-29 17:03:25,075][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 30.0 (TID 30) in 39 ms on localhost (executor driver) (1/1)
[INFO][2018-05-29 17:03:25,075][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 30.0, whose tasks have all completed, from pool 
[INFO][2018-05-29 17:03:25,075][org.apache.spark.scheduler.DAGScheduler]ResultStage 30 (foreachPartition at ReceiveKafkaData.scala:76) finished in 0.039 s
[INFO][2018-05-29 17:03:25,075][org.apache.spark.scheduler.DAGScheduler]Job 30 finished: foreachPartition at ReceiveKafkaData.scala:76, took 0.043025 s
[INFO][2018-05-29 17:03:25,076][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527584605000 ms.0 from job set of time 1527584605000 ms
[INFO][2018-05-29 17:03:25,076][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.075 s for time 1527584605000 ms (execution: 0.060 s)
[INFO][2018-05-29 17:03:25,076][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 33 from persistence list
[INFO][2018-05-29 17:03:25,076][org.apache.spark.storage.BlockManager]Removing RDD 33
[INFO][2018-05-29 17:03:25,076][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 32 from persistence list
[INFO][2018-05-29 17:03:25,076][org.apache.spark.storage.BlockManager]Removing RDD 32
[INFO][2018-05-29 17:03:25,076][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-29 17:03:25,076][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527584595000 ms
[INFO][2018-05-29 17:03:30,016][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527584610000 ms
[INFO][2018-05-29 17:03:30,019][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527584610000 ms.0 from job set of time 1527584610000 ms
[INFO][2018-05-29 17:03:30,027][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:75
[INFO][2018-05-29 17:03:30,027][org.apache.spark.scheduler.DAGScheduler]Got job 31 (isEmpty at ReceiveKafkaData.scala:75) with 1 output partitions
[INFO][2018-05-29 17:03:30,027][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 31 (isEmpty at ReceiveKafkaData.scala:75)
[INFO][2018-05-29 17:03:30,027][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-29 17:03:30,028][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-29 17:03:30,028][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 31 (MapPartitionsRDD[37] at map at ReceiveKafkaData.scala:74), which has no missing parents
[INFO][2018-05-29 17:03:30,031][org.apache.spark.storage.memory.MemoryStore]Block broadcast_31 stored as values in memory (estimated size 3.1 KB, free 912.2 MB)
[INFO][2018-05-29 17:03:30,032][org.apache.spark.storage.memory.MemoryStore]Block broadcast_31_piece0 stored as bytes in memory (estimated size 1973.0 B, free 912.2 MB)
[INFO][2018-05-29 17:03:30,032][org.apache.spark.storage.BlockManagerInfo]Added broadcast_31_piece0 in memory on 10.194.32.157:63705 (size: 1973.0 B, free: 912.3 MB)
[INFO][2018-05-29 17:03:30,033][org.apache.spark.SparkContext]Created broadcast 31 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-29 17:03:30,033][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 31 (MapPartitionsRDD[37] at map at ReceiveKafkaData.scala:74) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-29 17:03:30,033][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 31.0 with 1 tasks
[INFO][2018-05-29 17:03:30,033][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 31.0 (TID 31, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-29 17:03:30,033][org.apache.spark.executor.Executor]Running task 0.0 in stage 31.0 (TID 31)
[INFO][2018-05-29 17:03:30,034][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 188906 -> 188911
[INFO][2018-05-29 17:03:30,047][org.apache.spark.executor.Executor]Finished task 0.0 in stage 31.0 (TID 31). 961 bytes result sent to driver
[INFO][2018-05-29 17:03:30,048][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 31.0 (TID 31) in 15 ms on localhost (executor driver) (1/1)
[INFO][2018-05-29 17:03:30,048][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 31.0, whose tasks have all completed, from pool 
[INFO][2018-05-29 17:03:30,049][org.apache.spark.scheduler.DAGScheduler]ResultStage 31 (isEmpty at ReceiveKafkaData.scala:75) finished in 0.015 s
[INFO][2018-05-29 17:03:30,049][org.apache.spark.scheduler.DAGScheduler]Job 31 finished: isEmpty at ReceiveKafkaData.scala:75, took 0.022171 s
[INFO][2018-05-29 17:03:30,053][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:76
[INFO][2018-05-29 17:03:30,054][org.apache.spark.scheduler.DAGScheduler]Got job 32 (foreachPartition at ReceiveKafkaData.scala:76) with 1 output partitions
[INFO][2018-05-29 17:03:30,054][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 32 (foreachPartition at ReceiveKafkaData.scala:76)
[INFO][2018-05-29 17:03:30,054][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-29 17:03:30,054][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-29 17:03:30,054][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 32 (MapPartitionsRDD[37] at map at ReceiveKafkaData.scala:74), which has no missing parents
[INFO][2018-05-29 17:03:30,055][org.apache.spark.storage.memory.MemoryStore]Block broadcast_32 stored as values in memory (estimated size 3.4 KB, free 912.2 MB)
[INFO][2018-05-29 17:03:30,056][org.apache.spark.storage.memory.MemoryStore]Block broadcast_32_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.2 MB)
[INFO][2018-05-29 17:03:30,056][org.apache.spark.storage.BlockManagerInfo]Added broadcast_32_piece0 in memory on 10.194.32.157:63705 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-29 17:03:30,057][org.apache.spark.SparkContext]Created broadcast 32 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-29 17:03:30,057][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 32 (MapPartitionsRDD[37] at map at ReceiveKafkaData.scala:74) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-29 17:03:30,057][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 32.0 with 1 tasks
[INFO][2018-05-29 17:03:30,057][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 32.0 (TID 32, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-29 17:03:30,058][org.apache.spark.executor.Executor]Running task 0.0 in stage 32.0 (TID 32)
[INFO][2018-05-29 17:03:30,059][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 188906 -> 188911
[INFO][2018-05-29 17:03:30,059][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 188906
[INFO][2018-05-29 17:03:30,616][com.seven.spark.streaming.ReceiveKafkaData$]Inserting 5 lines of data to HBase is success . . .
[INFO][2018-05-29 17:03:30,633][com.seven.spark.streaming.ReceiveKafkaData$]Inserting 5 lines of data to ElasticSearch is success . . .
[INFO][2018-05-29 17:03:30,634][org.apache.spark.executor.Executor]Finished task 0.0 in stage 32.0 (TID 32). 708 bytes result sent to driver
[INFO][2018-05-29 17:03:30,634][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 32.0 (TID 32) in 577 ms on localhost (executor driver) (1/1)
[INFO][2018-05-29 17:03:30,634][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 32.0, whose tasks have all completed, from pool 
[INFO][2018-05-29 17:03:30,634][org.apache.spark.scheduler.DAGScheduler]ResultStage 32 (foreachPartition at ReceiveKafkaData.scala:76) finished in 0.577 s
[INFO][2018-05-29 17:03:30,635][org.apache.spark.scheduler.DAGScheduler]Job 32 finished: foreachPartition at ReceiveKafkaData.scala:76, took 0.581056 s
[INFO][2018-05-29 17:03:30,635][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527584610000 ms.0 from job set of time 1527584610000 ms
[INFO][2018-05-29 17:03:30,635][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.635 s for time 1527584610000 ms (execution: 0.616 s)
[INFO][2018-05-29 17:03:30,635][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 35 from persistence list
[INFO][2018-05-29 17:03:30,635][org.apache.spark.storage.BlockManager]Removing RDD 35
[INFO][2018-05-29 17:03:30,636][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 34 from persistence list
[INFO][2018-05-29 17:03:30,636][org.apache.spark.storage.BlockManager]Removing RDD 34
[INFO][2018-05-29 17:03:30,636][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-29 17:03:30,636][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527584600000 ms
[INFO][2018-05-29 17:03:35,015][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527584615000 ms
[INFO][2018-05-29 17:03:35,015][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527584615000 ms.0 from job set of time 1527584615000 ms
[INFO][2018-05-29 17:03:35,020][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:75
[INFO][2018-05-29 17:03:35,020][org.apache.spark.scheduler.DAGScheduler]Got job 33 (isEmpty at ReceiveKafkaData.scala:75) with 1 output partitions
[INFO][2018-05-29 17:03:35,020][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 33 (isEmpty at ReceiveKafkaData.scala:75)
[INFO][2018-05-29 17:03:35,020][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-29 17:03:35,020][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-29 17:03:35,021][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 33 (MapPartitionsRDD[39] at map at ReceiveKafkaData.scala:74), which has no missing parents
[INFO][2018-05-29 17:03:35,022][org.apache.spark.storage.memory.MemoryStore]Block broadcast_33 stored as values in memory (estimated size 3.1 KB, free 912.2 MB)
[INFO][2018-05-29 17:03:35,023][org.apache.spark.storage.memory.MemoryStore]Block broadcast_33_piece0 stored as bytes in memory (estimated size 1973.0 B, free 912.2 MB)
[INFO][2018-05-29 17:03:35,023][org.apache.spark.storage.BlockManagerInfo]Added broadcast_33_piece0 in memory on 10.194.32.157:63705 (size: 1973.0 B, free: 912.3 MB)
[INFO][2018-05-29 17:03:35,024][org.apache.spark.SparkContext]Created broadcast 33 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-29 17:03:35,024][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 33 (MapPartitionsRDD[39] at map at ReceiveKafkaData.scala:74) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-29 17:03:35,024][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 33.0 with 1 tasks
[INFO][2018-05-29 17:03:35,025][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 33.0 (TID 33, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-29 17:03:35,025][org.apache.spark.executor.Executor]Running task 0.0 in stage 33.0 (TID 33)
[INFO][2018-05-29 17:03:35,026][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 188911 -> 188916
[INFO][2018-05-29 17:03:35,026][org.apache.spark.executor.Executor]Finished task 0.0 in stage 33.0 (TID 33). 931 bytes result sent to driver
[INFO][2018-05-29 17:03:35,027][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 33.0 (TID 33) in 2 ms on localhost (executor driver) (1/1)
[INFO][2018-05-29 17:03:35,027][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 33.0, whose tasks have all completed, from pool 
[INFO][2018-05-29 17:03:35,027][org.apache.spark.scheduler.DAGScheduler]ResultStage 33 (isEmpty at ReceiveKafkaData.scala:75) finished in 0.003 s
[INFO][2018-05-29 17:03:35,027][org.apache.spark.scheduler.DAGScheduler]Job 33 finished: isEmpty at ReceiveKafkaData.scala:75, took 0.007477 s
[INFO][2018-05-29 17:03:35,032][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:76
[INFO][2018-05-29 17:03:35,033][org.apache.spark.scheduler.DAGScheduler]Got job 34 (foreachPartition at ReceiveKafkaData.scala:76) with 1 output partitions
[INFO][2018-05-29 17:03:35,033][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 34 (foreachPartition at ReceiveKafkaData.scala:76)
[INFO][2018-05-29 17:03:35,033][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-29 17:03:35,033][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-29 17:03:35,033][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 34 (MapPartitionsRDD[39] at map at ReceiveKafkaData.scala:74), which has no missing parents
[INFO][2018-05-29 17:03:35,036][org.apache.spark.storage.memory.MemoryStore]Block broadcast_34 stored as values in memory (estimated size 3.4 KB, free 912.2 MB)
[INFO][2018-05-29 17:03:35,037][org.apache.spark.storage.memory.MemoryStore]Block broadcast_34_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.2 MB)
[INFO][2018-05-29 17:03:35,038][org.apache.spark.storage.BlockManagerInfo]Added broadcast_34_piece0 in memory on 10.194.32.157:63705 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-29 17:03:35,038][org.apache.spark.SparkContext]Created broadcast 34 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-29 17:03:35,039][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 34 (MapPartitionsRDD[39] at map at ReceiveKafkaData.scala:74) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-29 17:03:35,039][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 34.0 with 1 tasks
[INFO][2018-05-29 17:03:35,039][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 34.0 (TID 34, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-29 17:03:35,040][org.apache.spark.executor.Executor]Running task 0.0 in stage 34.0 (TID 34)
[INFO][2018-05-29 17:03:35,042][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 188911 -> 188916
[INFO][2018-05-29 17:03:35,042][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 188911
[INFO][2018-05-29 17:03:35,067][com.seven.spark.streaming.ReceiveKafkaData$]Inserting 5 lines of data to HBase is success . . .
[INFO][2018-05-29 17:03:35,079][com.seven.spark.streaming.ReceiveKafkaData$]Inserting 5 lines of data to ElasticSearch is success . . .
[INFO][2018-05-29 17:03:35,079][org.apache.spark.executor.Executor]Finished task 0.0 in stage 34.0 (TID 34). 708 bytes result sent to driver
[INFO][2018-05-29 17:03:35,080][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 34.0 (TID 34) in 41 ms on localhost (executor driver) (1/1)
[INFO][2018-05-29 17:03:35,080][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 34.0, whose tasks have all completed, from pool 
[INFO][2018-05-29 17:03:35,081][org.apache.spark.scheduler.DAGScheduler]ResultStage 34 (foreachPartition at ReceiveKafkaData.scala:76) finished in 0.042 s
[INFO][2018-05-29 17:03:35,081][org.apache.spark.scheduler.DAGScheduler]Job 34 finished: foreachPartition at ReceiveKafkaData.scala:76, took 0.049284 s
[INFO][2018-05-29 17:03:35,082][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527584615000 ms.0 from job set of time 1527584615000 ms
[INFO][2018-05-29 17:03:35,082][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 37 from persistence list
[INFO][2018-05-29 17:03:35,082][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.082 s for time 1527584615000 ms (execution: 0.067 s)
[INFO][2018-05-29 17:03:35,083][org.apache.spark.storage.BlockManager]Removing RDD 37
[INFO][2018-05-29 17:03:35,083][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 36 from persistence list
[INFO][2018-05-29 17:03:35,084][org.apache.spark.storage.BlockManager]Removing RDD 36
[INFO][2018-05-29 17:03:35,084][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-29 17:03:35,084][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527584605000 ms
[INFO][2018-05-29 17:03:40,017][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527584620000 ms
[INFO][2018-05-29 17:03:40,017][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527584620000 ms.0 from job set of time 1527584620000 ms
[INFO][2018-05-29 17:03:40,022][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:75
[INFO][2018-05-29 17:03:40,023][org.apache.spark.scheduler.DAGScheduler]Got job 35 (isEmpty at ReceiveKafkaData.scala:75) with 1 output partitions
[INFO][2018-05-29 17:03:40,023][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 35 (isEmpty at ReceiveKafkaData.scala:75)
[INFO][2018-05-29 17:03:40,023][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-29 17:03:40,023][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-29 17:03:40,024][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 35 (MapPartitionsRDD[41] at map at ReceiveKafkaData.scala:74), which has no missing parents
[INFO][2018-05-29 17:03:40,026][org.apache.spark.storage.memory.MemoryStore]Block broadcast_35 stored as values in memory (estimated size 3.1 KB, free 912.2 MB)
[INFO][2018-05-29 17:03:40,027][org.apache.spark.storage.memory.MemoryStore]Block broadcast_35_piece0 stored as bytes in memory (estimated size 1973.0 B, free 912.2 MB)
[INFO][2018-05-29 17:03:40,027][org.apache.spark.storage.BlockManagerInfo]Added broadcast_35_piece0 in memory on 10.194.32.157:63705 (size: 1973.0 B, free: 912.3 MB)
[INFO][2018-05-29 17:03:40,028][org.apache.spark.SparkContext]Created broadcast 35 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-29 17:03:40,028][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 35 (MapPartitionsRDD[41] at map at ReceiveKafkaData.scala:74) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-29 17:03:40,028][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 35.0 with 1 tasks
[INFO][2018-05-29 17:03:40,029][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 35.0 (TID 35, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-29 17:03:40,029][org.apache.spark.executor.Executor]Running task 0.0 in stage 35.0 (TID 35)
[INFO][2018-05-29 17:03:40,030][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 188916 -> 188921
[INFO][2018-05-29 17:03:40,040][org.apache.spark.executor.Executor]Finished task 0.0 in stage 35.0 (TID 35). 916 bytes result sent to driver
[INFO][2018-05-29 17:03:40,041][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 35.0 (TID 35) in 12 ms on localhost (executor driver) (1/1)
[INFO][2018-05-29 17:03:40,041][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 35.0, whose tasks have all completed, from pool 
[INFO][2018-05-29 17:03:40,041][org.apache.spark.scheduler.DAGScheduler]ResultStage 35 (isEmpty at ReceiveKafkaData.scala:75) finished in 0.013 s
[INFO][2018-05-29 17:03:40,041][org.apache.spark.scheduler.DAGScheduler]Job 35 finished: isEmpty at ReceiveKafkaData.scala:75, took 0.018901 s
[INFO][2018-05-29 17:03:40,045][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:76
[INFO][2018-05-29 17:03:40,046][org.apache.spark.scheduler.DAGScheduler]Got job 36 (foreachPartition at ReceiveKafkaData.scala:76) with 1 output partitions
[INFO][2018-05-29 17:03:40,046][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 36 (foreachPartition at ReceiveKafkaData.scala:76)
[INFO][2018-05-29 17:03:40,046][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-29 17:03:40,046][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-29 17:03:40,046][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 36 (MapPartitionsRDD[41] at map at ReceiveKafkaData.scala:74), which has no missing parents
[INFO][2018-05-29 17:03:40,047][org.apache.spark.storage.memory.MemoryStore]Block broadcast_36 stored as values in memory (estimated size 3.4 KB, free 912.2 MB)
[INFO][2018-05-29 17:03:40,048][org.apache.spark.storage.memory.MemoryStore]Block broadcast_36_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.2 MB)
[INFO][2018-05-29 17:03:40,048][org.apache.spark.storage.BlockManagerInfo]Added broadcast_36_piece0 in memory on 10.194.32.157:63705 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-29 17:03:40,049][org.apache.spark.SparkContext]Created broadcast 36 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-29 17:03:40,049][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 36 (MapPartitionsRDD[41] at map at ReceiveKafkaData.scala:74) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-29 17:03:40,049][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 36.0 with 1 tasks
[INFO][2018-05-29 17:03:40,050][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 36.0 (TID 36, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-29 17:03:40,050][org.apache.spark.executor.Executor]Running task 0.0 in stage 36.0 (TID 36)
[INFO][2018-05-29 17:03:40,051][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 188916 -> 188921
[INFO][2018-05-29 17:03:40,051][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 188916
[INFO][2018-05-29 17:03:40,648][com.seven.spark.streaming.ReceiveKafkaData$]Inserting 5 lines of data to HBase is success . . .
[INFO][2018-05-29 17:03:40,662][com.seven.spark.streaming.ReceiveKafkaData$]Inserting 5 lines of data to ElasticSearch is success . . .
[INFO][2018-05-29 17:03:40,663][org.apache.spark.executor.Executor]Finished task 0.0 in stage 36.0 (TID 36). 708 bytes result sent to driver
[INFO][2018-05-29 17:03:40,663][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 36.0 (TID 36) in 614 ms on localhost (executor driver) (1/1)
[INFO][2018-05-29 17:03:40,663][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 36.0, whose tasks have all completed, from pool 
[INFO][2018-05-29 17:03:40,664][org.apache.spark.scheduler.DAGScheduler]ResultStage 36 (foreachPartition at ReceiveKafkaData.scala:76) finished in 0.615 s
[INFO][2018-05-29 17:03:40,664][org.apache.spark.scheduler.DAGScheduler]Job 36 finished: foreachPartition at ReceiveKafkaData.scala:76, took 0.618464 s
[INFO][2018-05-29 17:03:40,664][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527584620000 ms.0 from job set of time 1527584620000 ms
[INFO][2018-05-29 17:03:40,664][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.664 s for time 1527584620000 ms (execution: 0.647 s)
[INFO][2018-05-29 17:03:40,664][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 39 from persistence list
[INFO][2018-05-29 17:03:40,665][org.apache.spark.storage.BlockManager]Removing RDD 39
[INFO][2018-05-29 17:03:40,665][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 38 from persistence list
[INFO][2018-05-29 17:03:40,665][org.apache.spark.storage.BlockManager]Removing RDD 38
[INFO][2018-05-29 17:03:40,665][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-29 17:03:40,665][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527584610000 ms
[INFO][2018-05-29 17:03:45,023][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527584625000 ms
[INFO][2018-05-29 17:03:45,023][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527584625000 ms.0 from job set of time 1527584625000 ms
[INFO][2018-05-29 17:03:45,028][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:75
[INFO][2018-05-29 17:03:45,028][org.apache.spark.scheduler.DAGScheduler]Got job 37 (isEmpty at ReceiveKafkaData.scala:75) with 1 output partitions
[INFO][2018-05-29 17:03:45,028][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 37 (isEmpty at ReceiveKafkaData.scala:75)
[INFO][2018-05-29 17:03:45,028][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-29 17:03:45,028][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-29 17:03:45,028][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 37 (MapPartitionsRDD[43] at map at ReceiveKafkaData.scala:74), which has no missing parents
[INFO][2018-05-29 17:03:45,030][org.apache.spark.storage.memory.MemoryStore]Block broadcast_37 stored as values in memory (estimated size 3.1 KB, free 912.2 MB)
[INFO][2018-05-29 17:03:45,031][org.apache.spark.storage.memory.MemoryStore]Block broadcast_37_piece0 stored as bytes in memory (estimated size 1973.0 B, free 912.2 MB)
[INFO][2018-05-29 17:03:45,031][org.apache.spark.storage.BlockManagerInfo]Added broadcast_37_piece0 in memory on 10.194.32.157:63705 (size: 1973.0 B, free: 912.3 MB)
[INFO][2018-05-29 17:03:45,031][org.apache.spark.SparkContext]Created broadcast 37 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-29 17:03:45,032][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 37 (MapPartitionsRDD[43] at map at ReceiveKafkaData.scala:74) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-29 17:03:45,032][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 37.0 with 1 tasks
[INFO][2018-05-29 17:03:45,032][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 37.0 (TID 37, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-29 17:03:45,032][org.apache.spark.executor.Executor]Running task 0.0 in stage 37.0 (TID 37)
[INFO][2018-05-29 17:03:45,033][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 188921 -> 188926
[INFO][2018-05-29 17:03:45,034][org.apache.spark.executor.Executor]Finished task 0.0 in stage 37.0 (TID 37). 918 bytes result sent to driver
[INFO][2018-05-29 17:03:45,034][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 37.0 (TID 37) in 2 ms on localhost (executor driver) (1/1)
[INFO][2018-05-29 17:03:45,034][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 37.0, whose tasks have all completed, from pool 
[INFO][2018-05-29 17:03:45,034][org.apache.spark.scheduler.DAGScheduler]ResultStage 37 (isEmpty at ReceiveKafkaData.scala:75) finished in 0.002 s
[INFO][2018-05-29 17:03:45,035][org.apache.spark.scheduler.DAGScheduler]Job 37 finished: isEmpty at ReceiveKafkaData.scala:75, took 0.007014 s
[INFO][2018-05-29 17:03:45,038][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:76
[INFO][2018-05-29 17:03:45,038][org.apache.spark.scheduler.DAGScheduler]Got job 38 (foreachPartition at ReceiveKafkaData.scala:76) with 1 output partitions
[INFO][2018-05-29 17:03:45,038][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 38 (foreachPartition at ReceiveKafkaData.scala:76)
[INFO][2018-05-29 17:03:45,038][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-29 17:03:45,038][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-29 17:03:45,039][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 38 (MapPartitionsRDD[43] at map at ReceiveKafkaData.scala:74), which has no missing parents
[INFO][2018-05-29 17:03:45,040][org.apache.spark.storage.memory.MemoryStore]Block broadcast_38 stored as values in memory (estimated size 3.4 KB, free 912.2 MB)
[INFO][2018-05-29 17:03:45,040][org.apache.spark.storage.memory.MemoryStore]Block broadcast_38_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.2 MB)
[INFO][2018-05-29 17:03:45,041][org.apache.spark.storage.BlockManagerInfo]Added broadcast_38_piece0 in memory on 10.194.32.157:63705 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-29 17:03:45,041][org.apache.spark.SparkContext]Created broadcast 38 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-29 17:03:45,041][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 38 (MapPartitionsRDD[43] at map at ReceiveKafkaData.scala:74) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-29 17:03:45,041][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 38.0 with 1 tasks
[INFO][2018-05-29 17:03:45,041][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 38.0 (TID 38, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-29 17:03:45,042][org.apache.spark.executor.Executor]Running task 0.0 in stage 38.0 (TID 38)
[INFO][2018-05-29 17:03:45,043][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 188921 -> 188926
[INFO][2018-05-29 17:03:45,043][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 188921
[INFO][2018-05-29 17:03:45,072][com.seven.spark.streaming.ReceiveKafkaData$]Inserting 5 lines of data to HBase is success . . .
[INFO][2018-05-29 17:03:45,087][com.seven.spark.streaming.ReceiveKafkaData$]Inserting 5 lines of data to ElasticSearch is success . . .
[INFO][2018-05-29 17:03:45,088][org.apache.spark.executor.Executor]Finished task 0.0 in stage 38.0 (TID 38). 708 bytes result sent to driver
[INFO][2018-05-29 17:03:45,088][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 38.0 (TID 38) in 47 ms on localhost (executor driver) (1/1)
[INFO][2018-05-29 17:03:45,088][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 38.0, whose tasks have all completed, from pool 
[INFO][2018-05-29 17:03:45,088][org.apache.spark.scheduler.DAGScheduler]ResultStage 38 (foreachPartition at ReceiveKafkaData.scala:76) finished in 0.047 s
[INFO][2018-05-29 17:03:45,089][org.apache.spark.scheduler.DAGScheduler]Job 38 finished: foreachPartition at ReceiveKafkaData.scala:76, took 0.050615 s
[INFO][2018-05-29 17:03:45,089][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527584625000 ms.0 from job set of time 1527584625000 ms
[INFO][2018-05-29 17:03:45,089][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.089 s for time 1527584625000 ms (execution: 0.066 s)
[INFO][2018-05-29 17:03:45,089][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 41 from persistence list
[INFO][2018-05-29 17:03:45,090][org.apache.spark.storage.BlockManager]Removing RDD 41
[INFO][2018-05-29 17:03:45,090][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 40 from persistence list
[INFO][2018-05-29 17:03:45,090][org.apache.spark.storage.BlockManager]Removing RDD 40
[INFO][2018-05-29 17:03:45,090][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-29 17:03:45,090][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527584615000 ms
[INFO][2018-05-29 17:03:50,017][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527584630000 ms
[INFO][2018-05-29 17:03:50,017][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527584630000 ms.0 from job set of time 1527584630000 ms
[INFO][2018-05-29 17:03:50,022][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:75
[INFO][2018-05-29 17:03:50,022][org.apache.spark.scheduler.DAGScheduler]Got job 39 (isEmpty at ReceiveKafkaData.scala:75) with 1 output partitions
[INFO][2018-05-29 17:03:50,022][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 39 (isEmpty at ReceiveKafkaData.scala:75)
[INFO][2018-05-29 17:03:50,023][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-29 17:03:50,023][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-29 17:03:50,023][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 39 (MapPartitionsRDD[45] at map at ReceiveKafkaData.scala:74), which has no missing parents
[INFO][2018-05-29 17:03:50,024][org.apache.spark.storage.memory.MemoryStore]Block broadcast_39 stored as values in memory (estimated size 3.1 KB, free 912.2 MB)
[INFO][2018-05-29 17:03:50,025][org.apache.spark.storage.memory.MemoryStore]Block broadcast_39_piece0 stored as bytes in memory (estimated size 1973.0 B, free 912.2 MB)
[INFO][2018-05-29 17:03:50,026][org.apache.spark.storage.BlockManagerInfo]Added broadcast_39_piece0 in memory on 10.194.32.157:63705 (size: 1973.0 B, free: 912.3 MB)
[INFO][2018-05-29 17:03:50,026][org.apache.spark.SparkContext]Created broadcast 39 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-29 17:03:50,027][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 39 (MapPartitionsRDD[45] at map at ReceiveKafkaData.scala:74) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-29 17:03:50,027][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 39.0 with 1 tasks
[INFO][2018-05-29 17:03:50,027][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 39.0 (TID 39, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-29 17:03:50,027][org.apache.spark.executor.Executor]Running task 0.0 in stage 39.0 (TID 39)
[INFO][2018-05-29 17:03:50,028][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 188926 -> 188931
[INFO][2018-05-29 17:03:50,038][org.apache.spark.executor.Executor]Finished task 0.0 in stage 39.0 (TID 39). 957 bytes result sent to driver
[INFO][2018-05-29 17:03:50,039][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 39.0 (TID 39) in 12 ms on localhost (executor driver) (1/1)
[INFO][2018-05-29 17:03:50,039][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 39.0, whose tasks have all completed, from pool 
[INFO][2018-05-29 17:03:50,039][org.apache.spark.scheduler.DAGScheduler]ResultStage 39 (isEmpty at ReceiveKafkaData.scala:75) finished in 0.012 s
[INFO][2018-05-29 17:03:50,039][org.apache.spark.scheduler.DAGScheduler]Job 39 finished: isEmpty at ReceiveKafkaData.scala:75, took 0.017576 s
[INFO][2018-05-29 17:03:50,044][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:76
[INFO][2018-05-29 17:03:50,044][org.apache.spark.scheduler.DAGScheduler]Got job 40 (foreachPartition at ReceiveKafkaData.scala:76) with 1 output partitions
[INFO][2018-05-29 17:03:50,044][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 40 (foreachPartition at ReceiveKafkaData.scala:76)
[INFO][2018-05-29 17:03:50,044][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-29 17:03:50,044][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-29 17:03:50,045][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 40 (MapPartitionsRDD[45] at map at ReceiveKafkaData.scala:74), which has no missing parents
[INFO][2018-05-29 17:03:50,046][org.apache.spark.storage.memory.MemoryStore]Block broadcast_40 stored as values in memory (estimated size 3.4 KB, free 912.2 MB)
[INFO][2018-05-29 17:03:50,048][org.apache.spark.storage.memory.MemoryStore]Block broadcast_40_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.2 MB)
[INFO][2018-05-29 17:03:50,048][org.apache.spark.storage.BlockManagerInfo]Added broadcast_40_piece0 in memory on 10.194.32.157:63705 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-29 17:03:50,048][org.apache.spark.SparkContext]Created broadcast 40 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-29 17:03:50,048][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 40 (MapPartitionsRDD[45] at map at ReceiveKafkaData.scala:74) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-29 17:03:50,048][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 40.0 with 1 tasks
[INFO][2018-05-29 17:03:50,049][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 40.0 (TID 40, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-29 17:03:50,049][org.apache.spark.executor.Executor]Running task 0.0 in stage 40.0 (TID 40)
[INFO][2018-05-29 17:03:50,050][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 188926 -> 188931
[INFO][2018-05-29 17:03:50,050][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 188926
[INFO][2018-05-29 17:03:50,584][com.seven.spark.streaming.ReceiveKafkaData$]Inserting 5 lines of data to HBase is success . . .
[INFO][2018-05-29 17:03:50,597][com.seven.spark.streaming.ReceiveKafkaData$]Inserting 5 lines of data to ElasticSearch is success . . .
[INFO][2018-05-29 17:03:50,598][org.apache.spark.executor.Executor]Finished task 0.0 in stage 40.0 (TID 40). 665 bytes result sent to driver
[INFO][2018-05-29 17:03:50,599][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 40.0 (TID 40) in 549 ms on localhost (executor driver) (1/1)
[INFO][2018-05-29 17:03:50,599][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 40.0, whose tasks have all completed, from pool 
[INFO][2018-05-29 17:03:50,599][org.apache.spark.scheduler.DAGScheduler]ResultStage 40 (foreachPartition at ReceiveKafkaData.scala:76) finished in 0.550 s
[INFO][2018-05-29 17:03:50,599][org.apache.spark.scheduler.DAGScheduler]Job 40 finished: foreachPartition at ReceiveKafkaData.scala:76, took 0.555456 s
[INFO][2018-05-29 17:03:50,599][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527584630000 ms.0 from job set of time 1527584630000 ms
[INFO][2018-05-29 17:03:50,600][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 43 from persistence list
[INFO][2018-05-29 17:03:50,600][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.599 s for time 1527584630000 ms (execution: 0.582 s)
[INFO][2018-05-29 17:03:50,600][org.apache.spark.storage.BlockManager]Removing RDD 43
[INFO][2018-05-29 17:03:50,600][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 42 from persistence list
[INFO][2018-05-29 17:03:50,600][org.apache.spark.storage.BlockManager]Removing RDD 42
[INFO][2018-05-29 17:03:50,600][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-29 17:03:50,600][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527584620000 ms
[INFO][2018-05-29 17:03:55,018][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527584635000 ms
[INFO][2018-05-29 17:03:55,018][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527584635000 ms.0 from job set of time 1527584635000 ms
[INFO][2018-05-29 17:03:55,022][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:75
[INFO][2018-05-29 17:03:55,023][org.apache.spark.scheduler.DAGScheduler]Got job 41 (isEmpty at ReceiveKafkaData.scala:75) with 1 output partitions
[INFO][2018-05-29 17:03:55,023][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 41 (isEmpty at ReceiveKafkaData.scala:75)
[INFO][2018-05-29 17:03:55,023][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-29 17:03:55,023][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-29 17:03:55,023][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 41 (MapPartitionsRDD[47] at map at ReceiveKafkaData.scala:74), which has no missing parents
[INFO][2018-05-29 17:03:55,024][org.apache.spark.storage.memory.MemoryStore]Block broadcast_41 stored as values in memory (estimated size 3.1 KB, free 912.2 MB)
[INFO][2018-05-29 17:03:55,026][org.apache.spark.storage.memory.MemoryStore]Block broadcast_41_piece0 stored as bytes in memory (estimated size 1973.0 B, free 912.2 MB)
[INFO][2018-05-29 17:03:55,026][org.apache.spark.storage.BlockManagerInfo]Added broadcast_41_piece0 in memory on 10.194.32.157:63705 (size: 1973.0 B, free: 912.3 MB)
[INFO][2018-05-29 17:03:55,026][org.apache.spark.SparkContext]Created broadcast 41 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-29 17:03:55,027][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 41 (MapPartitionsRDD[47] at map at ReceiveKafkaData.scala:74) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-29 17:03:55,027][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 41.0 with 1 tasks
[INFO][2018-05-29 17:03:55,027][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 41.0 (TID 41, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-29 17:03:55,027][org.apache.spark.executor.Executor]Running task 0.0 in stage 41.0 (TID 41)
[INFO][2018-05-29 17:03:55,028][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 188931 -> 188936
[INFO][2018-05-29 17:03:55,029][org.apache.spark.executor.Executor]Finished task 0.0 in stage 41.0 (TID 41). 954 bytes result sent to driver
[INFO][2018-05-29 17:03:55,030][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 41.0 (TID 41) in 2 ms on localhost (executor driver) (1/1)
[INFO][2018-05-29 17:03:55,030][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 41.0, whose tasks have all completed, from pool 
[INFO][2018-05-29 17:03:55,030][org.apache.spark.scheduler.DAGScheduler]ResultStage 41 (isEmpty at ReceiveKafkaData.scala:75) finished in 0.003 s
[INFO][2018-05-29 17:03:55,030][org.apache.spark.scheduler.DAGScheduler]Job 41 finished: isEmpty at ReceiveKafkaData.scala:75, took 0.007717 s
[INFO][2018-05-29 17:03:55,034][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:76
[INFO][2018-05-29 17:03:55,034][org.apache.spark.scheduler.DAGScheduler]Got job 42 (foreachPartition at ReceiveKafkaData.scala:76) with 1 output partitions
[INFO][2018-05-29 17:03:55,034][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 42 (foreachPartition at ReceiveKafkaData.scala:76)
[INFO][2018-05-29 17:03:55,034][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-29 17:03:55,034][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-29 17:03:55,035][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 42 (MapPartitionsRDD[47] at map at ReceiveKafkaData.scala:74), which has no missing parents
[INFO][2018-05-29 17:03:55,036][org.apache.spark.storage.memory.MemoryStore]Block broadcast_42 stored as values in memory (estimated size 3.4 KB, free 912.2 MB)
[INFO][2018-05-29 17:03:55,037][org.apache.spark.storage.memory.MemoryStore]Block broadcast_42_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.2 MB)
[INFO][2018-05-29 17:03:55,037][org.apache.spark.storage.BlockManagerInfo]Added broadcast_42_piece0 in memory on 10.194.32.157:63705 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-29 17:03:55,038][org.apache.spark.SparkContext]Created broadcast 42 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-29 17:03:55,038][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 42 (MapPartitionsRDD[47] at map at ReceiveKafkaData.scala:74) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-29 17:03:55,038][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 42.0 with 1 tasks
[INFO][2018-05-29 17:03:55,038][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 42.0 (TID 42, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-29 17:03:55,039][org.apache.spark.executor.Executor]Running task 0.0 in stage 42.0 (TID 42)
[INFO][2018-05-29 17:03:55,039][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 188931 -> 188936
[INFO][2018-05-29 17:03:55,040][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 188931
[INFO][2018-05-29 17:03:55,064][com.seven.spark.streaming.ReceiveKafkaData$]Inserting 5 lines of data to HBase is success . . .
[INFO][2018-05-29 17:03:55,088][com.seven.spark.streaming.ReceiveKafkaData$]Inserting 5 lines of data to ElasticSearch is success . . .
[INFO][2018-05-29 17:03:55,089][org.apache.spark.executor.Executor]Finished task 0.0 in stage 42.0 (TID 42). 665 bytes result sent to driver
[INFO][2018-05-29 17:03:55,089][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 42.0 (TID 42) in 51 ms on localhost (executor driver) (1/1)
[INFO][2018-05-29 17:03:55,089][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 42.0, whose tasks have all completed, from pool 
[INFO][2018-05-29 17:03:55,090][org.apache.spark.scheduler.DAGScheduler]ResultStage 42 (foreachPartition at ReceiveKafkaData.scala:76) finished in 0.052 s
[INFO][2018-05-29 17:03:55,090][org.apache.spark.scheduler.DAGScheduler]Job 42 finished: foreachPartition at ReceiveKafkaData.scala:76, took 0.056223 s
[INFO][2018-05-29 17:03:55,090][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527584635000 ms.0 from job set of time 1527584635000 ms
[INFO][2018-05-29 17:03:55,091][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.090 s for time 1527584635000 ms (execution: 0.072 s)
[INFO][2018-05-29 17:03:55,091][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 45 from persistence list
[INFO][2018-05-29 17:03:55,091][org.apache.spark.storage.BlockManager]Removing RDD 45
[INFO][2018-05-29 17:03:55,091][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 44 from persistence list
[INFO][2018-05-29 17:03:55,091][org.apache.spark.storage.BlockManager]Removing RDD 44
[INFO][2018-05-29 17:03:55,092][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-29 17:03:55,092][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527584625000 ms
[INFO][2018-05-29 17:04:00,018][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527584640000 ms
[INFO][2018-05-29 17:04:00,018][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527584640000 ms.0 from job set of time 1527584640000 ms
[INFO][2018-05-29 17:04:00,022][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:75
[INFO][2018-05-29 17:04:00,022][org.apache.spark.scheduler.DAGScheduler]Got job 43 (isEmpty at ReceiveKafkaData.scala:75) with 1 output partitions
[INFO][2018-05-29 17:04:00,023][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 43 (isEmpty at ReceiveKafkaData.scala:75)
[INFO][2018-05-29 17:04:00,023][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-29 17:04:00,023][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-29 17:04:00,023][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 43 (MapPartitionsRDD[49] at map at ReceiveKafkaData.scala:74), which has no missing parents
[INFO][2018-05-29 17:04:00,024][org.apache.spark.storage.memory.MemoryStore]Block broadcast_43 stored as values in memory (estimated size 3.1 KB, free 912.2 MB)
[INFO][2018-05-29 17:04:00,025][org.apache.spark.storage.memory.MemoryStore]Block broadcast_43_piece0 stored as bytes in memory (estimated size 1976.0 B, free 912.2 MB)
[INFO][2018-05-29 17:04:00,025][org.apache.spark.storage.BlockManagerInfo]Added broadcast_43_piece0 in memory on 10.194.32.157:63705 (size: 1976.0 B, free: 912.3 MB)
[INFO][2018-05-29 17:04:00,026][org.apache.spark.SparkContext]Created broadcast 43 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-29 17:04:00,026][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 43 (MapPartitionsRDD[49] at map at ReceiveKafkaData.scala:74) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-29 17:04:00,026][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 43.0 with 1 tasks
[INFO][2018-05-29 17:04:00,027][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 43.0 (TID 43, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-29 17:04:00,027][org.apache.spark.executor.Executor]Running task 0.0 in stage 43.0 (TID 43)
[INFO][2018-05-29 17:04:00,028][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 188936 -> 188941
[INFO][2018-05-29 17:04:00,042][org.apache.spark.executor.Executor]Finished task 0.0 in stage 43.0 (TID 43). 974 bytes result sent to driver
[INFO][2018-05-29 17:04:00,043][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 43.0 (TID 43) in 17 ms on localhost (executor driver) (1/1)
[INFO][2018-05-29 17:04:00,043][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 43.0, whose tasks have all completed, from pool 
[INFO][2018-05-29 17:04:00,044][org.apache.spark.scheduler.DAGScheduler]ResultStage 43 (isEmpty at ReceiveKafkaData.scala:75) finished in 0.017 s
[INFO][2018-05-29 17:04:00,044][org.apache.spark.scheduler.DAGScheduler]Job 43 finished: isEmpty at ReceiveKafkaData.scala:75, took 0.021910 s
[INFO][2018-05-29 17:04:00,049][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:76
[INFO][2018-05-29 17:04:00,050][org.apache.spark.scheduler.DAGScheduler]Got job 44 (foreachPartition at ReceiveKafkaData.scala:76) with 1 output partitions
[INFO][2018-05-29 17:04:00,050][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 44 (foreachPartition at ReceiveKafkaData.scala:76)
[INFO][2018-05-29 17:04:00,050][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-29 17:04:00,050][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-29 17:04:00,050][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 44 (MapPartitionsRDD[49] at map at ReceiveKafkaData.scala:74), which has no missing parents
[INFO][2018-05-29 17:04:00,052][org.apache.spark.storage.memory.MemoryStore]Block broadcast_44 stored as values in memory (estimated size 3.4 KB, free 912.2 MB)
[INFO][2018-05-29 17:04:00,053][org.apache.spark.storage.memory.MemoryStore]Block broadcast_44_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.2 MB)
[INFO][2018-05-29 17:04:00,053][org.apache.spark.storage.BlockManagerInfo]Added broadcast_44_piece0 in memory on 10.194.32.157:63705 (size: 2.0 KB, free: 912.2 MB)
[INFO][2018-05-29 17:04:00,053][org.apache.spark.SparkContext]Created broadcast 44 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-29 17:04:00,053][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 44 (MapPartitionsRDD[49] at map at ReceiveKafkaData.scala:74) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-29 17:04:00,053][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 44.0 with 1 tasks
[INFO][2018-05-29 17:04:00,054][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 44.0 (TID 44, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-29 17:04:00,054][org.apache.spark.executor.Executor]Running task 0.0 in stage 44.0 (TID 44)
[INFO][2018-05-29 17:04:00,055][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 188936 -> 188941
[INFO][2018-05-29 17:04:00,055][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 188936
[INFO][2018-05-29 17:04:00,625][com.seven.spark.streaming.ReceiveKafkaData$]Inserting 5 lines of data to HBase is success . . .
[INFO][2018-05-29 17:04:00,640][com.seven.spark.streaming.ReceiveKafkaData$]Inserting 5 lines of data to ElasticSearch is success . . .
[INFO][2018-05-29 17:04:00,640][org.apache.spark.executor.Executor]Finished task 0.0 in stage 44.0 (TID 44). 665 bytes result sent to driver
[INFO][2018-05-29 17:04:00,640][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 44.0 (TID 44) in 586 ms on localhost (executor driver) (1/1)
[INFO][2018-05-29 17:04:00,640][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 44.0, whose tasks have all completed, from pool 
[INFO][2018-05-29 17:04:00,641][org.apache.spark.scheduler.DAGScheduler]ResultStage 44 (foreachPartition at ReceiveKafkaData.scala:76) finished in 0.587 s
[INFO][2018-05-29 17:04:00,641][org.apache.spark.scheduler.DAGScheduler]Job 44 finished: foreachPartition at ReceiveKafkaData.scala:76, took 0.591475 s
[INFO][2018-05-29 17:04:00,641][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527584640000 ms.0 from job set of time 1527584640000 ms
[INFO][2018-05-29 17:04:00,641][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.641 s for time 1527584640000 ms (execution: 0.623 s)
[INFO][2018-05-29 17:04:00,641][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 47 from persistence list
[INFO][2018-05-29 17:04:00,642][org.apache.spark.storage.BlockManager]Removing RDD 47
[INFO][2018-05-29 17:04:00,642][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 46 from persistence list
[INFO][2018-05-29 17:04:00,642][org.apache.spark.storage.BlockManager]Removing RDD 46
[INFO][2018-05-29 17:04:00,642][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-29 17:04:00,642][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527584630000 ms
[INFO][2018-05-29 17:04:01,699][org.apache.kafka.clients.producer.KafkaProducer]Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
[INFO][2018-05-29 17:04:01,709][org.apache.spark.util.ShutdownHookManager]Shutdown hook called
[INFO][2018-05-29 17:04:01,710][org.apache.spark.util.ShutdownHookManager]Deleting directory /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/spark-8cf97ca2-6d5d-4bcb-97e4-d0821ccc6e7b
[INFO][2018-05-29 17:04:05,012][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527584645000 ms
[INFO][2018-05-29 17:04:05,012][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527584645000 ms.0 from job set of time 1527584645000 ms
[INFO][2018-05-29 17:04:05,019][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:75
[INFO][2018-05-29 17:04:05,019][org.apache.spark.scheduler.DAGScheduler]Got job 45 (isEmpty at ReceiveKafkaData.scala:75) with 1 output partitions
[INFO][2018-05-29 17:04:05,019][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 45 (isEmpty at ReceiveKafkaData.scala:75)
[INFO][2018-05-29 17:04:05,019][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-29 17:04:05,019][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-29 17:04:05,020][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 45 (MapPartitionsRDD[51] at map at ReceiveKafkaData.scala:74), which has no missing parents
[INFO][2018-05-29 17:04:05,022][org.apache.spark.storage.memory.MemoryStore]Block broadcast_45 stored as values in memory (estimated size 3.1 KB, free 912.2 MB)
[INFO][2018-05-29 17:04:05,025][org.apache.spark.storage.memory.MemoryStore]Block broadcast_45_piece0 stored as bytes in memory (estimated size 1973.0 B, free 912.2 MB)
[INFO][2018-05-29 17:04:05,026][org.apache.spark.storage.BlockManagerInfo]Added broadcast_45_piece0 in memory on 10.194.32.157:63705 (size: 1973.0 B, free: 912.2 MB)
[INFO][2018-05-29 17:04:05,026][org.apache.spark.SparkContext]Created broadcast 45 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-29 17:04:05,026][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 45 (MapPartitionsRDD[51] at map at ReceiveKafkaData.scala:74) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-29 17:04:05,026][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 45.0 with 1 tasks
[INFO][2018-05-29 17:04:05,027][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 45.0 (TID 45, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-29 17:04:05,027][org.apache.spark.executor.Executor]Running task 0.0 in stage 45.0 (TID 45)
[INFO][2018-05-29 17:04:05,029][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 188941 -> 188942
[INFO][2018-05-29 17:04:05,031][org.apache.spark.executor.Executor]Finished task 0.0 in stage 45.0 (TID 45). 971 bytes result sent to driver
[INFO][2018-05-29 17:04:05,035][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 45.0 (TID 45) in 8 ms on localhost (executor driver) (1/1)
[INFO][2018-05-29 17:04:05,035][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 45.0, whose tasks have all completed, from pool 
[INFO][2018-05-29 17:04:05,035][org.apache.spark.scheduler.DAGScheduler]ResultStage 45 (isEmpty at ReceiveKafkaData.scala:75) finished in 0.008 s
[INFO][2018-05-29 17:04:05,036][org.apache.spark.scheduler.DAGScheduler]Job 45 finished: isEmpty at ReceiveKafkaData.scala:75, took 0.016879 s
[INFO][2018-05-29 17:04:05,041][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:76
[INFO][2018-05-29 17:04:05,041][org.apache.spark.scheduler.DAGScheduler]Got job 46 (foreachPartition at ReceiveKafkaData.scala:76) with 1 output partitions
[INFO][2018-05-29 17:04:05,042][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 46 (foreachPartition at ReceiveKafkaData.scala:76)
[INFO][2018-05-29 17:04:05,042][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-29 17:04:05,042][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-29 17:04:05,042][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 46 (MapPartitionsRDD[51] at map at ReceiveKafkaData.scala:74), which has no missing parents
[INFO][2018-05-29 17:04:05,043][org.apache.spark.storage.memory.MemoryStore]Block broadcast_46 stored as values in memory (estimated size 3.4 KB, free 912.2 MB)
[INFO][2018-05-29 17:04:05,044][org.apache.spark.storage.memory.MemoryStore]Block broadcast_46_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.2 MB)
[INFO][2018-05-29 17:04:05,044][org.apache.spark.storage.BlockManagerInfo]Added broadcast_46_piece0 in memory on 10.194.32.157:63705 (size: 2.0 KB, free: 912.2 MB)
[INFO][2018-05-29 17:04:05,047][org.apache.spark.SparkContext]Created broadcast 46 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-29 17:04:05,047][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 46 (MapPartitionsRDD[51] at map at ReceiveKafkaData.scala:74) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-29 17:04:05,047][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 46.0 with 1 tasks
[INFO][2018-05-29 17:04:05,048][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 46.0 (TID 46, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-29 17:04:05,048][org.apache.spark.executor.Executor]Running task 0.0 in stage 46.0 (TID 46)
[INFO][2018-05-29 17:04:05,050][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 188941 -> 188942
[INFO][2018-05-29 17:04:05,050][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 188941
[INFO][2018-05-29 17:04:05,636][com.seven.spark.streaming.ReceiveKafkaData$]Inserting 1 lines of data to HBase is success . . .
[INFO][2018-05-29 17:04:05,643][com.seven.spark.streaming.ReceiveKafkaData$]Inserting 1 lines of data to ElasticSearch is success . . .
[INFO][2018-05-29 17:04:05,644][org.apache.spark.executor.Executor]Finished task 0.0 in stage 46.0 (TID 46). 708 bytes result sent to driver
[INFO][2018-05-29 17:04:05,644][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 46.0 (TID 46) in 596 ms on localhost (executor driver) (1/1)
[INFO][2018-05-29 17:04:05,644][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 46.0, whose tasks have all completed, from pool 
[INFO][2018-05-29 17:04:05,644][org.apache.spark.scheduler.DAGScheduler]ResultStage 46 (foreachPartition at ReceiveKafkaData.scala:76) finished in 0.597 s
[INFO][2018-05-29 17:04:05,645][org.apache.spark.scheduler.DAGScheduler]Job 46 finished: foreachPartition at ReceiveKafkaData.scala:76, took 0.603592 s
[INFO][2018-05-29 17:04:05,645][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527584645000 ms.0 from job set of time 1527584645000 ms
[INFO][2018-05-29 17:04:05,645][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.645 s for time 1527584645000 ms (execution: 0.633 s)
[INFO][2018-05-29 17:04:05,645][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 49 from persistence list
[INFO][2018-05-29 17:04:05,645][org.apache.spark.storage.BlockManager]Removing RDD 49
[INFO][2018-05-29 17:04:05,645][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 48 from persistence list
[INFO][2018-05-29 17:04:05,646][org.apache.spark.storage.BlockManager]Removing RDD 48
[INFO][2018-05-29 17:04:05,646][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-29 17:04:05,646][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527584635000 ms
[INFO][2018-05-29 17:04:10,018][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527584650000 ms
[INFO][2018-05-29 17:04:10,018][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527584650000 ms.0 from job set of time 1527584650000 ms
[INFO][2018-05-29 17:04:10,024][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:75
[INFO][2018-05-29 17:04:10,024][org.apache.spark.scheduler.DAGScheduler]Got job 47 (isEmpty at ReceiveKafkaData.scala:75) with 1 output partitions
[INFO][2018-05-29 17:04:10,024][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 47 (isEmpty at ReceiveKafkaData.scala:75)
[INFO][2018-05-29 17:04:10,024][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-29 17:04:10,025][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-29 17:04:10,025][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 47 (MapPartitionsRDD[53] at map at ReceiveKafkaData.scala:74), which has no missing parents
[INFO][2018-05-29 17:04:10,026][org.apache.spark.storage.memory.MemoryStore]Block broadcast_47 stored as values in memory (estimated size 3.1 KB, free 912.2 MB)
[INFO][2018-05-29 17:04:10,027][org.apache.spark.storage.memory.MemoryStore]Block broadcast_47_piece0 stored as bytes in memory (estimated size 1973.0 B, free 912.2 MB)
[INFO][2018-05-29 17:04:10,028][org.apache.spark.storage.BlockManagerInfo]Added broadcast_47_piece0 in memory on 10.194.32.157:63705 (size: 1973.0 B, free: 912.2 MB)
[INFO][2018-05-29 17:04:10,028][org.apache.spark.SparkContext]Created broadcast 47 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-29 17:04:10,028][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 47 (MapPartitionsRDD[53] at map at ReceiveKafkaData.scala:74) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-29 17:04:10,028][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 47.0 with 1 tasks
[INFO][2018-05-29 17:04:10,029][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 47.0 (TID 47, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-29 17:04:10,029][org.apache.spark.executor.Executor]Running task 0.0 in stage 47.0 (TID 47)
[INFO][2018-05-29 17:04:10,030][org.apache.spark.streaming.kafka010.KafkaRDD]Beginning offset 188942 is the same as ending offset skipping seven 0
[INFO][2018-05-29 17:04:10,030][org.apache.spark.executor.Executor]Finished task 0.0 in stage 47.0 (TID 47). 665 bytes result sent to driver
[INFO][2018-05-29 17:04:10,031][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 47.0 (TID 47) in 2 ms on localhost (executor driver) (1/1)
[INFO][2018-05-29 17:04:10,031][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 47.0, whose tasks have all completed, from pool 
[INFO][2018-05-29 17:04:10,031][org.apache.spark.scheduler.DAGScheduler]ResultStage 47 (isEmpty at ReceiveKafkaData.scala:75) finished in 0.002 s
[INFO][2018-05-29 17:04:10,032][org.apache.spark.scheduler.DAGScheduler]Job 47 finished: isEmpty at ReceiveKafkaData.scala:75, took 0.007839 s
[INFO][2018-05-29 17:04:10,032][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527584650000 ms.0 from job set of time 1527584650000 ms
[INFO][2018-05-29 17:04:10,032][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 51 from persistence list
[INFO][2018-05-29 17:04:10,032][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.032 s for time 1527584650000 ms (execution: 0.014 s)
[INFO][2018-05-29 17:04:10,032][org.apache.spark.storage.BlockManager]Removing RDD 51
[INFO][2018-05-29 17:04:10,032][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 50 from persistence list
[INFO][2018-05-29 17:04:10,033][org.apache.spark.storage.BlockManager]Removing RDD 50
[INFO][2018-05-29 17:04:10,033][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-29 17:04:10,033][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527584640000 ms
[INFO][2018-05-29 17:04:15,017][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527584655000 ms
[INFO][2018-05-29 17:04:15,017][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527584655000 ms.0 from job set of time 1527584655000 ms
[INFO][2018-05-29 17:04:15,021][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:75
[INFO][2018-05-29 17:04:15,022][org.apache.spark.scheduler.DAGScheduler]Got job 48 (isEmpty at ReceiveKafkaData.scala:75) with 1 output partitions
[INFO][2018-05-29 17:04:15,022][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 48 (isEmpty at ReceiveKafkaData.scala:75)
[INFO][2018-05-29 17:04:15,022][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-29 17:04:15,022][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-29 17:04:15,022][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 48 (MapPartitionsRDD[55] at map at ReceiveKafkaData.scala:74), which has no missing parents
[INFO][2018-05-29 17:04:15,023][org.apache.spark.storage.memory.MemoryStore]Block broadcast_48 stored as values in memory (estimated size 3.1 KB, free 912.1 MB)
[INFO][2018-05-29 17:04:15,027][org.apache.spark.storage.memory.MemoryStore]Block broadcast_48_piece0 stored as bytes in memory (estimated size 1973.0 B, free 912.1 MB)
[INFO][2018-05-29 17:04:15,027][org.apache.spark.storage.BlockManagerInfo]Added broadcast_48_piece0 in memory on 10.194.32.157:63705 (size: 1973.0 B, free: 912.2 MB)
[INFO][2018-05-29 17:04:15,027][org.apache.spark.SparkContext]Created broadcast 48 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-29 17:04:15,028][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 48 (MapPartitionsRDD[55] at map at ReceiveKafkaData.scala:74) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-29 17:04:15,028][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 48.0 with 1 tasks
[INFO][2018-05-29 17:04:15,029][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 48.0 (TID 48, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-29 17:04:15,029][org.apache.spark.executor.Executor]Running task 0.0 in stage 48.0 (TID 48)
[INFO][2018-05-29 17:04:15,030][org.apache.spark.streaming.kafka010.KafkaRDD]Beginning offset 188942 is the same as ending offset skipping seven 0
[INFO][2018-05-29 17:04:15,030][org.apache.spark.executor.Executor]Finished task 0.0 in stage 48.0 (TID 48). 665 bytes result sent to driver
[INFO][2018-05-29 17:04:15,030][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 48.0 (TID 48) in 1 ms on localhost (executor driver) (1/1)
[INFO][2018-05-29 17:04:15,031][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 48.0, whose tasks have all completed, from pool 
[INFO][2018-05-29 17:04:15,031][org.apache.spark.scheduler.DAGScheduler]ResultStage 48 (isEmpty at ReceiveKafkaData.scala:75) finished in 0.003 s
[INFO][2018-05-29 17:04:15,031][org.apache.spark.scheduler.DAGScheduler]Job 48 finished: isEmpty at ReceiveKafkaData.scala:75, took 0.009617 s
[INFO][2018-05-29 17:04:15,031][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527584655000 ms.0 from job set of time 1527584655000 ms
[INFO][2018-05-29 17:04:15,031][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.031 s for time 1527584655000 ms (execution: 0.014 s)
[INFO][2018-05-29 17:04:15,031][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 53 from persistence list
[INFO][2018-05-29 17:04:15,032][org.apache.spark.storage.BlockManager]Removing RDD 53
[INFO][2018-05-29 17:04:15,032][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 52 from persistence list
[INFO][2018-05-29 17:04:15,032][org.apache.spark.storage.BlockManager]Removing RDD 52
[INFO][2018-05-29 17:04:15,032][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-29 17:04:15,032][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527584645000 ms
[INFO][2018-05-29 17:04:20,018][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527584660000 ms
[INFO][2018-05-29 17:04:20,018][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527584660000 ms.0 from job set of time 1527584660000 ms
[INFO][2018-05-29 17:04:20,023][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:75
[INFO][2018-05-29 17:04:20,024][org.apache.spark.scheduler.DAGScheduler]Got job 49 (isEmpty at ReceiveKafkaData.scala:75) with 1 output partitions
[INFO][2018-05-29 17:04:20,024][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 49 (isEmpty at ReceiveKafkaData.scala:75)
[INFO][2018-05-29 17:04:20,024][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-29 17:04:20,024][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-29 17:04:20,024][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 49 (MapPartitionsRDD[57] at map at ReceiveKafkaData.scala:74), which has no missing parents
[INFO][2018-05-29 17:04:20,025][org.apache.spark.storage.memory.MemoryStore]Block broadcast_49 stored as values in memory (estimated size 3.1 KB, free 912.1 MB)
[INFO][2018-05-29 17:04:20,030][org.apache.spark.storage.memory.MemoryStore]Block broadcast_49_piece0 stored as bytes in memory (estimated size 1973.0 B, free 912.1 MB)
[INFO][2018-05-29 17:04:20,030][org.apache.spark.storage.BlockManagerInfo]Added broadcast_49_piece0 in memory on 10.194.32.157:63705 (size: 1973.0 B, free: 912.2 MB)
[INFO][2018-05-29 17:04:20,031][org.apache.spark.SparkContext]Created broadcast 49 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-29 17:04:20,031][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 49 (MapPartitionsRDD[57] at map at ReceiveKafkaData.scala:74) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-29 17:04:20,032][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 49.0 with 1 tasks
[INFO][2018-05-29 17:04:20,032][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 49.0 (TID 49, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-29 17:04:20,032][org.apache.spark.executor.Executor]Running task 0.0 in stage 49.0 (TID 49)
[INFO][2018-05-29 17:04:20,033][org.apache.spark.streaming.kafka010.KafkaRDD]Beginning offset 188942 is the same as ending offset skipping seven 0
[INFO][2018-05-29 17:04:20,033][org.apache.spark.executor.Executor]Finished task 0.0 in stage 49.0 (TID 49). 665 bytes result sent to driver
[INFO][2018-05-29 17:04:20,034][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 49.0 (TID 49) in 2 ms on localhost (executor driver) (1/1)
[INFO][2018-05-29 17:04:20,034][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 49.0, whose tasks have all completed, from pool 
[INFO][2018-05-29 17:04:20,034][org.apache.spark.scheduler.DAGScheduler]ResultStage 49 (isEmpty at ReceiveKafkaData.scala:75) finished in 0.002 s
[INFO][2018-05-29 17:04:20,034][org.apache.spark.scheduler.DAGScheduler]Job 49 finished: isEmpty at ReceiveKafkaData.scala:75, took 0.011222 s
[INFO][2018-05-29 17:04:20,035][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527584660000 ms.0 from job set of time 1527584660000 ms
[INFO][2018-05-29 17:04:20,035][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 55 from persistence list
[INFO][2018-05-29 17:04:20,035][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.035 s for time 1527584660000 ms (execution: 0.017 s)
[INFO][2018-05-29 17:04:20,035][org.apache.spark.storage.BlockManager]Removing RDD 55
[INFO][2018-05-29 17:04:20,035][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 54 from persistence list
[INFO][2018-05-29 17:04:20,035][org.apache.spark.storage.BlockManager]Removing RDD 54
[INFO][2018-05-29 17:04:20,035][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-29 17:04:20,035][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527584650000 ms
[INFO][2018-05-29 17:04:20,932][org.apache.spark.streaming.StreamingContext]Invoking stop(stopGracefully=false) from shutdown hook
[INFO][2018-05-29 17:04:20,934][org.apache.spark.streaming.scheduler.ReceiverTracker]ReceiverTracker stopped
[INFO][2018-05-29 17:04:20,935][org.apache.spark.streaming.scheduler.JobGenerator]Stopping JobGenerator immediately
[INFO][2018-05-29 17:04:20,936][org.apache.spark.streaming.util.RecurringTimer]Stopped timer for JobGenerator after time 1527584660000
[INFO][2018-05-29 17:04:20,943][org.apache.spark.streaming.scheduler.JobGenerator]Stopped JobGenerator
[INFO][2018-05-29 17:04:20,945][org.apache.spark.streaming.scheduler.JobScheduler]Stopped JobScheduler
[INFO][2018-05-29 17:04:20,952][org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@57adfab0{/streaming,null,UNAVAILABLE,@Spark}
[INFO][2018-05-29 17:04:20,952][org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@2a22ad2b{/streaming/batch,null,UNAVAILABLE,@Spark}
[INFO][2018-05-29 17:04:20,954][org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@5dbf5634{/static/streaming,null,UNAVAILABLE,@Spark}
[INFO][2018-05-29 17:04:20,955][org.apache.spark.streaming.StreamingContext]StreamingContext stopped successfully
[INFO][2018-05-29 17:04:20,955][org.apache.spark.SparkContext]Invoking stop() from shutdown hook
[INFO][2018-05-29 17:04:20,961][org.spark_project.jetty.server.AbstractConnector]Stopped Spark@5fdff907{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-05-29 17:04:20,963][org.apache.spark.ui.SparkUI]Stopped Spark web UI at http://10.194.32.157:4040
[INFO][2018-05-29 17:04:20,972][org.apache.spark.MapOutputTrackerMasterEndpoint]MapOutputTrackerMasterEndpoint stopped!
[INFO][2018-05-29 17:04:20,991][org.apache.spark.storage.memory.MemoryStore]MemoryStore cleared
[INFO][2018-05-29 17:04:20,992][org.apache.spark.storage.BlockManager]BlockManager stopped
[INFO][2018-05-29 17:04:20,992][org.apache.spark.storage.BlockManagerMaster]BlockManagerMaster stopped
[INFO][2018-05-29 17:04:20,995][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint]OutputCommitCoordinator stopped!
[INFO][2018-05-29 17:04:20,996][org.apache.spark.SparkContext]Successfully stopped SparkContext
[INFO][2018-05-29 17:04:20,996][org.apache.spark.util.ShutdownHookManager]Shutdown hook called
[INFO][2018-05-29 17:04:20,997][org.apache.spark.util.ShutdownHookManager]Deleting directory /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/spark-e93b15ef-c9d7-4796-9f81-b63cadbb00aa
[INFO][2018-05-30 10:04:13,478][org.apache.spark.SparkContext]Running Spark version 2.2.0
[INFO][2018-05-30 10:04:14,906][org.apache.spark.SparkContext]Submitted application: SalesNetAvgMount$
[INFO][2018-05-30 10:04:14,946][org.apache.spark.SecurityManager]Changing view acls to: seven
[INFO][2018-05-30 10:04:14,947][org.apache.spark.SecurityManager]Changing modify acls to: seven
[INFO][2018-05-30 10:04:14,948][org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO][2018-05-30 10:04:14,949][org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO][2018-05-30 10:04:14,951][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(seven); groups with view permissions: Set(); users  with modify permissions: Set(seven); groups with modify permissions: Set()
[INFO][2018-05-30 10:04:15,440][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 52179.
[INFO][2018-05-30 10:04:15,478][org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO][2018-05-30 10:04:15,521][org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO][2018-05-30 10:04:15,531][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2018-05-30 10:04:15,532][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
[INFO][2018-05-30 10:04:15,549][org.apache.spark.storage.DiskBlockManager]Created local directory at /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/blockmgr-3aea7245-b017-4709-9773-c9ebc7b9c9a0
[INFO][2018-05-30 10:04:15,609][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 912.3 MB
[INFO][2018-05-30 10:04:15,740][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO][2018-05-30 10:04:15,908][org.spark_project.jetty.util.log]Logging initialized @4105ms
[INFO][2018-05-30 10:04:15,993][org.spark_project.jetty.server.Server]jetty-9.3.z-SNAPSHOT
[INFO][2018-05-30 10:04:16,011][org.spark_project.jetty.server.Server]Started @4210ms
[INFO][2018-05-30 10:04:16,043][org.spark_project.jetty.server.AbstractConnector]Started ServerConnector@1d18db87{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-05-30 10:04:16,044][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4040.
[INFO][2018-05-30 10:04:16,093][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3e2943ab{/jobs,null,AVAILABLE,@Spark}
[INFO][2018-05-30 10:04:16,096][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4602c2a9{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2018-05-30 10:04:16,097][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3e2822{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2018-05-30 10:04:16,098][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1849db1a{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2018-05-30 10:04:16,099][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1ca25c47{/stages,null,AVAILABLE,@Spark}
[INFO][2018-05-30 10:04:16,101][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@533b266e{/stages/json,null,AVAILABLE,@Spark}
[INFO][2018-05-30 10:04:16,102][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@89ff02e{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2018-05-30 10:04:16,104][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1d71006f{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-30 10:04:16,104][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5f2606b{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2018-05-30 10:04:16,105][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3ebff828{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2018-05-30 10:04:16,115][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@33352f32{/storage,null,AVAILABLE,@Spark}
[INFO][2018-05-30 10:04:16,118][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1e044120{/storage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-30 10:04:16,119][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3624da92{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2018-05-30 10:04:16,119][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@94f6bfb{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2018-05-30 10:04:16,121][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2484f433{/environment,null,AVAILABLE,@Spark}
[INFO][2018-05-30 10:04:16,122][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1255b1d1{/environment/json,null,AVAILABLE,@Spark}
[INFO][2018-05-30 10:04:16,123][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7c22d4f{/executors,null,AVAILABLE,@Spark}
[INFO][2018-05-30 10:04:16,126][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@60bdf15d{/executors/json,null,AVAILABLE,@Spark}
[INFO][2018-05-30 10:04:16,129][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@51e4ccb3{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2018-05-30 10:04:16,130][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@495083a0{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2018-05-30 10:04:16,144][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@28a0fd6c{/static,null,AVAILABLE,@Spark}
[INFO][2018-05-30 10:04:16,145][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4ef27d66{/,null,AVAILABLE,@Spark}
[INFO][2018-05-30 10:04:16,146][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1d9bec4d{/api,null,AVAILABLE,@Spark}
[INFO][2018-05-30 10:04:16,147][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5ab14cb9{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-30 10:04:16,148][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@439a8f59{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-30 10:04:16,150][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://10.194.67.11:4040
[INFO][2018-05-30 10:04:16,298][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
[INFO][2018-05-30 10:04:16,345][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 52180.
[INFO][2018-05-30 10:04:16,346][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 10.194.67.11:52180
[INFO][2018-05-30 10:04:16,355][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2018-05-30 10:04:16,359][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 10.194.67.11, 52180, None)
[INFO][2018-05-30 10:04:16,367][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 10.194.67.11:52180 with 912.3 MB RAM, BlockManagerId(driver, 10.194.67.11, 52180, None)
[INFO][2018-05-30 10:04:16,393][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 10.194.67.11, 52180, None)
[INFO][2018-05-30 10:04:16,394][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 10.194.67.11, 52180, None)
[INFO][2018-05-30 10:04:16,913][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@68ace111{/metrics/json,null,AVAILABLE,@Spark}
[INFO][2018-05-30 10:04:16,952][com.seven.spark.rdd.SalesNetAvgMount$]job is start . . . 
[INFO][2018-05-30 10:04:17,825][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0 stored as values in memory (estimated size 228.1 KB, free 912.1 MB)
[INFO][2018-05-30 10:04:18,119][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0_piece0 stored as bytes in memory (estimated size 22.1 KB, free 912.1 MB)
[INFO][2018-05-30 10:04:18,125][org.apache.spark.storage.BlockManagerInfo]Added broadcast_0_piece0 in memory on 10.194.67.11:52180 (size: 22.1 KB, free: 912.3 MB)
[INFO][2018-05-30 10:04:18,132][org.apache.spark.SparkContext]Created broadcast 0 from textFile at SalesNetAvgMount.scala:71
[WARN][2018-05-30 10:04:19,937][org.apache.hadoop.util.NativeCodeLoader]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO][2018-05-30 10:04:20,197][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 1
[INFO][2018-05-30 10:04:20,361][org.apache.spark.SparkContext]Starting job: collect at SalesNetAvgMount.scala:88
[INFO][2018-05-30 10:04:20,388][org.apache.spark.scheduler.DAGScheduler]Got job 0 (collect at SalesNetAvgMount.scala:88) with 2 output partitions
[INFO][2018-05-30 10:04:20,390][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 0 (collect at SalesNetAvgMount.scala:88)
[INFO][2018-05-30 10:04:20,391][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-30 10:04:20,393][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-30 10:04:20,413][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 0 (MapPartitionsRDD[2] at mapPartitions at SalesNetAvgMount.scala:71), which has no missing parents
[INFO][2018-05-30 10:04:20,462][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1 stored as values in memory (estimated size 3.5 KB, free 912.1 MB)
[INFO][2018-05-30 10:04:20,476][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1_piece0 stored as bytes in memory (estimated size 2028.0 B, free 912.1 MB)
[INFO][2018-05-30 10:04:20,477][org.apache.spark.storage.BlockManagerInfo]Added broadcast_1_piece0 in memory on 10.194.67.11:52180 (size: 2028.0 B, free: 912.3 MB)
[INFO][2018-05-30 10:04:20,478][org.apache.spark.SparkContext]Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-30 10:04:20,505][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at mapPartitions at SalesNetAvgMount.scala:71) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2018-05-30 10:04:20,506][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 0.0 with 2 tasks
[INFO][2018-05-30 10:04:20,562][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 4886 bytes)
[INFO][2018-05-30 10:04:20,565][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 4886 bytes)
[INFO][2018-05-30 10:04:20,573][org.apache.spark.executor.Executor]Running task 0.0 in stage 0.0 (TID 0)
[INFO][2018-05-30 10:04:20,573][org.apache.spark.executor.Executor]Running task 1.0 in stage 0.0 (TID 1)
[INFO][2018-05-30 10:04:20,633][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/operate/N/main/part-00000:0+34172
[INFO][2018-05-30 10:04:20,633][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/operate/N/main/part-00000:34172+34172
[INFO][2018-05-30 10:04:20,873][org.apache.spark.executor.Executor]Finished task 1.0 in stage 0.0 (TID 1). 17494 bytes result sent to driver
[INFO][2018-05-30 10:04:20,873][org.apache.spark.executor.Executor]Finished task 0.0 in stage 0.0 (TID 0). 17518 bytes result sent to driver
[INFO][2018-05-30 10:04:20,892][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 0.0 (TID 0) in 345 ms on localhost (executor driver) (1/2)
[INFO][2018-05-30 10:04:20,895][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 0.0 (TID 1) in 330 ms on localhost (executor driver) (2/2)
[INFO][2018-05-30 10:04:20,896][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO][2018-05-30 10:04:20,902][org.apache.spark.scheduler.DAGScheduler]ResultStage 0 (collect at SalesNetAvgMount.scala:88) finished in 0.369 s
[INFO][2018-05-30 10:04:20,911][org.apache.spark.scheduler.DAGScheduler]Job 0 finished: collect at SalesNetAvgMount.scala:88, took 0.550045 s
[INFO][2018-05-30 10:04:20,922][org.apache.spark.storage.memory.MemoryStore]Block broadcast_2 stored as values in memory (estimated size 110.1 KB, free 911.9 MB)
[INFO][2018-05-30 10:04:20,937][org.apache.spark.storage.memory.MemoryStore]Block broadcast_2_piece0 stored as bytes in memory (estimated size 17.5 KB, free 911.9 MB)
[INFO][2018-05-30 10:04:20,938][org.apache.spark.storage.BlockManagerInfo]Added broadcast_2_piece0 in memory on 10.194.67.11:52180 (size: 17.5 KB, free: 912.3 MB)
[INFO][2018-05-30 10:04:20,939][org.apache.spark.SparkContext]Created broadcast 2 from broadcast at SalesNetAvgMount.scala:45
[INFO][2018-05-30 10:04:20,946][org.apache.spark.storage.memory.MemoryStore]Block broadcast_3 stored as values in memory (estimated size 228.1 KB, free 911.7 MB)
[INFO][2018-05-30 10:04:20,981][org.apache.spark.storage.memory.MemoryStore]Block broadcast_3_piece0 stored as bytes in memory (estimated size 22.1 KB, free 911.7 MB)
[INFO][2018-05-30 10:04:20,982][org.apache.spark.storage.BlockManagerInfo]Added broadcast_3_piece0 in memory on 10.194.67.11:52180 (size: 22.1 KB, free: 912.2 MB)
[INFO][2018-05-30 10:04:20,983][org.apache.spark.SparkContext]Created broadcast 3 from textFile at NetTypeUtils.scala:53
[INFO][2018-05-30 10:04:20,988][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_1_piece0 on 10.194.67.11:52180 in memory (size: 2028.0 B, free: 912.2 MB)
[INFO][2018-05-30 10:04:21,011][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 1
[INFO][2018-05-30 10:04:21,028][org.apache.spark.SparkContext]Starting job: collect at NetTypeUtils.scala:62
[INFO][2018-05-30 10:04:21,029][org.apache.spark.scheduler.DAGScheduler]Got job 1 (collect at NetTypeUtils.scala:62) with 2 output partitions
[INFO][2018-05-30 10:04:21,029][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 1 (collect at NetTypeUtils.scala:62)
[INFO][2018-05-30 10:04:21,029][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-30 10:04:21,029][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-30 10:04:21,030][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 1 (MapPartitionsRDD[6] at mapPartitions at NetTypeUtils.scala:55), which has no missing parents
[INFO][2018-05-30 10:04:21,033][org.apache.spark.storage.memory.MemoryStore]Block broadcast_4 stored as values in memory (estimated size 3.7 KB, free 911.7 MB)
[INFO][2018-05-30 10:04:21,038][org.apache.spark.storage.memory.MemoryStore]Block broadcast_4_piece0 stored as bytes in memory (estimated size 2.0 KB, free 911.7 MB)
[INFO][2018-05-30 10:04:21,039][org.apache.spark.storage.BlockManagerInfo]Added broadcast_4_piece0 in memory on 10.194.67.11:52180 (size: 2.0 KB, free: 912.2 MB)
[INFO][2018-05-30 10:04:21,040][org.apache.spark.SparkContext]Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-30 10:04:21,041][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at mapPartitions at NetTypeUtils.scala:55) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2018-05-30 10:04:21,042][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 1.0 with 2 tasks
[INFO][2018-05-30 10:04:21,043][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 1.0 (TID 2, localhost, executor driver, partition 0, ANY, 4932 bytes)
[INFO][2018-05-30 10:04:21,043][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 1.0 (TID 3, localhost, executor driver, partition 1, ANY, 4932 bytes)
[INFO][2018-05-30 10:04:21,044][org.apache.spark.executor.Executor]Running task 1.0 in stage 1.0 (TID 3)
[INFO][2018-05-30 10:04:21,044][org.apache.spark.executor.Executor]Running task 0.0 in stage 1.0 (TID 2)
[INFO][2018-05-30 10:04:21,049][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/seven/data/netData/part-00000-5ea613c3-08fd-4310-b01a-b33de863caa2-c000.csv:160327+160327
[INFO][2018-05-30 10:04:21,049][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/seven/data/netData/part-00000-5ea613c3-08fd-4310-b01a-b33de863caa2-c000.csv:0+160327
[INFO][2018-05-30 10:04:21,176][org.apache.spark.executor.Executor]Finished task 0.0 in stage 1.0 (TID 2). 175704 bytes result sent to driver
[INFO][2018-05-30 10:04:21,188][org.apache.spark.executor.Executor]Finished task 1.0 in stage 1.0 (TID 3). 176889 bytes result sent to driver
[INFO][2018-05-30 10:04:21,190][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 1.0 (TID 2) in 148 ms on localhost (executor driver) (1/2)
[INFO][2018-05-30 10:04:21,195][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 1.0 (TID 3) in 152 ms on localhost (executor driver) (2/2)
[INFO][2018-05-30 10:04:21,196][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO][2018-05-30 10:04:21,198][org.apache.spark.scheduler.DAGScheduler]ResultStage 1 (collect at NetTypeUtils.scala:62) finished in 0.155 s
[INFO][2018-05-30 10:04:21,201][org.apache.spark.scheduler.DAGScheduler]Job 1 finished: collect at NetTypeUtils.scala:62, took 0.173335 s
[INFO][2018-05-30 10:04:21,209][org.apache.spark.storage.memory.MemoryStore]Block broadcast_5 stored as values in memory (estimated size 588.5 KB, free 911.1 MB)
[INFO][2018-05-30 10:04:21,236][org.apache.spark.storage.memory.MemoryStore]Block broadcast_5_piece0 stored as bytes in memory (estimated size 184.5 KB, free 910.9 MB)
[INFO][2018-05-30 10:04:21,237][org.apache.spark.storage.BlockManagerInfo]Added broadcast_5_piece0 in memory on 10.194.67.11:52180 (size: 184.5 KB, free: 912.1 MB)
[INFO][2018-05-30 10:04:21,241][org.apache.spark.SparkContext]Created broadcast 5 from broadcast at SalesNetAvgMount.scala:48
[INFO][2018-05-30 10:04:21,253][org.apache.spark.storage.memory.MemoryStore]Block broadcast_6 stored as values in memory (estimated size 228.1 KB, free 910.7 MB)
[INFO][2018-05-30 10:04:21,282][org.apache.spark.storage.memory.MemoryStore]Block broadcast_6_piece0 stored as bytes in memory (estimated size 22.1 KB, free 910.7 MB)
[INFO][2018-05-30 10:04:21,282][org.apache.spark.storage.BlockManagerInfo]Added broadcast_6_piece0 in memory on 10.194.67.11:52180 (size: 22.1 KB, free: 912.0 MB)
[INFO][2018-05-30 10:04:21,283][org.apache.spark.SparkContext]Created broadcast 6 from textFile at NetTypeUtils.scala:27
[INFO][2018-05-30 10:04:21,319][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 1
[INFO][2018-05-30 10:04:21,340][org.apache.spark.SparkContext]Starting job: collect at NetTypeUtils.scala:36
[INFO][2018-05-30 10:04:21,342][org.apache.spark.scheduler.DAGScheduler]Got job 2 (collect at NetTypeUtils.scala:36) with 2 output partitions
[INFO][2018-05-30 10:04:21,342][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 2 (collect at NetTypeUtils.scala:36)
[INFO][2018-05-30 10:04:21,342][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-30 10:04:21,342][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-30 10:04:21,343][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 2 (MapPartitionsRDD[10] at mapPartitions at NetTypeUtils.scala:29), which has no missing parents
[INFO][2018-05-30 10:04:21,345][org.apache.spark.storage.memory.MemoryStore]Block broadcast_7 stored as values in memory (estimated size 3.7 KB, free 910.7 MB)
[INFO][2018-05-30 10:04:21,352][org.apache.spark.storage.memory.MemoryStore]Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.0 KB, free 910.7 MB)
[INFO][2018-05-30 10:04:21,353][org.apache.spark.storage.BlockManagerInfo]Added broadcast_7_piece0 in memory on 10.194.67.11:52180 (size: 2.0 KB, free: 912.0 MB)
[INFO][2018-05-30 10:04:21,354][org.apache.spark.SparkContext]Created broadcast 7 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-30 10:04:21,357][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ResultStage 2 (MapPartitionsRDD[10] at mapPartitions at NetTypeUtils.scala:29) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2018-05-30 10:04:21,357][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 2.0 with 2 tasks
[INFO][2018-05-30 10:04:21,359][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 2.0 (TID 4, localhost, executor driver, partition 0, ANY, 4934 bytes)
[INFO][2018-05-30 10:04:21,360][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 2.0 (TID 5, localhost, executor driver, partition 1, ANY, 4934 bytes)
[INFO][2018-05-30 10:04:21,361][org.apache.spark.executor.Executor]Running task 0.0 in stage 2.0 (TID 4)
[INFO][2018-05-30 10:04:21,361][org.apache.spark.executor.Executor]Running task 1.0 in stage 2.0 (TID 5)
[INFO][2018-05-30 10:04:21,366][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/seven/data/pointData/part-00000-71581736-630f-4fbd-834f-e5e6283d573b-c000.csv:334642+334643
[INFO][2018-05-30 10:04:21,366][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/seven/data/pointData/part-00000-71581736-630f-4fbd-834f-e5e6283d573b-c000.csv:0+334642
[INFO][2018-05-30 10:04:21,562][org.apache.spark.executor.Executor]Finished task 0.0 in stage 2.0 (TID 4). 392358 bytes result sent to driver
[INFO][2018-05-30 10:04:21,570][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 2.0 (TID 4) in 212 ms on localhost (executor driver) (1/2)
[INFO][2018-05-30 10:04:21,573][org.apache.spark.executor.Executor]Finished task 1.0 in stage 2.0 (TID 5). 392427 bytes result sent to driver
[INFO][2018-05-30 10:04:21,581][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 2.0 (TID 5) in 222 ms on localhost (executor driver) (2/2)
[INFO][2018-05-30 10:04:21,581][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO][2018-05-30 10:04:21,582][org.apache.spark.scheduler.DAGScheduler]ResultStage 2 (collect at NetTypeUtils.scala:36) finished in 0.224 s
[INFO][2018-05-30 10:04:21,582][org.apache.spark.scheduler.DAGScheduler]Job 2 finished: collect at NetTypeUtils.scala:36, took 0.241437 s
[INFO][2018-05-30 10:04:21,593][org.apache.spark.storage.memory.MemoryStore]Block broadcast_8 stored as values in memory (estimated size 2.1 MB, free 908.6 MB)
[INFO][2018-05-30 10:04:21,620][org.apache.spark.storage.memory.MemoryStore]Block broadcast_8_piece0 stored as bytes in memory (estimated size 323.9 KB, free 908.3 MB)
[INFO][2018-05-30 10:04:21,621][org.apache.spark.storage.BlockManagerInfo]Added broadcast_8_piece0 in memory on 10.194.67.11:52180 (size: 323.9 KB, free: 911.7 MB)
[INFO][2018-05-30 10:04:21,623][org.apache.spark.SparkContext]Created broadcast 8 from broadcast at SalesNetAvgMount.scala:51
[INFO][2018-05-30 10:04:21,629][org.apache.spark.storage.memory.MemoryStore]Block broadcast_9 stored as values in memory (estimated size 228.1 KB, free 908.1 MB)
[INFO][2018-05-30 10:04:21,651][org.apache.spark.storage.memory.MemoryStore]Block broadcast_9_piece0 stored as bytes in memory (estimated size 22.1 KB, free 908.0 MB)
[INFO][2018-05-30 10:04:21,653][org.apache.spark.storage.BlockManagerInfo]Added broadcast_9_piece0 in memory on 10.194.67.11:52180 (size: 22.1 KB, free: 911.7 MB)
[INFO][2018-05-30 10:04:21,655][org.apache.spark.SparkContext]Created broadcast 9 from textFile at SalesNetAvgMount.scala:109
[INFO][2018-05-30 10:04:21,731][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 1
[INFO][2018-05-30 10:04:21,951][org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter]File Output Committer Algorithm version is 1
[INFO][2018-05-30 10:04:22,036][org.apache.spark.SparkContext]Starting job: saveAsTextFile at Utils.java:25
[INFO][2018-05-30 10:04:22,047][org.apache.spark.scheduler.DAGScheduler]Registering RDD 14 (mapPartitions at SalesNetAvgMount.scala:117)
[INFO][2018-05-30 10:04:22,047][org.apache.spark.scheduler.DAGScheduler]Registering RDD 17 (repartition at Utils.java:25)
[INFO][2018-05-30 10:04:22,048][org.apache.spark.scheduler.DAGScheduler]Got job 3 (saveAsTextFile at Utils.java:25) with 1 output partitions
[INFO][2018-05-30 10:04:22,048][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 5 (saveAsTextFile at Utils.java:25)
[INFO][2018-05-30 10:04:22,048][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 4)
[INFO][2018-05-30 10:04:22,048][org.apache.spark.scheduler.DAGScheduler]Missing parents: List(ShuffleMapStage 4)
[INFO][2018-05-30 10:04:22,052][org.apache.spark.scheduler.DAGScheduler]Submitting ShuffleMapStage 3 (MapPartitionsRDD[14] at mapPartitions at SalesNetAvgMount.scala:117), which has no missing parents
[INFO][2018-05-30 10:04:22,069][org.apache.spark.storage.memory.MemoryStore]Block broadcast_10 stored as values in memory (estimated size 48.1 KB, free 908.0 MB)
[INFO][2018-05-30 10:04:22,077][org.apache.spark.storage.memory.MemoryStore]Block broadcast_10_piece0 stored as bytes in memory (estimated size 21.5 KB, free 908.0 MB)
[INFO][2018-05-30 10:04:22,079][org.apache.spark.storage.BlockManagerInfo]Added broadcast_10_piece0 in memory on 10.194.67.11:52180 (size: 21.5 KB, free: 911.7 MB)
[INFO][2018-05-30 10:04:22,080][org.apache.spark.SparkContext]Created broadcast 10 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-30 10:04:22,083][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[14] at mapPartitions at SalesNetAvgMount.scala:117) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2018-05-30 10:04:22,083][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 3.0 with 2 tasks
[INFO][2018-05-30 10:04:22,089][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 3.0 (TID 6, localhost, executor driver, partition 0, ANY, 4872 bytes)
[INFO][2018-05-30 10:04:22,089][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 3.0 (TID 7, localhost, executor driver, partition 1, ANY, 4872 bytes)
[INFO][2018-05-30 10:04:22,089][org.apache.spark.executor.Executor]Running task 0.0 in stage 3.0 (TID 6)
[INFO][2018-05-30 10:04:22,090][org.apache.spark.executor.Executor]Running task 1.0 in stage 3.0 (TID 7)
[INFO][2018-05-30 10:04:22,110][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/part-00000:0+12229859
[INFO][2018-05-30 10:04:22,110][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/part-00000:12229859+12229860
[INFO][2018-05-30 10:04:22,222][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_7_piece0 on 10.194.67.11:52180 in memory (size: 2.0 KB, free: 911.7 MB)
[INFO][2018-05-30 10:04:22,224][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_4_piece0 on 10.194.67.11:52180 in memory (size: 2.0 KB, free: 911.7 MB)
[INFO][2018-05-30 10:04:22,227][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_3_piece0 on 10.194.67.11:52180 in memory (size: 22.1 KB, free: 911.7 MB)
[INFO][2018-05-30 10:04:22,228][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_6_piece0 on 10.194.67.11:52180 in memory (size: 22.1 KB, free: 911.7 MB)
[INFO][2018-05-30 10:04:23,993][org.apache.spark.executor.Executor]Finished task 0.0 in stage 3.0 (TID 6). 1085 bytes result sent to driver
[INFO][2018-05-30 10:04:24,015][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 3.0 (TID 6) in 1929 ms on localhost (executor driver) (1/2)
[INFO][2018-05-30 10:04:24,281][org.apache.spark.executor.Executor]Finished task 1.0 in stage 3.0 (TID 7). 1085 bytes result sent to driver
[INFO][2018-05-30 10:04:24,282][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 3.0 (TID 7) in 2193 ms on localhost (executor driver) (2/2)
[INFO][2018-05-30 10:04:24,282][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 3.0, whose tasks have all completed, from pool 
[INFO][2018-05-30 10:04:24,283][org.apache.spark.scheduler.DAGScheduler]ShuffleMapStage 3 (mapPartitions at SalesNetAvgMount.scala:117) finished in 2.198 s
[INFO][2018-05-30 10:04:24,284][org.apache.spark.scheduler.DAGScheduler]looking for newly runnable stages
[INFO][2018-05-30 10:04:24,284][org.apache.spark.scheduler.DAGScheduler]running: Set()
[INFO][2018-05-30 10:04:24,285][org.apache.spark.scheduler.DAGScheduler]waiting: Set(ResultStage 5, ShuffleMapStage 4)
[INFO][2018-05-30 10:04:24,285][org.apache.spark.scheduler.DAGScheduler]failed: Set()
[INFO][2018-05-30 10:04:24,290][org.apache.spark.scheduler.DAGScheduler]Submitting ShuffleMapStage 4 (MapPartitionsRDD[17] at repartition at Utils.java:25), which has no missing parents
[INFO][2018-05-30 10:04:24,298][org.apache.spark.storage.memory.MemoryStore]Block broadcast_11 stored as values in memory (estimated size 47.9 KB, free 908.4 MB)
[INFO][2018-05-30 10:04:24,301][org.apache.spark.storage.memory.MemoryStore]Block broadcast_11_piece0 stored as bytes in memory (estimated size 21.3 KB, free 908.4 MB)
[INFO][2018-05-30 10:04:24,302][org.apache.spark.storage.BlockManagerInfo]Added broadcast_11_piece0 in memory on 10.194.67.11:52180 (size: 21.3 KB, free: 911.7 MB)
[INFO][2018-05-30 10:04:24,303][org.apache.spark.SparkContext]Created broadcast 11 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-30 10:04:24,303][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[17] at repartition at Utils.java:25) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2018-05-30 10:04:24,303][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 4.0 with 2 tasks
[INFO][2018-05-30 10:04:24,305][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 4.0 (TID 8, localhost, executor driver, partition 0, ANY, 4610 bytes)
[INFO][2018-05-30 10:04:24,305][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 4.0 (TID 9, localhost, executor driver, partition 1, ANY, 4610 bytes)
[INFO][2018-05-30 10:04:24,305][org.apache.spark.executor.Executor]Running task 0.0 in stage 4.0 (TID 8)
[INFO][2018-05-30 10:04:24,305][org.apache.spark.executor.Executor]Running task 1.0 in stage 4.0 (TID 9)
[INFO][2018-05-30 10:04:24,331][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-30 10:04:24,331][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-30 10:04:24,333][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 6 ms
[INFO][2018-05-30 10:04:24,333][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 6 ms
[INFO][2018-05-30 10:04:24,467][org.apache.spark.storage.memory.MemoryStore]Block rdd_16_0 stored as values in memory (estimated size 86.9 KB, free 908.3 MB)
[INFO][2018-05-30 10:04:24,467][org.apache.spark.storage.BlockManagerInfo]Added rdd_16_0 in memory on 10.194.67.11:52180 (size: 86.9 KB, free: 911.6 MB)
[INFO][2018-05-30 10:04:24,468][org.apache.spark.storage.memory.MemoryStore]Block rdd_16_1 stored as values in memory (estimated size 87.1 KB, free 908.2 MB)
[INFO][2018-05-30 10:04:24,468][org.apache.spark.storage.BlockManagerInfo]Added rdd_16_1 in memory on 10.194.67.11:52180 (size: 87.1 KB, free: 911.5 MB)
[INFO][2018-05-30 10:04:24,503][org.apache.spark.executor.Executor]Finished task 0.0 in stage 4.0 (TID 8). 1910 bytes result sent to driver
[INFO][2018-05-30 10:04:24,503][org.apache.spark.executor.Executor]Finished task 1.0 in stage 4.0 (TID 9). 1910 bytes result sent to driver
[INFO][2018-05-30 10:04:24,504][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 4.0 (TID 8) in 200 ms on localhost (executor driver) (1/2)
[INFO][2018-05-30 10:04:24,505][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 4.0 (TID 9) in 199 ms on localhost (executor driver) (2/2)
[INFO][2018-05-30 10:04:24,505][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 4.0, whose tasks have all completed, from pool 
[INFO][2018-05-30 10:04:24,506][org.apache.spark.scheduler.DAGScheduler]ShuffleMapStage 4 (repartition at Utils.java:25) finished in 0.202 s
[INFO][2018-05-30 10:04:24,506][org.apache.spark.scheduler.DAGScheduler]looking for newly runnable stages
[INFO][2018-05-30 10:04:24,506][org.apache.spark.scheduler.DAGScheduler]running: Set()
[INFO][2018-05-30 10:04:24,506][org.apache.spark.scheduler.DAGScheduler]waiting: Set(ResultStage 5)
[INFO][2018-05-30 10:04:24,506][org.apache.spark.scheduler.DAGScheduler]failed: Set()
[INFO][2018-05-30 10:04:24,507][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 5 (MapPartitionsRDD[21] at saveAsTextFile at Utils.java:25), which has no missing parents
[INFO][2018-05-30 10:04:24,528][org.apache.spark.storage.memory.MemoryStore]Block broadcast_12 stored as values in memory (estimated size 70.0 KB, free 908.2 MB)
[INFO][2018-05-30 10:04:24,531][org.apache.spark.storage.memory.MemoryStore]Block broadcast_12_piece0 stored as bytes in memory (estimated size 25.4 KB, free 908.1 MB)
[INFO][2018-05-30 10:04:24,536][org.apache.spark.storage.BlockManagerInfo]Added broadcast_12_piece0 in memory on 10.194.67.11:52180 (size: 25.4 KB, free: 911.5 MB)
[INFO][2018-05-30 10:04:24,538][org.apache.spark.SparkContext]Created broadcast 12 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-30 10:04:24,539][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[21] at saveAsTextFile at Utils.java:25) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-30 10:04:24,539][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 5.0 with 1 tasks
[INFO][2018-05-30 10:04:24,553][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 5.0 (TID 10, localhost, executor driver, partition 0, ANY, 4897 bytes)
[INFO][2018-05-30 10:04:24,554][org.apache.spark.executor.Executor]Running task 0.0 in stage 5.0 (TID 10)
[INFO][2018-05-30 10:04:24,595][org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter]File Output Committer Algorithm version is 1
[INFO][2018-05-30 10:04:24,641][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-30 10:04:24,642][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO][2018-05-30 10:04:24,979][org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter]Saved output of task 'attempt_20180530100421_0005_m_000000_10' to hdfs://vm-xaj-bigdata-da-d01:8020/yst/seven/data/orderData/SalesNetAvgMount/_temporary/0/task_20180530100421_0005_m_000000
[INFO][2018-05-30 10:04:24,980][org.apache.spark.mapred.SparkHadoopMapRedUtil]attempt_20180530100421_0005_m_000000_10: Committed
[INFO][2018-05-30 10:04:24,984][org.apache.spark.executor.Executor]Finished task 0.0 in stage 5.0 (TID 10). 1010 bytes result sent to driver
[INFO][2018-05-30 10:04:24,985][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 5.0 (TID 10) in 445 ms on localhost (executor driver) (1/1)
[INFO][2018-05-30 10:04:24,985][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 5.0, whose tasks have all completed, from pool 
[INFO][2018-05-30 10:04:24,986][org.apache.spark.scheduler.DAGScheduler]ResultStage 5 (saveAsTextFile at Utils.java:25) finished in 0.446 s
[INFO][2018-05-30 10:04:24,987][org.apache.spark.scheduler.DAGScheduler]Job 3 finished: saveAsTextFile at Utils.java:25, took 2.950176 s
[INFO][2018-05-30 10:04:25,079][org.apache.spark.storage.memory.MemoryStore]Block broadcast_13 stored as values in memory (estimated size 228.1 KB, free 907.9 MB)
[INFO][2018-05-30 10:04:25,097][org.apache.spark.storage.memory.MemoryStore]Block broadcast_13_piece0 stored as bytes in memory (estimated size 22.1 KB, free 907.9 MB)
[INFO][2018-05-30 10:04:25,097][org.apache.spark.storage.BlockManagerInfo]Added broadcast_13_piece0 in memory on 10.194.67.11:52180 (size: 22.1 KB, free: 911.5 MB)
[INFO][2018-05-30 10:04:25,098][org.apache.spark.SparkContext]Created broadcast 13 from textFile at SalesNetAvgMount.scala:388
[INFO][2018-05-30 10:04:25,123][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 1
[INFO][2018-05-30 10:04:25,140][org.apache.spark.SparkContext]Starting job: collect at SalesNetAvgMount.scala:405
[INFO][2018-05-30 10:04:25,140][org.apache.spark.scheduler.DAGScheduler]Registering RDD 25 (mapPartitions at SalesNetAvgMount.scala:396)
[INFO][2018-05-30 10:04:25,141][org.apache.spark.scheduler.DAGScheduler]Got job 4 (collect at SalesNetAvgMount.scala:405) with 2 output partitions
[INFO][2018-05-30 10:04:25,141][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 7 (collect at SalesNetAvgMount.scala:405)
[INFO][2018-05-30 10:04:25,141][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 6)
[INFO][2018-05-30 10:04:25,141][org.apache.spark.scheduler.DAGScheduler]Missing parents: List(ShuffleMapStage 6)
[INFO][2018-05-30 10:04:25,141][org.apache.spark.scheduler.DAGScheduler]Submitting ShuffleMapStage 6 (MapPartitionsRDD[25] at mapPartitions at SalesNetAvgMount.scala:396), which has no missing parents
[INFO][2018-05-30 10:04:25,145][org.apache.spark.storage.memory.MemoryStore]Block broadcast_14 stored as values in memory (estimated size 48.1 KB, free 907.8 MB)
[INFO][2018-05-30 10:04:25,148][org.apache.spark.storage.memory.MemoryStore]Block broadcast_14_piece0 stored as bytes in memory (estimated size 21.5 KB, free 907.8 MB)
[INFO][2018-05-30 10:04:25,149][org.apache.spark.storage.BlockManagerInfo]Added broadcast_14_piece0 in memory on 10.194.67.11:52180 (size: 21.5 KB, free: 911.5 MB)
[INFO][2018-05-30 10:04:25,149][org.apache.spark.SparkContext]Created broadcast 14 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-30 10:04:25,150][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[25] at mapPartitions at SalesNetAvgMount.scala:396) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2018-05-30 10:04:25,150][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 6.0 with 2 tasks
[INFO][2018-05-30 10:04:25,151][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 6.0 (TID 11, localhost, executor driver, partition 0, ANY, 4872 bytes)
[INFO][2018-05-30 10:04:25,151][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 6.0 (TID 12, localhost, executor driver, partition 1, ANY, 4872 bytes)
[INFO][2018-05-30 10:04:25,151][org.apache.spark.executor.Executor]Running task 1.0 in stage 6.0 (TID 12)
[INFO][2018-05-30 10:04:25,151][org.apache.spark.executor.Executor]Running task 0.0 in stage 6.0 (TID 11)
[INFO][2018-05-30 10:04:25,157][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/part-00000:12229859+12229860
[INFO][2018-05-30 10:04:25,157][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/part-00000:0+12229859
[INFO][2018-05-30 10:04:25,744][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_11_piece0 on 10.194.67.11:52180 in memory (size: 21.3 KB, free: 911.5 MB)
[INFO][2018-05-30 10:04:25,747][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_12_piece0 on 10.194.67.11:52180 in memory (size: 25.4 KB, free: 911.5 MB)
[INFO][2018-05-30 10:04:26,519][org.apache.spark.executor.Executor]Finished task 0.0 in stage 6.0 (TID 11). 1085 bytes result sent to driver
[INFO][2018-05-30 10:04:26,520][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 6.0 (TID 11) in 1369 ms on localhost (executor driver) (1/2)
[INFO][2018-05-30 10:04:27,129][org.apache.spark.executor.Executor]Finished task 1.0 in stage 6.0 (TID 12). 1085 bytes result sent to driver
[INFO][2018-05-30 10:04:27,129][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 6.0 (TID 12) in 1978 ms on localhost (executor driver) (2/2)
[INFO][2018-05-30 10:04:27,129][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 6.0, whose tasks have all completed, from pool 
[INFO][2018-05-30 10:04:27,130][org.apache.spark.scheduler.DAGScheduler]ShuffleMapStage 6 (mapPartitions at SalesNetAvgMount.scala:396) finished in 1.980 s
[INFO][2018-05-30 10:04:27,130][org.apache.spark.scheduler.DAGScheduler]looking for newly runnable stages
[INFO][2018-05-30 10:04:27,130][org.apache.spark.scheduler.DAGScheduler]running: Set()
[INFO][2018-05-30 10:04:27,130][org.apache.spark.scheduler.DAGScheduler]waiting: Set(ResultStage 7)
[INFO][2018-05-30 10:04:27,130][org.apache.spark.scheduler.DAGScheduler]failed: Set()
[INFO][2018-05-30 10:04:27,131][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 7 (ShuffledRDD[26] at reduceByKey at SalesNetAvgMount.scala:405), which has no missing parents
[INFO][2018-05-30 10:04:27,133][org.apache.spark.storage.memory.MemoryStore]Block broadcast_15 stored as values in memory (estimated size 3.2 KB, free 908.0 MB)
[INFO][2018-05-30 10:04:27,136][org.apache.spark.storage.memory.MemoryStore]Block broadcast_15_piece0 stored as bytes in memory (estimated size 1953.0 B, free 908.0 MB)
[INFO][2018-05-30 10:04:27,136][org.apache.spark.storage.BlockManagerInfo]Added broadcast_15_piece0 in memory on 10.194.67.11:52180 (size: 1953.0 B, free: 911.5 MB)
[INFO][2018-05-30 10:04:27,136][org.apache.spark.SparkContext]Created broadcast 15 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-30 10:04:27,137][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ResultStage 7 (ShuffledRDD[26] at reduceByKey at SalesNetAvgMount.scala:405) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2018-05-30 10:04:27,137][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 7.0 with 2 tasks
[INFO][2018-05-30 10:04:27,138][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 7.0 (TID 13, localhost, executor driver, partition 0, ANY, 4621 bytes)
[INFO][2018-05-30 10:04:27,138][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 7.0 (TID 14, localhost, executor driver, partition 1, ANY, 4621 bytes)
[INFO][2018-05-30 10:04:27,138][org.apache.spark.executor.Executor]Running task 0.0 in stage 7.0 (TID 13)
[INFO][2018-05-30 10:04:27,138][org.apache.spark.executor.Executor]Running task 1.0 in stage 7.0 (TID 14)
[INFO][2018-05-30 10:04:27,140][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-30 10:04:27,141][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO][2018-05-30 10:04:27,140][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-30 10:04:27,141][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO][2018-05-30 10:04:27,155][org.apache.spark.executor.Executor]Finished task 1.0 in stage 7.0 (TID 14). 5755 bytes result sent to driver
[INFO][2018-05-30 10:04:27,156][org.apache.spark.executor.Executor]Finished task 0.0 in stage 7.0 (TID 13). 5372 bytes result sent to driver
[INFO][2018-05-30 10:04:27,157][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 7.0 (TID 14) in 19 ms on localhost (executor driver) (1/2)
[INFO][2018-05-30 10:04:27,157][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 7.0 (TID 13) in 19 ms on localhost (executor driver) (2/2)
[INFO][2018-05-30 10:04:27,157][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 7.0, whose tasks have all completed, from pool 
[INFO][2018-05-30 10:04:27,158][org.apache.spark.scheduler.DAGScheduler]ResultStage 7 (collect at SalesNetAvgMount.scala:405) finished in 0.021 s
[INFO][2018-05-30 10:04:27,158][org.apache.spark.scheduler.DAGScheduler]Job 4 finished: collect at SalesNetAvgMount.scala:405, took 2.018605 s
[INFO][2018-05-30 10:04:27,162][org.apache.spark.storage.memory.MemoryStore]Block broadcast_16 stored as values in memory (estimated size 228.1 KB, free 907.8 MB)
[INFO][2018-05-30 10:04:27,175][org.apache.spark.storage.memory.MemoryStore]Block broadcast_16_piece0 stored as bytes in memory (estimated size 22.1 KB, free 907.7 MB)
[INFO][2018-05-30 10:04:27,175][org.apache.spark.storage.BlockManagerInfo]Added broadcast_16_piece0 in memory on 10.194.67.11:52180 (size: 22.1 KB, free: 911.5 MB)
[INFO][2018-05-30 10:04:27,176][org.apache.spark.SparkContext]Created broadcast 16 from textFile at SalesNetAvgMount.scala:412
[INFO][2018-05-30 10:04:27,231][org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter]File Output Committer Algorithm version is 1
[INFO][2018-05-30 10:04:27,259][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 1
[INFO][2018-05-30 10:04:27,291][org.apache.spark.SparkContext]Starting job: saveAsTextFile at Utils.java:25
[INFO][2018-05-30 10:04:27,292][org.apache.spark.scheduler.DAGScheduler]Registering RDD 31 (repartition at Utils.java:25)
[INFO][2018-05-30 10:04:27,292][org.apache.spark.scheduler.DAGScheduler]Got job 5 (saveAsTextFile at Utils.java:25) with 1 output partitions
[INFO][2018-05-30 10:04:27,292][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 9 (saveAsTextFile at Utils.java:25)
[INFO][2018-05-30 10:04:27,292][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 8)
[INFO][2018-05-30 10:04:27,293][org.apache.spark.scheduler.DAGScheduler]Missing parents: List(ShuffleMapStage 8)
[INFO][2018-05-30 10:04:27,294][org.apache.spark.scheduler.DAGScheduler]Submitting ShuffleMapStage 8 (MapPartitionsRDD[31] at repartition at Utils.java:25), which has no missing parents
[INFO][2018-05-30 10:04:27,297][org.apache.spark.storage.memory.MemoryStore]Block broadcast_17 stored as values in memory (estimated size 59.2 KB, free 907.7 MB)
[INFO][2018-05-30 10:04:27,300][org.apache.spark.storage.memory.MemoryStore]Block broadcast_17_piece0 stored as bytes in memory (estimated size 28.7 KB, free 907.6 MB)
[INFO][2018-05-30 10:04:27,301][org.apache.spark.storage.BlockManagerInfo]Added broadcast_17_piece0 in memory on 10.194.67.11:52180 (size: 28.7 KB, free: 911.5 MB)
[INFO][2018-05-30 10:04:27,301][org.apache.spark.SparkContext]Created broadcast 17 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-30 10:04:27,302][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ShuffleMapStage 8 (MapPartitionsRDD[31] at repartition at Utils.java:25) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2018-05-30 10:04:27,302][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 8.0 with 2 tasks
[INFO][2018-05-30 10:04:27,303][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 8.0 (TID 15, localhost, executor driver, partition 0, ANY, 4921 bytes)
[INFO][2018-05-30 10:04:27,303][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 8.0 (TID 16, localhost, executor driver, partition 1, ANY, 4921 bytes)
[INFO][2018-05-30 10:04:27,303][org.apache.spark.executor.Executor]Running task 0.0 in stage 8.0 (TID 15)
[INFO][2018-05-30 10:04:27,303][org.apache.spark.executor.Executor]Running task 1.0 in stage 8.0 (TID 16)
[INFO][2018-05-30 10:04:27,309][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/seven/data/netData/part-00000-5ea613c3-08fd-4310-b01a-b33de863caa2-c000.csv:160327+160327
[INFO][2018-05-30 10:04:27,309][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/seven/data/netData/part-00000-5ea613c3-08fd-4310-b01a-b33de863caa2-c000.csv:0+160327
[INFO][2018-05-30 10:04:27,428][org.apache.spark.storage.memory.MemoryStore]Block rdd_30_1 stored as values in memory (estimated size 114.3 KB, free 907.5 MB)
[INFO][2018-05-30 10:04:27,428][org.apache.spark.storage.BlockManagerInfo]Added rdd_30_1 in memory on 10.194.67.11:52180 (size: 114.3 KB, free: 911.3 MB)
[INFO][2018-05-30 10:04:27,448][org.apache.spark.executor.Executor]Finished task 1.0 in stage 8.0 (TID 16). 1609 bytes result sent to driver
[INFO][2018-05-30 10:04:27,450][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 8.0 (TID 16) in 147 ms on localhost (executor driver) (1/2)
[INFO][2018-05-30 10:04:27,460][org.apache.spark.storage.memory.MemoryStore]Block rdd_30_0 stored as values in memory (estimated size 109.4 KB, free 907.4 MB)
[INFO][2018-05-30 10:04:27,460][org.apache.spark.storage.BlockManagerInfo]Added rdd_30_0 in memory on 10.194.67.11:52180 (size: 109.4 KB, free: 911.2 MB)
[INFO][2018-05-30 10:04:27,472][org.apache.spark.executor.Executor]Finished task 0.0 in stage 8.0 (TID 15). 1609 bytes result sent to driver
[INFO][2018-05-30 10:04:27,473][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 8.0 (TID 15) in 171 ms on localhost (executor driver) (2/2)
[INFO][2018-05-30 10:04:27,473][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 8.0, whose tasks have all completed, from pool 
[INFO][2018-05-30 10:04:27,474][org.apache.spark.scheduler.DAGScheduler]ShuffleMapStage 8 (repartition at Utils.java:25) finished in 0.172 s
[INFO][2018-05-30 10:04:27,474][org.apache.spark.scheduler.DAGScheduler]looking for newly runnable stages
[INFO][2018-05-30 10:04:27,474][org.apache.spark.scheduler.DAGScheduler]running: Set()
[INFO][2018-05-30 10:04:27,474][org.apache.spark.scheduler.DAGScheduler]waiting: Set(ResultStage 9)
[INFO][2018-05-30 10:04:27,474][org.apache.spark.scheduler.DAGScheduler]failed: Set()
[INFO][2018-05-30 10:04:27,474][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 9 (MapPartitionsRDD[35] at saveAsTextFile at Utils.java:25), which has no missing parents
[INFO][2018-05-30 10:04:27,491][org.apache.spark.storage.memory.MemoryStore]Block broadcast_18 stored as values in memory (estimated size 70.0 KB, free 907.4 MB)
[INFO][2018-05-30 10:04:27,498][org.apache.spark.storage.memory.MemoryStore]Block broadcast_18_piece0 stored as bytes in memory (estimated size 25.4 KB, free 907.3 MB)
[INFO][2018-05-30 10:04:27,499][org.apache.spark.storage.BlockManagerInfo]Added broadcast_18_piece0 in memory on 10.194.67.11:52180 (size: 25.4 KB, free: 911.2 MB)
[INFO][2018-05-30 10:04:27,499][org.apache.spark.SparkContext]Created broadcast 18 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-30 10:04:27,500][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[35] at saveAsTextFile at Utils.java:25) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-30 10:04:27,500][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 9.0 with 1 tasks
[INFO][2018-05-30 10:04:27,500][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 9.0 (TID 17, localhost, executor driver, partition 0, ANY, 4897 bytes)
[INFO][2018-05-30 10:04:27,501][org.apache.spark.executor.Executor]Running task 0.0 in stage 9.0 (TID 17)
[INFO][2018-05-30 10:04:27,510][org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter]File Output Committer Algorithm version is 1
[INFO][2018-05-30 10:04:27,523][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-30 10:04:27,523][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-30 10:04:27,689][org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter]Saved output of task 'attempt_20180530100427_0009_m_000000_17' to hdfs://vm-xaj-bigdata-da-d01:8020/yst/seven/data/orderData/getOrderNullDataNet/_temporary/0/task_20180530100427_0009_m_000000
[INFO][2018-05-30 10:04:27,690][org.apache.spark.mapred.SparkHadoopMapRedUtil]attempt_20180530100427_0009_m_000000_17: Committed
[INFO][2018-05-30 10:04:27,701][org.apache.spark.executor.Executor]Finished task 0.0 in stage 9.0 (TID 17). 1010 bytes result sent to driver
[INFO][2018-05-30 10:04:27,704][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 9.0 (TID 17) in 204 ms on localhost (executor driver) (1/1)
[INFO][2018-05-30 10:04:27,704][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 9.0, whose tasks have all completed, from pool 
[INFO][2018-05-30 10:04:27,705][org.apache.spark.scheduler.DAGScheduler]ResultStage 9 (saveAsTextFile at Utils.java:25) finished in 0.205 s
[INFO][2018-05-30 10:04:27,705][org.apache.spark.scheduler.DAGScheduler]Job 5 finished: saveAsTextFile at Utils.java:25, took 0.413958 s
[INFO][2018-05-30 10:04:27,846][com.seven.spark.rdd.SalesNetAvgMount$]job is success spend time is 0:00:10.889
[INFO][2018-05-30 10:04:27,849][org.apache.spark.SparkContext]Invoking stop() from shutdown hook
[INFO][2018-05-30 10:04:27,860][org.spark_project.jetty.server.AbstractConnector]Stopped Spark@1d18db87{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-05-30 10:04:27,862][org.apache.spark.ui.SparkUI]Stopped Spark web UI at http://10.194.67.11:4040
[INFO][2018-05-30 10:04:27,874][org.apache.spark.MapOutputTrackerMasterEndpoint]MapOutputTrackerMasterEndpoint stopped!
[INFO][2018-05-30 10:04:27,892][org.apache.spark.storage.memory.MemoryStore]MemoryStore cleared
[INFO][2018-05-30 10:04:27,892][org.apache.spark.storage.BlockManager]BlockManager stopped
[INFO][2018-05-30 10:04:27,894][org.apache.spark.storage.BlockManagerMaster]BlockManagerMaster stopped
[INFO][2018-05-30 10:04:27,897][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint]OutputCommitCoordinator stopped!
[INFO][2018-05-30 10:04:27,898][org.apache.spark.SparkContext]Successfully stopped SparkContext
[INFO][2018-05-30 10:04:27,898][org.apache.spark.util.ShutdownHookManager]Shutdown hook called
[INFO][2018-05-30 10:04:27,899][org.apache.spark.util.ShutdownHookManager]Deleting directory /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/spark-ae157845-7782-457c-bce2-1cf9ed2d454e
[INFO][2018-05-30 10:11:22,978][org.apache.spark.SparkContext]Running Spark version 2.2.0
[INFO][2018-05-30 10:11:24,055][org.apache.spark.SparkContext]Submitted application: SalesNetAvgMount$
[INFO][2018-05-30 10:11:24,121][org.apache.spark.SecurityManager]Changing view acls to: seven
[INFO][2018-05-30 10:11:24,122][org.apache.spark.SecurityManager]Changing modify acls to: seven
[INFO][2018-05-30 10:11:24,125][org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO][2018-05-30 10:11:24,126][org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO][2018-05-30 10:11:24,126][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(seven); groups with view permissions: Set(); users  with modify permissions: Set(seven); groups with modify permissions: Set()
[INFO][2018-05-30 10:11:24,633][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 52273.
[INFO][2018-05-30 10:11:24,672][org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO][2018-05-30 10:11:24,712][org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO][2018-05-30 10:11:24,716][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2018-05-30 10:11:24,717][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
[INFO][2018-05-30 10:11:24,735][org.apache.spark.storage.DiskBlockManager]Created local directory at /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/blockmgr-1aefa3bd-95de-4913-afc9-23c713a89704
[INFO][2018-05-30 10:11:24,801][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 912.3 MB
[INFO][2018-05-30 10:11:24,897][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO][2018-05-30 10:11:25,039][org.spark_project.jetty.util.log]Logging initialized @4785ms
[INFO][2018-05-30 10:11:25,144][org.spark_project.jetty.server.Server]jetty-9.3.z-SNAPSHOT
[INFO][2018-05-30 10:11:25,164][org.spark_project.jetty.server.Server]Started @4910ms
[INFO][2018-05-30 10:11:25,200][org.spark_project.jetty.server.AbstractConnector]Started ServerConnector@ea2d7b0{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-05-30 10:11:25,201][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4040.
[INFO][2018-05-30 10:11:25,231][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@222afc67{/jobs,null,AVAILABLE,@Spark}
[INFO][2018-05-30 10:11:25,231][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@d8d9199{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2018-05-30 10:11:25,232][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@602ae7b6{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2018-05-30 10:11:25,233][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@700f518a{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2018-05-30 10:11:25,234][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@13da7ab0{/stages,null,AVAILABLE,@Spark}
[INFO][2018-05-30 10:11:25,236][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@260ff5b7{/stages/json,null,AVAILABLE,@Spark}
[INFO][2018-05-30 10:11:25,238][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@77eb5790{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2018-05-30 10:11:25,241][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@81b5db0{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-30 10:11:25,242][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7139bd31{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2018-05-30 10:11:25,242][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4b3fe06e{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2018-05-30 10:11:25,243][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3e17a0a1{/storage,null,AVAILABLE,@Spark}
[INFO][2018-05-30 10:11:25,244][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4d8286c4{/storage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-30 10:11:25,246][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@161f6623{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2018-05-30 10:11:25,247][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6778aea6{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2018-05-30 10:11:25,248][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@69228e85{/environment,null,AVAILABLE,@Spark}
[INFO][2018-05-30 10:11:25,249][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5853495b{/environment/json,null,AVAILABLE,@Spark}
[INFO][2018-05-30 10:11:25,249][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2f61d591{/executors,null,AVAILABLE,@Spark}
[INFO][2018-05-30 10:11:25,259][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7173ae5b{/executors/json,null,AVAILABLE,@Spark}
[INFO][2018-05-30 10:11:25,260][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@53a9fcfd{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2018-05-30 10:11:25,261][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4d192aef{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2018-05-30 10:11:25,270][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@84487f4{/static,null,AVAILABLE,@Spark}
[INFO][2018-05-30 10:11:25,271][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@56913163{/,null,AVAILABLE,@Spark}
[INFO][2018-05-30 10:11:25,272][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@a18649a{/api,null,AVAILABLE,@Spark}
[INFO][2018-05-30 10:11:25,273][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@14229fa7{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-30 10:11:25,274][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7158daf2{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-30 10:11:25,277][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://10.194.67.11:4040
[INFO][2018-05-30 10:11:25,413][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
[INFO][2018-05-30 10:11:25,502][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 52274.
[INFO][2018-05-30 10:11:25,507][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 10.194.67.11:52274
[INFO][2018-05-30 10:11:25,515][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2018-05-30 10:11:25,520][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 10.194.67.11, 52274, None)
[INFO][2018-05-30 10:11:25,525][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 10.194.67.11:52274 with 912.3 MB RAM, BlockManagerId(driver, 10.194.67.11, 52274, None)
[INFO][2018-05-30 10:11:25,537][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 10.194.67.11, 52274, None)
[INFO][2018-05-30 10:11:25,538][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 10.194.67.11, 52274, None)
[INFO][2018-05-30 10:11:26,002][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3206174f{/metrics/json,null,AVAILABLE,@Spark}
[INFO][2018-05-30 10:11:26,048][com.seven.spark.rdd.SalesNetAvgMount$]job is start . . . 
[INFO][2018-05-30 10:11:29,158][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0 stored as values in memory (estimated size 228.1 KB, free 912.1 MB)
[INFO][2018-05-30 10:11:29,465][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0_piece0 stored as bytes in memory (estimated size 22.1 KB, free 912.1 MB)
[INFO][2018-05-30 10:11:29,468][org.apache.spark.storage.BlockManagerInfo]Added broadcast_0_piece0 in memory on 10.194.67.11:52274 (size: 22.1 KB, free: 912.3 MB)
[INFO][2018-05-30 10:11:29,475][org.apache.spark.SparkContext]Created broadcast 0 from textFile at SalesNetAvgMount.scala:71
[WARN][2018-05-30 10:11:31,768][org.apache.hadoop.util.NativeCodeLoader]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO][2018-05-30 10:11:31,971][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 1
[INFO][2018-05-30 10:11:32,098][org.apache.spark.SparkContext]Starting job: collect at SalesNetAvgMount.scala:88
[INFO][2018-05-30 10:11:32,122][org.apache.spark.scheduler.DAGScheduler]Got job 0 (collect at SalesNetAvgMount.scala:88) with 2 output partitions
[INFO][2018-05-30 10:11:32,123][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 0 (collect at SalesNetAvgMount.scala:88)
[INFO][2018-05-30 10:11:32,124][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-30 10:11:32,127][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-30 10:11:32,140][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 0 (MapPartitionsRDD[2] at mapPartitions at SalesNetAvgMount.scala:71), which has no missing parents
[INFO][2018-05-30 10:11:32,176][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1 stored as values in memory (estimated size 3.5 KB, free 912.1 MB)
[INFO][2018-05-30 10:11:32,187][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1_piece0 stored as bytes in memory (estimated size 2028.0 B, free 912.1 MB)
[INFO][2018-05-30 10:11:32,188][org.apache.spark.storage.BlockManagerInfo]Added broadcast_1_piece0 in memory on 10.194.67.11:52274 (size: 2028.0 B, free: 912.3 MB)
[INFO][2018-05-30 10:11:32,191][org.apache.spark.SparkContext]Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-30 10:11:32,213][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at mapPartitions at SalesNetAvgMount.scala:71) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2018-05-30 10:11:32,215][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 0.0 with 2 tasks
[INFO][2018-05-30 10:11:32,302][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 4886 bytes)
[INFO][2018-05-30 10:11:32,306][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 4886 bytes)
[INFO][2018-05-30 10:11:32,318][org.apache.spark.executor.Executor]Running task 0.0 in stage 0.0 (TID 0)
[INFO][2018-05-30 10:11:32,318][org.apache.spark.executor.Executor]Running task 1.0 in stage 0.0 (TID 1)
[INFO][2018-05-30 10:11:32,380][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/operate/N/main/part-00000:0+34172
[INFO][2018-05-30 10:11:32,380][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/operate/N/main/part-00000:34172+34172
[INFO][2018-05-30 10:11:32,618][org.apache.spark.executor.Executor]Finished task 0.0 in stage 0.0 (TID 0). 17518 bytes result sent to driver
[INFO][2018-05-30 10:11:32,627][org.apache.spark.executor.Executor]Finished task 1.0 in stage 0.0 (TID 1). 17494 bytes result sent to driver
[INFO][2018-05-30 10:11:32,639][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 0.0 (TID 0) in 355 ms on localhost (executor driver) (1/2)
[INFO][2018-05-30 10:11:32,641][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 0.0 (TID 1) in 336 ms on localhost (executor driver) (2/2)
[INFO][2018-05-30 10:11:32,642][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO][2018-05-30 10:11:32,650][org.apache.spark.scheduler.DAGScheduler]ResultStage 0 (collect at SalesNetAvgMount.scala:88) finished in 0.401 s
[INFO][2018-05-30 10:11:32,671][org.apache.spark.scheduler.DAGScheduler]Job 0 finished: collect at SalesNetAvgMount.scala:88, took 0.572108 s
[INFO][2018-05-30 10:11:33,041][org.apache.spark.storage.memory.MemoryStore]Block broadcast_2 stored as values in memory (estimated size 110.1 KB, free 911.9 MB)
[INFO][2018-05-30 10:11:33,051][org.apache.spark.storage.memory.MemoryStore]Block broadcast_2_piece0 stored as bytes in memory (estimated size 17.5 KB, free 911.9 MB)
[INFO][2018-05-30 10:11:33,052][org.apache.spark.storage.BlockManagerInfo]Added broadcast_2_piece0 in memory on 10.194.67.11:52274 (size: 17.5 KB, free: 912.3 MB)
[INFO][2018-05-30 10:11:33,053][org.apache.spark.SparkContext]Created broadcast 2 from broadcast at SalesNetAvgMount.scala:45
[INFO][2018-05-30 10:11:37,751][org.apache.spark.storage.memory.MemoryStore]Block broadcast_3 stored as values in memory (estimated size 228.1 KB, free 911.7 MB)
[INFO][2018-05-30 10:11:37,769][org.apache.spark.storage.memory.MemoryStore]Block broadcast_3_piece0 stored as bytes in memory (estimated size 22.1 KB, free 911.7 MB)
[INFO][2018-05-30 10:11:37,770][org.apache.spark.storage.BlockManagerInfo]Added broadcast_3_piece0 in memory on 10.194.67.11:52274 (size: 22.1 KB, free: 912.2 MB)
[INFO][2018-05-30 10:11:37,771][org.apache.spark.SparkContext]Created broadcast 3 from textFile at NetTypeUtils.scala:53
[INFO][2018-05-30 10:11:37,813][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 1
[INFO][2018-05-30 10:11:37,836][org.apache.spark.SparkContext]Starting job: collect at NetTypeUtils.scala:62
[INFO][2018-05-30 10:11:37,837][org.apache.spark.scheduler.DAGScheduler]Got job 1 (collect at NetTypeUtils.scala:62) with 2 output partitions
[INFO][2018-05-30 10:11:37,837][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 1 (collect at NetTypeUtils.scala:62)
[INFO][2018-05-30 10:11:37,837][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-30 10:11:37,838][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-30 10:11:37,838][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 1 (MapPartitionsRDD[6] at mapPartitions at NetTypeUtils.scala:55), which has no missing parents
[INFO][2018-05-30 10:11:37,843][org.apache.spark.storage.memory.MemoryStore]Block broadcast_4 stored as values in memory (estimated size 3.7 KB, free 911.7 MB)
[INFO][2018-05-30 10:11:37,854][org.apache.spark.storage.memory.MemoryStore]Block broadcast_4_piece0 stored as bytes in memory (estimated size 2.0 KB, free 911.7 MB)
[INFO][2018-05-30 10:11:37,857][org.apache.spark.storage.BlockManagerInfo]Added broadcast_4_piece0 in memory on 10.194.67.11:52274 (size: 2.0 KB, free: 912.2 MB)
[INFO][2018-05-30 10:11:37,857][org.apache.spark.SparkContext]Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-30 10:11:37,859][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at mapPartitions at NetTypeUtils.scala:55) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2018-05-30 10:11:37,860][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 1.0 with 2 tasks
[INFO][2018-05-30 10:11:37,862][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 1.0 (TID 2, localhost, executor driver, partition 0, ANY, 4932 bytes)
[INFO][2018-05-30 10:11:37,862][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 1.0 (TID 3, localhost, executor driver, partition 1, ANY, 4932 bytes)
[INFO][2018-05-30 10:11:37,863][org.apache.spark.executor.Executor]Running task 0.0 in stage 1.0 (TID 2)
[INFO][2018-05-30 10:11:37,863][org.apache.spark.executor.Executor]Running task 1.0 in stage 1.0 (TID 3)
[INFO][2018-05-30 10:11:37,872][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/seven/data/netData/part-00000-5ea613c3-08fd-4310-b01a-b33de863caa2-c000.csv:160327+160327
[INFO][2018-05-30 10:11:37,872][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/seven/data/netData/part-00000-5ea613c3-08fd-4310-b01a-b33de863caa2-c000.csv:0+160327
[INFO][2018-05-30 10:11:37,984][org.apache.spark.executor.Executor]Finished task 1.0 in stage 1.0 (TID 3). 176889 bytes result sent to driver
[INFO][2018-05-30 10:11:37,989][org.apache.spark.executor.Executor]Finished task 0.0 in stage 1.0 (TID 2). 175704 bytes result sent to driver
[INFO][2018-05-30 10:11:38,005][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 1.0 (TID 3) in 143 ms on localhost (executor driver) (1/2)
[INFO][2018-05-30 10:11:38,005][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 1.0 (TID 2) in 144 ms on localhost (executor driver) (2/2)
[INFO][2018-05-30 10:11:38,005][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO][2018-05-30 10:11:38,006][org.apache.spark.scheduler.DAGScheduler]ResultStage 1 (collect at NetTypeUtils.scala:62) finished in 0.145 s
[INFO][2018-05-30 10:11:38,007][org.apache.spark.scheduler.DAGScheduler]Job 1 finished: collect at NetTypeUtils.scala:62, took 0.170374 s
[INFO][2018-05-30 10:11:38,014][org.apache.spark.storage.memory.MemoryStore]Block broadcast_5 stored as values in memory (estimated size 588.5 KB, free 911.1 MB)
[INFO][2018-05-30 10:11:38,025][org.apache.spark.storage.memory.MemoryStore]Block broadcast_5_piece0 stored as bytes in memory (estimated size 184.5 KB, free 910.9 MB)
[INFO][2018-05-30 10:11:38,026][org.apache.spark.storage.BlockManagerInfo]Added broadcast_5_piece0 in memory on 10.194.67.11:52274 (size: 184.5 KB, free: 912.1 MB)
[INFO][2018-05-30 10:11:38,027][org.apache.spark.SparkContext]Created broadcast 5 from broadcast at SalesNetAvgMount.scala:48
[INFO][2018-05-30 10:11:38,032][org.apache.spark.storage.memory.MemoryStore]Block broadcast_6 stored as values in memory (estimated size 228.1 KB, free 910.7 MB)
[INFO][2018-05-30 10:11:38,050][org.apache.spark.storage.memory.MemoryStore]Block broadcast_6_piece0 stored as bytes in memory (estimated size 22.1 KB, free 910.7 MB)
[INFO][2018-05-30 10:11:38,051][org.apache.spark.storage.BlockManagerInfo]Added broadcast_6_piece0 in memory on 10.194.67.11:52274 (size: 22.1 KB, free: 912.0 MB)
[INFO][2018-05-30 10:11:38,052][org.apache.spark.SparkContext]Created broadcast 6 from textFile at NetTypeUtils.scala:27
[INFO][2018-05-30 10:11:38,080][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 1
[INFO][2018-05-30 10:11:38,101][org.apache.spark.SparkContext]Starting job: collect at NetTypeUtils.scala:36
[INFO][2018-05-30 10:11:38,103][org.apache.spark.scheduler.DAGScheduler]Got job 2 (collect at NetTypeUtils.scala:36) with 2 output partitions
[INFO][2018-05-30 10:11:38,103][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 2 (collect at NetTypeUtils.scala:36)
[INFO][2018-05-30 10:11:38,103][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-30 10:11:38,103][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-30 10:11:38,104][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 2 (MapPartitionsRDD[10] at mapPartitions at NetTypeUtils.scala:29), which has no missing parents
[INFO][2018-05-30 10:11:38,106][org.apache.spark.storage.memory.MemoryStore]Block broadcast_7 stored as values in memory (estimated size 3.7 KB, free 910.7 MB)
[INFO][2018-05-30 10:11:38,114][org.apache.spark.storage.memory.MemoryStore]Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.0 KB, free 910.7 MB)
[INFO][2018-05-30 10:11:38,114][org.apache.spark.storage.BlockManagerInfo]Added broadcast_7_piece0 in memory on 10.194.67.11:52274 (size: 2.0 KB, free: 912.0 MB)
[INFO][2018-05-30 10:11:38,115][org.apache.spark.SparkContext]Created broadcast 7 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-30 10:11:38,116][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ResultStage 2 (MapPartitionsRDD[10] at mapPartitions at NetTypeUtils.scala:29) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2018-05-30 10:11:38,116][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 2.0 with 2 tasks
[INFO][2018-05-30 10:11:38,117][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 2.0 (TID 4, localhost, executor driver, partition 0, ANY, 4934 bytes)
[INFO][2018-05-30 10:11:38,118][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 2.0 (TID 5, localhost, executor driver, partition 1, ANY, 4934 bytes)
[INFO][2018-05-30 10:11:38,118][org.apache.spark.executor.Executor]Running task 0.0 in stage 2.0 (TID 4)
[INFO][2018-05-30 10:11:38,118][org.apache.spark.executor.Executor]Running task 1.0 in stage 2.0 (TID 5)
[INFO][2018-05-30 10:11:38,123][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/seven/data/pointData/part-00000-71581736-630f-4fbd-834f-e5e6283d573b-c000.csv:334642+334643
[INFO][2018-05-30 10:11:38,125][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/seven/data/pointData/part-00000-71581736-630f-4fbd-834f-e5e6283d573b-c000.csv:0+334642
[INFO][2018-05-30 10:11:38,291][org.apache.spark.executor.Executor]Finished task 0.0 in stage 2.0 (TID 4). 392358 bytes result sent to driver
[INFO][2018-05-30 10:11:38,311][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 2.0 (TID 4) in 194 ms on localhost (executor driver) (1/2)
[INFO][2018-05-30 10:11:38,313][org.apache.spark.executor.Executor]Finished task 1.0 in stage 2.0 (TID 5). 392470 bytes result sent to driver
[INFO][2018-05-30 10:11:38,320][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 2.0 (TID 5) in 203 ms on localhost (executor driver) (2/2)
[INFO][2018-05-30 10:11:38,320][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO][2018-05-30 10:11:38,321][org.apache.spark.scheduler.DAGScheduler]ResultStage 2 (collect at NetTypeUtils.scala:36) finished in 0.204 s
[INFO][2018-05-30 10:11:38,321][org.apache.spark.scheduler.DAGScheduler]Job 2 finished: collect at NetTypeUtils.scala:36, took 0.219370 s
[INFO][2018-05-30 10:11:38,329][org.apache.spark.storage.memory.MemoryStore]Block broadcast_8 stored as values in memory (estimated size 2.1 MB, free 908.6 MB)
[INFO][2018-05-30 10:11:38,340][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_4_piece0 on 10.194.67.11:52274 in memory (size: 2.0 KB, free: 912.0 MB)
[INFO][2018-05-30 10:11:38,345][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_3_piece0 on 10.194.67.11:52274 in memory (size: 22.1 KB, free: 912.1 MB)
[INFO][2018-05-30 10:11:38,354][org.apache.spark.storage.memory.MemoryStore]Block broadcast_8_piece0 stored as bytes in memory (estimated size 323.9 KB, free 908.5 MB)
[INFO][2018-05-30 10:11:38,354][org.apache.spark.storage.BlockManagerInfo]Added broadcast_8_piece0 in memory on 10.194.67.11:52274 (size: 323.9 KB, free: 911.7 MB)
[INFO][2018-05-30 10:11:38,355][org.apache.spark.SparkContext]Created broadcast 8 from broadcast at SalesNetAvgMount.scala:51
[INFO][2018-05-30 10:11:39,676][org.apache.spark.storage.memory.MemoryStore]Block broadcast_9 stored as values in memory (estimated size 228.1 KB, free 908.3 MB)
[INFO][2018-05-30 10:11:39,699][org.apache.spark.storage.memory.MemoryStore]Block broadcast_9_piece0 stored as bytes in memory (estimated size 22.1 KB, free 908.3 MB)
[INFO][2018-05-30 10:11:39,700][org.apache.spark.storage.BlockManagerInfo]Added broadcast_9_piece0 in memory on 10.194.67.11:52274 (size: 22.1 KB, free: 911.7 MB)
[INFO][2018-05-30 10:11:39,701][org.apache.spark.SparkContext]Created broadcast 9 from textFile at SalesNetAvgMount.scala:109
[INFO][2018-05-30 10:11:40,499][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 1
[INFO][2018-05-30 10:11:40,723][org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter]File Output Committer Algorithm version is 1
[INFO][2018-05-30 10:11:40,785][org.apache.spark.SparkContext]Starting job: saveAsTextFile at Utils.java:25
[INFO][2018-05-30 10:11:40,796][org.apache.spark.scheduler.DAGScheduler]Registering RDD 14 (mapPartitions at SalesNetAvgMount.scala:117)
[INFO][2018-05-30 10:11:40,797][org.apache.spark.scheduler.DAGScheduler]Registering RDD 17 (repartition at Utils.java:25)
[INFO][2018-05-30 10:11:40,797][org.apache.spark.scheduler.DAGScheduler]Got job 3 (saveAsTextFile at Utils.java:25) with 1 output partitions
[INFO][2018-05-30 10:11:40,797][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 5 (saveAsTextFile at Utils.java:25)
[INFO][2018-05-30 10:11:40,797][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 4)
[INFO][2018-05-30 10:11:40,797][org.apache.spark.scheduler.DAGScheduler]Missing parents: List(ShuffleMapStage 4)
[INFO][2018-05-30 10:11:40,801][org.apache.spark.scheduler.DAGScheduler]Submitting ShuffleMapStage 3 (MapPartitionsRDD[14] at mapPartitions at SalesNetAvgMount.scala:117), which has no missing parents
[INFO][2018-05-30 10:11:40,818][org.apache.spark.storage.memory.MemoryStore]Block broadcast_10 stored as values in memory (estimated size 48.1 KB, free 908.2 MB)
[INFO][2018-05-30 10:11:40,822][org.apache.spark.storage.memory.MemoryStore]Block broadcast_10_piece0 stored as bytes in memory (estimated size 21.5 KB, free 908.2 MB)
[INFO][2018-05-30 10:11:40,823][org.apache.spark.storage.BlockManagerInfo]Added broadcast_10_piece0 in memory on 10.194.67.11:52274 (size: 21.5 KB, free: 911.7 MB)
[INFO][2018-05-30 10:11:40,823][org.apache.spark.SparkContext]Created broadcast 10 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-30 10:11:40,826][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[14] at mapPartitions at SalesNetAvgMount.scala:117) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2018-05-30 10:11:40,826][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 3.0 with 2 tasks
[INFO][2018-05-30 10:11:40,828][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 3.0 (TID 6, localhost, executor driver, partition 0, ANY, 4872 bytes)
[INFO][2018-05-30 10:11:40,828][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 3.0 (TID 7, localhost, executor driver, partition 1, ANY, 4872 bytes)
[INFO][2018-05-30 10:11:40,828][org.apache.spark.executor.Executor]Running task 0.0 in stage 3.0 (TID 6)
[INFO][2018-05-30 10:11:40,828][org.apache.spark.executor.Executor]Running task 1.0 in stage 3.0 (TID 7)
[INFO][2018-05-30 10:11:40,842][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/part-00000:12229859+12229860
[INFO][2018-05-30 10:11:40,842][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/part-00000:0+12229859
[INFO][2018-05-30 10:11:42,009][org.apache.spark.executor.Executor]Finished task 1.0 in stage 3.0 (TID 7). 1128 bytes result sent to driver
[INFO][2018-05-30 10:11:42,035][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 3.0 (TID 7) in 1207 ms on localhost (executor driver) (1/2)
[INFO][2018-05-30 10:11:51,037][org.apache.spark.executor.Executor]Finished task 0.0 in stage 3.0 (TID 6). 1085 bytes result sent to driver
[INFO][2018-05-30 10:11:51,038][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 3.0 (TID 6) in 10211 ms on localhost (executor driver) (2/2)
[INFO][2018-05-30 10:11:51,039][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 3.0, whose tasks have all completed, from pool 
[INFO][2018-05-30 10:11:51,039][org.apache.spark.scheduler.DAGScheduler]ShuffleMapStage 3 (mapPartitions at SalesNetAvgMount.scala:117) finished in 10.212 s
[INFO][2018-05-30 10:11:51,040][org.apache.spark.scheduler.DAGScheduler]looking for newly runnable stages
[INFO][2018-05-30 10:11:51,041][org.apache.spark.scheduler.DAGScheduler]running: Set()
[INFO][2018-05-30 10:11:51,042][org.apache.spark.scheduler.DAGScheduler]waiting: Set(ResultStage 5, ShuffleMapStage 4)
[INFO][2018-05-30 10:11:51,043][org.apache.spark.scheduler.DAGScheduler]failed: Set()
[INFO][2018-05-30 10:11:51,048][org.apache.spark.scheduler.DAGScheduler]Submitting ShuffleMapStage 4 (MapPartitionsRDD[17] at repartition at Utils.java:25), which has no missing parents
[INFO][2018-05-30 10:11:51,055][org.apache.spark.storage.memory.MemoryStore]Block broadcast_11 stored as values in memory (estimated size 47.9 KB, free 908.2 MB)
[INFO][2018-05-30 10:11:51,059][org.apache.spark.storage.memory.MemoryStore]Block broadcast_11_piece0 stored as bytes in memory (estimated size 21.3 KB, free 908.1 MB)
[INFO][2018-05-30 10:11:51,060][org.apache.spark.storage.BlockManagerInfo]Added broadcast_11_piece0 in memory on 10.194.67.11:52274 (size: 21.3 KB, free: 911.7 MB)
[INFO][2018-05-30 10:11:51,060][org.apache.spark.SparkContext]Created broadcast 11 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-30 10:11:51,061][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[17] at repartition at Utils.java:25) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2018-05-30 10:11:51,061][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 4.0 with 2 tasks
[INFO][2018-05-30 10:11:51,063][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 4.0 (TID 8, localhost, executor driver, partition 0, ANY, 4610 bytes)
[INFO][2018-05-30 10:11:51,063][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 4.0 (TID 9, localhost, executor driver, partition 1, ANY, 4610 bytes)
[INFO][2018-05-30 10:11:51,063][org.apache.spark.executor.Executor]Running task 0.0 in stage 4.0 (TID 8)
[INFO][2018-05-30 10:11:51,063][org.apache.spark.executor.Executor]Running task 1.0 in stage 4.0 (TID 9)
[INFO][2018-05-30 10:11:51,091][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-30 10:11:51,091][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-30 10:11:51,094][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 8 ms
[INFO][2018-05-30 10:11:51,094][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 8 ms
[INFO][2018-05-30 10:11:53,719][org.apache.spark.storage.memory.MemoryStore]Block rdd_16_1 stored as values in memory (estimated size 87.0 KB, free 908.0 MB)
[INFO][2018-05-30 10:11:53,719][org.apache.spark.storage.memory.MemoryStore]Block rdd_16_0 stored as values in memory (estimated size 86.9 KB, free 908.0 MB)
[INFO][2018-05-30 10:11:53,720][org.apache.spark.storage.BlockManagerInfo]Added rdd_16_1 in memory on 10.194.67.11:52274 (size: 87.0 KB, free: 911.6 MB)
[INFO][2018-05-30 10:11:53,721][org.apache.spark.storage.BlockManagerInfo]Added rdd_16_0 in memory on 10.194.67.11:52274 (size: 86.9 KB, free: 911.5 MB)
[INFO][2018-05-30 10:11:53,759][org.apache.spark.executor.Executor]Finished task 0.0 in stage 4.0 (TID 8). 1953 bytes result sent to driver
[INFO][2018-05-30 10:11:53,759][org.apache.spark.executor.Executor]Finished task 1.0 in stage 4.0 (TID 9). 1953 bytes result sent to driver
[INFO][2018-05-30 10:11:53,760][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 4.0 (TID 8) in 2698 ms on localhost (executor driver) (1/2)
[INFO][2018-05-30 10:11:53,761][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 4.0 (TID 9) in 2697 ms on localhost (executor driver) (2/2)
[INFO][2018-05-30 10:11:53,761][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 4.0, whose tasks have all completed, from pool 
[INFO][2018-05-30 10:11:53,761][org.apache.spark.scheduler.DAGScheduler]ShuffleMapStage 4 (repartition at Utils.java:25) finished in 2.699 s
[INFO][2018-05-30 10:11:53,762][org.apache.spark.scheduler.DAGScheduler]looking for newly runnable stages
[INFO][2018-05-30 10:11:53,762][org.apache.spark.scheduler.DAGScheduler]running: Set()
[INFO][2018-05-30 10:11:53,762][org.apache.spark.scheduler.DAGScheduler]waiting: Set(ResultStage 5)
[INFO][2018-05-30 10:11:53,762][org.apache.spark.scheduler.DAGScheduler]failed: Set()
[INFO][2018-05-30 10:11:53,762][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 5 (MapPartitionsRDD[21] at saveAsTextFile at Utils.java:25), which has no missing parents
[INFO][2018-05-30 10:11:53,778][org.apache.spark.storage.memory.MemoryStore]Block broadcast_12 stored as values in memory (estimated size 70.0 KB, free 907.9 MB)
[INFO][2018-05-30 10:11:53,782][org.apache.spark.storage.memory.MemoryStore]Block broadcast_12_piece0 stored as bytes in memory (estimated size 25.4 KB, free 907.9 MB)
[INFO][2018-05-30 10:11:53,783][org.apache.spark.storage.BlockManagerInfo]Added broadcast_12_piece0 in memory on 10.194.67.11:52274 (size: 25.4 KB, free: 911.5 MB)
[INFO][2018-05-30 10:11:53,784][org.apache.spark.SparkContext]Created broadcast 12 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-30 10:11:53,784][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[21] at saveAsTextFile at Utils.java:25) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-30 10:11:53,784][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 5.0 with 1 tasks
[INFO][2018-05-30 10:11:53,810][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 5.0 (TID 10, localhost, executor driver, partition 0, ANY, 4897 bytes)
[INFO][2018-05-30 10:11:53,810][org.apache.spark.executor.Executor]Running task 0.0 in stage 5.0 (TID 10)
[INFO][2018-05-30 10:11:53,844][org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter]File Output Committer Algorithm version is 1
[INFO][2018-05-30 10:11:53,897][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-30 10:11:53,898][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO][2018-05-30 10:11:54,265][org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter]Saved output of task 'attempt_20180530101140_0005_m_000000_10' to hdfs://vm-xaj-bigdata-da-d01:8020/yst/seven/data/orderData/SalesNetAvgMount/_temporary/0/task_20180530101140_0005_m_000000
[INFO][2018-05-30 10:11:54,266][org.apache.spark.mapred.SparkHadoopMapRedUtil]attempt_20180530101140_0005_m_000000_10: Committed
[INFO][2018-05-30 10:11:54,271][org.apache.spark.executor.Executor]Finished task 0.0 in stage 5.0 (TID 10). 1010 bytes result sent to driver
[INFO][2018-05-30 10:11:54,272][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 5.0 (TID 10) in 487 ms on localhost (executor driver) (1/1)
[INFO][2018-05-30 10:11:54,272][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 5.0, whose tasks have all completed, from pool 
[INFO][2018-05-30 10:11:54,273][org.apache.spark.scheduler.DAGScheduler]ResultStage 5 (saveAsTextFile at Utils.java:25) finished in 0.487 s
[INFO][2018-05-30 10:11:54,273][org.apache.spark.scheduler.DAGScheduler]Job 3 finished: saveAsTextFile at Utils.java:25, took 13.487325 s
[INFO][2018-05-30 10:11:54,368][org.apache.spark.storage.memory.MemoryStore]Block broadcast_13 stored as values in memory (estimated size 228.1 KB, free 907.7 MB)
[INFO][2018-05-30 10:11:54,385][org.apache.spark.storage.memory.MemoryStore]Block broadcast_13_piece0 stored as bytes in memory (estimated size 22.1 KB, free 907.6 MB)
[INFO][2018-05-30 10:11:54,386][org.apache.spark.storage.BlockManagerInfo]Added broadcast_13_piece0 in memory on 10.194.67.11:52274 (size: 22.1 KB, free: 911.5 MB)
[INFO][2018-05-30 10:11:54,386][org.apache.spark.SparkContext]Created broadcast 13 from textFile at SalesNetAvgMount.scala:388
[INFO][2018-05-30 10:11:54,811][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 1
[INFO][2018-05-30 10:11:54,836][org.apache.spark.SparkContext]Starting job: collect at SalesNetAvgMount.scala:405
[INFO][2018-05-30 10:11:54,837][org.apache.spark.scheduler.DAGScheduler]Registering RDD 25 (mapPartitions at SalesNetAvgMount.scala:396)
[INFO][2018-05-30 10:11:54,837][org.apache.spark.scheduler.DAGScheduler]Got job 4 (collect at SalesNetAvgMount.scala:405) with 2 output partitions
[INFO][2018-05-30 10:11:54,837][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 7 (collect at SalesNetAvgMount.scala:405)
[INFO][2018-05-30 10:11:54,837][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 6)
[INFO][2018-05-30 10:11:54,838][org.apache.spark.scheduler.DAGScheduler]Missing parents: List(ShuffleMapStage 6)
[INFO][2018-05-30 10:11:54,838][org.apache.spark.scheduler.DAGScheduler]Submitting ShuffleMapStage 6 (MapPartitionsRDD[25] at mapPartitions at SalesNetAvgMount.scala:396), which has no missing parents
[INFO][2018-05-30 10:11:54,841][org.apache.spark.storage.memory.MemoryStore]Block broadcast_14 stored as values in memory (estimated size 48.1 KB, free 907.6 MB)
[INFO][2018-05-30 10:11:54,845][org.apache.spark.storage.memory.MemoryStore]Block broadcast_14_piece0 stored as bytes in memory (estimated size 21.5 KB, free 907.6 MB)
[INFO][2018-05-30 10:11:54,846][org.apache.spark.storage.BlockManagerInfo]Added broadcast_14_piece0 in memory on 10.194.67.11:52274 (size: 21.5 KB, free: 911.4 MB)
[INFO][2018-05-30 10:11:54,846][org.apache.spark.SparkContext]Created broadcast 14 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-30 10:11:54,847][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[25] at mapPartitions at SalesNetAvgMount.scala:396) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2018-05-30 10:11:54,847][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 6.0 with 2 tasks
[INFO][2018-05-30 10:11:54,848][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 6.0 (TID 11, localhost, executor driver, partition 0, ANY, 4872 bytes)
[INFO][2018-05-30 10:11:54,848][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 6.0 (TID 12, localhost, executor driver, partition 1, ANY, 4872 bytes)
[INFO][2018-05-30 10:11:54,849][org.apache.spark.executor.Executor]Running task 0.0 in stage 6.0 (TID 11)
[INFO][2018-05-30 10:11:54,849][org.apache.spark.executor.Executor]Running task 1.0 in stage 6.0 (TID 12)
[INFO][2018-05-30 10:11:54,853][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/part-00000:12229859+12229860
[INFO][2018-05-30 10:11:54,855][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/part-00000:0+12229859
[INFO][2018-05-30 10:11:55,319][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_12_piece0 on 10.194.67.11:52274 in memory (size: 25.4 KB, free: 911.5 MB)
[INFO][2018-05-30 10:11:55,321][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_11_piece0 on 10.194.67.11:52274 in memory (size: 21.3 KB, free: 911.5 MB)
[INFO][2018-05-30 10:11:56,534][org.apache.spark.executor.Executor]Finished task 0.0 in stage 6.0 (TID 11). 1085 bytes result sent to driver
[INFO][2018-05-30 10:11:56,535][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 6.0 (TID 11) in 1687 ms on localhost (executor driver) (1/2)
[INFO][2018-05-30 10:11:56,815][org.apache.spark.executor.Executor]Finished task 1.0 in stage 6.0 (TID 12). 1085 bytes result sent to driver
[INFO][2018-05-30 10:11:56,816][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 6.0 (TID 12) in 1968 ms on localhost (executor driver) (2/2)
[INFO][2018-05-30 10:11:56,816][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 6.0, whose tasks have all completed, from pool 
[INFO][2018-05-30 10:11:56,817][org.apache.spark.scheduler.DAGScheduler]ShuffleMapStage 6 (mapPartitions at SalesNetAvgMount.scala:396) finished in 1.970 s
[INFO][2018-05-30 10:11:56,817][org.apache.spark.scheduler.DAGScheduler]looking for newly runnable stages
[INFO][2018-05-30 10:11:56,817][org.apache.spark.scheduler.DAGScheduler]running: Set()
[INFO][2018-05-30 10:11:56,817][org.apache.spark.scheduler.DAGScheduler]waiting: Set(ResultStage 7)
[INFO][2018-05-30 10:11:56,817][org.apache.spark.scheduler.DAGScheduler]failed: Set()
[INFO][2018-05-30 10:11:56,817][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 7 (ShuffledRDD[26] at reduceByKey at SalesNetAvgMount.scala:405), which has no missing parents
[INFO][2018-05-30 10:11:56,819][org.apache.spark.storage.memory.MemoryStore]Block broadcast_15 stored as values in memory (estimated size 3.2 KB, free 907.7 MB)
[INFO][2018-05-30 10:11:56,824][org.apache.spark.storage.memory.MemoryStore]Block broadcast_15_piece0 stored as bytes in memory (estimated size 1953.0 B, free 907.7 MB)
[INFO][2018-05-30 10:11:56,824][org.apache.spark.storage.BlockManagerInfo]Added broadcast_15_piece0 in memory on 10.194.67.11:52274 (size: 1953.0 B, free: 911.5 MB)
[INFO][2018-05-30 10:11:56,825][org.apache.spark.SparkContext]Created broadcast 15 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-30 10:11:56,827][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ResultStage 7 (ShuffledRDD[26] at reduceByKey at SalesNetAvgMount.scala:405) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2018-05-30 10:11:56,827][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 7.0 with 2 tasks
[INFO][2018-05-30 10:11:56,828][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 7.0 (TID 13, localhost, executor driver, partition 0, ANY, 4621 bytes)
[INFO][2018-05-30 10:11:56,829][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 7.0 (TID 14, localhost, executor driver, partition 1, ANY, 4621 bytes)
[INFO][2018-05-30 10:11:56,830][org.apache.spark.executor.Executor]Running task 1.0 in stage 7.0 (TID 14)
[INFO][2018-05-30 10:11:56,830][org.apache.spark.executor.Executor]Running task 0.0 in stage 7.0 (TID 13)
[INFO][2018-05-30 10:11:56,849][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-30 10:11:56,849][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-30 10:11:56,849][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-30 10:11:56,849][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-30 10:11:56,866][org.apache.spark.executor.Executor]Finished task 1.0 in stage 7.0 (TID 14). 5755 bytes result sent to driver
[INFO][2018-05-30 10:11:56,867][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 7.0 (TID 14) in 39 ms on localhost (executor driver) (1/2)
[INFO][2018-05-30 10:11:56,868][org.apache.spark.executor.Executor]Finished task 0.0 in stage 7.0 (TID 13). 5415 bytes result sent to driver
[INFO][2018-05-30 10:11:56,869][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 7.0 (TID 13) in 41 ms on localhost (executor driver) (2/2)
[INFO][2018-05-30 10:11:56,869][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 7.0, whose tasks have all completed, from pool 
[INFO][2018-05-30 10:11:56,869][org.apache.spark.scheduler.DAGScheduler]ResultStage 7 (collect at SalesNetAvgMount.scala:405) finished in 0.041 s
[INFO][2018-05-30 10:11:56,870][org.apache.spark.scheduler.DAGScheduler]Job 4 finished: collect at SalesNetAvgMount.scala:405, took 2.033224 s
[INFO][2018-05-30 10:12:59,095][org.apache.spark.storage.memory.MemoryStore]Block broadcast_16 stored as values in memory (estimated size 228.1 KB, free 907.5 MB)
[INFO][2018-05-30 10:12:59,118][org.apache.spark.storage.memory.MemoryStore]Block broadcast_16_piece0 stored as bytes in memory (estimated size 22.1 KB, free 907.5 MB)
[INFO][2018-05-30 10:12:59,118][org.apache.spark.storage.BlockManagerInfo]Added broadcast_16_piece0 in memory on 10.194.67.11:52274 (size: 22.1 KB, free: 911.5 MB)
[INFO][2018-05-30 10:12:59,119][org.apache.spark.SparkContext]Created broadcast 16 from textFile at SalesNetAvgMount.scala:412
[INFO][2018-05-30 10:12:59,180][org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter]File Output Committer Algorithm version is 1
[INFO][2018-05-30 10:12:59,222][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 1
[INFO][2018-05-30 10:12:59,231][org.apache.spark.SparkContext]Invoking stop() from shutdown hook
[INFO][2018-05-30 10:12:59,239][com.seven.spark.rdd.SalesNetAvgMount$]job is success spend time is 0:01:33.189
[INFO][2018-05-30 10:12:59,243][org.spark_project.jetty.server.AbstractConnector]Stopped Spark@ea2d7b0{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-05-30 10:12:59,244][org.apache.spark.ui.SparkUI]Stopped Spark web UI at http://10.194.67.11:4040
[INFO][2018-05-30 10:12:59,257][org.apache.spark.MapOutputTrackerMasterEndpoint]MapOutputTrackerMasterEndpoint stopped!
[INFO][2018-05-30 10:12:59,289][org.apache.spark.storage.memory.MemoryStore]MemoryStore cleared
[INFO][2018-05-30 10:12:59,289][org.apache.spark.storage.BlockManager]BlockManager stopped
[INFO][2018-05-30 10:12:59,290][org.apache.spark.storage.BlockManagerMaster]BlockManagerMaster stopped
[INFO][2018-05-30 10:12:59,293][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint]OutputCommitCoordinator stopped!
[INFO][2018-05-30 10:12:59,294][org.apache.spark.SparkContext]Successfully stopped SparkContext
[INFO][2018-05-30 10:12:59,295][org.apache.spark.util.ShutdownHookManager]Shutdown hook called
[INFO][2018-05-30 10:12:59,296][org.apache.spark.util.ShutdownHookManager]Deleting directory /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/spark-86e2420a-790c-4b14-85ed-e6aa6009cd43
[INFO][2018-05-30 10:14:06,390][org.apache.spark.SparkContext]Running Spark version 2.2.0
[INFO][2018-05-30 10:14:07,567][org.apache.spark.SparkContext]Submitted application: SalesNetAvgMount$
[INFO][2018-05-30 10:14:07,587][org.apache.spark.SecurityManager]Changing view acls to: seven
[INFO][2018-05-30 10:14:07,587][org.apache.spark.SecurityManager]Changing modify acls to: seven
[INFO][2018-05-30 10:14:07,587][org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO][2018-05-30 10:14:07,588][org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO][2018-05-30 10:14:07,589][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(seven); groups with view permissions: Set(); users  with modify permissions: Set(seven); groups with modify permissions: Set()
[INFO][2018-05-30 10:14:07,954][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 52321.
[INFO][2018-05-30 10:14:07,983][org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO][2018-05-30 10:14:08,001][org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO][2018-05-30 10:14:08,004][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2018-05-30 10:14:08,004][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
[INFO][2018-05-30 10:14:08,014][org.apache.spark.storage.DiskBlockManager]Created local directory at /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/blockmgr-02418146-c5bf-4cf2-b6bc-cad6ba3b1f22
[INFO][2018-05-30 10:14:08,031][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 912.3 MB
[INFO][2018-05-30 10:14:08,106][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO][2018-05-30 10:14:08,186][org.spark_project.jetty.util.log]Logging initialized @3032ms
[INFO][2018-05-30 10:14:08,260][org.spark_project.jetty.server.Server]jetty-9.3.z-SNAPSHOT
[INFO][2018-05-30 10:14:08,272][org.spark_project.jetty.server.Server]Started @3119ms
[INFO][2018-05-30 10:14:08,290][org.spark_project.jetty.server.AbstractConnector]Started ServerConnector@34d1162d{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-05-30 10:14:08,290][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4040.
[INFO][2018-05-30 10:14:08,313][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@26fb628{/jobs,null,AVAILABLE,@Spark}
[INFO][2018-05-30 10:14:08,314][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6a62689d{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2018-05-30 10:14:08,315][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@60fa3495{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2018-05-30 10:14:08,316][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@29a60c27{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2018-05-30 10:14:08,316][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@69c79f09{/stages,null,AVAILABLE,@Spark}
[INFO][2018-05-30 10:14:08,317][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5fcacc0{/stages/json,null,AVAILABLE,@Spark}
[INFO][2018-05-30 10:14:08,317][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6d1d4d7{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2018-05-30 10:14:08,319][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6a988392{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-30 10:14:08,319][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5b6813df{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2018-05-30 10:14:08,320][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2b58f754{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2018-05-30 10:14:08,322][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2552f2cb{/storage,null,AVAILABLE,@Spark}
[INFO][2018-05-30 10:14:08,322][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5f3b9c57{/storage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-30 10:14:08,323][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2cf23c81{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2018-05-30 10:14:08,324][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@35fe2125{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2018-05-30 10:14:08,325][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@34645867{/environment,null,AVAILABLE,@Spark}
[INFO][2018-05-30 10:14:08,326][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@60b71e8f{/environment/json,null,AVAILABLE,@Spark}
[INFO][2018-05-30 10:14:08,327][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@464649c{/executors,null,AVAILABLE,@Spark}
[INFO][2018-05-30 10:14:08,328][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5f59185e{/executors/json,null,AVAILABLE,@Spark}
[INFO][2018-05-30 10:14:08,329][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@47da3952{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2018-05-30 10:14:08,329][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@46e8a539{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2018-05-30 10:14:08,338][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5fd62371{/static,null,AVAILABLE,@Spark}
[INFO][2018-05-30 10:14:08,338][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@303e3593{/,null,AVAILABLE,@Spark}
[INFO][2018-05-30 10:14:08,346][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@362a019c{/api,null,AVAILABLE,@Spark}
[INFO][2018-05-30 10:14:08,347][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3f23a3a0{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-30 10:14:08,347][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5fb97279{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-30 10:14:08,350][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://10.194.67.11:4040
[INFO][2018-05-30 10:14:08,437][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
[INFO][2018-05-30 10:14:08,461][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 52322.
[INFO][2018-05-30 10:14:08,462][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 10.194.67.11:52322
[INFO][2018-05-30 10:14:08,467][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2018-05-30 10:14:08,469][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 10.194.67.11, 52322, None)
[INFO][2018-05-30 10:14:08,473][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 10.194.67.11:52322 with 912.3 MB RAM, BlockManagerId(driver, 10.194.67.11, 52322, None)
[INFO][2018-05-30 10:14:08,475][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 10.194.67.11, 52322, None)
[INFO][2018-05-30 10:14:08,476][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 10.194.67.11, 52322, None)
[INFO][2018-05-30 10:14:08,763][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@584f5497{/metrics/json,null,AVAILABLE,@Spark}
[INFO][2018-05-30 10:14:08,795][com.seven.spark.rdd.SalesNetAvgMount$]job is start . . . 
[INFO][2018-05-30 10:14:09,331][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0 stored as values in memory (estimated size 228.1 KB, free 912.1 MB)
[INFO][2018-05-30 10:14:09,578][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0_piece0 stored as bytes in memory (estimated size 22.1 KB, free 912.1 MB)
[INFO][2018-05-30 10:14:09,580][org.apache.spark.storage.BlockManagerInfo]Added broadcast_0_piece0 in memory on 10.194.67.11:52322 (size: 22.1 KB, free: 912.3 MB)
[INFO][2018-05-30 10:14:09,593][org.apache.spark.SparkContext]Created broadcast 0 from textFile at SalesNetAvgMount.scala:71
[WARN][2018-05-30 10:14:11,032][org.apache.hadoop.util.NativeCodeLoader]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO][2018-05-30 10:14:11,132][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 1
[INFO][2018-05-30 10:14:11,205][org.apache.spark.SparkContext]Starting job: collect at SalesNetAvgMount.scala:88
[INFO][2018-05-30 10:14:11,214][org.apache.spark.scheduler.DAGScheduler]Got job 0 (collect at SalesNetAvgMount.scala:88) with 2 output partitions
[INFO][2018-05-30 10:14:11,214][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 0 (collect at SalesNetAvgMount.scala:88)
[INFO][2018-05-30 10:14:11,215][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-30 10:14:11,216][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-30 10:14:11,222][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 0 (MapPartitionsRDD[2] at mapPartitions at SalesNetAvgMount.scala:71), which has no missing parents
[INFO][2018-05-30 10:14:11,238][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1 stored as values in memory (estimated size 3.5 KB, free 912.1 MB)
[INFO][2018-05-30 10:14:11,248][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1_piece0 stored as bytes in memory (estimated size 2028.0 B, free 912.1 MB)
[INFO][2018-05-30 10:14:11,249][org.apache.spark.storage.BlockManagerInfo]Added broadcast_1_piece0 in memory on 10.194.67.11:52322 (size: 2028.0 B, free: 912.3 MB)
[INFO][2018-05-30 10:14:11,250][org.apache.spark.SparkContext]Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-30 10:14:11,278][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at mapPartitions at SalesNetAvgMount.scala:71) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2018-05-30 10:14:11,280][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 0.0 with 2 tasks
[INFO][2018-05-30 10:14:11,327][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 4886 bytes)
[INFO][2018-05-30 10:14:11,334][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 4886 bytes)
[INFO][2018-05-30 10:14:11,343][org.apache.spark.executor.Executor]Running task 1.0 in stage 0.0 (TID 1)
[INFO][2018-05-30 10:14:11,343][org.apache.spark.executor.Executor]Running task 0.0 in stage 0.0 (TID 0)
[INFO][2018-05-30 10:14:11,404][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/operate/N/main/part-00000:34172+34172
[INFO][2018-05-30 10:14:11,406][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/operate/N/main/part-00000:0+34172
[INFO][2018-05-30 10:14:11,620][org.apache.spark.executor.Executor]Finished task 1.0 in stage 0.0 (TID 1). 17494 bytes result sent to driver
[INFO][2018-05-30 10:14:11,621][org.apache.spark.executor.Executor]Finished task 0.0 in stage 0.0 (TID 0). 17518 bytes result sent to driver
[INFO][2018-05-30 10:14:11,641][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 0.0 (TID 1) in 306 ms on localhost (executor driver) (1/2)
[INFO][2018-05-30 10:14:11,643][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 0.0 (TID 0) in 331 ms on localhost (executor driver) (2/2)
[INFO][2018-05-30 10:14:11,646][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO][2018-05-30 10:14:11,657][org.apache.spark.scheduler.DAGScheduler]ResultStage 0 (collect at SalesNetAvgMount.scala:88) finished in 0.349 s
[INFO][2018-05-30 10:14:11,663][org.apache.spark.scheduler.DAGScheduler]Job 0 finished: collect at SalesNetAvgMount.scala:88, took 0.458037 s
[INFO][2018-05-30 10:14:11,673][org.apache.spark.storage.memory.MemoryStore]Block broadcast_2 stored as values in memory (estimated size 110.1 KB, free 911.9 MB)
[INFO][2018-05-30 10:14:11,686][org.apache.spark.storage.memory.MemoryStore]Block broadcast_2_piece0 stored as bytes in memory (estimated size 17.5 KB, free 911.9 MB)
[INFO][2018-05-30 10:14:11,687][org.apache.spark.storage.BlockManagerInfo]Added broadcast_2_piece0 in memory on 10.194.67.11:52322 (size: 17.5 KB, free: 912.3 MB)
[INFO][2018-05-30 10:14:11,688][org.apache.spark.SparkContext]Created broadcast 2 from broadcast at SalesNetAvgMount.scala:45
[INFO][2018-05-30 10:14:11,694][org.apache.spark.storage.memory.MemoryStore]Block broadcast_3 stored as values in memory (estimated size 228.1 KB, free 911.7 MB)
[INFO][2018-05-30 10:14:11,733][org.apache.spark.storage.memory.MemoryStore]Block broadcast_3_piece0 stored as bytes in memory (estimated size 22.1 KB, free 911.7 MB)
[INFO][2018-05-30 10:14:11,735][org.apache.spark.storage.BlockManagerInfo]Added broadcast_3_piece0 in memory on 10.194.67.11:52322 (size: 22.1 KB, free: 912.2 MB)
[INFO][2018-05-30 10:14:11,736][org.apache.spark.SparkContext]Created broadcast 3 from textFile at NetTypeUtils.scala:53
[INFO][2018-05-30 10:14:11,747][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_1_piece0 on 10.194.67.11:52322 in memory (size: 2028.0 B, free: 912.2 MB)
[INFO][2018-05-30 10:14:11,764][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 1
[INFO][2018-05-30 10:14:11,779][org.apache.spark.SparkContext]Starting job: collect at NetTypeUtils.scala:62
[INFO][2018-05-30 10:14:11,780][org.apache.spark.scheduler.DAGScheduler]Got job 1 (collect at NetTypeUtils.scala:62) with 2 output partitions
[INFO][2018-05-30 10:14:11,780][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 1 (collect at NetTypeUtils.scala:62)
[INFO][2018-05-30 10:14:11,780][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-30 10:14:11,781][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-30 10:14:11,782][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 1 (MapPartitionsRDD[6] at mapPartitions at NetTypeUtils.scala:55), which has no missing parents
[INFO][2018-05-30 10:14:11,785][org.apache.spark.storage.memory.MemoryStore]Block broadcast_4 stored as values in memory (estimated size 3.7 KB, free 911.7 MB)
[INFO][2018-05-30 10:14:11,791][org.apache.spark.storage.memory.MemoryStore]Block broadcast_4_piece0 stored as bytes in memory (estimated size 2.0 KB, free 911.7 MB)
[INFO][2018-05-30 10:14:11,791][org.apache.spark.storage.BlockManagerInfo]Added broadcast_4_piece0 in memory on 10.194.67.11:52322 (size: 2.0 KB, free: 912.2 MB)
[INFO][2018-05-30 10:14:11,792][org.apache.spark.SparkContext]Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-30 10:14:11,793][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at mapPartitions at NetTypeUtils.scala:55) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2018-05-30 10:14:11,793][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 1.0 with 2 tasks
[INFO][2018-05-30 10:14:11,795][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 1.0 (TID 2, localhost, executor driver, partition 0, ANY, 4932 bytes)
[INFO][2018-05-30 10:14:11,795][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 1.0 (TID 3, localhost, executor driver, partition 1, ANY, 4932 bytes)
[INFO][2018-05-30 10:14:11,796][org.apache.spark.executor.Executor]Running task 1.0 in stage 1.0 (TID 3)
[INFO][2018-05-30 10:14:11,796][org.apache.spark.executor.Executor]Running task 0.0 in stage 1.0 (TID 2)
[INFO][2018-05-30 10:14:11,800][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/seven/data/netData/part-00000-5ea613c3-08fd-4310-b01a-b33de863caa2-c000.csv:160327+160327
[INFO][2018-05-30 10:14:11,801][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/seven/data/netData/part-00000-5ea613c3-08fd-4310-b01a-b33de863caa2-c000.csv:0+160327
[INFO][2018-05-30 10:14:11,927][org.apache.spark.executor.Executor]Finished task 1.0 in stage 1.0 (TID 3). 176889 bytes result sent to driver
[INFO][2018-05-30 10:14:11,929][org.apache.spark.executor.Executor]Finished task 0.0 in stage 1.0 (TID 2). 175704 bytes result sent to driver
[INFO][2018-05-30 10:14:11,946][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 1.0 (TID 3) in 151 ms on localhost (executor driver) (1/2)
[INFO][2018-05-30 10:14:11,946][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 1.0 (TID 2) in 152 ms on localhost (executor driver) (2/2)
[INFO][2018-05-30 10:14:11,946][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO][2018-05-30 10:14:11,947][org.apache.spark.scheduler.DAGScheduler]ResultStage 1 (collect at NetTypeUtils.scala:62) finished in 0.153 s
[INFO][2018-05-30 10:14:11,948][org.apache.spark.scheduler.DAGScheduler]Job 1 finished: collect at NetTypeUtils.scala:62, took 0.168166 s
[INFO][2018-05-30 10:14:11,953][org.apache.spark.storage.memory.MemoryStore]Block broadcast_5 stored as values in memory (estimated size 588.5 KB, free 911.1 MB)
[INFO][2018-05-30 10:14:11,971][org.apache.spark.storage.memory.MemoryStore]Block broadcast_5_piece0 stored as bytes in memory (estimated size 184.5 KB, free 910.9 MB)
[INFO][2018-05-30 10:14:11,973][org.apache.spark.storage.BlockManagerInfo]Added broadcast_5_piece0 in memory on 10.194.67.11:52322 (size: 184.5 KB, free: 912.1 MB)
[INFO][2018-05-30 10:14:11,974][org.apache.spark.SparkContext]Created broadcast 5 from broadcast at SalesNetAvgMount.scala:48
[INFO][2018-05-30 10:14:11,979][org.apache.spark.storage.memory.MemoryStore]Block broadcast_6 stored as values in memory (estimated size 228.1 KB, free 910.7 MB)
[INFO][2018-05-30 10:14:12,000][org.apache.spark.storage.memory.MemoryStore]Block broadcast_6_piece0 stored as bytes in memory (estimated size 22.1 KB, free 910.7 MB)
[INFO][2018-05-30 10:14:12,003][org.apache.spark.storage.BlockManagerInfo]Added broadcast_6_piece0 in memory on 10.194.67.11:52322 (size: 22.1 KB, free: 912.0 MB)
[INFO][2018-05-30 10:14:12,004][org.apache.spark.SparkContext]Created broadcast 6 from textFile at NetTypeUtils.scala:27
[INFO][2018-05-30 10:14:12,028][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 1
[INFO][2018-05-30 10:14:12,047][org.apache.spark.SparkContext]Starting job: collect at NetTypeUtils.scala:36
[INFO][2018-05-30 10:14:12,048][org.apache.spark.scheduler.DAGScheduler]Got job 2 (collect at NetTypeUtils.scala:36) with 2 output partitions
[INFO][2018-05-30 10:14:12,048][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 2 (collect at NetTypeUtils.scala:36)
[INFO][2018-05-30 10:14:12,048][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-30 10:14:12,049][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-30 10:14:12,049][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 2 (MapPartitionsRDD[10] at mapPartitions at NetTypeUtils.scala:29), which has no missing parents
[INFO][2018-05-30 10:14:12,051][org.apache.spark.storage.memory.MemoryStore]Block broadcast_7 stored as values in memory (estimated size 3.7 KB, free 910.7 MB)
[INFO][2018-05-30 10:14:12,056][org.apache.spark.storage.memory.MemoryStore]Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.0 KB, free 910.7 MB)
[INFO][2018-05-30 10:14:12,057][org.apache.spark.storage.BlockManagerInfo]Added broadcast_7_piece0 in memory on 10.194.67.11:52322 (size: 2.0 KB, free: 912.0 MB)
[INFO][2018-05-30 10:14:12,057][org.apache.spark.SparkContext]Created broadcast 7 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-30 10:14:12,058][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ResultStage 2 (MapPartitionsRDD[10] at mapPartitions at NetTypeUtils.scala:29) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2018-05-30 10:14:12,058][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 2.0 with 2 tasks
[INFO][2018-05-30 10:14:12,059][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 2.0 (TID 4, localhost, executor driver, partition 0, ANY, 4934 bytes)
[INFO][2018-05-30 10:14:12,060][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 2.0 (TID 5, localhost, executor driver, partition 1, ANY, 4934 bytes)
[INFO][2018-05-30 10:14:12,060][org.apache.spark.executor.Executor]Running task 0.0 in stage 2.0 (TID 4)
[INFO][2018-05-30 10:14:12,060][org.apache.spark.executor.Executor]Running task 1.0 in stage 2.0 (TID 5)
[INFO][2018-05-30 10:14:12,064][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/seven/data/pointData/part-00000-71581736-630f-4fbd-834f-e5e6283d573b-c000.csv:0+334642
[INFO][2018-05-30 10:14:12,066][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/seven/data/pointData/part-00000-71581736-630f-4fbd-834f-e5e6283d573b-c000.csv:334642+334643
[INFO][2018-05-30 10:14:12,196][org.apache.spark.executor.Executor]Finished task 1.0 in stage 2.0 (TID 5). 392427 bytes result sent to driver
[INFO][2018-05-30 10:14:12,204][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 2.0 (TID 5) in 145 ms on localhost (executor driver) (1/2)
[INFO][2018-05-30 10:14:12,355][org.apache.spark.executor.Executor]Finished task 0.0 in stage 2.0 (TID 4). 392358 bytes result sent to driver
[INFO][2018-05-30 10:14:12,361][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 2.0 (TID 4) in 302 ms on localhost (executor driver) (2/2)
[INFO][2018-05-30 10:14:12,361][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO][2018-05-30 10:14:12,362][org.apache.spark.scheduler.DAGScheduler]ResultStage 2 (collect at NetTypeUtils.scala:36) finished in 0.303 s
[INFO][2018-05-30 10:14:12,363][org.apache.spark.scheduler.DAGScheduler]Job 2 finished: collect at NetTypeUtils.scala:36, took 0.314653 s
[INFO][2018-05-30 10:14:12,372][org.apache.spark.storage.memory.MemoryStore]Block broadcast_8 stored as values in memory (estimated size 2.1 MB, free 908.6 MB)
[INFO][2018-05-30 10:14:12,384][org.apache.spark.storage.memory.MemoryStore]Block broadcast_8_piece0 stored as bytes in memory (estimated size 323.9 KB, free 908.3 MB)
[INFO][2018-05-30 10:14:12,385][org.apache.spark.storage.BlockManagerInfo]Added broadcast_8_piece0 in memory on 10.194.67.11:52322 (size: 323.9 KB, free: 911.7 MB)
[INFO][2018-05-30 10:14:12,385][org.apache.spark.SparkContext]Created broadcast 8 from broadcast at SalesNetAvgMount.scala:51
[INFO][2018-05-30 10:14:12,390][org.apache.spark.storage.memory.MemoryStore]Block broadcast_9 stored as values in memory (estimated size 228.1 KB, free 908.1 MB)
[INFO][2018-05-30 10:14:12,410][org.apache.spark.storage.memory.MemoryStore]Block broadcast_9_piece0 stored as bytes in memory (estimated size 22.1 KB, free 908.0 MB)
[INFO][2018-05-30 10:14:12,410][org.apache.spark.storage.BlockManagerInfo]Added broadcast_9_piece0 in memory on 10.194.67.11:52322 (size: 22.1 KB, free: 911.7 MB)
[INFO][2018-05-30 10:14:12,412][org.apache.spark.SparkContext]Created broadcast 9 from textFile at SalesNetAvgMount.scala:109
[INFO][2018-05-30 10:14:12,479][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 1
[INFO][2018-05-30 10:14:12,600][org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter]File Output Committer Algorithm version is 1
[INFO][2018-05-30 10:14:12,657][org.apache.spark.SparkContext]Starting job: saveAsTextFile at Utils.java:25
[INFO][2018-05-30 10:14:12,665][org.apache.spark.scheduler.DAGScheduler]Registering RDD 14 (mapPartitions at SalesNetAvgMount.scala:117)
[INFO][2018-05-30 10:14:12,665][org.apache.spark.scheduler.DAGScheduler]Registering RDD 17 (repartition at Utils.java:25)
[INFO][2018-05-30 10:14:12,666][org.apache.spark.scheduler.DAGScheduler]Got job 3 (saveAsTextFile at Utils.java:25) with 1 output partitions
[INFO][2018-05-30 10:14:12,666][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 5 (saveAsTextFile at Utils.java:25)
[INFO][2018-05-30 10:14:12,666][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 4)
[INFO][2018-05-30 10:14:12,666][org.apache.spark.scheduler.DAGScheduler]Missing parents: List(ShuffleMapStage 4)
[INFO][2018-05-30 10:14:12,670][org.apache.spark.scheduler.DAGScheduler]Submitting ShuffleMapStage 3 (MapPartitionsRDD[14] at mapPartitions at SalesNetAvgMount.scala:117), which has no missing parents
[INFO][2018-05-30 10:14:12,681][org.apache.spark.storage.memory.MemoryStore]Block broadcast_10 stored as values in memory (estimated size 48.1 KB, free 908.0 MB)
[INFO][2018-05-30 10:14:12,686][org.apache.spark.storage.memory.MemoryStore]Block broadcast_10_piece0 stored as bytes in memory (estimated size 21.5 KB, free 908.0 MB)
[INFO][2018-05-30 10:14:12,686][org.apache.spark.storage.BlockManagerInfo]Added broadcast_10_piece0 in memory on 10.194.67.11:52322 (size: 21.5 KB, free: 911.7 MB)
[INFO][2018-05-30 10:14:12,687][org.apache.spark.SparkContext]Created broadcast 10 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-30 10:14:12,689][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[14] at mapPartitions at SalesNetAvgMount.scala:117) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2018-05-30 10:14:12,689][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 3.0 with 2 tasks
[INFO][2018-05-30 10:14:12,691][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 3.0 (TID 6, localhost, executor driver, partition 0, ANY, 4872 bytes)
[INFO][2018-05-30 10:14:12,691][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 3.0 (TID 7, localhost, executor driver, partition 1, ANY, 4872 bytes)
[INFO][2018-05-30 10:14:12,692][org.apache.spark.executor.Executor]Running task 0.0 in stage 3.0 (TID 6)
[INFO][2018-05-30 10:14:12,692][org.apache.spark.executor.Executor]Running task 1.0 in stage 3.0 (TID 7)
[INFO][2018-05-30 10:14:12,704][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/part-00000:12229859+12229860
[INFO][2018-05-30 10:14:12,704][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/part-00000:0+12229859
[INFO][2018-05-30 10:14:12,819][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_4_piece0 on 10.194.67.11:52322 in memory (size: 2.0 KB, free: 911.7 MB)
[INFO][2018-05-30 10:14:12,820][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_7_piece0 on 10.194.67.11:52322 in memory (size: 2.0 KB, free: 911.7 MB)
[INFO][2018-05-30 10:14:12,821][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_3_piece0 on 10.194.67.11:52322 in memory (size: 22.1 KB, free: 911.7 MB)
[INFO][2018-05-30 10:14:12,822][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_6_piece0 on 10.194.67.11:52322 in memory (size: 22.1 KB, free: 911.7 MB)
[INFO][2018-05-30 10:14:14,231][org.apache.spark.executor.Executor]Finished task 0.0 in stage 3.0 (TID 6). 1085 bytes result sent to driver
[INFO][2018-05-30 10:14:14,250][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 3.0 (TID 6) in 1560 ms on localhost (executor driver) (1/2)
[INFO][2018-05-30 10:14:23,480][org.apache.spark.executor.Executor]Finished task 1.0 in stage 3.0 (TID 7). 1085 bytes result sent to driver
[INFO][2018-05-30 10:14:23,482][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 3.0 (TID 7) in 10791 ms on localhost (executor driver) (2/2)
[INFO][2018-05-30 10:14:23,482][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 3.0, whose tasks have all completed, from pool 
[INFO][2018-05-30 10:14:23,483][org.apache.spark.scheduler.DAGScheduler]ShuffleMapStage 3 (mapPartitions at SalesNetAvgMount.scala:117) finished in 10.792 s
[INFO][2018-05-30 10:14:23,483][org.apache.spark.scheduler.DAGScheduler]looking for newly runnable stages
[INFO][2018-05-30 10:14:23,484][org.apache.spark.scheduler.DAGScheduler]running: Set()
[INFO][2018-05-30 10:14:23,484][org.apache.spark.scheduler.DAGScheduler]waiting: Set(ResultStage 5, ShuffleMapStage 4)
[INFO][2018-05-30 10:14:23,485][org.apache.spark.scheduler.DAGScheduler]failed: Set()
[INFO][2018-05-30 10:14:23,490][org.apache.spark.scheduler.DAGScheduler]Submitting ShuffleMapStage 4 (MapPartitionsRDD[17] at repartition at Utils.java:25), which has no missing parents
[INFO][2018-05-30 10:14:23,496][org.apache.spark.storage.memory.MemoryStore]Block broadcast_11 stored as values in memory (estimated size 47.9 KB, free 908.4 MB)
[INFO][2018-05-30 10:14:23,500][org.apache.spark.storage.memory.MemoryStore]Block broadcast_11_piece0 stored as bytes in memory (estimated size 21.3 KB, free 908.4 MB)
[INFO][2018-05-30 10:14:23,500][org.apache.spark.storage.BlockManagerInfo]Added broadcast_11_piece0 in memory on 10.194.67.11:52322 (size: 21.3 KB, free: 911.7 MB)
[INFO][2018-05-30 10:14:23,501][org.apache.spark.SparkContext]Created broadcast 11 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-30 10:14:23,502][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[17] at repartition at Utils.java:25) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2018-05-30 10:14:23,502][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 4.0 with 2 tasks
[INFO][2018-05-30 10:14:23,504][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 4.0 (TID 8, localhost, executor driver, partition 0, ANY, 4610 bytes)
[INFO][2018-05-30 10:14:23,504][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 4.0 (TID 9, localhost, executor driver, partition 1, ANY, 4610 bytes)
[INFO][2018-05-30 10:14:23,504][org.apache.spark.executor.Executor]Running task 0.0 in stage 4.0 (TID 8)
[INFO][2018-05-30 10:14:23,504][org.apache.spark.executor.Executor]Running task 1.0 in stage 4.0 (TID 9)
[INFO][2018-05-30 10:14:23,525][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-30 10:14:23,525][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-30 10:14:23,527][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 6 ms
[INFO][2018-05-30 10:14:23,527][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 5 ms
[INFO][2018-05-30 10:14:23,670][org.apache.spark.storage.memory.MemoryStore]Block rdd_16_1 stored as values in memory (estimated size 87.0 KB, free 908.3 MB)
[INFO][2018-05-30 10:14:23,671][org.apache.spark.storage.BlockManagerInfo]Added rdd_16_1 in memory on 10.194.67.11:52322 (size: 87.0 KB, free: 911.6 MB)
[INFO][2018-05-30 10:14:23,677][org.apache.spark.storage.memory.MemoryStore]Block rdd_16_0 stored as values in memory (estimated size 86.9 KB, free 908.2 MB)
[INFO][2018-05-30 10:14:23,678][org.apache.spark.storage.BlockManagerInfo]Added rdd_16_0 in memory on 10.194.67.11:52322 (size: 86.9 KB, free: 911.5 MB)
[INFO][2018-05-30 10:14:23,700][org.apache.spark.executor.Executor]Finished task 1.0 in stage 4.0 (TID 9). 1910 bytes result sent to driver
[INFO][2018-05-30 10:14:23,701][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 4.0 (TID 9) in 197 ms on localhost (executor driver) (1/2)
[INFO][2018-05-30 10:14:23,703][org.apache.spark.executor.Executor]Finished task 0.0 in stage 4.0 (TID 8). 1910 bytes result sent to driver
[INFO][2018-05-30 10:14:23,703][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 4.0 (TID 8) in 200 ms on localhost (executor driver) (2/2)
[INFO][2018-05-30 10:14:23,704][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 4.0, whose tasks have all completed, from pool 
[INFO][2018-05-30 10:14:23,705][org.apache.spark.scheduler.DAGScheduler]ShuffleMapStage 4 (repartition at Utils.java:25) finished in 0.201 s
[INFO][2018-05-30 10:14:23,705][org.apache.spark.scheduler.DAGScheduler]looking for newly runnable stages
[INFO][2018-05-30 10:14:23,705][org.apache.spark.scheduler.DAGScheduler]running: Set()
[INFO][2018-05-30 10:14:23,705][org.apache.spark.scheduler.DAGScheduler]waiting: Set(ResultStage 5)
[INFO][2018-05-30 10:14:23,705][org.apache.spark.scheduler.DAGScheduler]failed: Set()
[INFO][2018-05-30 10:14:23,705][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 5 (MapPartitionsRDD[21] at saveAsTextFile at Utils.java:25), which has no missing parents
[INFO][2018-05-30 10:14:23,721][org.apache.spark.storage.memory.MemoryStore]Block broadcast_12 stored as values in memory (estimated size 70.0 KB, free 908.2 MB)
[INFO][2018-05-30 10:14:23,725][org.apache.spark.storage.memory.MemoryStore]Block broadcast_12_piece0 stored as bytes in memory (estimated size 25.4 KB, free 908.1 MB)
[INFO][2018-05-30 10:14:23,725][org.apache.spark.storage.BlockManagerInfo]Added broadcast_12_piece0 in memory on 10.194.67.11:52322 (size: 25.4 KB, free: 911.5 MB)
[INFO][2018-05-30 10:14:23,726][org.apache.spark.SparkContext]Created broadcast 12 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-30 10:14:23,726][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[21] at saveAsTextFile at Utils.java:25) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-30 10:14:23,726][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 5.0 with 1 tasks
[INFO][2018-05-30 10:14:23,735][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 5.0 (TID 10, localhost, executor driver, partition 0, ANY, 4897 bytes)
[INFO][2018-05-30 10:14:23,736][org.apache.spark.executor.Executor]Running task 0.0 in stage 5.0 (TID 10)
[INFO][2018-05-30 10:14:23,763][org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter]File Output Committer Algorithm version is 1
[INFO][2018-05-30 10:14:23,804][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-30 10:14:23,805][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO][2018-05-30 10:14:24,082][org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter]Saved output of task 'attempt_20180530101412_0005_m_000000_10' to hdfs://vm-xaj-bigdata-da-d01:8020/yst/seven/data/orderData/SalesNetAvgMount/_temporary/0/task_20180530101412_0005_m_000000
[INFO][2018-05-30 10:14:24,083][org.apache.spark.mapred.SparkHadoopMapRedUtil]attempt_20180530101412_0005_m_000000_10: Committed
[INFO][2018-05-30 10:14:24,087][org.apache.spark.executor.Executor]Finished task 0.0 in stage 5.0 (TID 10). 1010 bytes result sent to driver
[INFO][2018-05-30 10:14:24,088][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 5.0 (TID 10) in 361 ms on localhost (executor driver) (1/1)
[INFO][2018-05-30 10:14:24,088][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 5.0, whose tasks have all completed, from pool 
[INFO][2018-05-30 10:14:24,089][org.apache.spark.scheduler.DAGScheduler]ResultStage 5 (saveAsTextFile at Utils.java:25) finished in 0.362 s
[INFO][2018-05-30 10:14:24,089][org.apache.spark.scheduler.DAGScheduler]Job 3 finished: saveAsTextFile at Utils.java:25, took 11.431471 s
[INFO][2018-05-30 10:14:24,177][org.apache.spark.storage.memory.MemoryStore]Block broadcast_13 stored as values in memory (estimated size 228.1 KB, free 907.9 MB)
[INFO][2018-05-30 10:14:24,196][org.apache.spark.storage.memory.MemoryStore]Block broadcast_13_piece0 stored as bytes in memory (estimated size 22.1 KB, free 907.9 MB)
[INFO][2018-05-30 10:14:24,197][org.apache.spark.storage.BlockManagerInfo]Added broadcast_13_piece0 in memory on 10.194.67.11:52322 (size: 22.1 KB, free: 911.5 MB)
[INFO][2018-05-30 10:14:24,197][org.apache.spark.SparkContext]Created broadcast 13 from textFile at SalesNetAvgMount.scala:388
[INFO][2018-05-30 10:14:24,223][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 1
[INFO][2018-05-30 10:14:24,244][org.apache.spark.SparkContext]Starting job: collect at SalesNetAvgMount.scala:405
[INFO][2018-05-30 10:14:24,245][org.apache.spark.scheduler.DAGScheduler]Registering RDD 25 (mapPartitions at SalesNetAvgMount.scala:396)
[INFO][2018-05-30 10:14:24,245][org.apache.spark.scheduler.DAGScheduler]Got job 4 (collect at SalesNetAvgMount.scala:405) with 2 output partitions
[INFO][2018-05-30 10:14:24,246][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 7 (collect at SalesNetAvgMount.scala:405)
[INFO][2018-05-30 10:14:24,246][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 6)
[INFO][2018-05-30 10:14:24,246][org.apache.spark.scheduler.DAGScheduler]Missing parents: List(ShuffleMapStage 6)
[INFO][2018-05-30 10:14:24,246][org.apache.spark.scheduler.DAGScheduler]Submitting ShuffleMapStage 6 (MapPartitionsRDD[25] at mapPartitions at SalesNetAvgMount.scala:396), which has no missing parents
[INFO][2018-05-30 10:14:24,249][org.apache.spark.storage.memory.MemoryStore]Block broadcast_14 stored as values in memory (estimated size 48.1 KB, free 907.8 MB)
[INFO][2018-05-30 10:14:24,253][org.apache.spark.storage.memory.MemoryStore]Block broadcast_14_piece0 stored as bytes in memory (estimated size 21.5 KB, free 907.8 MB)
[INFO][2018-05-30 10:14:24,253][org.apache.spark.storage.BlockManagerInfo]Added broadcast_14_piece0 in memory on 10.194.67.11:52322 (size: 21.5 KB, free: 911.5 MB)
[INFO][2018-05-30 10:14:24,254][org.apache.spark.SparkContext]Created broadcast 14 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-30 10:14:24,254][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[25] at mapPartitions at SalesNetAvgMount.scala:396) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2018-05-30 10:14:24,254][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 6.0 with 2 tasks
[INFO][2018-05-30 10:14:24,255][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 6.0 (TID 11, localhost, executor driver, partition 0, ANY, 4872 bytes)
[INFO][2018-05-30 10:14:24,255][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 6.0 (TID 12, localhost, executor driver, partition 1, ANY, 4872 bytes)
[INFO][2018-05-30 10:14:24,255][org.apache.spark.executor.Executor]Running task 1.0 in stage 6.0 (TID 12)
[INFO][2018-05-30 10:14:24,255][org.apache.spark.executor.Executor]Running task 0.0 in stage 6.0 (TID 11)
[INFO][2018-05-30 10:14:24,259][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/part-00000:12229859+12229860
[INFO][2018-05-30 10:14:24,260][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/part-00000:0+12229859
[INFO][2018-05-30 10:14:27,909][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_11_piece0 on 10.194.67.11:52322 in memory (size: 21.3 KB, free: 911.5 MB)
[INFO][2018-05-30 10:14:27,910][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_12_piece0 on 10.194.67.11:52322 in memory (size: 25.4 KB, free: 911.5 MB)
[INFO][2018-05-30 10:14:35,249][org.apache.spark.executor.Executor]Finished task 0.0 in stage 6.0 (TID 11). 1085 bytes result sent to driver
[INFO][2018-05-30 10:14:35,251][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 6.0 (TID 11) in 10995 ms on localhost (executor driver) (1/2)
[INFO][2018-05-30 10:14:35,338][org.apache.spark.executor.Executor]Finished task 1.0 in stage 6.0 (TID 12). 1085 bytes result sent to driver
[INFO][2018-05-30 10:14:35,339][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 6.0 (TID 12) in 11084 ms on localhost (executor driver) (2/2)
[INFO][2018-05-30 10:14:35,339][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 6.0, whose tasks have all completed, from pool 
[INFO][2018-05-30 10:14:35,340][org.apache.spark.scheduler.DAGScheduler]ShuffleMapStage 6 (mapPartitions at SalesNetAvgMount.scala:396) finished in 11.085 s
[INFO][2018-05-30 10:14:35,340][org.apache.spark.scheduler.DAGScheduler]looking for newly runnable stages
[INFO][2018-05-30 10:14:35,340][org.apache.spark.scheduler.DAGScheduler]running: Set()
[INFO][2018-05-30 10:14:35,340][org.apache.spark.scheduler.DAGScheduler]waiting: Set(ResultStage 7)
[INFO][2018-05-30 10:14:35,340][org.apache.spark.scheduler.DAGScheduler]failed: Set()
[INFO][2018-05-30 10:14:35,340][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 7 (ShuffledRDD[26] at reduceByKey at SalesNetAvgMount.scala:405), which has no missing parents
[INFO][2018-05-30 10:14:35,342][org.apache.spark.storage.memory.MemoryStore]Block broadcast_15 stored as values in memory (estimated size 3.2 KB, free 908.0 MB)
[INFO][2018-05-30 10:14:35,345][org.apache.spark.storage.memory.MemoryStore]Block broadcast_15_piece0 stored as bytes in memory (estimated size 1953.0 B, free 908.0 MB)
[INFO][2018-05-30 10:14:35,345][org.apache.spark.storage.BlockManagerInfo]Added broadcast_15_piece0 in memory on 10.194.67.11:52322 (size: 1953.0 B, free: 911.5 MB)
[INFO][2018-05-30 10:14:35,346][org.apache.spark.SparkContext]Created broadcast 15 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-30 10:14:35,346][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ResultStage 7 (ShuffledRDD[26] at reduceByKey at SalesNetAvgMount.scala:405) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2018-05-30 10:14:35,346][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 7.0 with 2 tasks
[INFO][2018-05-30 10:14:35,347][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 7.0 (TID 13, localhost, executor driver, partition 0, ANY, 4621 bytes)
[INFO][2018-05-30 10:14:35,348][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 7.0 (TID 14, localhost, executor driver, partition 1, ANY, 4621 bytes)
[INFO][2018-05-30 10:14:35,348][org.apache.spark.executor.Executor]Running task 0.0 in stage 7.0 (TID 13)
[INFO][2018-05-30 10:14:35,348][org.apache.spark.executor.Executor]Running task 1.0 in stage 7.0 (TID 14)
[INFO][2018-05-30 10:14:35,350][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-30 10:14:35,350][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-30 10:14:35,350][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-30 10:14:35,350][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-30 10:14:35,367][org.apache.spark.executor.Executor]Finished task 0.0 in stage 7.0 (TID 13). 5372 bytes result sent to driver
[INFO][2018-05-30 10:14:35,367][org.apache.spark.executor.Executor]Finished task 1.0 in stage 7.0 (TID 14). 5712 bytes result sent to driver
[INFO][2018-05-30 10:14:35,368][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 7.0 (TID 13) in 21 ms on localhost (executor driver) (1/2)
[INFO][2018-05-30 10:14:35,368][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 7.0 (TID 14) in 21 ms on localhost (executor driver) (2/2)
[INFO][2018-05-30 10:14:35,368][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 7.0, whose tasks have all completed, from pool 
[INFO][2018-05-30 10:14:35,369][org.apache.spark.scheduler.DAGScheduler]ResultStage 7 (collect at SalesNetAvgMount.scala:405) finished in 0.022 s
[INFO][2018-05-30 10:14:35,369][org.apache.spark.scheduler.DAGScheduler]Job 4 finished: collect at SalesNetAvgMount.scala:405, took 11.124438 s
[INFO][2018-05-30 10:14:35,373][org.apache.spark.storage.memory.MemoryStore]Block broadcast_16 stored as values in memory (estimated size 228.1 KB, free 907.8 MB)
[INFO][2018-05-30 10:14:35,386][org.apache.spark.storage.memory.MemoryStore]Block broadcast_16_piece0 stored as bytes in memory (estimated size 22.1 KB, free 907.7 MB)
[INFO][2018-05-30 10:14:35,386][org.apache.spark.storage.BlockManagerInfo]Added broadcast_16_piece0 in memory on 10.194.67.11:52322 (size: 22.1 KB, free: 911.5 MB)
[INFO][2018-05-30 10:14:35,387][org.apache.spark.SparkContext]Created broadcast 16 from textFile at SalesNetAvgMount.scala:412
[INFO][2018-05-30 10:14:35,412][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_15_piece0 on 10.194.67.11:52322 in memory (size: 1953.0 B, free: 911.5 MB)
[INFO][2018-05-30 10:14:35,453][org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter]File Output Committer Algorithm version is 1
[INFO][2018-05-30 10:14:35,482][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 1
[INFO][2018-05-30 10:14:35,513][org.apache.spark.SparkContext]Starting job: saveAsTextFile at Utils.java:25
[INFO][2018-05-30 10:14:35,513][org.apache.spark.scheduler.DAGScheduler]Registering RDD 31 (repartition at Utils.java:25)
[INFO][2018-05-30 10:14:35,514][org.apache.spark.scheduler.DAGScheduler]Got job 5 (saveAsTextFile at Utils.java:25) with 1 output partitions
[INFO][2018-05-30 10:14:35,514][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 9 (saveAsTextFile at Utils.java:25)
[INFO][2018-05-30 10:14:35,514][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 8)
[INFO][2018-05-30 10:14:35,514][org.apache.spark.scheduler.DAGScheduler]Missing parents: List(ShuffleMapStage 8)
[INFO][2018-05-30 10:14:35,515][org.apache.spark.scheduler.DAGScheduler]Submitting ShuffleMapStage 8 (MapPartitionsRDD[31] at repartition at Utils.java:25), which has no missing parents
[INFO][2018-05-30 10:14:35,518][org.apache.spark.storage.memory.MemoryStore]Block broadcast_17 stored as values in memory (estimated size 59.2 KB, free 907.7 MB)
[INFO][2018-05-30 10:14:35,521][org.apache.spark.storage.memory.MemoryStore]Block broadcast_17_piece0 stored as bytes in memory (estimated size 28.7 KB, free 907.7 MB)
[INFO][2018-05-30 10:14:35,522][org.apache.spark.storage.BlockManagerInfo]Added broadcast_17_piece0 in memory on 10.194.67.11:52322 (size: 28.7 KB, free: 911.5 MB)
[INFO][2018-05-30 10:14:35,522][org.apache.spark.SparkContext]Created broadcast 17 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-30 10:14:35,523][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ShuffleMapStage 8 (MapPartitionsRDD[31] at repartition at Utils.java:25) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2018-05-30 10:14:35,523][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 8.0 with 2 tasks
[INFO][2018-05-30 10:14:35,524][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 8.0 (TID 15, localhost, executor driver, partition 0, ANY, 4921 bytes)
[INFO][2018-05-30 10:14:35,524][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 8.0 (TID 16, localhost, executor driver, partition 1, ANY, 4921 bytes)
[INFO][2018-05-30 10:14:35,524][org.apache.spark.executor.Executor]Running task 1.0 in stage 8.0 (TID 16)
[INFO][2018-05-30 10:14:35,524][org.apache.spark.executor.Executor]Running task 0.0 in stage 8.0 (TID 15)
[INFO][2018-05-30 10:14:35,530][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/seven/data/netData/part-00000-5ea613c3-08fd-4310-b01a-b33de863caa2-c000.csv:160327+160327
[INFO][2018-05-30 10:14:35,530][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/seven/data/netData/part-00000-5ea613c3-08fd-4310-b01a-b33de863caa2-c000.csv:0+160327
[INFO][2018-05-30 10:14:35,625][org.apache.spark.storage.memory.MemoryStore]Block rdd_30_1 stored as values in memory (estimated size 80.6 KB, free 907.6 MB)
[INFO][2018-05-30 10:14:35,626][org.apache.spark.storage.BlockManagerInfo]Added rdd_30_1 in memory on 10.194.67.11:52322 (size: 80.6 KB, free: 911.4 MB)
[INFO][2018-05-30 10:14:35,626][org.apache.spark.storage.memory.MemoryStore]Block rdd_30_0 stored as values in memory (estimated size 27.1 KB, free 907.5 MB)
[INFO][2018-05-30 10:14:35,627][org.apache.spark.storage.BlockManagerInfo]Added rdd_30_0 in memory on 10.194.67.11:52322 (size: 27.1 KB, free: 911.4 MB)
[INFO][2018-05-30 10:14:35,637][org.apache.spark.executor.Executor]Finished task 1.0 in stage 8.0 (TID 16). 1609 bytes result sent to driver
[INFO][2018-05-30 10:14:35,637][org.apache.spark.executor.Executor]Finished task 0.0 in stage 8.0 (TID 15). 1609 bytes result sent to driver
[INFO][2018-05-30 10:14:35,639][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 8.0 (TID 16) in 115 ms on localhost (executor driver) (1/2)
[INFO][2018-05-30 10:14:35,639][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 8.0 (TID 15) in 116 ms on localhost (executor driver) (2/2)
[INFO][2018-05-30 10:14:35,639][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 8.0, whose tasks have all completed, from pool 
[INFO][2018-05-30 10:14:35,639][org.apache.spark.scheduler.DAGScheduler]ShuffleMapStage 8 (repartition at Utils.java:25) finished in 0.116 s
[INFO][2018-05-30 10:14:35,639][org.apache.spark.scheduler.DAGScheduler]looking for newly runnable stages
[INFO][2018-05-30 10:14:35,640][org.apache.spark.scheduler.DAGScheduler]running: Set()
[INFO][2018-05-30 10:14:35,640][org.apache.spark.scheduler.DAGScheduler]waiting: Set(ResultStage 9)
[INFO][2018-05-30 10:14:35,640][org.apache.spark.scheduler.DAGScheduler]failed: Set()
[INFO][2018-05-30 10:14:35,640][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 9 (MapPartitionsRDD[35] at saveAsTextFile at Utils.java:25), which has no missing parents
[INFO][2018-05-30 10:14:35,653][org.apache.spark.storage.memory.MemoryStore]Block broadcast_18 stored as values in memory (estimated size 70.0 KB, free 907.5 MB)
[INFO][2018-05-30 10:14:35,657][org.apache.spark.storage.memory.MemoryStore]Block broadcast_18_piece0 stored as bytes in memory (estimated size 25.4 KB, free 907.5 MB)
[INFO][2018-05-30 10:14:35,657][org.apache.spark.storage.BlockManagerInfo]Added broadcast_18_piece0 in memory on 10.194.67.11:52322 (size: 25.4 KB, free: 911.3 MB)
[INFO][2018-05-30 10:14:35,658][org.apache.spark.SparkContext]Created broadcast 18 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-30 10:14:35,658][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[35] at saveAsTextFile at Utils.java:25) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-30 10:14:35,658][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 9.0 with 1 tasks
[INFO][2018-05-30 10:14:35,659][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 9.0 (TID 17, localhost, executor driver, partition 0, ANY, 4897 bytes)
[INFO][2018-05-30 10:14:35,659][org.apache.spark.executor.Executor]Running task 0.0 in stage 9.0 (TID 17)
[INFO][2018-05-30 10:14:35,669][org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter]File Output Committer Algorithm version is 1
[INFO][2018-05-30 10:14:35,684][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-30 10:14:35,684][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-30 10:14:35,839][org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter]Saved output of task 'attempt_20180530101435_0009_m_000000_17' to hdfs://vm-xaj-bigdata-da-d01:8020/yst/seven/data/orderData/getOrderNullDataNet/_temporary/0/task_20180530101435_0009_m_000000
[INFO][2018-05-30 10:14:35,839][org.apache.spark.mapred.SparkHadoopMapRedUtil]attempt_20180530101435_0009_m_000000_17: Committed
[INFO][2018-05-30 10:14:35,842][org.apache.spark.executor.Executor]Finished task 0.0 in stage 9.0 (TID 17). 1010 bytes result sent to driver
[INFO][2018-05-30 10:14:35,843][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 9.0 (TID 17) in 184 ms on localhost (executor driver) (1/1)
[INFO][2018-05-30 10:14:35,843][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 9.0, whose tasks have all completed, from pool 
[INFO][2018-05-30 10:14:35,844][org.apache.spark.scheduler.DAGScheduler]ResultStage 9 (saveAsTextFile at Utils.java:25) finished in 0.185 s
[INFO][2018-05-30 10:14:35,844][org.apache.spark.scheduler.DAGScheduler]Job 5 finished: saveAsTextFile at Utils.java:25, took 0.330973 s
[INFO][2018-05-30 10:14:35,939][com.seven.spark.rdd.SalesNetAvgMount$]job is success spend time is 0:00:27.142
[INFO][2018-05-30 10:14:35,942][org.apache.spark.SparkContext]Invoking stop() from shutdown hook
[INFO][2018-05-30 10:14:35,948][org.spark_project.jetty.server.AbstractConnector]Stopped Spark@34d1162d{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-05-30 10:14:35,950][org.apache.spark.ui.SparkUI]Stopped Spark web UI at http://10.194.67.11:4040
[INFO][2018-05-30 10:14:35,959][org.apache.spark.MapOutputTrackerMasterEndpoint]MapOutputTrackerMasterEndpoint stopped!
[INFO][2018-05-30 10:14:35,979][org.apache.spark.storage.memory.MemoryStore]MemoryStore cleared
[INFO][2018-05-30 10:14:35,979][org.apache.spark.storage.BlockManager]BlockManager stopped
[INFO][2018-05-30 10:14:35,980][org.apache.spark.storage.BlockManagerMaster]BlockManagerMaster stopped
[INFO][2018-05-30 10:14:35,982][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint]OutputCommitCoordinator stopped!
[INFO][2018-05-30 10:14:35,984][org.apache.spark.SparkContext]Successfully stopped SparkContext
[INFO][2018-05-30 10:14:35,984][org.apache.spark.util.ShutdownHookManager]Shutdown hook called
[INFO][2018-05-30 10:14:35,985][org.apache.spark.util.ShutdownHookManager]Deleting directory /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/spark-2cc366c0-e726-42e8-baa4-9e02ccc5b9f2
[INFO][2018-05-30 13:12:39,021][org.apache.spark.SparkContext]Running Spark version 2.2.0
[INFO][2018-05-30 13:12:40,099][org.apache.spark.SparkContext]Submitted application: ReceiveKafkaData$
[INFO][2018-05-30 13:12:40,128][org.apache.spark.SecurityManager]Changing view acls to: seven
[INFO][2018-05-30 13:12:40,129][org.apache.spark.SecurityManager]Changing modify acls to: seven
[INFO][2018-05-30 13:12:40,130][org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO][2018-05-30 13:12:40,131][org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO][2018-05-30 13:12:40,132][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(seven); groups with view permissions: Set(); users  with modify permissions: Set(seven); groups with modify permissions: Set()
[INFO][2018-05-30 13:12:40,490][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 56668.
[INFO][2018-05-30 13:12:40,516][org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO][2018-05-30 13:12:40,543][org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO][2018-05-30 13:12:40,546][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2018-05-30 13:12:40,546][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
[INFO][2018-05-30 13:12:40,559][org.apache.spark.storage.DiskBlockManager]Created local directory at /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/blockmgr-733f994e-69c9-40df-a4f3-c8f8c4d8fa31
[INFO][2018-05-30 13:12:40,578][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 912.3 MB
[INFO][2018-05-30 13:12:40,662][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO][2018-05-30 13:12:40,789][org.spark_project.jetty.util.log]Logging initialized @3189ms
[INFO][2018-05-30 13:12:40,867][org.spark_project.jetty.server.Server]jetty-9.3.z-SNAPSHOT
[INFO][2018-05-30 13:12:40,884][org.spark_project.jetty.server.Server]Started @3286ms
[INFO][2018-05-30 13:12:40,914][org.spark_project.jetty.server.AbstractConnector]Started ServerConnector@5fdff907{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-05-30 13:12:40,914][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4040.
[INFO][2018-05-30 13:12:40,938][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3e2822{/jobs,null,AVAILABLE,@Spark}
[INFO][2018-05-30 13:12:40,938][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@35fe2125{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2018-05-30 13:12:40,939][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@34645867{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2018-05-30 13:12:40,940][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1255b1d1{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2018-05-30 13:12:40,941][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7c22d4f{/stages,null,AVAILABLE,@Spark}
[INFO][2018-05-30 13:12:40,941][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@60bdf15d{/stages/json,null,AVAILABLE,@Spark}
[INFO][2018-05-30 13:12:40,942][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@51e4ccb3{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2018-05-30 13:12:40,943][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@28a0fd6c{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-30 13:12:40,944][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@66629f63{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2018-05-30 13:12:40,944][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@27a5328c{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2018-05-30 13:12:40,945][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6c345c5f{/storage,null,AVAILABLE,@Spark}
[INFO][2018-05-30 13:12:40,946][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@65e61854{/storage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-30 13:12:40,947][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4fcee388{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2018-05-30 13:12:40,947][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3af17be2{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2018-05-30 13:12:40,948][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@37f21974{/environment,null,AVAILABLE,@Spark}
[INFO][2018-05-30 13:12:40,950][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6e521c1e{/environment/json,null,AVAILABLE,@Spark}
[INFO][2018-05-30 13:12:40,951][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5d5d9e5{/executors,null,AVAILABLE,@Spark}
[INFO][2018-05-30 13:12:40,952][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4ef27d66{/executors/json,null,AVAILABLE,@Spark}
[INFO][2018-05-30 13:12:40,953][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1d9bec4d{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2018-05-30 13:12:40,954][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@10c8f62{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2018-05-30 13:12:40,963][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@25f7391e{/static,null,AVAILABLE,@Spark}
[INFO][2018-05-30 13:12:40,964][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@593e824f{/,null,AVAILABLE,@Spark}
[INFO][2018-05-30 13:12:40,966][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6d8792db{/api,null,AVAILABLE,@Spark}
[INFO][2018-05-30 13:12:40,967][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2f162cc0{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-30 13:12:40,967][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7c041b41{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-30 13:12:40,971][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://10.194.67.11:4040
[INFO][2018-05-30 13:12:41,122][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
[INFO][2018-05-30 13:12:41,168][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 56669.
[INFO][2018-05-30 13:12:41,178][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 10.194.67.11:56669
[INFO][2018-05-30 13:12:41,182][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2018-05-30 13:12:41,185][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 10.194.67.11, 56669, None)
[INFO][2018-05-30 13:12:41,191][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 10.194.67.11:56669 with 912.3 MB RAM, BlockManagerId(driver, 10.194.67.11, 56669, None)
[INFO][2018-05-30 13:12:41,210][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 10.194.67.11, 56669, None)
[INFO][2018-05-30 13:12:41,215][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 10.194.67.11, 56669, None)
[INFO][2018-05-30 13:12:41,692][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@55b62629{/metrics/json,null,AVAILABLE,@Spark}
[WARN][2018-05-30 13:12:42,100][org.apache.spark.streaming.kafka010.KafkaUtils]overriding enable.auto.commit to false for executor
[WARN][2018-05-30 13:12:42,101][org.apache.spark.streaming.kafka010.KafkaUtils]overriding auto.offset.reset to none for executor
[WARN][2018-05-30 13:12:42,102][org.apache.spark.streaming.kafka010.KafkaUtils]overriding executor group.id to spark-executor-use_a_separate_group_id_for_each_stream
[WARN][2018-05-30 13:12:42,103][org.apache.spark.streaming.kafka010.KafkaUtils]overriding receive.buffer.bytes to 65536 see KAFKA-3135
[INFO][2018-05-30 13:12:42,243][org.apache.spark.streaming.kafka010.DirectKafkaInputDStream]Slide time = 5000 ms
[INFO][2018-05-30 13:12:42,244][org.apache.spark.streaming.kafka010.DirectKafkaInputDStream]Storage level = Serialized 1x Replicated
[INFO][2018-05-30 13:12:42,245][org.apache.spark.streaming.kafka010.DirectKafkaInputDStream]Checkpoint interval = null
[INFO][2018-05-30 13:12:42,246][org.apache.spark.streaming.kafka010.DirectKafkaInputDStream]Remember interval = 5000 ms
[INFO][2018-05-30 13:12:42,247][org.apache.spark.streaming.kafka010.DirectKafkaInputDStream]Initialized and validated org.apache.spark.streaming.kafka010.DirectKafkaInputDStream@62376a9f
[INFO][2018-05-30 13:12:42,247][org.apache.spark.streaming.dstream.MappedDStream]Slide time = 5000 ms
[INFO][2018-05-30 13:12:42,248][org.apache.spark.streaming.dstream.MappedDStream]Storage level = Serialized 1x Replicated
[INFO][2018-05-30 13:12:42,248][org.apache.spark.streaming.dstream.MappedDStream]Checkpoint interval = null
[INFO][2018-05-30 13:12:42,248][org.apache.spark.streaming.dstream.MappedDStream]Remember interval = 5000 ms
[INFO][2018-05-30 13:12:42,248][org.apache.spark.streaming.dstream.MappedDStream]Initialized and validated org.apache.spark.streaming.dstream.MappedDStream@89969c7
[INFO][2018-05-30 13:12:42,248][org.apache.spark.streaming.dstream.ForEachDStream]Slide time = 5000 ms
[INFO][2018-05-30 13:12:42,248][org.apache.spark.streaming.dstream.ForEachDStream]Storage level = Serialized 1x Replicated
[INFO][2018-05-30 13:12:42,248][org.apache.spark.streaming.dstream.ForEachDStream]Checkpoint interval = null
[INFO][2018-05-30 13:12:42,248][org.apache.spark.streaming.dstream.ForEachDStream]Remember interval = 5000 ms
[INFO][2018-05-30 13:12:42,248][org.apache.spark.streaming.dstream.ForEachDStream]Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@54a1eb55
[INFO][2018-05-30 13:12:42,360][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	metric.reporters = []
	metadata.max.age.ms = 300000
	partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor]
	reconnect.backoff.ms = 50
	sasl.kerberos.ticket.renew.window.factor = 0.8
	max.partition.fetch.bytes = 1048576
	bootstrap.servers = [vm-xaj-bigdata-da-d01:9092, vm-xaj-bigdata-da-d02:9092, vm-xaj-bigdata-da-d03:9092, vm-xaj-bigdata-da-d04:9092, vm-xaj-bigdata-da-d05:9092, vm-xaj-bigdata-da-d06:9092, vm-xaj-bigdata-da-d07:9092]
	ssl.keystore.type = JKS
	enable.auto.commit = false
	sasl.mechanism = GSSAPI
	interceptor.classes = null
	exclude.internal.topics = true
	ssl.truststore.password = null
	client.id = 
	ssl.endpoint.identification.algorithm = null
	max.poll.records = 2147483647
	check.crcs = true
	request.timeout.ms = 40000
	heartbeat.interval.ms = 3000
	auto.commit.interval.ms = 5000
	receive.buffer.bytes = 65536
	ssl.truststore.type = JKS
	ssl.truststore.location = null
	ssl.keystore.password = null
	fetch.min.bytes = 1
	send.buffer.bytes = 131072
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	group.id = use_a_separate_group_id_for_each_stream
	retry.backoff.ms = 100
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	ssl.trustmanager.algorithm = PKIX
	ssl.key.password = null
	fetch.max.wait.ms = 500
	sasl.kerberos.min.time.before.relogin = 60000
	connections.max.idle.ms = 540000
	session.timeout.ms = 30000
	metrics.num.samples = 2
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	ssl.protocol = TLS
	ssl.provider = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.keystore.location = null
	ssl.cipher.suites = null
	security.protocol = PLAINTEXT
	ssl.keymanager.algorithm = SunX509
	metrics.sample.window.ms = 30000
	auto.offset.reset = latest

[INFO][2018-05-30 13:12:49,545][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	metric.reporters = []
	metadata.max.age.ms = 300000
	partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor]
	reconnect.backoff.ms = 50
	sasl.kerberos.ticket.renew.window.factor = 0.8
	max.partition.fetch.bytes = 1048576
	bootstrap.servers = [vm-xaj-bigdata-da-d01:9092, vm-xaj-bigdata-da-d02:9092, vm-xaj-bigdata-da-d03:9092, vm-xaj-bigdata-da-d04:9092, vm-xaj-bigdata-da-d05:9092, vm-xaj-bigdata-da-d06:9092, vm-xaj-bigdata-da-d07:9092]
	ssl.keystore.type = JKS
	enable.auto.commit = false
	sasl.mechanism = GSSAPI
	interceptor.classes = null
	exclude.internal.topics = true
	ssl.truststore.password = null
	client.id = consumer-1
	ssl.endpoint.identification.algorithm = null
	max.poll.records = 2147483647
	check.crcs = true
	request.timeout.ms = 40000
	heartbeat.interval.ms = 3000
	auto.commit.interval.ms = 5000
	receive.buffer.bytes = 65536
	ssl.truststore.type = JKS
	ssl.truststore.location = null
	ssl.keystore.password = null
	fetch.min.bytes = 1
	send.buffer.bytes = 131072
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	group.id = use_a_separate_group_id_for_each_stream
	retry.backoff.ms = 100
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	ssl.trustmanager.algorithm = PKIX
	ssl.key.password = null
	fetch.max.wait.ms = 500
	sasl.kerberos.min.time.before.relogin = 60000
	connections.max.idle.ms = 540000
	session.timeout.ms = 30000
	metrics.num.samples = 2
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	ssl.protocol = TLS
	ssl.provider = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.keystore.location = null
	ssl.cipher.suites = null
	security.protocol = PLAINTEXT
	ssl.keymanager.algorithm = SunX509
	metrics.sample.window.ms = 30000
	auto.offset.reset = latest

[INFO][2018-05-30 13:12:49,578][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 0.10.0.1
[INFO][2018-05-30 13:12:49,579][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a7a17cdec9eaa6c5
[INFO][2018-05-30 13:12:49,737][org.apache.kafka.clients.consumer.internals.AbstractCoordinator]Discovered coordinator vm-xaj-bigdata-da-d03:9092 (id: 2147483531 rack: null) for group use_a_separate_group_id_for_each_stream.
[INFO][2018-05-30 13:12:49,738][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]Revoking previously assigned partitions [] for group use_a_separate_group_id_for_each_stream
[INFO][2018-05-30 13:12:49,738][org.apache.kafka.clients.consumer.internals.AbstractCoordinator](Re-)joining group use_a_separate_group_id_for_each_stream
[INFO][2018-05-30 13:12:52,888][org.apache.kafka.clients.consumer.internals.AbstractCoordinator]Successfully joined group use_a_separate_group_id_for_each_stream with generation 1
[INFO][2018-05-30 13:12:52,889][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]Setting newly assigned partitions [seven-0] for group use_a_separate_group_id_for_each_stream
[INFO][2018-05-30 13:12:52,926][org.apache.spark.streaming.util.RecurringTimer]Started timer for JobGenerator at time 1527657165000
[INFO][2018-05-30 13:12:52,927][org.apache.spark.streaming.scheduler.JobGenerator]Started JobGenerator at 1527657165000 ms
[INFO][2018-05-30 13:12:52,934][org.apache.spark.streaming.scheduler.JobScheduler]Started JobScheduler
[INFO][2018-05-30 13:12:52,941][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@57adfab0{/streaming,null,AVAILABLE,@Spark}
[INFO][2018-05-30 13:12:52,942][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5af97169{/streaming/json,null,AVAILABLE,@Spark}
[INFO][2018-05-30 13:12:52,942][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2a22ad2b{/streaming/batch,null,AVAILABLE,@Spark}
[INFO][2018-05-30 13:12:52,943][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@121c54fa{/streaming/batch/json,null,AVAILABLE,@Spark}
[INFO][2018-05-30 13:12:52,950][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5dbf5634{/static/streaming,null,AVAILABLE,@Spark}
[INFO][2018-05-30 13:12:52,951][org.apache.spark.streaming.StreamingContext]StreamingContext started
[INFO][2018-05-30 13:12:53,531][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527657165000 ms
[INFO][2018-05-30 13:12:53,534][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527657165000 ms.0 from job set of time 1527657165000 ms
[INFO][2018-05-30 13:12:53,543][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527657170000 ms
[INFO][2018-05-30 13:12:53,609][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:75
[INFO][2018-05-30 13:12:53,629][org.apache.spark.scheduler.DAGScheduler]Got job 0 (isEmpty at ReceiveKafkaData.scala:75) with 1 output partitions
[INFO][2018-05-30 13:12:53,631][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 0 (isEmpty at ReceiveKafkaData.scala:75)
[INFO][2018-05-30 13:12:53,631][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-30 13:12:53,633][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-30 13:12:53,646][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 0 (MapPartitionsRDD[1] at map at ReceiveKafkaData.scala:74), which has no missing parents
[INFO][2018-05-30 13:12:53,811][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-30 13:12:53,863][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0_piece0 stored as bytes in memory (estimated size 1972.0 B, free 912.3 MB)
[INFO][2018-05-30 13:12:53,865][org.apache.spark.storage.BlockManagerInfo]Added broadcast_0_piece0 in memory on 10.194.67.11:56669 (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-30 13:12:53,870][org.apache.spark.SparkContext]Created broadcast 0 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-30 13:12:53,899][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at map at ReceiveKafkaData.scala:74) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-30 13:12:53,905][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 0.0 with 1 tasks
[INFO][2018-05-30 13:12:54,007][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-30 13:12:54,019][org.apache.spark.executor.Executor]Running task 0.0 in stage 0.0 (TID 0)
[INFO][2018-05-30 13:12:54,063][org.apache.spark.streaming.kafka010.KafkaRDD]Beginning offset 188942 is the same as ending offset skipping seven 0
[INFO][2018-05-30 13:12:54,083][org.apache.spark.executor.Executor]Finished task 0.0 in stage 0.0 (TID 0). 708 bytes result sent to driver
[INFO][2018-05-30 13:12:54,091][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 0.0 (TID 0) in 98 ms on localhost (executor driver) (1/1)
[INFO][2018-05-30 13:12:54,094][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO][2018-05-30 13:12:54,128][org.apache.spark.scheduler.DAGScheduler]ResultStage 0 (isEmpty at ReceiveKafkaData.scala:75) finished in 0.161 s
[INFO][2018-05-30 13:12:54,156][org.apache.spark.scheduler.DAGScheduler]Job 0 finished: isEmpty at ReceiveKafkaData.scala:75, took 0.545943 s
[INFO][2018-05-30 13:12:54,162][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527657165000 ms.0 from job set of time 1527657165000 ms
[INFO][2018-05-30 13:12:54,165][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 9.162 s for time 1527657165000 ms (execution: 0.629 s)
[INFO][2018-05-30 13:12:54,165][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527657170000 ms.0 from job set of time 1527657170000 ms
[INFO][2018-05-30 13:12:54,189][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-30 13:12:54,190][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:75
[INFO][2018-05-30 13:12:54,196][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 
[INFO][2018-05-30 13:12:54,216][org.apache.spark.scheduler.DAGScheduler]Got job 1 (isEmpty at ReceiveKafkaData.scala:75) with 1 output partitions
[INFO][2018-05-30 13:12:54,216][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 1 (isEmpty at ReceiveKafkaData.scala:75)
[INFO][2018-05-30 13:12:54,216][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-30 13:12:54,216][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-30 13:12:54,217][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 1 (MapPartitionsRDD[3] at map at ReceiveKafkaData.scala:74), which has no missing parents
[INFO][2018-05-30 13:12:54,220][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-30 13:12:54,223][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1_piece0 stored as bytes in memory (estimated size 1973.0 B, free 912.3 MB)
[INFO][2018-05-30 13:12:54,224][org.apache.spark.storage.BlockManagerInfo]Added broadcast_1_piece0 in memory on 10.194.67.11:56669 (size: 1973.0 B, free: 912.3 MB)
[INFO][2018-05-30 13:12:54,237][org.apache.spark.SparkContext]Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-30 13:12:54,241][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at map at ReceiveKafkaData.scala:74) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-30 13:12:54,241][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 1.0 with 1 tasks
[INFO][2018-05-30 13:12:54,243][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-30 13:12:54,243][org.apache.spark.executor.Executor]Running task 0.0 in stage 1.0 (TID 1)
[INFO][2018-05-30 13:12:54,249][org.apache.spark.streaming.kafka010.KafkaRDD]Beginning offset 188942 is the same as ending offset skipping seven 0
[INFO][2018-05-30 13:12:54,250][org.apache.spark.executor.Executor]Finished task 0.0 in stage 1.0 (TID 1). 708 bytes result sent to driver
[INFO][2018-05-30 13:12:54,252][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 1.0 (TID 1) in 10 ms on localhost (executor driver) (1/1)
[INFO][2018-05-30 13:12:54,253][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO][2018-05-30 13:12:54,255][org.apache.spark.scheduler.DAGScheduler]ResultStage 1 (isEmpty at ReceiveKafkaData.scala:75) finished in 0.012 s
[INFO][2018-05-30 13:12:54,256][org.apache.spark.scheduler.DAGScheduler]Job 1 finished: isEmpty at ReceiveKafkaData.scala:75, took 0.066132 s
[INFO][2018-05-30 13:12:54,261][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527657170000 ms.0 from job set of time 1527657170000 ms
[INFO][2018-05-30 13:12:54,261][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 4.261 s for time 1527657170000 ms (execution: 0.096 s)
[INFO][2018-05-30 13:12:54,264][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 1 from persistence list
[INFO][2018-05-30 13:12:54,282][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 0 from persistence list
[INFO][2018-05-30 13:12:54,282][org.apache.spark.storage.BlockManager]Removing RDD 1
[INFO][2018-05-30 13:12:54,285][org.apache.spark.storage.BlockManager]Removing RDD 0
[INFO][2018-05-30 13:12:54,285][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-30 13:12:54,287][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 
[INFO][2018-05-30 13:12:55,014][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527657175000 ms
[INFO][2018-05-30 13:12:55,015][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527657175000 ms.0 from job set of time 1527657175000 ms
[INFO][2018-05-30 13:12:55,026][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:75
[INFO][2018-05-30 13:12:55,032][org.apache.spark.scheduler.DAGScheduler]Got job 2 (isEmpty at ReceiveKafkaData.scala:75) with 1 output partitions
[INFO][2018-05-30 13:12:55,032][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 2 (isEmpty at ReceiveKafkaData.scala:75)
[INFO][2018-05-30 13:12:55,032][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-30 13:12:55,032][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-30 13:12:55,033][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 2 (MapPartitionsRDD[5] at map at ReceiveKafkaData.scala:74), which has no missing parents
[INFO][2018-05-30 13:12:55,035][org.apache.spark.storage.memory.MemoryStore]Block broadcast_2 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-30 13:12:55,037][org.apache.spark.storage.memory.MemoryStore]Block broadcast_2_piece0 stored as bytes in memory (estimated size 1973.0 B, free 912.3 MB)
[INFO][2018-05-30 13:12:55,038][org.apache.spark.storage.BlockManagerInfo]Added broadcast_2_piece0 in memory on 10.194.67.11:56669 (size: 1973.0 B, free: 912.3 MB)
[INFO][2018-05-30 13:12:55,039][org.apache.spark.SparkContext]Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-30 13:12:55,040][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[5] at map at ReceiveKafkaData.scala:74) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-30 13:12:55,040][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 2.0 with 1 tasks
[INFO][2018-05-30 13:12:55,041][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-30 13:12:55,041][org.apache.spark.executor.Executor]Running task 0.0 in stage 2.0 (TID 2)
[INFO][2018-05-30 13:12:55,045][org.apache.spark.streaming.kafka010.KafkaRDD]Beginning offset 188942 is the same as ending offset skipping seven 0
[INFO][2018-05-30 13:12:55,047][org.apache.spark.executor.Executor]Finished task 0.0 in stage 2.0 (TID 2). 708 bytes result sent to driver
[INFO][2018-05-30 13:12:55,048][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 2.0 (TID 2) in 7 ms on localhost (executor driver) (1/1)
[INFO][2018-05-30 13:12:55,048][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO][2018-05-30 13:12:55,049][org.apache.spark.scheduler.DAGScheduler]ResultStage 2 (isEmpty at ReceiveKafkaData.scala:75) finished in 0.009 s
[INFO][2018-05-30 13:12:55,049][org.apache.spark.scheduler.DAGScheduler]Job 2 finished: isEmpty at ReceiveKafkaData.scala:75, took 0.022563 s
[INFO][2018-05-30 13:12:55,050][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527657175000 ms.0 from job set of time 1527657175000 ms
[INFO][2018-05-30 13:12:55,050][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 3 from persistence list
[INFO][2018-05-30 13:12:55,050][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.050 s for time 1527657175000 ms (execution: 0.036 s)
[INFO][2018-05-30 13:12:55,050][org.apache.spark.storage.BlockManager]Removing RDD 3
[INFO][2018-05-30 13:12:55,051][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 2 from persistence list
[INFO][2018-05-30 13:12:55,051][org.apache.spark.storage.BlockManager]Removing RDD 2
[INFO][2018-05-30 13:12:55,051][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-30 13:12:55,051][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527657165000 ms
[INFO][2018-05-30 13:13:00,020][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527657180000 ms
[INFO][2018-05-30 13:13:00,021][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527657180000 ms.0 from job set of time 1527657180000 ms
[INFO][2018-05-30 13:13:00,028][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:75
[INFO][2018-05-30 13:13:00,029][org.apache.spark.scheduler.DAGScheduler]Got job 3 (isEmpty at ReceiveKafkaData.scala:75) with 1 output partitions
[INFO][2018-05-30 13:13:00,030][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 3 (isEmpty at ReceiveKafkaData.scala:75)
[INFO][2018-05-30 13:13:00,030][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-30 13:13:00,030][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-30 13:13:00,030][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 3 (MapPartitionsRDD[7] at map at ReceiveKafkaData.scala:74), which has no missing parents
[INFO][2018-05-30 13:13:00,034][org.apache.spark.storage.memory.MemoryStore]Block broadcast_3 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-30 13:13:00,036][org.apache.spark.storage.memory.MemoryStore]Block broadcast_3_piece0 stored as bytes in memory (estimated size 1971.0 B, free 912.3 MB)
[INFO][2018-05-30 13:13:00,036][org.apache.spark.storage.BlockManagerInfo]Added broadcast_3_piece0 in memory on 10.194.67.11:56669 (size: 1971.0 B, free: 912.3 MB)
[INFO][2018-05-30 13:13:00,037][org.apache.spark.SparkContext]Created broadcast 3 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-30 13:13:00,038][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[7] at map at ReceiveKafkaData.scala:74) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-30 13:13:00,038][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 3.0 with 1 tasks
[INFO][2018-05-30 13:13:00,039][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 3.0 (TID 3, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-30 13:13:00,040][org.apache.spark.executor.Executor]Running task 0.0 in stage 3.0 (TID 3)
[INFO][2018-05-30 13:13:00,045][org.apache.spark.streaming.kafka010.KafkaRDD]Beginning offset 188942 is the same as ending offset skipping seven 0
[INFO][2018-05-30 13:13:00,046][org.apache.spark.executor.Executor]Finished task 0.0 in stage 3.0 (TID 3). 708 bytes result sent to driver
[INFO][2018-05-30 13:13:00,047][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 3.0 (TID 3) in 8 ms on localhost (executor driver) (1/1)
[INFO][2018-05-30 13:13:00,048][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 3.0, whose tasks have all completed, from pool 
[INFO][2018-05-30 13:13:00,048][org.apache.spark.scheduler.DAGScheduler]ResultStage 3 (isEmpty at ReceiveKafkaData.scala:75) finished in 0.009 s
[INFO][2018-05-30 13:13:00,049][org.apache.spark.scheduler.DAGScheduler]Job 3 finished: isEmpty at ReceiveKafkaData.scala:75, took 0.019708 s
[INFO][2018-05-30 13:13:00,049][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527657180000 ms.0 from job set of time 1527657180000 ms
[INFO][2018-05-30 13:13:00,049][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 5 from persistence list
[INFO][2018-05-30 13:13:00,049][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.049 s for time 1527657180000 ms (execution: 0.028 s)
[INFO][2018-05-30 13:13:00,050][org.apache.spark.storage.BlockManager]Removing RDD 5
[INFO][2018-05-30 13:13:00,050][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 4 from persistence list
[INFO][2018-05-30 13:13:00,050][org.apache.spark.storage.BlockManager]Removing RDD 4
[INFO][2018-05-30 13:13:00,051][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-30 13:13:00,051][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527657170000 ms
[INFO][2018-05-30 13:13:03,235][org.apache.spark.SparkContext]Running Spark version 2.2.0
[INFO][2018-05-30 13:13:04,362][org.apache.spark.SparkContext]Submitted application: SimulationKafkaSendOutData$
[INFO][2018-05-30 13:13:04,408][org.apache.spark.SecurityManager]Changing view acls to: seven
[INFO][2018-05-30 13:13:04,409][org.apache.spark.SecurityManager]Changing modify acls to: seven
[INFO][2018-05-30 13:13:04,410][org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO][2018-05-30 13:13:04,410][org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO][2018-05-30 13:13:04,411][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(seven); groups with view permissions: Set(); users  with modify permissions: Set(seven); groups with modify permissions: Set()
[INFO][2018-05-30 13:13:04,861][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 56679.
[INFO][2018-05-30 13:13:04,884][org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO][2018-05-30 13:13:04,899][org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO][2018-05-30 13:13:04,903][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2018-05-30 13:13:04,904][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
[INFO][2018-05-30 13:13:04,913][org.apache.spark.storage.DiskBlockManager]Created local directory at /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/blockmgr-4ac8adc8-c349-4c1b-b96a-1b0e4a6950b7
[INFO][2018-05-30 13:13:04,930][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 912.3 MB
[INFO][2018-05-30 13:13:05,016][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527657185000 ms
[INFO][2018-05-30 13:13:05,016][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527657185000 ms.0 from job set of time 1527657185000 ms
[INFO][2018-05-30 13:13:05,019][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO][2018-05-30 13:13:05,024][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:75
[INFO][2018-05-30 13:13:05,026][org.apache.spark.scheduler.DAGScheduler]Got job 4 (isEmpty at ReceiveKafkaData.scala:75) with 1 output partitions
[INFO][2018-05-30 13:13:05,026][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 4 (isEmpty at ReceiveKafkaData.scala:75)
[INFO][2018-05-30 13:13:05,026][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-30 13:13:05,026][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-30 13:13:05,027][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 4 (MapPartitionsRDD[9] at map at ReceiveKafkaData.scala:74), which has no missing parents
[INFO][2018-05-30 13:13:05,029][org.apache.spark.storage.memory.MemoryStore]Block broadcast_4 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-30 13:13:05,038][org.apache.spark.storage.memory.MemoryStore]Block broadcast_4_piece0 stored as bytes in memory (estimated size 1973.0 B, free 912.3 MB)
[INFO][2018-05-30 13:13:05,039][org.apache.spark.storage.BlockManagerInfo]Added broadcast_4_piece0 in memory on 10.194.67.11:56669 (size: 1973.0 B, free: 912.3 MB)
[INFO][2018-05-30 13:13:05,039][org.apache.spark.SparkContext]Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-30 13:13:05,040][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[9] at map at ReceiveKafkaData.scala:74) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-30 13:13:05,040][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 4.0 with 1 tasks
[INFO][2018-05-30 13:13:05,041][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 4.0 (TID 4, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-30 13:13:05,041][org.apache.spark.executor.Executor]Running task 0.0 in stage 4.0 (TID 4)
[INFO][2018-05-30 13:13:05,046][org.apache.spark.streaming.kafka010.KafkaRDD]Beginning offset 188942 is the same as ending offset skipping seven 0
[INFO][2018-05-30 13:13:05,047][org.apache.spark.executor.Executor]Finished task 0.0 in stage 4.0 (TID 4). 708 bytes result sent to driver
[INFO][2018-05-30 13:13:05,048][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 4.0 (TID 4) in 7 ms on localhost (executor driver) (1/1)
[INFO][2018-05-30 13:13:05,048][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 4.0, whose tasks have all completed, from pool 
[INFO][2018-05-30 13:13:05,049][org.apache.spark.scheduler.DAGScheduler]ResultStage 4 (isEmpty at ReceiveKafkaData.scala:75) finished in 0.007 s
[INFO][2018-05-30 13:13:05,049][org.apache.spark.scheduler.DAGScheduler]Job 4 finished: isEmpty at ReceiveKafkaData.scala:75, took 0.024480 s
[INFO][2018-05-30 13:13:05,050][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527657185000 ms.0 from job set of time 1527657185000 ms
[INFO][2018-05-30 13:13:05,050][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.050 s for time 1527657185000 ms (execution: 0.034 s)
[INFO][2018-05-30 13:13:05,050][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 7 from persistence list
[INFO][2018-05-30 13:13:05,051][org.apache.spark.storage.BlockManager]Removing RDD 7
[INFO][2018-05-30 13:13:05,051][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 6 from persistence list
[INFO][2018-05-30 13:13:05,051][org.apache.spark.storage.BlockManager]Removing RDD 6
[INFO][2018-05-30 13:13:05,051][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-30 13:13:05,052][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527657175000 ms
[INFO][2018-05-30 13:13:05,140][org.spark_project.jetty.util.log]Logging initialized @2892ms
[INFO][2018-05-30 13:13:05,219][org.spark_project.jetty.server.Server]jetty-9.3.z-SNAPSHOT
[INFO][2018-05-30 13:13:05,235][org.spark_project.jetty.server.Server]Started @2989ms
[WARN][2018-05-30 13:13:05,260][org.apache.spark.util.Utils]Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[INFO][2018-05-30 13:13:05,272][org.spark_project.jetty.server.AbstractConnector]Started ServerConnector@4642361e{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}
[INFO][2018-05-30 13:13:05,273][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4041.
[INFO][2018-05-30 13:13:05,315][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4a9f80d3{/jobs,null,AVAILABLE,@Spark}
[INFO][2018-05-30 13:13:05,315][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3e2822{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2018-05-30 13:13:05,316][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@29a60c27{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2018-05-30 13:13:05,317][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1ca25c47{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2018-05-30 13:13:05,317][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@533b266e{/stages,null,AVAILABLE,@Spark}
[INFO][2018-05-30 13:13:05,318][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@89ff02e{/stages/json,null,AVAILABLE,@Spark}
[INFO][2018-05-30 13:13:05,318][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@62679465{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2018-05-30 13:13:05,321][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5f2606b{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-30 13:13:05,323][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3ebff828{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2018-05-30 13:13:05,325][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@33352f32{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2018-05-30 13:13:05,326][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1e044120{/storage,null,AVAILABLE,@Spark}
[INFO][2018-05-30 13:13:05,327][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3624da92{/storage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-30 13:13:05,327][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@94f6bfb{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2018-05-30 13:13:05,328][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2484f433{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2018-05-30 13:13:05,329][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1255b1d1{/environment,null,AVAILABLE,@Spark}
[INFO][2018-05-30 13:13:05,329][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7c22d4f{/environment/json,null,AVAILABLE,@Spark}
[INFO][2018-05-30 13:13:05,330][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@60bdf15d{/executors,null,AVAILABLE,@Spark}
[INFO][2018-05-30 13:13:05,331][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@51e4ccb3{/executors/json,null,AVAILABLE,@Spark}
[INFO][2018-05-30 13:13:05,331][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@495083a0{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2018-05-30 13:13:05,332][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@28a0fd6c{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2018-05-30 13:13:05,341][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@66629f63{/static,null,AVAILABLE,@Spark}
[INFO][2018-05-30 13:13:05,342][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1d9bec4d{/,null,AVAILABLE,@Spark}
[INFO][2018-05-30 13:13:05,350][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@10c8f62{/api,null,AVAILABLE,@Spark}
[INFO][2018-05-30 13:13:05,351][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@439a8f59{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-30 13:13:05,352][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@31024624{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-30 13:13:05,365][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://10.194.67.11:4041
[INFO][2018-05-30 13:13:05,532][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
[INFO][2018-05-30 13:13:05,577][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 56680.
[INFO][2018-05-30 13:13:05,579][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 10.194.67.11:56680
[INFO][2018-05-30 13:13:05,581][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2018-05-30 13:13:05,583][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 10.194.67.11, 56680, None)
[INFO][2018-05-30 13:13:05,591][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 10.194.67.11:56680 with 912.3 MB RAM, BlockManagerId(driver, 10.194.67.11, 56680, None)
[INFO][2018-05-30 13:13:05,595][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 10.194.67.11, 56680, None)
[INFO][2018-05-30 13:13:05,596][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 10.194.67.11, 56680, None)
[INFO][2018-05-30 13:13:05,956][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6df20ade{/metrics/json,null,AVAILABLE,@Spark}
[INFO][2018-05-30 13:13:06,697][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0 stored as values in memory (estimated size 228.1 KB, free 912.1 MB)
[INFO][2018-05-30 13:13:06,773][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0_piece0 stored as bytes in memory (estimated size 22.1 KB, free 912.1 MB)
[INFO][2018-05-30 13:13:06,776][org.apache.spark.storage.BlockManagerInfo]Added broadcast_0_piece0 in memory on 10.194.67.11:56680 (size: 22.1 KB, free: 912.3 MB)
[INFO][2018-05-30 13:13:06,781][org.apache.spark.SparkContext]Created broadcast 0 from textFile at SimulationKafkaSendOutData.scala:24
[WARN][2018-05-30 13:13:07,334][org.apache.hadoop.util.NativeCodeLoader]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO][2018-05-30 13:13:07,568][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 1
[INFO][2018-05-30 13:13:07,690][org.apache.spark.SparkContext]Starting job: collect at SimulationKafkaSendOutData.scala:24
[INFO][2018-05-30 13:13:07,709][org.apache.spark.scheduler.DAGScheduler]Got job 0 (collect at SimulationKafkaSendOutData.scala:24) with 2 output partitions
[INFO][2018-05-30 13:13:07,710][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 0 (collect at SimulationKafkaSendOutData.scala:24)
[INFO][2018-05-30 13:13:07,710][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-30 13:13:07,711][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-30 13:13:07,718][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 0 (hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/* MapPartitionsRDD[1] at textFile at SimulationKafkaSendOutData.scala:24), which has no missing parents
[INFO][2018-05-30 13:13:07,738][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1 stored as values in memory (estimated size 3.3 KB, free 912.1 MB)
[INFO][2018-05-30 13:13:07,741][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1_piece0 stored as bytes in memory (estimated size 1969.0 B, free 912.1 MB)
[INFO][2018-05-30 13:13:07,741][org.apache.spark.storage.BlockManagerInfo]Added broadcast_1_piece0 in memory on 10.194.67.11:56680 (size: 1969.0 B, free: 912.3 MB)
[INFO][2018-05-30 13:13:07,742][org.apache.spark.SparkContext]Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-30 13:13:07,755][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ResultStage 0 (hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/* MapPartitionsRDD[1] at textFile at SimulationKafkaSendOutData.scala:24) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2018-05-30 13:13:07,756][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 0.0 with 2 tasks
[INFO][2018-05-30 13:13:07,813][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 4883 bytes)
[INFO][2018-05-30 13:13:07,817][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 4883 bytes)
[INFO][2018-05-30 13:13:07,832][org.apache.spark.executor.Executor]Running task 0.0 in stage 0.0 (TID 0)
[INFO][2018-05-30 13:13:07,834][org.apache.spark.executor.Executor]Running task 1.0 in stage 0.0 (TID 1)
[INFO][2018-05-30 13:13:07,927][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/part-00000:12229859+12229860
[INFO][2018-05-30 13:13:07,927][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/part-00000:0+12229859
[INFO][2018-05-30 13:13:10,039][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527657190000 ms
[INFO][2018-05-30 13:13:10,044][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527657190000 ms.0 from job set of time 1527657190000 ms
[INFO][2018-05-30 13:13:10,052][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:75
[INFO][2018-05-30 13:13:10,053][org.apache.spark.scheduler.DAGScheduler]Got job 5 (isEmpty at ReceiveKafkaData.scala:75) with 1 output partitions
[INFO][2018-05-30 13:13:10,053][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 5 (isEmpty at ReceiveKafkaData.scala:75)
[INFO][2018-05-30 13:13:10,053][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-30 13:13:10,053][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-30 13:13:10,054][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 5 (MapPartitionsRDD[11] at map at ReceiveKafkaData.scala:74), which has no missing parents
[INFO][2018-05-30 13:13:10,056][org.apache.spark.storage.memory.MemoryStore]Block broadcast_5 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-30 13:13:10,066][org.apache.spark.storage.memory.MemoryStore]Block broadcast_5_piece0 stored as bytes in memory (estimated size 1973.0 B, free 912.3 MB)
[INFO][2018-05-30 13:13:10,067][org.apache.spark.storage.BlockManagerInfo]Added broadcast_5_piece0 in memory on 10.194.67.11:56669 (size: 1973.0 B, free: 912.3 MB)
[INFO][2018-05-30 13:13:10,068][org.apache.spark.SparkContext]Created broadcast 5 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-30 13:13:10,069][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[11] at map at ReceiveKafkaData.scala:74) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-30 13:13:10,069][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 5.0 with 1 tasks
[INFO][2018-05-30 13:13:10,070][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 5.0 (TID 5, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-30 13:13:10,070][org.apache.spark.executor.Executor]Running task 0.0 in stage 5.0 (TID 5)
[INFO][2018-05-30 13:13:10,074][org.apache.spark.streaming.kafka010.KafkaRDD]Beginning offset 188942 is the same as ending offset skipping seven 0
[INFO][2018-05-30 13:13:10,076][org.apache.spark.executor.Executor]Finished task 0.0 in stage 5.0 (TID 5). 708 bytes result sent to driver
[INFO][2018-05-30 13:13:10,077][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 5.0 (TID 5) in 8 ms on localhost (executor driver) (1/1)
[INFO][2018-05-30 13:13:10,077][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 5.0, whose tasks have all completed, from pool 
[INFO][2018-05-30 13:13:10,078][org.apache.spark.scheduler.DAGScheduler]ResultStage 5 (isEmpty at ReceiveKafkaData.scala:75) finished in 0.008 s
[INFO][2018-05-30 13:13:10,078][org.apache.spark.scheduler.DAGScheduler]Job 5 finished: isEmpty at ReceiveKafkaData.scala:75, took 0.025968 s
[INFO][2018-05-30 13:13:10,079][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527657190000 ms.0 from job set of time 1527657190000 ms
[INFO][2018-05-30 13:13:10,079][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.078 s for time 1527657190000 ms (execution: 0.035 s)
[INFO][2018-05-30 13:13:10,079][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 9 from persistence list
[INFO][2018-05-30 13:13:10,079][org.apache.spark.storage.BlockManager]Removing RDD 9
[INFO][2018-05-30 13:13:10,079][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 8 from persistence list
[INFO][2018-05-30 13:13:10,080][org.apache.spark.storage.BlockManager]Removing RDD 8
[INFO][2018-05-30 13:13:10,080][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-30 13:13:10,080][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527657180000 ms
[INFO][2018-05-30 13:13:10,341][org.apache.spark.storage.memory.MemoryStore]Block taskresult_1 stored as bytes in memory (estimated size 11.8 MB, free 900.2 MB)
[INFO][2018-05-30 13:13:10,343][org.apache.spark.storage.BlockManagerInfo]Added taskresult_1 in memory on 10.194.67.11:56680 (size: 11.8 MB, free: 900.5 MB)
[INFO][2018-05-30 13:13:10,345][org.apache.spark.executor.Executor]Finished task 1.0 in stage 0.0 (TID 1). 12387903 bytes result sent via BlockManager)
[INFO][2018-05-30 13:13:10,380][org.apache.spark.storage.memory.MemoryStore]Block taskresult_0 stored as bytes in memory (estimated size 11.8 MB, free 888.4 MB)
[INFO][2018-05-30 13:13:10,381][org.apache.spark.storage.BlockManagerInfo]Added taskresult_0 in memory on 10.194.67.11:56680 (size: 11.8 MB, free: 888.6 MB)
[INFO][2018-05-30 13:13:10,381][org.apache.spark.executor.Executor]Finished task 0.0 in stage 0.0 (TID 0). 12388162 bytes result sent via BlockManager)
[INFO][2018-05-30 13:13:10,391][org.apache.spark.network.client.TransportClientFactory]Successfully created connection to /10.194.67.11:56680 after 25 ms (0 ms spent in bootstraps)
[INFO][2018-05-30 13:13:10,850][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 0.0 (TID 1) in 3034 ms on localhost (executor driver) (1/2)
[INFO][2018-05-30 13:13:10,858][org.apache.spark.storage.BlockManagerInfo]Removed taskresult_1 on 10.194.67.11:56680 in memory (size: 11.8 MB, free: 900.5 MB)
[INFO][2018-05-30 13:13:10,858][org.apache.spark.storage.BlockManagerInfo]Removed taskresult_0 on 10.194.67.11:56680 in memory (size: 11.8 MB, free: 912.3 MB)
[INFO][2018-05-30 13:13:10,861][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 0.0 (TID 0) in 3068 ms on localhost (executor driver) (2/2)
[INFO][2018-05-30 13:13:10,865][org.apache.spark.scheduler.DAGScheduler]ResultStage 0 (collect at SimulationKafkaSendOutData.scala:24) finished in 3.087 s
[INFO][2018-05-30 13:13:10,864][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO][2018-05-30 13:13:10,871][org.apache.spark.scheduler.DAGScheduler]Job 0 finished: collect at SimulationKafkaSendOutData.scala:24, took 3.180018 s
[INFO][2018-05-30 13:13:11,295][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_0_piece0 on 10.194.67.11:56680 in memory (size: 22.1 KB, free: 912.3 MB)
[INFO][2018-05-30 13:13:11,301][org.spark_project.jetty.server.AbstractConnector]Stopped Spark@4642361e{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}
[INFO][2018-05-30 13:13:11,301][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_1_piece0 on 10.194.67.11:56680 in memory (size: 1969.0 B, free: 912.3 MB)
[INFO][2018-05-30 13:13:11,304][org.apache.spark.ui.SparkUI]Stopped Spark web UI at http://10.194.67.11:4041
[INFO][2018-05-30 13:13:11,320][org.apache.spark.MapOutputTrackerMasterEndpoint]MapOutputTrackerMasterEndpoint stopped!
[INFO][2018-05-30 13:13:11,336][org.apache.spark.storage.memory.MemoryStore]MemoryStore cleared
[INFO][2018-05-30 13:13:11,336][org.apache.spark.storage.BlockManager]BlockManager stopped
[INFO][2018-05-30 13:13:11,338][org.apache.spark.storage.BlockManagerMaster]BlockManagerMaster stopped
[INFO][2018-05-30 13:13:11,341][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint]OutputCommitCoordinator stopped!
[INFO][2018-05-30 13:13:11,342][org.apache.spark.SparkContext]Successfully stopped SparkContext
[INFO][2018-05-30 13:13:11,360][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	metric.reporters = []
	metadata.max.age.ms = 300000
	reconnect.backoff.ms = 50
	sasl.kerberos.ticket.renew.window.factor = 0.8
	bootstrap.servers = [vm-xaj-bigdata-da-d01:9092, vm-xaj-bigdata-da-d02:9092, vm-xaj-bigdata-da-d03:9092, vm-xaj-bigdata-da-d04:9092, vm-xaj-bigdata-da-d05:9092, vm-xaj-bigdata-da-d06:9092, vm-xaj-bigdata-da-d07:9092]
	ssl.keystore.type = JKS
	sasl.mechanism = GSSAPI
	max.block.ms = 60000
	interceptor.classes = null
	ssl.truststore.password = null
	client.id = 
	ssl.endpoint.identification.algorithm = null
	request.timeout.ms = 30000
	acks = all
	receive.buffer.bytes = 32768
	ssl.truststore.type = JKS
	retries = 0
	ssl.truststore.location = null
	ssl.keystore.password = null
	send.buffer.bytes = 131072
	compression.type = none
	metadata.fetch.timeout.ms = 60000
	retry.backoff.ms = 100
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	buffer.memory = 33554432
	timeout.ms = 30000
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	ssl.trustmanager.algorithm = PKIX
	block.on.buffer.full = false
	ssl.key.password = null
	sasl.kerberos.min.time.before.relogin = 60000
	connections.max.idle.ms = 540000
	max.in.flight.requests.per.connection = 5
	metrics.num.samples = 2
	ssl.protocol = TLS
	ssl.provider = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	batch.size = 16384
	ssl.keystore.location = null
	ssl.cipher.suites = null
	security.protocol = PLAINTEXT
	max.request.size = 1048576
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
	ssl.keymanager.algorithm = SunX509
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	linger.ms = 1

[INFO][2018-05-30 13:13:11,395][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	metric.reporters = []
	metadata.max.age.ms = 300000
	reconnect.backoff.ms = 50
	sasl.kerberos.ticket.renew.window.factor = 0.8
	bootstrap.servers = [vm-xaj-bigdata-da-d01:9092, vm-xaj-bigdata-da-d02:9092, vm-xaj-bigdata-da-d03:9092, vm-xaj-bigdata-da-d04:9092, vm-xaj-bigdata-da-d05:9092, vm-xaj-bigdata-da-d06:9092, vm-xaj-bigdata-da-d07:9092]
	ssl.keystore.type = JKS
	sasl.mechanism = GSSAPI
	max.block.ms = 60000
	interceptor.classes = null
	ssl.truststore.password = null
	client.id = producer-1
	ssl.endpoint.identification.algorithm = null
	request.timeout.ms = 30000
	acks = all
	receive.buffer.bytes = 32768
	ssl.truststore.type = JKS
	retries = 0
	ssl.truststore.location = null
	ssl.keystore.password = null
	send.buffer.bytes = 131072
	compression.type = none
	metadata.fetch.timeout.ms = 60000
	retry.backoff.ms = 100
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	buffer.memory = 33554432
	timeout.ms = 30000
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	ssl.trustmanager.algorithm = PKIX
	block.on.buffer.full = false
	ssl.key.password = null
	sasl.kerberos.min.time.before.relogin = 60000
	connections.max.idle.ms = 540000
	max.in.flight.requests.per.connection = 5
	metrics.num.samples = 2
	ssl.protocol = TLS
	ssl.provider = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	batch.size = 16384
	ssl.keystore.location = null
	ssl.cipher.suites = null
	security.protocol = PLAINTEXT
	max.request.size = 1048576
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
	ssl.keymanager.algorithm = SunX509
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	linger.ms = 1

[INFO][2018-05-30 13:13:11,398][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 0.10.0.1
[INFO][2018-05-30 13:13:11,398][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a7a17cdec9eaa6c5
[INFO][2018-05-30 13:13:15,012][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527657195000 ms
[INFO][2018-05-30 13:13:15,013][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527657195000 ms.0 from job set of time 1527657195000 ms
[INFO][2018-05-30 13:13:15,021][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:75
[INFO][2018-05-30 13:13:15,022][org.apache.spark.scheduler.DAGScheduler]Got job 6 (isEmpty at ReceiveKafkaData.scala:75) with 1 output partitions
[INFO][2018-05-30 13:13:15,022][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 6 (isEmpty at ReceiveKafkaData.scala:75)
[INFO][2018-05-30 13:13:15,022][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-30 13:13:15,022][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-30 13:13:15,022][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 6 (MapPartitionsRDD[13] at map at ReceiveKafkaData.scala:74), which has no missing parents
[INFO][2018-05-30 13:13:15,025][org.apache.spark.storage.memory.MemoryStore]Block broadcast_6 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-30 13:13:15,029][org.apache.spark.storage.memory.MemoryStore]Block broadcast_6_piece0 stored as bytes in memory (estimated size 1973.0 B, free 912.3 MB)
[INFO][2018-05-30 13:13:15,030][org.apache.spark.storage.BlockManagerInfo]Added broadcast_6_piece0 in memory on 10.194.67.11:56669 (size: 1973.0 B, free: 912.3 MB)
[INFO][2018-05-30 13:13:15,031][org.apache.spark.SparkContext]Created broadcast 6 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-30 13:13:15,031][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[13] at map at ReceiveKafkaData.scala:74) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-30 13:13:15,031][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 6.0 with 1 tasks
[INFO][2018-05-30 13:13:15,032][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 6.0 (TID 6, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-30 13:13:15,033][org.apache.spark.executor.Executor]Running task 0.0 in stage 6.0 (TID 6)
[INFO][2018-05-30 13:13:15,039][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 188942 -> 188975
[INFO][2018-05-30 13:13:15,042][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initializing cache 16 64 0.75
[INFO][2018-05-30 13:13:15,044][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Cache miss for CacheKey(spark-executor-use_a_separate_group_id_for_each_stream,seven,0)
[INFO][2018-05-30 13:13:15,047][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	metric.reporters = []
	metadata.max.age.ms = 300000
	partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor]
	reconnect.backoff.ms = 50
	sasl.kerberos.ticket.renew.window.factor = 0.8
	max.partition.fetch.bytes = 1048576
	bootstrap.servers = [vm-xaj-bigdata-da-d01:9092, vm-xaj-bigdata-da-d02:9092, vm-xaj-bigdata-da-d03:9092, vm-xaj-bigdata-da-d04:9092, vm-xaj-bigdata-da-d05:9092, vm-xaj-bigdata-da-d06:9092, vm-xaj-bigdata-da-d07:9092]
	ssl.keystore.type = JKS
	enable.auto.commit = false
	sasl.mechanism = GSSAPI
	interceptor.classes = null
	exclude.internal.topics = true
	ssl.truststore.password = null
	client.id = 
	ssl.endpoint.identification.algorithm = null
	max.poll.records = 2147483647
	check.crcs = true
	request.timeout.ms = 40000
	heartbeat.interval.ms = 3000
	auto.commit.interval.ms = 5000
	receive.buffer.bytes = 65536
	ssl.truststore.type = JKS
	ssl.truststore.location = null
	ssl.keystore.password = null
	fetch.min.bytes = 1
	send.buffer.bytes = 131072
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	group.id = spark-executor-use_a_separate_group_id_for_each_stream
	retry.backoff.ms = 100
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	ssl.trustmanager.algorithm = PKIX
	ssl.key.password = null
	fetch.max.wait.ms = 500
	sasl.kerberos.min.time.before.relogin = 60000
	connections.max.idle.ms = 540000
	session.timeout.ms = 30000
	metrics.num.samples = 2
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	ssl.protocol = TLS
	ssl.provider = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.keystore.location = null
	ssl.cipher.suites = null
	security.protocol = PLAINTEXT
	ssl.keymanager.algorithm = SunX509
	metrics.sample.window.ms = 30000
	auto.offset.reset = none

[INFO][2018-05-30 13:13:15,051][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	metric.reporters = []
	metadata.max.age.ms = 300000
	partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor]
	reconnect.backoff.ms = 50
	sasl.kerberos.ticket.renew.window.factor = 0.8
	max.partition.fetch.bytes = 1048576
	bootstrap.servers = [vm-xaj-bigdata-da-d01:9092, vm-xaj-bigdata-da-d02:9092, vm-xaj-bigdata-da-d03:9092, vm-xaj-bigdata-da-d04:9092, vm-xaj-bigdata-da-d05:9092, vm-xaj-bigdata-da-d06:9092, vm-xaj-bigdata-da-d07:9092]
	ssl.keystore.type = JKS
	enable.auto.commit = false
	sasl.mechanism = GSSAPI
	interceptor.classes = null
	exclude.internal.topics = true
	ssl.truststore.password = null
	client.id = consumer-2
	ssl.endpoint.identification.algorithm = null
	max.poll.records = 2147483647
	check.crcs = true
	request.timeout.ms = 40000
	heartbeat.interval.ms = 3000
	auto.commit.interval.ms = 5000
	receive.buffer.bytes = 65536
	ssl.truststore.type = JKS
	ssl.truststore.location = null
	ssl.keystore.password = null
	fetch.min.bytes = 1
	send.buffer.bytes = 131072
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	group.id = spark-executor-use_a_separate_group_id_for_each_stream
	retry.backoff.ms = 100
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	ssl.trustmanager.algorithm = PKIX
	ssl.key.password = null
	fetch.max.wait.ms = 500
	sasl.kerberos.min.time.before.relogin = 60000
	connections.max.idle.ms = 540000
	session.timeout.ms = 30000
	metrics.num.samples = 2
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	ssl.protocol = TLS
	ssl.provider = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.keystore.location = null
	ssl.cipher.suites = null
	security.protocol = PLAINTEXT
	ssl.keymanager.algorithm = SunX509
	metrics.sample.window.ms = 30000
	auto.offset.reset = none

[INFO][2018-05-30 13:13:15,053][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 0.10.0.1
[INFO][2018-05-30 13:13:15,053][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a7a17cdec9eaa6c5
[INFO][2018-05-30 13:13:15,056][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 188942
[INFO][2018-05-30 13:13:15,171][org.apache.kafka.clients.consumer.internals.AbstractCoordinator]Discovered coordinator vm-xaj-bigdata-da-d06:9092 (id: 2147483530 rack: null) for group spark-executor-use_a_separate_group_id_for_each_stream.
[INFO][2018-05-30 13:13:15,224][org.apache.spark.executor.Executor]Finished task 0.0 in stage 6.0 (TID 6). 1012 bytes result sent to driver
[INFO][2018-05-30 13:13:15,225][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 6.0 (TID 6) in 193 ms on localhost (executor driver) (1/1)
[INFO][2018-05-30 13:13:15,226][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 6.0, whose tasks have all completed, from pool 
[INFO][2018-05-30 13:13:15,226][org.apache.spark.scheduler.DAGScheduler]ResultStage 6 (isEmpty at ReceiveKafkaData.scala:75) finished in 0.194 s
[INFO][2018-05-30 13:13:15,227][org.apache.spark.scheduler.DAGScheduler]Job 6 finished: isEmpty at ReceiveKafkaData.scala:75, took 0.205291 s
[INFO][2018-05-30 13:13:15,239][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:76
[INFO][2018-05-30 13:13:15,239][org.apache.spark.scheduler.DAGScheduler]Got job 7 (foreachPartition at ReceiveKafkaData.scala:76) with 1 output partitions
[INFO][2018-05-30 13:13:15,239][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 7 (foreachPartition at ReceiveKafkaData.scala:76)
[INFO][2018-05-30 13:13:15,239][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-30 13:13:15,240][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-30 13:13:15,240][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 7 (MapPartitionsRDD[13] at map at ReceiveKafkaData.scala:74), which has no missing parents
[INFO][2018-05-30 13:13:15,241][org.apache.spark.storage.memory.MemoryStore]Block broadcast_7 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-30 13:13:15,245][org.apache.spark.storage.memory.MemoryStore]Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.3 MB)
[INFO][2018-05-30 13:13:15,246][org.apache.spark.storage.BlockManagerInfo]Added broadcast_7_piece0 in memory on 10.194.67.11:56669 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-30 13:13:15,246][org.apache.spark.SparkContext]Created broadcast 7 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-30 13:13:15,247][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[13] at map at ReceiveKafkaData.scala:74) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-30 13:13:15,247][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 7.0 with 1 tasks
[INFO][2018-05-30 13:13:15,248][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 7.0 (TID 7, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-30 13:13:15,248][org.apache.spark.executor.Executor]Running task 0.0 in stage 7.0 (TID 7)
[INFO][2018-05-30 13:13:15,252][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 188942 -> 188975
[INFO][2018-05-30 13:13:15,252][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 188942
[INFO][2018-05-30 13:13:15,496][org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper]Process identifier=hconnection-0x5b1b21af connecting to ZooKeeper ensemble=vm-xaj-bigdata-da-d01:2181,vm-xaj-bigdata-da-d02:2181,vm-xaj-bigdata-da-d03:2181
[INFO][2018-05-30 13:13:15,507][org.apache.zookeeper.ZooKeeper]Client environment:zookeeper.version=3.4.5-cdh5.7.0--1, built on 04/06/2016 14:24 GMT
[INFO][2018-05-30 13:13:15,508][org.apache.zookeeper.ZooKeeper]Client environment:host.name=10.194.67.11
[INFO][2018-05-30 13:13:15,508][org.apache.zookeeper.ZooKeeper]Client environment:java.version=1.8.0_161
[INFO][2018-05-30 13:13:15,508][org.apache.zookeeper.ZooKeeper]Client environment:java.vendor=Oracle Corporation
[INFO][2018-05-30 13:13:15,508][org.apache.zookeeper.ZooKeeper]Client environment:java.home=/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre
[INFO][2018-05-30 13:13:15,508][org.apache.zookeeper.ZooKeeper]Client environment:java.class.path=/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/charsets.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/deploy.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/cldrdata.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/dnsns.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/jaccess.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/jfxrt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/localedata.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/nashorn.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/sunec.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/sunjce_provider.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/sunpkcs11.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/zipfs.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/javaws.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/jce.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/jfr.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/jfxswt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/jsse.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/management-agent.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/plugin.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/resources.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/rt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/ant-javafx.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/dt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/javafx-mx.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/jconsole.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/packager.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/sa-jdi.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/tools.jar:/Users/seven/project/github/dataMining/target/classes:/Users/seven/software/maven/repository/org/scala-lang/scala-library/2.11.12/scala-library-2.11.12.jar:/Users/seven/software/maven/repository/org/scala-lang/scala-compiler/2.11.12/scala-compiler-2.11.12.jar:/Users/seven/software/maven/repository/org/scala-lang/modules/scala-xml_2.11/1.0.5/scala-xml_2.11-1.0.5.jar:/Users/seven/software/maven/repository/org/scala-lang/modules/scala-parser-combinators_2.11/1.0.4/scala-parser-combinators_2.11-1.0.4.jar:/Users/seven/software/maven/repository/org/scala-lang/scala-reflect/2.11.12/scala-reflect-2.11.12.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-client/2.6.0-cdh5.7.0/hadoop-client-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-common/2.6.0-cdh5.7.0/hadoop-common-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/xmlenc/xmlenc/0.52/xmlenc-0.52.jar:/Users/seven/software/maven/repository/commons-net/commons-net/3.1/commons-net-3.1.jar:/Users/seven/software/maven/repository/commons-configuration/commons-configuration/1.6/commons-configuration-1.6.jar:/Users/seven/software/maven/repository/commons-digester/commons-digester/1.8/commons-digester-1.8.jar:/Users/seven/software/maven/repository/commons-beanutils/commons-beanutils/1.7.0/commons-beanutils-1.7.0.jar:/Users/seven/software/maven/repository/org/slf4j/slf4j-api/1.7.5/slf4j-api-1.7.5.jar:/Users/seven/software/maven/repository/org/slf4j/slf4j-log4j12/1.7.5/slf4j-log4j12-1.7.5.jar:/Users/seven/software/maven/repository/org/apache/avro/avro/1.7.6-cdh5.7.0/avro-1.7.6-cdh5.7.0.jar:/Users/seven/software/maven/repository/com/google/code/gson/gson/2.2.4/gson-2.2.4.jar:/Users/seven/software/maven/repository/org/apache/curator/curator-client/2.7.1/curator-client-2.7.1.jar:/Users/seven/software/maven/repository/org/apache/curator/curator-recipes/2.7.1/curator-recipes-2.7.1.jar:/Users/seven/software/maven/repository/com/google/code/findbugs/jsr305/3.0.0/jsr305-3.0.0.jar:/Users/seven/software/maven/repository/org/apache/htrace/htrace-core4/4.0.1-incubating/htrace-core4-4.0.1-incubating.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-compress/1.4.1/commons-compress-1.4.1.jar:/Users/seven/software/maven/repository/org/tukaani/xz/1.0/xz-1.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-hdfs/2.6.0-cdh5.7.0/hadoop-hdfs-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/io/netty/netty/3.6.2.Final/netty-3.6.2.Final.jar:/Users/seven/software/maven/repository/xerces/xercesImpl/2.9.1/xercesImpl-2.9.1.jar:/Users/seven/software/maven/repository/xml-apis/xml-apis/1.3.04/xml-apis-1.3.04.jar:/Users/seven/software/maven/repository/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-app/2.6.0-cdh5.7.0/hadoop-mapreduce-client-app-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-common/2.6.0-cdh5.7.0/hadoop-mapreduce-client-common-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-yarn-client/2.6.0-cdh5.7.0/hadoop-yarn-client-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-yarn-server-common/2.6.0-cdh5.7.0/hadoop-yarn-server-common-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-shuffle/2.6.0-cdh5.7.0/hadoop-mapreduce-client-shuffle-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-yarn-api/2.6.0-cdh5.7.0/hadoop-yarn-api-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-core/2.6.0-cdh5.7.0/hadoop-mapreduce-client-core-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-yarn-common/2.6.0-cdh5.7.0/hadoop-yarn-common-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/javax/xml/bind/jaxb-api/2.2.2/jaxb-api-2.2.2.jar:/Users/seven/software/maven/repository/javax/xml/stream/stax-api/1.0-2/stax-api-1.0-2.jar:/Users/seven/software/maven/repository/javax/activation/activation/1.1/activation-1.1.jar:/Users/seven/software/maven/repository/com/sun/jersey/jersey-client/1.9/jersey-client-1.9.jar:/Users/seven/software/maven/repository/org/codehaus/jackson/jackson-xc/1.8.8/jackson-xc-1.8.8.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.6.0-cdh5.7.0/hadoop-mapreduce-client-jobclient-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-aws/2.6.0-cdh5.7.0/hadoop-aws-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/com/amazonaws/aws-java-sdk-s3/1.10.6/aws-java-sdk-s3-1.10.6.jar:/Users/seven/software/maven/repository/com/amazonaws/aws-java-sdk-kms/1.10.6/aws-java-sdk-kms-1.10.6.jar:/Users/seven/software/maven/repository/com/amazonaws/aws-java-sdk-core/1.10.6/aws-java-sdk-core-1.10.6.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-annotations/2.6.0-cdh5.7.0/hadoop-annotations-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-client/1.2.0-cdh5.7.0/hbase-client-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-annotations/1.2.0-cdh5.7.0/hbase-annotations-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-common/1.2.0-cdh5.7.0/hbase-common-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-protocol/1.2.0-cdh5.7.0/hbase-protocol-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/commons-codec/commons-codec/1.9/commons-codec-1.9.jar:/Users/seven/software/maven/repository/commons-io/commons-io/2.4/commons-io-2.4.jar:/Users/seven/software/maven/repository/commons-lang/commons-lang/2.6/commons-lang-2.6.jar:/Users/seven/software/maven/repository/commons-logging/commons-logging/1.2/commons-logging-1.2.jar:/Users/seven/software/maven/repository/com/google/guava/guava/12.0.1/guava-12.0.1.jar:/Users/seven/software/maven/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/Users/seven/software/maven/repository/io/netty/netty-all/4.0.23.Final/netty-all-4.0.23.Final.jar:/Users/seven/software/maven/repository/org/apache/zookeeper/zookeeper/3.4.5-cdh5.7.0/zookeeper-3.4.5-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/htrace/htrace-core/3.2.0-incubating/htrace-core-3.2.0-incubating.jar:/Users/seven/software/maven/repository/org/codehaus/jackson/jackson-mapper-asl/1.8.8/jackson-mapper-asl-1.8.8.jar:/Users/seven/software/maven/repository/org/jruby/jcodings/jcodings/1.0.8/jcodings-1.0.8.jar:/Users/seven/software/maven/repository/org/jruby/joni/joni/2.1.2/joni-2.1.2.jar:/Users/seven/software/maven/repository/com/yammer/metrics/metrics-core/2.2.0/metrics-core-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-auth/2.6.0-cdh5.7.0/hadoop-auth-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/httpcomponents/httpclient/4.2.5/httpclient-4.2.5.jar:/Users/seven/software/maven/repository/org/apache/httpcomponents/httpcore/4.2.4/httpcore-4.2.4.jar:/Users/seven/software/maven/repository/org/apache/directory/server/apacheds-kerberos-codec/2.0.0-M15/apacheds-kerberos-codec-2.0.0-M15.jar:/Users/seven/software/maven/repository/org/apache/directory/server/apacheds-i18n/2.0.0-M15/apacheds-i18n-2.0.0-M15.jar:/Users/seven/software/maven/repository/org/apache/directory/api/api-asn1-api/1.0.0-M20/api-asn1-api-1.0.0-M20.jar:/Users/seven/software/maven/repository/org/apache/directory/api/api-util/1.0.0-M20/api-util-1.0.0-M20.jar:/Users/seven/software/maven/repository/org/apache/curator/curator-framework/2.7.1/curator-framework-2.7.1.jar:/Users/seven/software/maven/repository/com/github/stephenc/findbugs/findbugs-annotations/1.3.9-1/findbugs-annotations-1.3.9-1.jar:/Users/seven/software/maven/repository/junit/junit/4.12/junit-4.12.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-server/1.2.0-cdh5.7.0/hbase-server-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-procedure/1.2.0-cdh5.7.0/hbase-procedure-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-common/1.2.0-cdh5.7.0/hbase-common-1.2.0-cdh5.7.0-tests.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-prefix-tree/1.2.0-cdh5.7.0/hbase-prefix-tree-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar:/Users/seven/software/maven/repository/commons-collections/commons-collections/3.2.2/commons-collections-3.2.2.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-hadoop-compat/1.2.0-cdh5.7.0/hbase-hadoop-compat-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-hadoop2-compat/1.2.0-cdh5.7.0/hbase-hadoop2-compat-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/com/sun/jersey/jersey-core/1.9/jersey-core-1.9.jar:/Users/seven/software/maven/repository/com/sun/jersey/jersey-server/1.9/jersey-server-1.9.jar:/Users/seven/software/maven/repository/asm/asm/3.1/asm-3.1.jar:/Users/seven/software/maven/repository/commons-cli/commons-cli/1.2/commons-cli-1.2.jar:/Users/seven/software/maven/repository/com/github/stephenc/high-scale-lib/high-scale-lib/1.1.1/high-scale-lib-1.1.1.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-math/2.1/commons-math-2.1.jar:/Users/seven/software/maven/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jetty/6.1.26.cloudera.4/jetty-6.1.26.cloudera.4.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jetty-util/6.1.26.cloudera.4/jetty-util-6.1.26.cloudera.4.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jetty-sslengine/6.1.26.cloudera.4/jetty-sslengine-6.1.26.cloudera.4.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jsp-2.1/6.1.14/jsp-2.1-6.1.14.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jsp-api-2.1/6.1.14/jsp-api-2.1-6.1.14.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/servlet-api-2.5/6.1.14/servlet-api-2.5-6.1.14.jar:/Users/seven/software/maven/repository/org/codehaus/jackson/jackson-core-asl/1.8.8/jackson-core-asl-1.8.8.jar:/Users/seven/software/maven/repository/org/codehaus/jackson/jackson-jaxrs/1.8.8/jackson-jaxrs-1.8.8.jar:/Users/seven/software/maven/repository/tomcat/jasper-compiler/5.5.23/jasper-compiler-5.5.23.jar:/Users/seven/software/maven/repository/tomcat/jasper-runtime/5.5.23/jasper-runtime-5.5.23.jar:/Users/seven/software/maven/repository/commons-el/commons-el/1.0/commons-el-1.0.jar:/Users/seven/software/maven/repository/org/jamon/jamon-runtime/2.4.1/jamon-runtime-2.4.1.jar:/Users/seven/software/maven/repository/com/lmax/disruptor/3.3.0/disruptor-3.3.0.jar:/Users/seven/software/maven/repository/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar:/Users/seven/software/maven/repository/org/owasp/esapi/esapi/2.1.0/esapi-2.1.0.jar:/Users/seven/software/maven/repository/commons-beanutils/commons-beanutils-core/1.7.0/commons-beanutils-core-1.7.0.jar:/Users/seven/software/maven/repository/commons-fileupload/commons-fileupload/1.2/commons-fileupload-1.2.jar:/Users/seven/software/maven/repository/xom/xom/1.2.5/xom-1.2.5.jar:/Users/seven/software/maven/repository/xalan/xalan/2.7.0/xalan-2.7.0.jar:/Users/seven/software/maven/repository/org/beanshell/bsh-core/2.0b4/bsh-core-2.0b4.jar:/Users/seven/software/maven/repository/org/owasp/antisamy/antisamy/1.4.3/antisamy-1.4.3.jar:/Users/seven/software/maven/repository/org/apache/xmlgraphics/batik-css/1.7/batik-css-1.7.jar:/Users/seven/software/maven/repository/org/apache/xmlgraphics/batik-ext/1.7/batik-ext-1.7.jar:/Users/seven/software/maven/repository/org/apache/xmlgraphics/batik-util/1.7/batik-util-1.7.jar:/Users/seven/software/maven/repository/xml-apis/xml-apis-ext/1.3.04/xml-apis-ext-1.3.04.jar:/Users/seven/software/maven/repository/net/sourceforge/nekohtml/nekohtml/1.9.12/nekohtml-1.9.12.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-core/2.6.0-mr1-cdh5.7.0/hadoop-core-2.6.0-mr1-cdh5.7.0.jar:/Users/seven/software/maven/repository/javax/servlet/jsp/jsp-api/2.1/jsp-api-2.1.jar:/Users/seven/software/maven/repository/net/java/dev/jets3t/jets3t/0.6.1/jets3t-0.6.1.jar:/Users/seven/software/maven/repository/javax/servlet/servlet-api/2.5/servlet-api-2.5.jar:/Users/seven/software/maven/repository/hsqldb/hsqldb/1.8.0.10/hsqldb-1.8.0.10.jar:/Users/seven/software/maven/repository/org/eclipse/jdt/core/3.1.1/core-3.1.1.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-hdfs/2.6.0-cdh5.7.0/hadoop-hdfs-2.6.0-cdh5.7.0-tests.jar:/Users/seven/software/maven/repository/commons-daemon/commons-daemon/1.0.13/commons-daemon-1.0.13.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-lang3/3.5/commons-lang3-3.5.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-pool2/2.4.2/commons-pool2-2.4.2.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-mllib_2.11/2.2.0/spark-mllib_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-core_2.11/2.2.0/spark-core_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/avro/avro-mapred/1.7.7/avro-mapred-1.7.7-hadoop2.jar:/Users/seven/software/maven/repository/org/apache/avro/avro-ipc/1.7.7/avro-ipc-1.7.7-tests.jar:/Users/seven/software/maven/repository/com/twitter/chill_2.11/0.8.0/chill_2.11-0.8.0.jar:/Users/seven/software/maven/repository/com/esotericsoftware/kryo-shaded/3.0.3/kryo-shaded-3.0.3.jar:/Users/seven/software/maven/repository/com/esotericsoftware/minlog/1.3.0/minlog-1.3.0.jar:/Users/seven/software/maven/repository/org/objenesis/objenesis/2.1/objenesis-2.1.jar:/Users/seven/software/maven/repository/com/twitter/chill-java/0.8.0/chill-java-0.8.0.jar:/Users/seven/software/maven/repository/org/apache/xbean/xbean-asm5-shaded/4.4/xbean-asm5-shaded-4.4.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-launcher_2.11/2.2.0/spark-launcher_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-network-common_2.11/2.2.0/spark-network-common_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-network-shuffle_2.11/2.2.0/spark-network-shuffle_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-unsafe_2.11/2.2.0/spark-unsafe_2.11-2.2.0.jar:/Users/seven/software/maven/repository/javax/servlet/javax.servlet-api/3.1.0/javax.servlet-api-3.1.0.jar:/Users/seven/software/maven/repository/org/slf4j/jul-to-slf4j/1.7.16/jul-to-slf4j-1.7.16.jar:/Users/seven/software/maven/repository/org/slf4j/jcl-over-slf4j/1.7.16/jcl-over-slf4j-1.7.16.jar:/Users/seven/software/maven/repository/com/ning/compress-lzf/1.0.3/compress-lzf-1.0.3.jar:/Users/seven/software/maven/repository/org/xerial/snappy/snappy-java/1.1.2.6/snappy-java-1.1.2.6.jar:/Users/seven/software/maven/repository/org/roaringbitmap/RoaringBitmap/0.5.11/RoaringBitmap-0.5.11.jar:/Users/seven/software/maven/repository/org/json4s/json4s-jackson_2.11/3.2.11/json4s-jackson_2.11-3.2.11.jar:/Users/seven/software/maven/repository/org/json4s/json4s-core_2.11/3.2.11/json4s-core_2.11-3.2.11.jar:/Users/seven/software/maven/repository/org/json4s/json4s-ast_2.11/3.2.11/json4s-ast_2.11-3.2.11.jar:/Users/seven/software/maven/repository/org/scala-lang/scalap/2.11.0/scalap-2.11.0.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/core/jersey-client/2.22.2/jersey-client-2.22.2.jar:/Users/seven/software/maven/repository/javax/ws/rs/javax.ws.rs-api/2.0.1/javax.ws.rs-api-2.0.1.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/hk2-api/2.4.0-b34/hk2-api-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/hk2-utils/2.4.0-b34/hk2-utils-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/external/aopalliance-repackaged/2.4.0-b34/aopalliance-repackaged-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/external/javax.inject/2.4.0-b34/javax.inject-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/hk2-locator/2.4.0-b34/hk2-locator-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/javassist/javassist/3.18.1-GA/javassist-3.18.1-GA.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/core/jersey-common/2.22.2/jersey-common-2.22.2.jar:/Users/seven/software/maven/repository/javax/annotation/javax.annotation-api/1.2/javax.annotation-api-1.2.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/bundles/repackaged/jersey-guava/2.22.2/jersey-guava-2.22.2.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/osgi-resource-locator/1.0.1/osgi-resource-locator-1.0.1.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/core/jersey-server/2.22.2/jersey-server-2.22.2.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/media/jersey-media-jaxb/2.22.2/jersey-media-jaxb-2.22.2.jar:/Users/seven/software/maven/repository/javax/validation/validation-api/1.1.0.Final/validation-api-1.1.0.Final.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/containers/jersey-container-servlet/2.22.2/jersey-container-servlet-2.22.2.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/containers/jersey-container-servlet-core/2.22.2/jersey-container-servlet-core-2.22.2.jar:/Users/seven/software/maven/repository/com/clearspring/analytics/stream/2.7.0/stream-2.7.0.jar:/Users/seven/software/maven/repository/io/dropwizard/metrics/metrics-core/3.1.2/metrics-core-3.1.2.jar:/Users/seven/software/maven/repository/io/dropwizard/metrics/metrics-jvm/3.1.2/metrics-jvm-3.1.2.jar:/Users/seven/software/maven/repository/io/dropwizard/metrics/metrics-json/3.1.2/metrics-json-3.1.2.jar:/Users/seven/software/maven/repository/io/dropwizard/metrics/metrics-graphite/3.1.2/metrics-graphite-3.1.2.jar:/Users/seven/software/maven/repository/org/apache/ivy/ivy/2.4.0/ivy-2.4.0.jar:/Users/seven/software/maven/repository/oro/oro/2.0.8/oro-2.0.8.jar:/Users/seven/software/maven/repository/net/razorvine/pyrolite/4.13/pyrolite-4.13.jar:/Users/seven/software/maven/repository/net/sf/py4j/py4j/0.10.4/py4j-0.10.4.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-crypto/1.0.0/commons-crypto-1.0.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-streaming_2.11/2.2.0/spark-streaming_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-sql_2.11/2.2.0/spark-sql_2.11-2.2.0.jar:/Users/seven/software/maven/repository/com/univocity/univocity-parsers/2.2.1/univocity-parsers-2.2.1.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-sketch_2.11/2.2.0/spark-sketch_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-catalyst_2.11/2.2.0/spark-catalyst_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/codehaus/janino/janino/3.0.0/janino-3.0.0.jar:/Users/seven/software/maven/repository/org/codehaus/janino/commons-compiler/3.0.0/commons-compiler-3.0.0.jar:/Users/seven/software/maven/repository/org/antlr/antlr4-runtime/4.5.3/antlr4-runtime-4.5.3.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-column/1.8.2/parquet-column-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-common/1.8.2/parquet-common-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-encoding/1.8.2/parquet-encoding-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-hadoop/1.8.2/parquet-hadoop-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-format/2.3.1/parquet-format-2.3.1.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-jackson/1.8.2/parquet-jackson-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-graphx_2.11/2.2.0/spark-graphx_2.11-2.2.0.jar:/Users/seven/software/maven/repository/com/github/fommil/netlib/core/1.1.2/core-1.1.2.jar:/Users/seven/software/maven/repository/net/sourceforge/f2j/arpack_combined_all/0.1/arpack_combined_all-0.1.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-mllib-local_2.11/2.2.0/spark-mllib-local_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/scalanlp/breeze_2.11/0.13.1/breeze_2.11-0.13.1.jar:/Users/seven/software/maven/repository/org/scalanlp/breeze-macros_2.11/0.13.1/breeze-macros_2.11-0.13.1.jar:/Users/seven/software/maven/repository/net/sf/opencsv/opencsv/2.3/opencsv-2.3.jar:/Users/seven/software/maven/repository/com/github/rwl/jtransforms/2.4.0/jtransforms-2.4.0.jar:/Users/seven/software/maven/repository/org/spire-math/spire_2.11/0.13.0/spire_2.11-0.13.0.jar:/Users/seven/software/maven/repository/org/spire-math/spire-macros_2.11/0.13.0/spire-macros_2.11-0.13.0.jar:/Users/seven/software/maven/repository/org/typelevel/machinist_2.11/0.6.1/machinist_2.11-0.6.1.jar:/Users/seven/software/maven/repository/com/chuusai/shapeless_2.11/2.3.2/shapeless_2.11-2.3.2.jar:/Users/seven/software/maven/repository/org/typelevel/macro-compat_2.11/1.1.1/macro-compat_2.11-1.1.1.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-math3/3.4.1/commons-math3-3.4.1.jar:/Users/seven/software/maven/repository/org/jpmml/pmml-model/1.2.15/pmml-model-1.2.15.jar:/Users/seven/software/maven/repository/org/jpmml/pmml-schema/1.2.15/pmml-schema-1.2.15.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-tags_2.11/2.2.0/spark-tags_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-streaming-kafka-0-10_2.11/2.2.0/spark-streaming-kafka-0-10_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/kafka/kafka_2.11/0.10.0.1/kafka_2.11-0.10.0.1.jar:/Users/seven/software/maven/repository/com/101tec/zkclient/0.8/zkclient-0.8.jar:/Users/seven/software/maven/repository/org/apache/kafka/kafka-clients/0.10.0.1/kafka-clients-0.10.0.1.jar:/Users/seven/software/maven/repository/mysql/mysql-connector-java/5.1.31/mysql-connector-java-5.1.31.jar:/Users/seven/software/maven/repository/org/elasticsearch/client/transport/5.1.2/transport-5.1.2.jar:/Users/seven/software/maven/repository/org/elasticsearch/plugin/transport-netty3-client/5.1.2/transport-netty3-client-5.1.2.jar:/Users/seven/software/maven/repository/org/elasticsearch/plugin/transport-netty4-client/5.1.2/transport-netty4-client-5.1.2.jar:/Users/seven/software/maven/repository/io/netty/netty-buffer/4.1.6.Final/netty-buffer-4.1.6.Final.jar:/Users/seven/software/maven/repository/io/netty/netty-codec/4.1.6.Final/netty-codec-4.1.6.Final.jar:/Users/seven/software/maven/repository/io/netty/netty-codec-http/4.1.6.Final/netty-codec-http-4.1.6.Final.jar:/Users/seven/software/maven/repository/io/netty/netty-common/4.1.6.Final/netty-common-4.1.6.Final.jar:/Users/seven/software/maven/repository/io/netty/netty-handler/4.1.6.Final/netty-handler-4.1.6.Final.jar:/Users/seven/software/maven/repository/io/netty/netty-resolver/4.1.6.Final/netty-resolver-4.1.6.Final.jar:/Users/seven/software/maven/repository/io/netty/netty-transport/4.1.6.Final/netty-transport-4.1.6.Final.jar:/Users/seven/software/maven/repository/org/elasticsearch/plugin/reindex-client/5.1.2/reindex-client-5.1.2.jar:/Users/seven/software/maven/repository/org/elasticsearch/client/rest/5.1.2/rest-5.1.2.jar:/Users/seven/software/maven/repository/org/apache/httpcomponents/httpasyncclient/4.1.2/httpasyncclient-4.1.2.jar:/Users/seven/software/maven/repository/org/apache/httpcomponents/httpcore-nio/4.4.5/httpcore-nio-4.4.5.jar:/Users/seven/software/maven/repository/org/elasticsearch/plugin/lang-mustache-client/5.1.2/lang-mustache-client-5.1.2.jar:/Users/seven/software/maven/repository/com/github/spullara/mustache/java/compiler/0.9.3/compiler-0.9.3.jar:/Users/seven/software/maven/repository/org/elasticsearch/plugin/percolator-client/5.1.2/percolator-client-5.1.2.jar:/Users/seven/software/maven/repository/org/elasticsearch/elasticsearch/5.1.2/elasticsearch-5.1.2.jar:/Users/seven/software/maven/repository/org/apache/lucene/lucene-core/6.3.0/lucene-core-6.3.0.jar:/Users/seven/software/maven/repository/org/apache/lucene/lucene-analyzers-common/6.3.0/lucene-analyzers-common-6.3.0.jar:/Users/seven/software/maven/repository/org/apache/lucene/lucene-backward-codecs/6.3.0/lucene-backward-codecs-6.3.0.jar:/Users/seven/software/maven/repository/org/apache/lucene/lucene-grouping/6.3.0/lucene-grouping-6.3.0.jar:/Users/seven/software/maven/repository/org/apache/lucene/lucene-highlighter/6.3.0/lucene-highlighter-6.3.0.jar:/Users/seven/software/maven/repository/org/apache/lucene/lucene-join/6.3.0/lucene-join-6.3.0.jar:/Users/seven/software/maven/repository/org/apache/lucene/lucene-memory/6.3.0/lucene-memory-6.3.0.jar:/Users/seven/software/maven/repository/org/apache/lucene/lucene-misc/6.3.0/lucene-misc-6.3.0.jar:/Users/seven/software/maven/repository/org/apache/lucene/lucene-queries/6.3.0/lucene-queries-6.3.0.jar:/Users/seven/software/maven/repository/org/apache/lucene/lucene-queryparser/6.3.0/lucene-queryparser-6.3.0.jar:/Users/seven/software/maven/repository/org/apache/lucene/lucene-sandbox/6.3.0/lucene-sandbox-6.3.0.jar:/Users/seven/software/maven/repository/org/apache/lucene/lucene-spatial/6.3.0/lucene-spatial-6.3.0.jar:/Users/seven/software/maven/repository/org/apache/lucene/lucene-spatial-extras/6.3.0/lucene-spatial-extras-6.3.0.jar:/Users/seven/software/maven/repository/org/apache/lucene/lucene-spatial3d/6.3.0/lucene-spatial3d-6.3.0.jar:/Users/seven/software/maven/repository/org/apache/lucene/lucene-suggest/6.3.0/lucene-suggest-6.3.0.jar:/Users/seven/software/maven/repository/org/elasticsearch/securesm/1.1/securesm-1.1.jar:/Users/seven/software/maven/repository/net/sf/jopt-simple/jopt-simple/5.0.2/jopt-simple-5.0.2.jar:/Users/seven/software/maven/repository/com/carrotsearch/hppc/0.7.1/hppc-0.7.1.jar:/Users/seven/software/maven/repository/joda-time/joda-time/2.9.5/joda-time-2.9.5.jar:/Users/seven/software/maven/repository/org/yaml/snakeyaml/1.15/snakeyaml-1.15.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/core/jackson-core/2.8.1/jackson-core-2.8.1.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/dataformat/jackson-dataformat-smile/2.8.1/jackson-dataformat-smile-2.8.1.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/dataformat/jackson-dataformat-yaml/2.8.1/jackson-dataformat-yaml-2.8.1.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/dataformat/jackson-dataformat-cbor/2.8.1/jackson-dataformat-cbor-2.8.1.jar:/Users/seven/software/maven/repository/com/tdunning/t-digest/3.0/t-digest-3.0.jar:/Users/seven/software/maven/repository/org/hdrhistogram/HdrHistogram/2.1.6/HdrHistogram-2.1.6.jar:/Users/seven/software/maven/repository/net/java/dev/jna/jna/4.2.2/jna-4.2.2.jar:/Users/seven/software/maven/repository/org/apache/logging/log4j/log4j-api/2.6.2/log4j-api-2.6.2.jar:/Users/seven/software/maven/repository/org/apache/logging/log4j/log4j-core/2.6.2/log4j-core-2.6.2.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/module/jackson-module-scala_2.11/2.6.5/jackson-module-scala_2.11-2.6.5.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/core/jackson-annotations/2.6.5/jackson-annotations-2.6.5.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/core/jackson-databind/2.6.5/jackson-databind-2.6.5.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/module/jackson-module-paranamer/2.6.5/jackson-module-paranamer-2.6.5.jar:/Users/seven/software/maven/repository/com/thoughtworks/paranamer/paranamer/2.6/paranamer-2.6.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-streaming-flume_2.11/2.2.0/spark-streaming-flume_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/flume/flume-ng-core/1.6.0/flume-ng-core-1.6.0.jar:/Users/seven/software/maven/repository/org/apache/flume/flume-ng-configuration/1.6.0/flume-ng-configuration-1.6.0.jar:/Users/seven/software/maven/repository/org/apache/avro/avro-ipc/1.7.4/avro-ipc-1.7.4.jar:/Users/seven/software/maven/repository/org/apache/velocity/velocity/1.7/velocity-1.7.jar:/Users/seven/software/maven/repository/org/apache/mina/mina-core/2.0.4/mina-core-2.0.4.jar:/Users/seven/software/maven/repository/org/apache/flume/flume-ng-sdk/1.6.0/flume-ng-sdk-1.6.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-streaming-flume-sink_2.11/2.2.0/spark-streaming-flume-sink_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/flume/flume-ng-clients/flume-ng-log4jappender/1.6.0/flume-ng-log4jappender-1.6.0.jar:/Users/seven/software/maven/repository/net/jpountz/lz4/lz4/1.3.0/lz4-1.3.0.jar:/Applications/IntelliJ IDEA.app/Contents/lib/idea_rt.jar
[INFO][2018-05-30 13:13:15,510][org.apache.zookeeper.ZooKeeper]Client environment:java.library.path=/Users/seven/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:.
[INFO][2018-05-30 13:13:15,510][org.apache.zookeeper.ZooKeeper]Client environment:java.io.tmpdir=/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/
[INFO][2018-05-30 13:13:15,510][org.apache.zookeeper.ZooKeeper]Client environment:java.compiler=<NA>
[INFO][2018-05-30 13:13:15,510][org.apache.zookeeper.ZooKeeper]Client environment:os.name=Mac OS X
[INFO][2018-05-30 13:13:15,510][org.apache.zookeeper.ZooKeeper]Client environment:os.arch=x86_64
[INFO][2018-05-30 13:13:15,511][org.apache.zookeeper.ZooKeeper]Client environment:os.version=10.13.4
[INFO][2018-05-30 13:13:15,511][org.apache.zookeeper.ZooKeeper]Client environment:user.name=seven
[INFO][2018-05-30 13:13:15,511][org.apache.zookeeper.ZooKeeper]Client environment:user.home=/Users/seven
[INFO][2018-05-30 13:13:15,511][org.apache.zookeeper.ZooKeeper]Client environment:user.dir=/Users/seven/project/github/dataMining
[INFO][2018-05-30 13:13:15,511][org.apache.zookeeper.ZooKeeper]Initiating client connection, connectString=vm-xaj-bigdata-da-d01:2181,vm-xaj-bigdata-da-d02:2181,vm-xaj-bigdata-da-d03:2181 sessionTimeout=60000 watcher=hconnection-0x5b1b21af0x0, quorum=vm-xaj-bigdata-da-d01:2181,vm-xaj-bigdata-da-d02:2181,vm-xaj-bigdata-da-d03:2181, baseZNode=/hbase
[INFO][2018-05-30 13:13:15,535][org.apache.zookeeper.ClientCnxn]Opening socket connection to server vm-xaj-bigdata-da-d02/10.213.4.26:2181. Will not attempt to authenticate using SASL (unknown error)
[INFO][2018-05-30 13:13:15,558][org.apache.zookeeper.ClientCnxn]Socket connection established, initiating session, client: /10.194.67.11:56697, server: vm-xaj-bigdata-da-d02/10.213.4.26:2181
[INFO][2018-05-30 13:13:15,580][org.apache.zookeeper.ClientCnxn]Session establishment complete on server vm-xaj-bigdata-da-d02/10.213.4.26:2181, sessionid = 0x262b4dc569b8e27, negotiated timeout = 60000
[INFO][2018-05-30 13:13:16,051][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_4_piece0 on 10.194.67.11:56669 in memory (size: 1973.0 B, free: 912.3 MB)
[INFO][2018-05-30 13:13:16,054][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_0_piece0 on 10.194.67.11:56669 in memory (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-30 13:13:16,057][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_1_piece0 on 10.194.67.11:56669 in memory (size: 1973.0 B, free: 912.3 MB)
[INFO][2018-05-30 13:13:16,059][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_5_piece0 on 10.194.67.11:56669 in memory (size: 1973.0 B, free: 912.3 MB)
[INFO][2018-05-30 13:13:16,060][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_3_piece0 on 10.194.67.11:56669 in memory (size: 1971.0 B, free: 912.3 MB)
[INFO][2018-05-30 13:13:16,061][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_6_piece0 on 10.194.67.11:56669 in memory (size: 1973.0 B, free: 912.3 MB)
[INFO][2018-05-30 13:13:16,062][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_2_piece0 on 10.194.67.11:56669 in memory (size: 1973.0 B, free: 912.3 MB)
[WARN][2018-05-30 13:13:16,190][org.apache.hadoop.util.NativeCodeLoader]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO][2018-05-30 13:13:16,753][com.seven.spark.streaming.ReceiveKafkaData$]Inserting 33 lines of data to HBase is success . . .
[INFO][2018-05-30 13:13:19,963][com.seven.spark.streaming.ReceiveKafkaData$]Inserting 33 lines of data to ElasticSearch is success . . .
[INFO][2018-05-30 13:13:19,965][org.apache.spark.executor.Executor]Finished task 0.0 in stage 7.0 (TID 7). 794 bytes result sent to driver
[INFO][2018-05-30 13:13:19,967][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 7.0 (TID 7) in 4719 ms on localhost (executor driver) (1/1)
[INFO][2018-05-30 13:13:19,967][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 7.0, whose tasks have all completed, from pool 
[INFO][2018-05-30 13:13:19,969][org.apache.spark.scheduler.DAGScheduler]ResultStage 7 (foreachPartition at ReceiveKafkaData.scala:76) finished in 4.720 s
[INFO][2018-05-30 13:13:19,969][org.apache.spark.scheduler.DAGScheduler]Job 7 finished: foreachPartition at ReceiveKafkaData.scala:76, took 4.730305 s
[INFO][2018-05-30 13:13:19,970][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527657195000 ms.0 from job set of time 1527657195000 ms
[INFO][2018-05-30 13:13:19,970][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 4.970 s for time 1527657195000 ms (execution: 4.957 s)
[INFO][2018-05-30 13:13:19,971][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 11 from persistence list
[INFO][2018-05-30 13:13:19,973][org.apache.spark.storage.BlockManager]Removing RDD 11
[INFO][2018-05-30 13:13:19,974][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 10 from persistence list
[INFO][2018-05-30 13:13:19,975][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-30 13:13:19,976][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527657185000 ms
[INFO][2018-05-30 13:13:19,976][org.apache.spark.storage.BlockManager]Removing RDD 10
[INFO][2018-05-30 13:13:20,020][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527657200000 ms
[INFO][2018-05-30 13:13:20,021][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527657200000 ms.0 from job set of time 1527657200000 ms
[INFO][2018-05-30 13:13:20,031][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:75
[INFO][2018-05-30 13:13:20,032][org.apache.spark.scheduler.DAGScheduler]Got job 8 (isEmpty at ReceiveKafkaData.scala:75) with 1 output partitions
[INFO][2018-05-30 13:13:20,032][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 8 (isEmpty at ReceiveKafkaData.scala:75)
[INFO][2018-05-30 13:13:20,032][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-30 13:13:20,032][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-30 13:13:20,032][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 8 (MapPartitionsRDD[15] at map at ReceiveKafkaData.scala:74), which has no missing parents
[INFO][2018-05-30 13:13:20,035][org.apache.spark.storage.memory.MemoryStore]Block broadcast_8 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-30 13:13:20,043][org.apache.spark.storage.memory.MemoryStore]Block broadcast_8_piece0 stored as bytes in memory (estimated size 1973.0 B, free 912.3 MB)
[INFO][2018-05-30 13:13:20,044][org.apache.spark.storage.BlockManagerInfo]Added broadcast_8_piece0 in memory on 10.194.67.11:56669 (size: 1973.0 B, free: 912.3 MB)
[INFO][2018-05-30 13:13:20,045][org.apache.spark.SparkContext]Created broadcast 8 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-30 13:13:20,046][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[15] at map at ReceiveKafkaData.scala:74) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-30 13:13:20,047][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 8.0 with 1 tasks
[INFO][2018-05-30 13:13:20,048][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 8.0 (TID 8, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-30 13:13:20,048][org.apache.spark.executor.Executor]Running task 0.0 in stage 8.0 (TID 8)
[INFO][2018-05-30 13:13:20,086][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 188975 -> 189023
[INFO][2018-05-30 13:13:20,087][org.apache.spark.executor.Executor]Finished task 0.0 in stage 8.0 (TID 8). 971 bytes result sent to driver
[INFO][2018-05-30 13:13:20,088][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 8.0 (TID 8) in 41 ms on localhost (executor driver) (1/1)
[INFO][2018-05-30 13:13:20,088][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 8.0, whose tasks have all completed, from pool 
[INFO][2018-05-30 13:13:20,089][org.apache.spark.scheduler.DAGScheduler]ResultStage 8 (isEmpty at ReceiveKafkaData.scala:75) finished in 0.041 s
[INFO][2018-05-30 13:13:20,089][org.apache.spark.scheduler.DAGScheduler]Job 8 finished: isEmpty at ReceiveKafkaData.scala:75, took 0.057571 s
[INFO][2018-05-30 13:13:20,098][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:76
[INFO][2018-05-30 13:13:20,098][org.apache.spark.scheduler.DAGScheduler]Got job 9 (foreachPartition at ReceiveKafkaData.scala:76) with 1 output partitions
[INFO][2018-05-30 13:13:20,099][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 9 (foreachPartition at ReceiveKafkaData.scala:76)
[INFO][2018-05-30 13:13:20,099][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-30 13:13:20,099][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-30 13:13:20,099][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 9 (MapPartitionsRDD[15] at map at ReceiveKafkaData.scala:74), which has no missing parents
[INFO][2018-05-30 13:13:20,103][org.apache.spark.storage.memory.MemoryStore]Block broadcast_9 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-30 13:13:20,118][org.apache.spark.storage.memory.MemoryStore]Block broadcast_9_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.3 MB)
[INFO][2018-05-30 13:13:20,119][org.apache.spark.storage.BlockManagerInfo]Added broadcast_9_piece0 in memory on 10.194.67.11:56669 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-30 13:13:20,120][org.apache.spark.SparkContext]Created broadcast 9 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-30 13:13:20,120][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[15] at map at ReceiveKafkaData.scala:74) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-30 13:13:20,120][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 9.0 with 1 tasks
[INFO][2018-05-30 13:13:20,121][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 9.0 (TID 9, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-30 13:13:20,122][org.apache.spark.executor.Executor]Running task 0.0 in stage 9.0 (TID 9)
[INFO][2018-05-30 13:13:20,127][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 188975 -> 189023
[INFO][2018-05-30 13:13:20,127][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 188975
[INFO][2018-05-30 13:13:20,195][com.seven.spark.streaming.ReceiveKafkaData$]Inserting 48 lines of data to HBase is success . . .
[INFO][2018-05-30 13:13:20,294][com.seven.spark.streaming.ReceiveKafkaData$]Inserting 48 lines of data to ElasticSearch is success . . .
[INFO][2018-05-30 13:13:20,295][org.apache.spark.executor.Executor]Finished task 0.0 in stage 9.0 (TID 9). 708 bytes result sent to driver
[INFO][2018-05-30 13:13:20,295][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 9.0 (TID 9) in 174 ms on localhost (executor driver) (1/1)
[INFO][2018-05-30 13:13:20,295][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 9.0, whose tasks have all completed, from pool 
[INFO][2018-05-30 13:13:20,296][org.apache.spark.scheduler.DAGScheduler]ResultStage 9 (foreachPartition at ReceiveKafkaData.scala:76) finished in 0.175 s
[INFO][2018-05-30 13:13:20,296][org.apache.spark.scheduler.DAGScheduler]Job 9 finished: foreachPartition at ReceiveKafkaData.scala:76, took 0.198420 s
[INFO][2018-05-30 13:13:20,297][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527657200000 ms.0 from job set of time 1527657200000 ms
[INFO][2018-05-30 13:13:20,297][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.297 s for time 1527657200000 ms (execution: 0.276 s)
[INFO][2018-05-30 13:13:20,297][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 13 from persistence list
[INFO][2018-05-30 13:13:20,297][org.apache.spark.storage.BlockManager]Removing RDD 13
[INFO][2018-05-30 13:13:20,298][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 12 from persistence list
[INFO][2018-05-30 13:13:20,298][org.apache.spark.storage.BlockManager]Removing RDD 12
[INFO][2018-05-30 13:13:20,298][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-30 13:13:20,298][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527657190000 ms
[INFO][2018-05-30 13:13:22,019][org.apache.kafka.clients.producer.KafkaProducer]Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
[INFO][2018-05-30 13:13:22,030][org.apache.spark.util.ShutdownHookManager]Shutdown hook called
[INFO][2018-05-30 13:13:22,031][org.apache.spark.util.ShutdownHookManager]Deleting directory /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/spark-49210874-4292-4b92-870a-2c7c2a056ec5
[INFO][2018-05-30 13:13:25,019][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527657205000 ms
[INFO][2018-05-30 13:13:25,019][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527657205000 ms.0 from job set of time 1527657205000 ms
[INFO][2018-05-30 13:13:25,025][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:75
[INFO][2018-05-30 13:13:25,025][org.apache.spark.scheduler.DAGScheduler]Got job 10 (isEmpty at ReceiveKafkaData.scala:75) with 1 output partitions
[INFO][2018-05-30 13:13:25,025][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 10 (isEmpty at ReceiveKafkaData.scala:75)
[INFO][2018-05-30 13:13:25,025][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-30 13:13:25,025][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-30 13:13:25,026][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 10 (MapPartitionsRDD[17] at map at ReceiveKafkaData.scala:74), which has no missing parents
[INFO][2018-05-30 13:13:25,028][org.apache.spark.storage.memory.MemoryStore]Block broadcast_10 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-30 13:13:25,035][org.apache.spark.storage.memory.MemoryStore]Block broadcast_10_piece0 stored as bytes in memory (estimated size 1970.0 B, free 912.3 MB)
[INFO][2018-05-30 13:13:25,036][org.apache.spark.storage.BlockManagerInfo]Added broadcast_10_piece0 in memory on 10.194.67.11:56669 (size: 1970.0 B, free: 912.3 MB)
[INFO][2018-05-30 13:13:25,036][org.apache.spark.SparkContext]Created broadcast 10 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-30 13:13:25,037][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[17] at map at ReceiveKafkaData.scala:74) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-30 13:13:25,037][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 10.0 with 1 tasks
[INFO][2018-05-30 13:13:25,037][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 10.0 (TID 10, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-30 13:13:25,038][org.apache.spark.executor.Executor]Running task 0.0 in stage 10.0 (TID 10)
[INFO][2018-05-30 13:13:25,039][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 189023 -> 189042
[INFO][2018-05-30 13:13:25,040][org.apache.spark.executor.Executor]Finished task 0.0 in stage 10.0 (TID 10). 970 bytes result sent to driver
[INFO][2018-05-30 13:13:25,041][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 10.0 (TID 10) in 4 ms on localhost (executor driver) (1/1)
[INFO][2018-05-30 13:13:25,041][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 10.0, whose tasks have all completed, from pool 
[INFO][2018-05-30 13:13:25,041][org.apache.spark.scheduler.DAGScheduler]ResultStage 10 (isEmpty at ReceiveKafkaData.scala:75) finished in 0.004 s
[INFO][2018-05-30 13:13:25,042][org.apache.spark.scheduler.DAGScheduler]Job 10 finished: isEmpty at ReceiveKafkaData.scala:75, took 0.017089 s
[INFO][2018-05-30 13:13:25,049][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:76
[INFO][2018-05-30 13:13:25,061][org.apache.spark.scheduler.DAGScheduler]Got job 11 (foreachPartition at ReceiveKafkaData.scala:76) with 1 output partitions
[INFO][2018-05-30 13:13:25,061][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 11 (foreachPartition at ReceiveKafkaData.scala:76)
[INFO][2018-05-30 13:13:25,061][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-30 13:13:25,062][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-30 13:13:25,063][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 11 (MapPartitionsRDD[17] at map at ReceiveKafkaData.scala:74), which has no missing parents
[INFO][2018-05-30 13:13:25,064][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_9_piece0 on 10.194.67.11:56669 in memory (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-30 13:13:25,065][org.apache.spark.storage.memory.MemoryStore]Block broadcast_11 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-30 13:13:25,066][org.apache.spark.storage.memory.MemoryStore]Block broadcast_11_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.3 MB)
[INFO][2018-05-30 13:13:25,067][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_8_piece0 on 10.194.67.11:56669 in memory (size: 1973.0 B, free: 912.3 MB)
[INFO][2018-05-30 13:13:25,067][org.apache.spark.storage.BlockManagerInfo]Added broadcast_11_piece0 in memory on 10.194.67.11:56669 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-30 13:13:25,067][org.apache.spark.SparkContext]Created broadcast 11 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-30 13:13:25,068][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[17] at map at ReceiveKafkaData.scala:74) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-30 13:13:25,068][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 11.0 with 1 tasks
[INFO][2018-05-30 13:13:25,068][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_10_piece0 on 10.194.67.11:56669 in memory (size: 1970.0 B, free: 912.3 MB)
[INFO][2018-05-30 13:13:25,069][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 11.0 (TID 11, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-30 13:13:25,069][org.apache.spark.executor.Executor]Running task 0.0 in stage 11.0 (TID 11)
[INFO][2018-05-30 13:13:25,072][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 189023 -> 189042
[INFO][2018-05-30 13:13:25,072][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 189023
[INFO][2018-05-30 13:13:25,109][com.seven.spark.streaming.ReceiveKafkaData$]Inserting 19 lines of data to HBase is success . . .
[INFO][2018-05-30 13:13:25,144][com.seven.spark.streaming.ReceiveKafkaData$]Inserting 19 lines of data to ElasticSearch is success . . .
[INFO][2018-05-30 13:13:25,145][org.apache.spark.executor.Executor]Finished task 0.0 in stage 11.0 (TID 11). 751 bytes result sent to driver
[INFO][2018-05-30 13:13:25,147][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 11.0 (TID 11) in 78 ms on localhost (executor driver) (1/1)
[INFO][2018-05-30 13:13:25,147][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 11.0, whose tasks have all completed, from pool 
[INFO][2018-05-30 13:13:25,148][org.apache.spark.scheduler.DAGScheduler]ResultStage 11 (foreachPartition at ReceiveKafkaData.scala:76) finished in 0.079 s
[INFO][2018-05-30 13:13:25,148][org.apache.spark.scheduler.DAGScheduler]Job 11 finished: foreachPartition at ReceiveKafkaData.scala:76, took 0.099106 s
[INFO][2018-05-30 13:13:25,149][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527657205000 ms.0 from job set of time 1527657205000 ms
[INFO][2018-05-30 13:13:25,149][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 15 from persistence list
[INFO][2018-05-30 13:13:25,149][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.149 s for time 1527657205000 ms (execution: 0.130 s)
[INFO][2018-05-30 13:13:25,149][org.apache.spark.storage.BlockManager]Removing RDD 15
[INFO][2018-05-30 13:13:25,149][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 14 from persistence list
[INFO][2018-05-30 13:13:25,150][org.apache.spark.storage.BlockManager]Removing RDD 14
[INFO][2018-05-30 13:13:25,150][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-30 13:13:25,150][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527657195000 ms
[INFO][2018-05-30 13:13:30,019][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527657210000 ms
[INFO][2018-05-30 13:13:30,020][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527657210000 ms.0 from job set of time 1527657210000 ms
[INFO][2018-05-30 13:13:30,026][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:75
[INFO][2018-05-30 13:13:30,027][org.apache.spark.scheduler.DAGScheduler]Got job 12 (isEmpty at ReceiveKafkaData.scala:75) with 1 output partitions
[INFO][2018-05-30 13:13:30,027][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 12 (isEmpty at ReceiveKafkaData.scala:75)
[INFO][2018-05-30 13:13:30,027][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-30 13:13:30,027][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-30 13:13:30,027][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 12 (MapPartitionsRDD[19] at map at ReceiveKafkaData.scala:74), which has no missing parents
[INFO][2018-05-30 13:13:30,029][org.apache.spark.storage.memory.MemoryStore]Block broadcast_12 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-30 13:13:30,030][org.apache.spark.storage.memory.MemoryStore]Block broadcast_12_piece0 stored as bytes in memory (estimated size 1973.0 B, free 912.3 MB)
[INFO][2018-05-30 13:13:30,031][org.apache.spark.storage.BlockManagerInfo]Added broadcast_12_piece0 in memory on 10.194.67.11:56669 (size: 1973.0 B, free: 912.3 MB)
[INFO][2018-05-30 13:13:30,031][org.apache.spark.SparkContext]Created broadcast 12 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-30 13:13:30,032][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[19] at map at ReceiveKafkaData.scala:74) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-30 13:13:30,032][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 12.0 with 1 tasks
[INFO][2018-05-30 13:13:30,032][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 12.0 (TID 12, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-30 13:13:30,033][org.apache.spark.executor.Executor]Running task 0.0 in stage 12.0 (TID 12)
[INFO][2018-05-30 13:13:30,035][org.apache.spark.streaming.kafka010.KafkaRDD]Beginning offset 189042 is the same as ending offset skipping seven 0
[INFO][2018-05-30 13:13:30,037][org.apache.spark.executor.Executor]Finished task 0.0 in stage 12.0 (TID 12). 708 bytes result sent to driver
[INFO][2018-05-30 13:13:30,038][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 12.0 (TID 12) in 6 ms on localhost (executor driver) (1/1)
[INFO][2018-05-30 13:13:30,038][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 12.0, whose tasks have all completed, from pool 
[INFO][2018-05-30 13:13:30,038][org.apache.spark.scheduler.DAGScheduler]ResultStage 12 (isEmpty at ReceiveKafkaData.scala:75) finished in 0.006 s
[INFO][2018-05-30 13:13:30,039][org.apache.spark.scheduler.DAGScheduler]Job 12 finished: isEmpty at ReceiveKafkaData.scala:75, took 0.012401 s
[INFO][2018-05-30 13:13:30,039][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527657210000 ms.0 from job set of time 1527657210000 ms
[INFO][2018-05-30 13:13:30,040][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 17 from persistence list
[INFO][2018-05-30 13:13:30,040][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.039 s for time 1527657210000 ms (execution: 0.019 s)
[INFO][2018-05-30 13:13:30,041][org.apache.spark.storage.BlockManager]Removing RDD 17
[INFO][2018-05-30 13:13:30,041][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 16 from persistence list
[INFO][2018-05-30 13:13:30,041][org.apache.spark.storage.BlockManager]Removing RDD 16
[INFO][2018-05-30 13:13:30,042][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-30 13:13:30,042][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527657200000 ms
[INFO][2018-05-30 13:13:35,017][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527657215000 ms
[INFO][2018-05-30 13:13:35,017][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527657215000 ms.0 from job set of time 1527657215000 ms
[INFO][2018-05-30 13:13:35,024][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:75
[INFO][2018-05-30 13:13:35,024][org.apache.spark.scheduler.DAGScheduler]Got job 13 (isEmpty at ReceiveKafkaData.scala:75) with 1 output partitions
[INFO][2018-05-30 13:13:35,024][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 13 (isEmpty at ReceiveKafkaData.scala:75)
[INFO][2018-05-30 13:13:35,024][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-30 13:13:35,025][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-30 13:13:35,025][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 13 (MapPartitionsRDD[21] at map at ReceiveKafkaData.scala:74), which has no missing parents
[INFO][2018-05-30 13:13:35,027][org.apache.spark.storage.memory.MemoryStore]Block broadcast_13 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-30 13:13:35,030][org.apache.spark.storage.memory.MemoryStore]Block broadcast_13_piece0 stored as bytes in memory (estimated size 1973.0 B, free 912.3 MB)
[INFO][2018-05-30 13:13:35,031][org.apache.spark.storage.BlockManagerInfo]Added broadcast_13_piece0 in memory on 10.194.67.11:56669 (size: 1973.0 B, free: 912.3 MB)
[INFO][2018-05-30 13:13:35,031][org.apache.spark.SparkContext]Created broadcast 13 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-30 13:13:35,032][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[21] at map at ReceiveKafkaData.scala:74) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-30 13:13:35,032][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 13.0 with 1 tasks
[INFO][2018-05-30 13:13:35,032][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 13.0 (TID 13, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-30 13:13:35,033][org.apache.spark.executor.Executor]Running task 0.0 in stage 13.0 (TID 13)
[INFO][2018-05-30 13:13:35,036][org.apache.spark.streaming.kafka010.KafkaRDD]Beginning offset 189042 is the same as ending offset skipping seven 0
[INFO][2018-05-30 13:13:35,038][org.apache.spark.executor.Executor]Finished task 0.0 in stage 13.0 (TID 13). 708 bytes result sent to driver
[INFO][2018-05-30 13:13:35,039][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 13.0 (TID 13) in 6 ms on localhost (executor driver) (1/1)
[INFO][2018-05-30 13:13:35,039][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 13.0, whose tasks have all completed, from pool 
[INFO][2018-05-30 13:13:35,040][org.apache.spark.scheduler.DAGScheduler]ResultStage 13 (isEmpty at ReceiveKafkaData.scala:75) finished in 0.006 s
[INFO][2018-05-30 13:13:35,040][org.apache.spark.scheduler.DAGScheduler]Job 13 finished: isEmpty at ReceiveKafkaData.scala:75, took 0.015989 s
[INFO][2018-05-30 13:13:35,040][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527657215000 ms.0 from job set of time 1527657215000 ms
[INFO][2018-05-30 13:13:35,041][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.040 s for time 1527657215000 ms (execution: 0.023 s)
[INFO][2018-05-30 13:13:35,041][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 19 from persistence list
[INFO][2018-05-30 13:13:35,042][org.apache.spark.storage.BlockManager]Removing RDD 19
[INFO][2018-05-30 13:13:35,042][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 18 from persistence list
[INFO][2018-05-30 13:13:35,044][org.apache.spark.storage.BlockManager]Removing RDD 18
[INFO][2018-05-30 13:13:35,044][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-30 13:13:35,045][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527657205000 ms
[INFO][2018-05-30 13:13:40,020][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527657220000 ms
[INFO][2018-05-30 13:13:40,020][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527657220000 ms.0 from job set of time 1527657220000 ms
[INFO][2018-05-30 13:13:40,025][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:75
[INFO][2018-05-30 13:13:40,026][org.apache.spark.scheduler.DAGScheduler]Got job 14 (isEmpty at ReceiveKafkaData.scala:75) with 1 output partitions
[INFO][2018-05-30 13:13:40,026][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 14 (isEmpty at ReceiveKafkaData.scala:75)
[INFO][2018-05-30 13:13:40,026][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-30 13:13:40,026][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-30 13:13:40,027][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 14 (MapPartitionsRDD[23] at map at ReceiveKafkaData.scala:74), which has no missing parents
[INFO][2018-05-30 13:13:40,029][org.apache.spark.storage.memory.MemoryStore]Block broadcast_14 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-30 13:13:40,031][org.apache.spark.storage.memory.MemoryStore]Block broadcast_14_piece0 stored as bytes in memory (estimated size 1973.0 B, free 912.3 MB)
[INFO][2018-05-30 13:13:40,031][org.apache.spark.storage.BlockManagerInfo]Added broadcast_14_piece0 in memory on 10.194.67.11:56669 (size: 1973.0 B, free: 912.3 MB)
[INFO][2018-05-30 13:13:40,032][org.apache.spark.SparkContext]Created broadcast 14 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-30 13:13:40,032][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[23] at map at ReceiveKafkaData.scala:74) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-30 13:13:40,032][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 14.0 with 1 tasks
[INFO][2018-05-30 13:13:40,033][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 14.0 (TID 14, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-30 13:13:40,033][org.apache.spark.executor.Executor]Running task 0.0 in stage 14.0 (TID 14)
[INFO][2018-05-30 13:13:40,035][org.apache.spark.streaming.kafka010.KafkaRDD]Beginning offset 189042 is the same as ending offset skipping seven 0
[INFO][2018-05-30 13:13:40,036][org.apache.spark.executor.Executor]Finished task 0.0 in stage 14.0 (TID 14). 708 bytes result sent to driver
[INFO][2018-05-30 13:13:40,037][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 14.0 (TID 14) in 4 ms on localhost (executor driver) (1/1)
[INFO][2018-05-30 13:13:40,037][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 14.0, whose tasks have all completed, from pool 
[INFO][2018-05-30 13:13:40,037][org.apache.spark.scheduler.DAGScheduler]ResultStage 14 (isEmpty at ReceiveKafkaData.scala:75) finished in 0.004 s
[INFO][2018-05-30 13:13:40,037][org.apache.spark.scheduler.DAGScheduler]Job 14 finished: isEmpty at ReceiveKafkaData.scala:75, took 0.011952 s
[INFO][2018-05-30 13:13:40,038][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527657220000 ms.0 from job set of time 1527657220000 ms
[INFO][2018-05-30 13:13:40,038][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.038 s for time 1527657220000 ms (execution: 0.018 s)
[INFO][2018-05-30 13:13:40,038][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 21 from persistence list
[INFO][2018-05-30 13:13:40,038][org.apache.spark.storage.BlockManager]Removing RDD 21
[INFO][2018-05-30 13:13:40,038][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 20 from persistence list
[INFO][2018-05-30 13:13:40,039][org.apache.spark.storage.BlockManager]Removing RDD 20
[INFO][2018-05-30 13:13:40,039][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-30 13:13:40,039][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527657210000 ms
[INFO][2018-05-30 13:13:45,024][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527657225000 ms
[INFO][2018-05-30 13:13:45,024][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527657225000 ms.0 from job set of time 1527657225000 ms
[INFO][2018-05-30 13:13:45,029][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:75
[INFO][2018-05-30 13:13:45,029][org.apache.spark.scheduler.DAGScheduler]Got job 15 (isEmpty at ReceiveKafkaData.scala:75) with 1 output partitions
[INFO][2018-05-30 13:13:45,030][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 15 (isEmpty at ReceiveKafkaData.scala:75)
[INFO][2018-05-30 13:13:45,030][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-30 13:13:45,030][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-30 13:13:45,030][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 15 (MapPartitionsRDD[25] at map at ReceiveKafkaData.scala:74), which has no missing parents
[INFO][2018-05-30 13:13:45,032][org.apache.spark.storage.memory.MemoryStore]Block broadcast_15 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-30 13:13:45,033][org.apache.spark.storage.memory.MemoryStore]Block broadcast_15_piece0 stored as bytes in memory (estimated size 1973.0 B, free 912.3 MB)
[INFO][2018-05-30 13:13:45,033][org.apache.spark.storage.BlockManagerInfo]Added broadcast_15_piece0 in memory on 10.194.67.11:56669 (size: 1973.0 B, free: 912.3 MB)
[INFO][2018-05-30 13:13:45,034][org.apache.spark.SparkContext]Created broadcast 15 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-30 13:13:45,035][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 15 (MapPartitionsRDD[25] at map at ReceiveKafkaData.scala:74) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-30 13:13:45,035][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 15.0 with 1 tasks
[INFO][2018-05-30 13:13:45,036][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 15.0 (TID 15, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-30 13:13:45,036][org.apache.spark.executor.Executor]Running task 0.0 in stage 15.0 (TID 15)
[INFO][2018-05-30 13:13:45,039][org.apache.spark.streaming.kafka010.KafkaRDD]Beginning offset 189042 is the same as ending offset skipping seven 0
[INFO][2018-05-30 13:13:45,039][org.apache.spark.executor.Executor]Finished task 0.0 in stage 15.0 (TID 15). 708 bytes result sent to driver
[INFO][2018-05-30 13:13:45,040][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 15.0 (TID 15) in 4 ms on localhost (executor driver) (1/1)
[INFO][2018-05-30 13:13:45,040][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 15.0, whose tasks have all completed, from pool 
[INFO][2018-05-30 13:13:45,040][org.apache.spark.scheduler.DAGScheduler]ResultStage 15 (isEmpty at ReceiveKafkaData.scala:75) finished in 0.005 s
[INFO][2018-05-30 13:13:45,041][org.apache.spark.scheduler.DAGScheduler]Job 15 finished: isEmpty at ReceiveKafkaData.scala:75, took 0.011468 s
[INFO][2018-05-30 13:13:45,041][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527657225000 ms.0 from job set of time 1527657225000 ms
[INFO][2018-05-30 13:13:45,041][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 23 from persistence list
[INFO][2018-05-30 13:13:45,041][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.041 s for time 1527657225000 ms (execution: 0.017 s)
[INFO][2018-05-30 13:13:45,042][org.apache.spark.storage.BlockManager]Removing RDD 23
[INFO][2018-05-30 13:13:45,042][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 22 from persistence list
[INFO][2018-05-30 13:13:45,043][org.apache.spark.storage.BlockManager]Removing RDD 22
[INFO][2018-05-30 13:13:45,043][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-30 13:13:45,043][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527657215000 ms
[INFO][2018-05-30 13:13:50,019][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527657230000 ms
[INFO][2018-05-30 13:13:50,020][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527657230000 ms.0 from job set of time 1527657230000 ms
[INFO][2018-05-30 13:13:50,025][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:75
[INFO][2018-05-30 13:13:50,026][org.apache.spark.scheduler.DAGScheduler]Got job 16 (isEmpty at ReceiveKafkaData.scala:75) with 1 output partitions
[INFO][2018-05-30 13:13:50,026][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 16 (isEmpty at ReceiveKafkaData.scala:75)
[INFO][2018-05-30 13:13:50,026][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-30 13:13:50,026][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-30 13:13:50,026][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 16 (MapPartitionsRDD[27] at map at ReceiveKafkaData.scala:74), which has no missing parents
[INFO][2018-05-30 13:13:50,028][org.apache.spark.storage.memory.MemoryStore]Block broadcast_16 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-30 13:13:50,029][org.apache.spark.storage.memory.MemoryStore]Block broadcast_16_piece0 stored as bytes in memory (estimated size 1973.0 B, free 912.3 MB)
[INFO][2018-05-30 13:13:50,030][org.apache.spark.storage.BlockManagerInfo]Added broadcast_16_piece0 in memory on 10.194.67.11:56669 (size: 1973.0 B, free: 912.3 MB)
[INFO][2018-05-30 13:13:50,032][org.apache.spark.SparkContext]Created broadcast 16 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-30 13:13:50,034][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 16 (MapPartitionsRDD[27] at map at ReceiveKafkaData.scala:74) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-30 13:13:50,034][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 16.0 with 1 tasks
[INFO][2018-05-30 13:13:50,035][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 16.0 (TID 16, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-30 13:13:50,035][org.apache.spark.executor.Executor]Running task 0.0 in stage 16.0 (TID 16)
[INFO][2018-05-30 13:13:50,036][org.apache.spark.streaming.kafka010.KafkaRDD]Beginning offset 189042 is the same as ending offset skipping seven 0
[INFO][2018-05-30 13:13:50,037][org.apache.spark.executor.Executor]Finished task 0.0 in stage 16.0 (TID 16). 665 bytes result sent to driver
[INFO][2018-05-30 13:13:50,038][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 16.0 (TID 16) in 4 ms on localhost (executor driver) (1/1)
[INFO][2018-05-30 13:13:50,038][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 16.0, whose tasks have all completed, from pool 
[INFO][2018-05-30 13:13:50,039][org.apache.spark.scheduler.DAGScheduler]ResultStage 16 (isEmpty at ReceiveKafkaData.scala:75) finished in 0.005 s
[INFO][2018-05-30 13:13:50,040][org.apache.spark.scheduler.DAGScheduler]Job 16 finished: isEmpty at ReceiveKafkaData.scala:75, took 0.014106 s
[INFO][2018-05-30 13:13:50,040][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527657230000 ms.0 from job set of time 1527657230000 ms
[INFO][2018-05-30 13:13:50,040][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.040 s for time 1527657230000 ms (execution: 0.021 s)
[INFO][2018-05-30 13:13:50,040][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 25 from persistence list
[INFO][2018-05-30 13:13:50,041][org.apache.spark.storage.BlockManager]Removing RDD 25
[INFO][2018-05-30 13:13:50,041][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 24 from persistence list
[INFO][2018-05-30 13:13:50,041][org.apache.spark.storage.BlockManager]Removing RDD 24
[INFO][2018-05-30 13:13:50,041][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-30 13:13:50,041][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527657220000 ms
[INFO][2018-05-30 13:13:55,018][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527657235000 ms
[INFO][2018-05-30 13:13:55,019][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527657235000 ms.0 from job set of time 1527657235000 ms
[INFO][2018-05-30 13:13:55,023][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:75
[INFO][2018-05-30 13:13:55,024][org.apache.spark.scheduler.DAGScheduler]Got job 17 (isEmpty at ReceiveKafkaData.scala:75) with 1 output partitions
[INFO][2018-05-30 13:13:55,024][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 17 (isEmpty at ReceiveKafkaData.scala:75)
[INFO][2018-05-30 13:13:55,024][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-30 13:13:55,024][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-30 13:13:55,025][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 17 (MapPartitionsRDD[29] at map at ReceiveKafkaData.scala:74), which has no missing parents
[INFO][2018-05-30 13:13:55,026][org.apache.spark.storage.memory.MemoryStore]Block broadcast_17 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-30 13:13:55,027][org.apache.spark.storage.memory.MemoryStore]Block broadcast_17_piece0 stored as bytes in memory (estimated size 1973.0 B, free 912.3 MB)
[INFO][2018-05-30 13:13:55,027][org.apache.spark.storage.BlockManagerInfo]Added broadcast_17_piece0 in memory on 10.194.67.11:56669 (size: 1973.0 B, free: 912.3 MB)
[INFO][2018-05-30 13:13:55,028][org.apache.spark.SparkContext]Created broadcast 17 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-30 13:13:55,028][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[29] at map at ReceiveKafkaData.scala:74) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-30 13:13:55,028][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 17.0 with 1 tasks
[INFO][2018-05-30 13:13:55,029][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 17.0 (TID 17, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-30 13:13:55,029][org.apache.spark.executor.Executor]Running task 0.0 in stage 17.0 (TID 17)
[INFO][2018-05-30 13:13:55,030][org.apache.spark.streaming.kafka010.KafkaRDD]Beginning offset 189042 is the same as ending offset skipping seven 0
[INFO][2018-05-30 13:13:55,031][org.apache.spark.executor.Executor]Finished task 0.0 in stage 17.0 (TID 17). 665 bytes result sent to driver
[INFO][2018-05-30 13:13:55,031][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 17.0 (TID 17) in 2 ms on localhost (executor driver) (1/1)
[INFO][2018-05-30 13:13:55,031][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 17.0, whose tasks have all completed, from pool 
[INFO][2018-05-30 13:13:55,032][org.apache.spark.scheduler.DAGScheduler]ResultStage 17 (isEmpty at ReceiveKafkaData.scala:75) finished in 0.004 s
[INFO][2018-05-30 13:13:55,032][org.apache.spark.scheduler.DAGScheduler]Job 17 finished: isEmpty at ReceiveKafkaData.scala:75, took 0.008297 s
[INFO][2018-05-30 13:13:55,032][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527657235000 ms.0 from job set of time 1527657235000 ms
[INFO][2018-05-30 13:13:55,032][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 27 from persistence list
[INFO][2018-05-30 13:13:55,032][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.032 s for time 1527657235000 ms (execution: 0.013 s)
[INFO][2018-05-30 13:13:55,033][org.apache.spark.storage.BlockManager]Removing RDD 27
[INFO][2018-05-30 13:13:55,033][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 26 from persistence list
[INFO][2018-05-30 13:13:55,033][org.apache.spark.storage.BlockManager]Removing RDD 26
[INFO][2018-05-30 13:13:55,033][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-30 13:13:55,034][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527657225000 ms
[INFO][2018-05-30 13:14:00,020][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527657240000 ms
[INFO][2018-05-30 13:14:00,020][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527657240000 ms.0 from job set of time 1527657240000 ms
[INFO][2018-05-30 13:14:00,024][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:75
[INFO][2018-05-30 13:14:00,025][org.apache.spark.scheduler.DAGScheduler]Got job 18 (isEmpty at ReceiveKafkaData.scala:75) with 1 output partitions
[INFO][2018-05-30 13:14:00,025][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 18 (isEmpty at ReceiveKafkaData.scala:75)
[INFO][2018-05-30 13:14:00,025][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-30 13:14:00,025][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-30 13:14:00,025][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 18 (MapPartitionsRDD[31] at map at ReceiveKafkaData.scala:74), which has no missing parents
[INFO][2018-05-30 13:14:00,029][org.apache.spark.storage.memory.MemoryStore]Block broadcast_18 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-30 13:14:00,030][org.apache.spark.storage.memory.MemoryStore]Block broadcast_18_piece0 stored as bytes in memory (estimated size 1973.0 B, free 912.3 MB)
[INFO][2018-05-30 13:14:00,031][org.apache.spark.storage.BlockManagerInfo]Added broadcast_18_piece0 in memory on 10.194.67.11:56669 (size: 1973.0 B, free: 912.3 MB)
[INFO][2018-05-30 13:14:00,031][org.apache.spark.SparkContext]Created broadcast 18 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-30 13:14:00,032][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 18 (MapPartitionsRDD[31] at map at ReceiveKafkaData.scala:74) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-30 13:14:00,032][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 18.0 with 1 tasks
[INFO][2018-05-30 13:14:00,033][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 18.0 (TID 18, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-30 13:14:00,033][org.apache.spark.executor.Executor]Running task 0.0 in stage 18.0 (TID 18)
[INFO][2018-05-30 13:14:00,034][org.apache.spark.streaming.kafka010.KafkaRDD]Beginning offset 189042 is the same as ending offset skipping seven 0
[INFO][2018-05-30 13:14:00,034][org.apache.spark.executor.Executor]Finished task 0.0 in stage 18.0 (TID 18). 665 bytes result sent to driver
[INFO][2018-05-30 13:14:00,035][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 18.0 (TID 18) in 3 ms on localhost (executor driver) (1/1)
[INFO][2018-05-30 13:14:00,035][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 18.0, whose tasks have all completed, from pool 
[INFO][2018-05-30 13:14:00,035][org.apache.spark.scheduler.DAGScheduler]ResultStage 18 (isEmpty at ReceiveKafkaData.scala:75) finished in 0.003 s
[INFO][2018-05-30 13:14:00,036][org.apache.spark.scheduler.DAGScheduler]Job 18 finished: isEmpty at ReceiveKafkaData.scala:75, took 0.011231 s
[INFO][2018-05-30 13:14:00,036][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527657240000 ms.0 from job set of time 1527657240000 ms
[INFO][2018-05-30 13:14:00,036][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.036 s for time 1527657240000 ms (execution: 0.016 s)
[INFO][2018-05-30 13:14:00,036][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 29 from persistence list
[INFO][2018-05-30 13:14:00,038][org.apache.spark.storage.BlockManager]Removing RDD 29
[INFO][2018-05-30 13:14:00,039][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 28 from persistence list
[INFO][2018-05-30 13:14:00,041][org.apache.spark.storage.BlockManager]Removing RDD 28
[INFO][2018-05-30 13:14:00,042][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-30 13:14:00,043][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527657230000 ms
[INFO][2018-05-30 13:14:05,019][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527657245000 ms
[INFO][2018-05-30 13:14:05,019][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527657245000 ms.0 from job set of time 1527657245000 ms
[INFO][2018-05-30 13:14:05,025][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:75
[INFO][2018-05-30 13:14:05,026][org.apache.spark.scheduler.DAGScheduler]Got job 19 (isEmpty at ReceiveKafkaData.scala:75) with 1 output partitions
[INFO][2018-05-30 13:14:05,026][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 19 (isEmpty at ReceiveKafkaData.scala:75)
[INFO][2018-05-30 13:14:05,026][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-30 13:14:05,026][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-30 13:14:05,026][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 19 (MapPartitionsRDD[33] at map at ReceiveKafkaData.scala:74), which has no missing parents
[INFO][2018-05-30 13:14:05,029][org.apache.spark.storage.memory.MemoryStore]Block broadcast_19 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-30 13:14:05,030][org.apache.spark.storage.memory.MemoryStore]Block broadcast_19_piece0 stored as bytes in memory (estimated size 1973.0 B, free 912.3 MB)
[INFO][2018-05-30 13:14:05,030][org.apache.spark.storage.BlockManagerInfo]Added broadcast_19_piece0 in memory on 10.194.67.11:56669 (size: 1973.0 B, free: 912.3 MB)
[INFO][2018-05-30 13:14:05,031][org.apache.spark.SparkContext]Created broadcast 19 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-30 13:14:05,032][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 19 (MapPartitionsRDD[33] at map at ReceiveKafkaData.scala:74) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-30 13:14:05,032][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 19.0 with 1 tasks
[INFO][2018-05-30 13:14:05,032][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 19.0 (TID 19, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-30 13:14:05,033][org.apache.spark.executor.Executor]Running task 0.0 in stage 19.0 (TID 19)
[INFO][2018-05-30 13:14:05,035][org.apache.spark.streaming.kafka010.KafkaRDD]Beginning offset 189042 is the same as ending offset skipping seven 0
[INFO][2018-05-30 13:14:05,035][org.apache.spark.executor.Executor]Finished task 0.0 in stage 19.0 (TID 19). 665 bytes result sent to driver
[INFO][2018-05-30 13:14:05,036][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 19.0 (TID 19) in 4 ms on localhost (executor driver) (1/1)
[INFO][2018-05-30 13:14:05,036][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 19.0, whose tasks have all completed, from pool 
[INFO][2018-05-30 13:14:05,037][org.apache.spark.scheduler.DAGScheduler]ResultStage 19 (isEmpty at ReceiveKafkaData.scala:75) finished in 0.005 s
[INFO][2018-05-30 13:14:05,037][org.apache.spark.scheduler.DAGScheduler]Job 19 finished: isEmpty at ReceiveKafkaData.scala:75, took 0.012166 s
[INFO][2018-05-30 13:14:05,038][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527657245000 ms.0 from job set of time 1527657245000 ms
[INFO][2018-05-30 13:14:05,038][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.038 s for time 1527657245000 ms (execution: 0.019 s)
[INFO][2018-05-30 13:14:05,038][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 31 from persistence list
[INFO][2018-05-30 13:14:05,039][org.apache.spark.storage.BlockManager]Removing RDD 31
[INFO][2018-05-30 13:14:05,039][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 30 from persistence list
[INFO][2018-05-30 13:14:05,040][org.apache.spark.storage.BlockManager]Removing RDD 30
[INFO][2018-05-30 13:14:05,040][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-30 13:14:05,040][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527657235000 ms
[INFO][2018-05-30 13:14:10,018][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527657250000 ms
[INFO][2018-05-30 13:14:10,019][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527657250000 ms.0 from job set of time 1527657250000 ms
[INFO][2018-05-30 13:14:10,024][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:75
[INFO][2018-05-30 13:14:10,025][org.apache.spark.scheduler.DAGScheduler]Got job 20 (isEmpty at ReceiveKafkaData.scala:75) with 1 output partitions
[INFO][2018-05-30 13:14:10,025][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 20 (isEmpty at ReceiveKafkaData.scala:75)
[INFO][2018-05-30 13:14:10,025][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-30 13:14:10,025][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-30 13:14:10,026][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 20 (MapPartitionsRDD[35] at map at ReceiveKafkaData.scala:74), which has no missing parents
[INFO][2018-05-30 13:14:10,027][org.apache.spark.storage.memory.MemoryStore]Block broadcast_20 stored as values in memory (estimated size 3.1 KB, free 912.2 MB)
[INFO][2018-05-30 13:14:10,028][org.apache.spark.storage.memory.MemoryStore]Block broadcast_20_piece0 stored as bytes in memory (estimated size 1973.0 B, free 912.2 MB)
[INFO][2018-05-30 13:14:10,029][org.apache.spark.storage.BlockManagerInfo]Added broadcast_20_piece0 in memory on 10.194.67.11:56669 (size: 1973.0 B, free: 912.3 MB)
[INFO][2018-05-30 13:14:10,029][org.apache.spark.SparkContext]Created broadcast 20 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-30 13:14:10,030][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 20 (MapPartitionsRDD[35] at map at ReceiveKafkaData.scala:74) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-30 13:14:10,030][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 20.0 with 1 tasks
[INFO][2018-05-30 13:14:10,030][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 20.0 (TID 20, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-30 13:14:10,031][org.apache.spark.executor.Executor]Running task 0.0 in stage 20.0 (TID 20)
[INFO][2018-05-30 13:14:10,033][org.apache.spark.streaming.kafka010.KafkaRDD]Beginning offset 189042 is the same as ending offset skipping seven 0
[INFO][2018-05-30 13:14:10,033][org.apache.spark.executor.Executor]Finished task 0.0 in stage 20.0 (TID 20). 708 bytes result sent to driver
[INFO][2018-05-30 13:14:10,034][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 20.0 (TID 20) in 4 ms on localhost (executor driver) (1/1)
[INFO][2018-05-30 13:14:10,034][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 20.0, whose tasks have all completed, from pool 
[INFO][2018-05-30 13:14:10,035][org.apache.spark.scheduler.DAGScheduler]ResultStage 20 (isEmpty at ReceiveKafkaData.scala:75) finished in 0.005 s
[INFO][2018-05-30 13:14:10,036][org.apache.spark.scheduler.DAGScheduler]Job 20 finished: isEmpty at ReceiveKafkaData.scala:75, took 0.011294 s
[INFO][2018-05-30 13:14:10,036][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527657250000 ms.0 from job set of time 1527657250000 ms
[INFO][2018-05-30 13:14:10,036][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.036 s for time 1527657250000 ms (execution: 0.017 s)
[INFO][2018-05-30 13:14:10,036][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 33 from persistence list
[INFO][2018-05-30 13:14:10,036][org.apache.spark.storage.BlockManager]Removing RDD 33
[INFO][2018-05-30 13:14:10,037][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 32 from persistence list
[INFO][2018-05-30 13:14:10,037][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-30 13:14:10,037][org.apache.spark.storage.BlockManager]Removing RDD 32
[INFO][2018-05-30 13:14:10,038][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527657240000 ms
[INFO][2018-05-30 13:14:12,903][org.apache.spark.streaming.StreamingContext]Invoking stop(stopGracefully=false) from shutdown hook
[INFO][2018-05-30 13:14:12,907][org.apache.spark.streaming.scheduler.ReceiverTracker]ReceiverTracker stopped
[INFO][2018-05-30 13:14:12,907][org.apache.spark.streaming.scheduler.JobGenerator]Stopping JobGenerator immediately
[INFO][2018-05-30 13:14:12,908][org.apache.spark.streaming.util.RecurringTimer]Stopped timer for JobGenerator after time 1527657250000
[INFO][2018-05-30 13:14:12,917][org.apache.spark.streaming.scheduler.JobGenerator]Stopped JobGenerator
[INFO][2018-05-30 13:14:12,920][org.apache.spark.streaming.scheduler.JobScheduler]Stopped JobScheduler
[INFO][2018-05-30 13:14:12,926][org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@57adfab0{/streaming,null,UNAVAILABLE,@Spark}
[INFO][2018-05-30 13:14:12,926][org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@2a22ad2b{/streaming/batch,null,UNAVAILABLE,@Spark}
[INFO][2018-05-30 13:14:12,928][org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@5dbf5634{/static/streaming,null,UNAVAILABLE,@Spark}
[INFO][2018-05-30 13:14:12,929][org.apache.spark.streaming.StreamingContext]StreamingContext stopped successfully
[INFO][2018-05-30 13:14:12,930][org.apache.spark.SparkContext]Invoking stop() from shutdown hook
[INFO][2018-05-30 13:14:12,936][org.spark_project.jetty.server.AbstractConnector]Stopped Spark@5fdff907{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-05-30 13:14:12,940][org.apache.spark.ui.SparkUI]Stopped Spark web UI at http://10.194.67.11:4040
[INFO][2018-05-30 13:14:12,948][org.apache.spark.MapOutputTrackerMasterEndpoint]MapOutputTrackerMasterEndpoint stopped!
[INFO][2018-05-30 13:14:12,965][org.apache.spark.storage.memory.MemoryStore]MemoryStore cleared
[INFO][2018-05-30 13:14:12,965][org.apache.spark.storage.BlockManager]BlockManager stopped
[INFO][2018-05-30 13:14:12,966][org.apache.spark.storage.BlockManagerMaster]BlockManagerMaster stopped
[INFO][2018-05-30 13:14:12,968][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint]OutputCommitCoordinator stopped!
[INFO][2018-05-30 13:14:12,969][org.apache.spark.SparkContext]Successfully stopped SparkContext
[INFO][2018-05-30 13:14:12,969][org.apache.spark.util.ShutdownHookManager]Shutdown hook called
[INFO][2018-05-30 13:14:12,970][org.apache.spark.util.ShutdownHookManager]Deleting directory /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/spark-f3c5a830-ee55-4f12-9ab8-4b08ac674bae
[INFO][2018-05-30 13:25:11,103][org.apache.spark.SparkContext]Running Spark version 2.2.0
[INFO][2018-05-30 13:25:11,857][org.apache.spark.SparkContext]Submitted application: ReceiveKafkaData$
[INFO][2018-05-30 13:25:11,882][org.apache.spark.SecurityManager]Changing view acls to: seven
[INFO][2018-05-30 13:25:11,883][org.apache.spark.SecurityManager]Changing modify acls to: seven
[INFO][2018-05-30 13:25:11,884][org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO][2018-05-30 13:25:11,897][org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO][2018-05-30 13:25:11,898][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(seven); groups with view permissions: Set(); users  with modify permissions: Set(seven); groups with modify permissions: Set()
[INFO][2018-05-30 13:25:12,619][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 57227.
[INFO][2018-05-30 13:25:12,653][org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO][2018-05-30 13:25:12,692][org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO][2018-05-30 13:25:12,696][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2018-05-30 13:25:12,697][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
[INFO][2018-05-30 13:25:12,724][org.apache.spark.storage.DiskBlockManager]Created local directory at /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/blockmgr-95cec8d2-ba16-4bd3-bb59-decd905e482e
[INFO][2018-05-30 13:25:12,760][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 912.3 MB
[INFO][2018-05-30 13:25:12,979][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO][2018-05-30 13:25:13,178][org.spark_project.jetty.util.log]Logging initialized @3033ms
[INFO][2018-05-30 13:25:13,304][org.spark_project.jetty.server.Server]jetty-9.3.z-SNAPSHOT
[INFO][2018-05-30 13:25:13,334][org.spark_project.jetty.server.Server]Started @3190ms
[INFO][2018-05-30 13:25:13,386][org.spark_project.jetty.server.AbstractConnector]Started ServerConnector@df60c0{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-05-30 13:25:13,389][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4040.
[INFO][2018-05-30 13:25:13,462][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3e2822{/jobs,null,AVAILABLE,@Spark}
[INFO][2018-05-30 13:25:13,463][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@35fe2125{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2018-05-30 13:25:13,464][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@34645867{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2018-05-30 13:25:13,470][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1255b1d1{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2018-05-30 13:25:13,472][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7c22d4f{/stages,null,AVAILABLE,@Spark}
[INFO][2018-05-30 13:25:13,473][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@60bdf15d{/stages/json,null,AVAILABLE,@Spark}
[INFO][2018-05-30 13:25:13,473][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@51e4ccb3{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2018-05-30 13:25:13,475][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@28a0fd6c{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-30 13:25:13,504][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@66629f63{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2018-05-30 13:25:13,514][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@27a5328c{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2018-05-30 13:25:13,516][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6c345c5f{/storage,null,AVAILABLE,@Spark}
[INFO][2018-05-30 13:25:13,517][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@65e61854{/storage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-30 13:25:13,518][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4fcee388{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2018-05-30 13:25:13,520][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3af17be2{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2018-05-30 13:25:13,520][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@37f21974{/environment,null,AVAILABLE,@Spark}
[INFO][2018-05-30 13:25:13,524][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6e521c1e{/environment/json,null,AVAILABLE,@Spark}
[INFO][2018-05-30 13:25:13,525][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5d5d9e5{/executors,null,AVAILABLE,@Spark}
[INFO][2018-05-30 13:25:13,526][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4ef27d66{/executors/json,null,AVAILABLE,@Spark}
[INFO][2018-05-30 13:25:13,526][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1d9bec4d{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2018-05-30 13:25:13,526][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@10c8f62{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2018-05-30 13:25:13,534][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@25f7391e{/static,null,AVAILABLE,@Spark}
[INFO][2018-05-30 13:25:13,535][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@593e824f{/,null,AVAILABLE,@Spark}
[INFO][2018-05-30 13:25:13,536][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6d8792db{/api,null,AVAILABLE,@Spark}
[INFO][2018-05-30 13:25:13,538][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2f162cc0{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-30 13:25:13,540][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7c041b41{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-30 13:25:13,547][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://10.194.67.11:4040
[INFO][2018-05-30 13:25:13,695][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
[INFO][2018-05-30 13:25:13,734][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 57228.
[INFO][2018-05-30 13:25:13,734][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 10.194.67.11:57228
[INFO][2018-05-30 13:25:13,739][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2018-05-30 13:25:13,747][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 10.194.67.11, 57228, None)
[INFO][2018-05-30 13:25:13,751][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 10.194.67.11:57228 with 912.3 MB RAM, BlockManagerId(driver, 10.194.67.11, 57228, None)
[INFO][2018-05-30 13:25:13,754][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 10.194.67.11, 57228, None)
[INFO][2018-05-30 13:25:13,754][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 10.194.67.11, 57228, None)
[INFO][2018-05-30 13:25:14,005][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@55b62629{/metrics/json,null,AVAILABLE,@Spark}
[WARN][2018-05-30 13:25:14,607][org.apache.spark.streaming.kafka010.KafkaUtils]overriding enable.auto.commit to false for executor
[WARN][2018-05-30 13:25:14,608][org.apache.spark.streaming.kafka010.KafkaUtils]overriding auto.offset.reset to none for executor
[WARN][2018-05-30 13:25:14,609][org.apache.spark.streaming.kafka010.KafkaUtils]overriding executor group.id to spark-executor-use_a_separate_group_id_for_each_stream
[WARN][2018-05-30 13:25:14,611][org.apache.spark.streaming.kafka010.KafkaUtils]overriding receive.buffer.bytes to 65536 see KAFKA-3135
[INFO][2018-05-30 13:25:14,778][org.apache.spark.streaming.kafka010.DirectKafkaInputDStream]Slide time = 5000 ms
[INFO][2018-05-30 13:25:14,778][org.apache.spark.streaming.kafka010.DirectKafkaInputDStream]Storage level = Serialized 1x Replicated
[INFO][2018-05-30 13:25:14,779][org.apache.spark.streaming.kafka010.DirectKafkaInputDStream]Checkpoint interval = null
[INFO][2018-05-30 13:25:14,780][org.apache.spark.streaming.kafka010.DirectKafkaInputDStream]Remember interval = 5000 ms
[INFO][2018-05-30 13:25:14,781][org.apache.spark.streaming.kafka010.DirectKafkaInputDStream]Initialized and validated org.apache.spark.streaming.kafka010.DirectKafkaInputDStream@5b13d624
[INFO][2018-05-30 13:25:14,781][org.apache.spark.streaming.dstream.MappedDStream]Slide time = 5000 ms
[INFO][2018-05-30 13:25:14,781][org.apache.spark.streaming.dstream.MappedDStream]Storage level = Serialized 1x Replicated
[INFO][2018-05-30 13:25:14,781][org.apache.spark.streaming.dstream.MappedDStream]Checkpoint interval = null
[INFO][2018-05-30 13:25:14,781][org.apache.spark.streaming.dstream.MappedDStream]Remember interval = 5000 ms
[INFO][2018-05-30 13:25:14,782][org.apache.spark.streaming.dstream.MappedDStream]Initialized and validated org.apache.spark.streaming.dstream.MappedDStream@4014f2e9
[INFO][2018-05-30 13:25:14,782][org.apache.spark.streaming.dstream.ForEachDStream]Slide time = 5000 ms
[INFO][2018-05-30 13:25:14,782][org.apache.spark.streaming.dstream.ForEachDStream]Storage level = Serialized 1x Replicated
[INFO][2018-05-30 13:25:14,782][org.apache.spark.streaming.dstream.ForEachDStream]Checkpoint interval = null
[INFO][2018-05-30 13:25:14,782][org.apache.spark.streaming.dstream.ForEachDStream]Remember interval = 5000 ms
[INFO][2018-05-30 13:25:14,782][org.apache.spark.streaming.dstream.ForEachDStream]Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@25c0a476
[INFO][2018-05-30 13:25:14,882][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	metric.reporters = []
	metadata.max.age.ms = 300000
	partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor]
	reconnect.backoff.ms = 50
	sasl.kerberos.ticket.renew.window.factor = 0.8
	max.partition.fetch.bytes = 1048576
	bootstrap.servers = [vm-xaj-bigdata-da-d01:9092, vm-xaj-bigdata-da-d02:9092, vm-xaj-bigdata-da-d03:9092, vm-xaj-bigdata-da-d04:9092, vm-xaj-bigdata-da-d05:9092, vm-xaj-bigdata-da-d06:9092, vm-xaj-bigdata-da-d07:9092]
	ssl.keystore.type = JKS
	enable.auto.commit = false
	sasl.mechanism = GSSAPI
	interceptor.classes = null
	exclude.internal.topics = true
	ssl.truststore.password = null
	client.id = 
	ssl.endpoint.identification.algorithm = null
	max.poll.records = 2147483647
	check.crcs = true
	request.timeout.ms = 40000
	heartbeat.interval.ms = 3000
	auto.commit.interval.ms = 5000
	receive.buffer.bytes = 65536
	ssl.truststore.type = JKS
	ssl.truststore.location = null
	ssl.keystore.password = null
	fetch.min.bytes = 1
	send.buffer.bytes = 131072
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	group.id = use_a_separate_group_id_for_each_stream
	retry.backoff.ms = 100
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	ssl.trustmanager.algorithm = PKIX
	ssl.key.password = null
	fetch.max.wait.ms = 500
	sasl.kerberos.min.time.before.relogin = 60000
	connections.max.idle.ms = 540000
	session.timeout.ms = 30000
	metrics.num.samples = 2
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	ssl.protocol = TLS
	ssl.provider = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.keystore.location = null
	ssl.cipher.suites = null
	security.protocol = PLAINTEXT
	ssl.keymanager.algorithm = SunX509
	metrics.sample.window.ms = 30000
	auto.offset.reset = latest

[INFO][2018-05-30 13:25:18,380][org.apache.spark.SparkContext]Running Spark version 2.2.0
[INFO][2018-05-30 13:25:19,157][org.apache.spark.SparkContext]Submitted application: SimulationKafkaSendOutData$
[INFO][2018-05-30 13:25:19,177][org.apache.spark.SecurityManager]Changing view acls to: seven
[INFO][2018-05-30 13:25:19,178][org.apache.spark.SecurityManager]Changing modify acls to: seven
[INFO][2018-05-30 13:25:19,180][org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO][2018-05-30 13:25:19,181][org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO][2018-05-30 13:25:19,181][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(seven); groups with view permissions: Set(); users  with modify permissions: Set(seven); groups with modify permissions: Set()
[INFO][2018-05-30 13:25:19,514][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 57233.
[INFO][2018-05-30 13:25:19,531][org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO][2018-05-30 13:25:19,547][org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO][2018-05-30 13:25:19,550][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2018-05-30 13:25:19,550][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
[INFO][2018-05-30 13:25:19,560][org.apache.spark.storage.DiskBlockManager]Created local directory at /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/blockmgr-8057c05a-52f8-4d95-8d2d-79987a25e481
[INFO][2018-05-30 13:25:19,577][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 912.3 MB
[INFO][2018-05-30 13:25:19,663][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO][2018-05-30 13:25:19,755][org.spark_project.jetty.util.log]Logging initialized @2326ms
[INFO][2018-05-30 13:25:19,809][org.spark_project.jetty.server.Server]jetty-9.3.z-SNAPSHOT
[INFO][2018-05-30 13:25:19,823][org.spark_project.jetty.server.Server]Started @2396ms
[WARN][2018-05-30 13:25:19,836][org.apache.spark.util.Utils]Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[INFO][2018-05-30 13:25:19,844][org.spark_project.jetty.server.AbstractConnector]Started ServerConnector@5427d3ac{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}
[INFO][2018-05-30 13:25:19,844][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4041.
[INFO][2018-05-30 13:25:19,871][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@41fe9859{/jobs,null,AVAILABLE,@Spark}
[INFO][2018-05-30 13:25:19,872][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@29a60c27{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2018-05-30 13:25:19,872][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@69c79f09{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2018-05-30 13:25:19,873][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@533b266e{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2018-05-30 13:25:19,874][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@89ff02e{/stages,null,AVAILABLE,@Spark}
[INFO][2018-05-30 13:25:19,875][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@62679465{/stages/json,null,AVAILABLE,@Spark}
[INFO][2018-05-30 13:25:19,875][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1d71006f{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2018-05-30 13:25:19,877][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3ebff828{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-30 13:25:19,878][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@33352f32{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2018-05-30 13:25:19,878][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1e044120{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2018-05-30 13:25:19,879][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3624da92{/storage,null,AVAILABLE,@Spark}
[INFO][2018-05-30 13:25:19,881][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@94f6bfb{/storage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-30 13:25:19,882][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2484f433{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2018-05-30 13:25:19,883][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1255b1d1{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2018-05-30 13:25:19,884][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7c22d4f{/environment,null,AVAILABLE,@Spark}
[INFO][2018-05-30 13:25:19,887][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@60bdf15d{/environment/json,null,AVAILABLE,@Spark}
[INFO][2018-05-30 13:25:19,888][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@51e4ccb3{/executors,null,AVAILABLE,@Spark}
[INFO][2018-05-30 13:25:19,889][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@495083a0{/executors/json,null,AVAILABLE,@Spark}
[INFO][2018-05-30 13:25:19,891][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@28a0fd6c{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2018-05-30 13:25:19,892][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@66629f63{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2018-05-30 13:25:19,901][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@27a5328c{/static,null,AVAILABLE,@Spark}
[INFO][2018-05-30 13:25:19,906][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@10c8f62{/,null,AVAILABLE,@Spark}
[INFO][2018-05-30 13:25:19,908][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@25f7391e{/api,null,AVAILABLE,@Spark}
[INFO][2018-05-30 13:25:19,910][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@31024624{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-30 13:25:19,910][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@32cb636e{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-30 13:25:19,912][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://10.194.67.11:4041
[INFO][2018-05-30 13:25:20,032][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
[INFO][2018-05-30 13:25:20,059][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 57234.
[INFO][2018-05-30 13:25:20,060][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 10.194.67.11:57234
[INFO][2018-05-30 13:25:20,062][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2018-05-30 13:25:20,063][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 10.194.67.11, 57234, None)
[INFO][2018-05-30 13:25:20,067][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 10.194.67.11:57234 with 912.3 MB RAM, BlockManagerId(driver, 10.194.67.11, 57234, None)
[INFO][2018-05-30 13:25:20,083][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 10.194.67.11, 57234, None)
[INFO][2018-05-30 13:25:20,084][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 10.194.67.11, 57234, None)
[INFO][2018-05-30 13:25:20,391][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4010d494{/metrics/json,null,AVAILABLE,@Spark}
[INFO][2018-05-30 13:25:20,954][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0 stored as values in memory (estimated size 228.1 KB, free 912.1 MB)
[INFO][2018-05-30 13:25:21,008][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0_piece0 stored as bytes in memory (estimated size 22.1 KB, free 912.1 MB)
[INFO][2018-05-30 13:25:21,010][org.apache.spark.storage.BlockManagerInfo]Added broadcast_0_piece0 in memory on 10.194.67.11:57234 (size: 22.1 KB, free: 912.3 MB)
[INFO][2018-05-30 13:25:21,014][org.apache.spark.SparkContext]Created broadcast 0 from textFile at SimulationKafkaSendOutData.scala:24
[WARN][2018-05-30 13:25:21,371][org.apache.hadoop.util.NativeCodeLoader]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO][2018-05-30 13:25:21,507][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 1
[INFO][2018-05-30 13:25:21,589][org.apache.spark.SparkContext]Starting job: collect at SimulationKafkaSendOutData.scala:24
[INFO][2018-05-30 13:25:21,598][org.apache.spark.scheduler.DAGScheduler]Got job 0 (collect at SimulationKafkaSendOutData.scala:24) with 2 output partitions
[INFO][2018-05-30 13:25:21,599][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 0 (collect at SimulationKafkaSendOutData.scala:24)
[INFO][2018-05-30 13:25:21,599][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-30 13:25:21,600][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-30 13:25:21,611][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 0 (hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/* MapPartitionsRDD[1] at textFile at SimulationKafkaSendOutData.scala:24), which has no missing parents
[INFO][2018-05-30 13:25:21,627][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1 stored as values in memory (estimated size 3.3 KB, free 912.1 MB)
[INFO][2018-05-30 13:25:21,629][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1_piece0 stored as bytes in memory (estimated size 1969.0 B, free 912.1 MB)
[INFO][2018-05-30 13:25:21,629][org.apache.spark.storage.BlockManagerInfo]Added broadcast_1_piece0 in memory on 10.194.67.11:57234 (size: 1969.0 B, free: 912.3 MB)
[INFO][2018-05-30 13:25:21,630][org.apache.spark.SparkContext]Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-30 13:25:21,641][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ResultStage 0 (hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/* MapPartitionsRDD[1] at textFile at SimulationKafkaSendOutData.scala:24) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2018-05-30 13:25:21,642][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 0.0 with 2 tasks
[INFO][2018-05-30 13:25:21,676][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 4883 bytes)
[INFO][2018-05-30 13:25:21,679][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 4883 bytes)
[INFO][2018-05-30 13:25:21,684][org.apache.spark.executor.Executor]Running task 0.0 in stage 0.0 (TID 0)
[INFO][2018-05-30 13:25:21,685][org.apache.spark.executor.Executor]Running task 1.0 in stage 0.0 (TID 1)
[INFO][2018-05-30 13:25:21,727][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/part-00000:12229859+12229860
[INFO][2018-05-30 13:25:21,727][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/part-00000:0+12229859
[INFO][2018-05-30 13:25:22,060][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	metric.reporters = []
	metadata.max.age.ms = 300000
	partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor]
	reconnect.backoff.ms = 50
	sasl.kerberos.ticket.renew.window.factor = 0.8
	max.partition.fetch.bytes = 1048576
	bootstrap.servers = [vm-xaj-bigdata-da-d01:9092, vm-xaj-bigdata-da-d02:9092, vm-xaj-bigdata-da-d03:9092, vm-xaj-bigdata-da-d04:9092, vm-xaj-bigdata-da-d05:9092, vm-xaj-bigdata-da-d06:9092, vm-xaj-bigdata-da-d07:9092]
	ssl.keystore.type = JKS
	enable.auto.commit = false
	sasl.mechanism = GSSAPI
	interceptor.classes = null
	exclude.internal.topics = true
	ssl.truststore.password = null
	client.id = consumer-1
	ssl.endpoint.identification.algorithm = null
	max.poll.records = 2147483647
	check.crcs = true
	request.timeout.ms = 40000
	heartbeat.interval.ms = 3000
	auto.commit.interval.ms = 5000
	receive.buffer.bytes = 65536
	ssl.truststore.type = JKS
	ssl.truststore.location = null
	ssl.keystore.password = null
	fetch.min.bytes = 1
	send.buffer.bytes = 131072
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	group.id = use_a_separate_group_id_for_each_stream
	retry.backoff.ms = 100
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	ssl.trustmanager.algorithm = PKIX
	ssl.key.password = null
	fetch.max.wait.ms = 500
	sasl.kerberos.min.time.before.relogin = 60000
	connections.max.idle.ms = 540000
	session.timeout.ms = 30000
	metrics.num.samples = 2
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	ssl.protocol = TLS
	ssl.provider = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.keystore.location = null
	ssl.cipher.suites = null
	security.protocol = PLAINTEXT
	ssl.keymanager.algorithm = SunX509
	metrics.sample.window.ms = 30000
	auto.offset.reset = latest

[INFO][2018-05-30 13:25:22,081][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 0.10.0.1
[INFO][2018-05-30 13:25:22,081][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a7a17cdec9eaa6c5
[INFO][2018-05-30 13:25:22,253][org.apache.kafka.clients.consumer.internals.AbstractCoordinator]Discovered coordinator vm-xaj-bigdata-da-d03:9092 (id: 2147483531 rack: null) for group use_a_separate_group_id_for_each_stream.
[INFO][2018-05-30 13:25:22,254][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]Revoking previously assigned partitions [] for group use_a_separate_group_id_for_each_stream
[INFO][2018-05-30 13:25:22,254][org.apache.kafka.clients.consumer.internals.AbstractCoordinator](Re-)joining group use_a_separate_group_id_for_each_stream
[INFO][2018-05-30 13:25:24,800][org.apache.spark.storage.memory.MemoryStore]Block taskresult_1 stored as bytes in memory (estimated size 11.8 MB, free 900.2 MB)
[INFO][2018-05-30 13:25:24,803][org.apache.spark.storage.BlockManagerInfo]Added taskresult_1 in memory on 10.194.67.11:57234 (size: 11.8 MB, free: 900.5 MB)
[INFO][2018-05-30 13:25:24,804][org.apache.spark.executor.Executor]Finished task 1.0 in stage 0.0 (TID 1). 12387903 bytes result sent via BlockManager)
[INFO][2018-05-30 13:25:24,840][org.apache.spark.network.client.TransportClientFactory]Successfully created connection to /10.194.67.11:57234 after 19 ms (0 ms spent in bootstraps)
[INFO][2018-05-30 13:25:24,846][org.apache.spark.storage.memory.MemoryStore]Block taskresult_0 stored as bytes in memory (estimated size 11.8 MB, free 888.4 MB)
[INFO][2018-05-30 13:25:24,847][org.apache.spark.storage.BlockManagerInfo]Added taskresult_0 in memory on 10.194.67.11:57234 (size: 11.8 MB, free: 888.6 MB)
[INFO][2018-05-30 13:25:24,848][org.apache.spark.executor.Executor]Finished task 0.0 in stage 0.0 (TID 0). 12388162 bytes result sent via BlockManager)
[INFO][2018-05-30 13:25:25,350][org.apache.kafka.clients.consumer.internals.AbstractCoordinator]Successfully joined group use_a_separate_group_id_for_each_stream with generation 1
[INFO][2018-05-30 13:25:25,351][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]Setting newly assigned partitions [seven-0] for group use_a_separate_group_id_for_each_stream
[INFO][2018-05-30 13:25:25,388][org.apache.spark.streaming.util.RecurringTimer]Started timer for JobGenerator at time 1527657915000
[INFO][2018-05-30 13:25:25,390][org.apache.spark.streaming.scheduler.JobGenerator]Started JobGenerator at 1527657915000 ms
[INFO][2018-05-30 13:25:25,391][org.apache.spark.streaming.scheduler.JobScheduler]Started JobScheduler
[INFO][2018-05-30 13:25:25,395][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@57adfab0{/streaming,null,AVAILABLE,@Spark}
[INFO][2018-05-30 13:25:25,395][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5af97169{/streaming/json,null,AVAILABLE,@Spark}
[INFO][2018-05-30 13:25:25,396][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2a22ad2b{/streaming/batch,null,AVAILABLE,@Spark}
[INFO][2018-05-30 13:25:25,397][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@121c54fa{/streaming/batch/json,null,AVAILABLE,@Spark}
[INFO][2018-05-30 13:25:25,398][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5dbf5634{/static/streaming,null,AVAILABLE,@Spark}
[INFO][2018-05-30 13:25:25,399][org.apache.spark.streaming.StreamingContext]StreamingContext started
[INFO][2018-05-30 13:25:25,434][org.apache.spark.storage.BlockManagerInfo]Removed taskresult_1 on 10.194.67.11:57234 in memory (size: 11.8 MB, free: 900.5 MB)
[INFO][2018-05-30 13:25:25,434][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 0.0 (TID 1) in 3752 ms on localhost (executor driver) (1/2)
[INFO][2018-05-30 13:25:25,446][org.apache.spark.storage.BlockManagerInfo]Removed taskresult_0 on 10.194.67.11:57234 in memory (size: 11.8 MB, free: 912.3 MB)
[INFO][2018-05-30 13:25:25,448][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 0.0 (TID 0) in 3782 ms on localhost (executor driver) (2/2)
[INFO][2018-05-30 13:25:25,450][org.apache.spark.scheduler.DAGScheduler]ResultStage 0 (collect at SimulationKafkaSendOutData.scala:24) finished in 3.793 s
[INFO][2018-05-30 13:25:25,454][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO][2018-05-30 13:25:25,455][org.apache.spark.scheduler.DAGScheduler]Job 0 finished: collect at SimulationKafkaSendOutData.scala:24, took 3.866321 s
[INFO][2018-05-30 13:25:25,543][org.spark_project.jetty.server.AbstractConnector]Stopped Spark@5427d3ac{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}
[INFO][2018-05-30 13:25:25,548][org.apache.spark.ui.SparkUI]Stopped Spark web UI at http://10.194.67.11:4041
[INFO][2018-05-30 13:25:25,558][org.apache.spark.MapOutputTrackerMasterEndpoint]MapOutputTrackerMasterEndpoint stopped!
[INFO][2018-05-30 13:25:25,572][org.apache.spark.storage.memory.MemoryStore]MemoryStore cleared
[INFO][2018-05-30 13:25:25,572][org.apache.spark.storage.BlockManager]BlockManager stopped
[INFO][2018-05-30 13:25:25,573][org.apache.spark.storage.BlockManagerMaster]BlockManagerMaster stopped
[INFO][2018-05-30 13:25:25,576][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint]OutputCommitCoordinator stopped!
[INFO][2018-05-30 13:25:25,577][org.apache.spark.SparkContext]Successfully stopped SparkContext
[INFO][2018-05-30 13:25:25,730][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	metric.reporters = []
	metadata.max.age.ms = 300000
	reconnect.backoff.ms = 50
	sasl.kerberos.ticket.renew.window.factor = 0.8
	bootstrap.servers = [vm-xaj-bigdata-da-d01:9092, vm-xaj-bigdata-da-d02:9092, vm-xaj-bigdata-da-d03:9092, vm-xaj-bigdata-da-d04:9092, vm-xaj-bigdata-da-d05:9092, vm-xaj-bigdata-da-d06:9092, vm-xaj-bigdata-da-d07:9092]
	ssl.keystore.type = JKS
	sasl.mechanism = GSSAPI
	max.block.ms = 60000
	interceptor.classes = null
	ssl.truststore.password = null
	client.id = 
	ssl.endpoint.identification.algorithm = null
	request.timeout.ms = 30000
	acks = all
	receive.buffer.bytes = 32768
	ssl.truststore.type = JKS
	retries = 0
	ssl.truststore.location = null
	ssl.keystore.password = null
	send.buffer.bytes = 131072
	compression.type = none
	metadata.fetch.timeout.ms = 60000
	retry.backoff.ms = 100
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	buffer.memory = 33554432
	timeout.ms = 30000
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	ssl.trustmanager.algorithm = PKIX
	block.on.buffer.full = false
	ssl.key.password = null
	sasl.kerberos.min.time.before.relogin = 60000
	connections.max.idle.ms = 540000
	max.in.flight.requests.per.connection = 5
	metrics.num.samples = 2
	ssl.protocol = TLS
	ssl.provider = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	batch.size = 16384
	ssl.keystore.location = null
	ssl.cipher.suites = null
	security.protocol = PLAINTEXT
	max.request.size = 1048576
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
	ssl.keymanager.algorithm = SunX509
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	linger.ms = 1

[INFO][2018-05-30 13:25:25,772][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	metric.reporters = []
	metadata.max.age.ms = 300000
	reconnect.backoff.ms = 50
	sasl.kerberos.ticket.renew.window.factor = 0.8
	bootstrap.servers = [vm-xaj-bigdata-da-d01:9092, vm-xaj-bigdata-da-d02:9092, vm-xaj-bigdata-da-d03:9092, vm-xaj-bigdata-da-d04:9092, vm-xaj-bigdata-da-d05:9092, vm-xaj-bigdata-da-d06:9092, vm-xaj-bigdata-da-d07:9092]
	ssl.keystore.type = JKS
	sasl.mechanism = GSSAPI
	max.block.ms = 60000
	interceptor.classes = null
	ssl.truststore.password = null
	client.id = producer-1
	ssl.endpoint.identification.algorithm = null
	request.timeout.ms = 30000
	acks = all
	receive.buffer.bytes = 32768
	ssl.truststore.type = JKS
	retries = 0
	ssl.truststore.location = null
	ssl.keystore.password = null
	send.buffer.bytes = 131072
	compression.type = none
	metadata.fetch.timeout.ms = 60000
	retry.backoff.ms = 100
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	buffer.memory = 33554432
	timeout.ms = 30000
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	ssl.trustmanager.algorithm = PKIX
	block.on.buffer.full = false
	ssl.key.password = null
	sasl.kerberos.min.time.before.relogin = 60000
	connections.max.idle.ms = 540000
	max.in.flight.requests.per.connection = 5
	metrics.num.samples = 2
	ssl.protocol = TLS
	ssl.provider = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	batch.size = 16384
	ssl.keystore.location = null
	ssl.cipher.suites = null
	security.protocol = PLAINTEXT
	max.request.size = 1048576
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
	ssl.keymanager.algorithm = SunX509
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	linger.ms = 1

[INFO][2018-05-30 13:25:25,775][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 0.10.0.1
[INFO][2018-05-30 13:25:25,775][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a7a17cdec9eaa6c5
[INFO][2018-05-30 13:25:26,099][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527657915000 ms
[INFO][2018-05-30 13:25:26,108][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527657915000 ms.0 from job set of time 1527657915000 ms
[INFO][2018-05-30 13:25:26,150][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527657920000 ms
[INFO][2018-05-30 13:25:26,175][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527657925000 ms
[INFO][2018-05-30 13:25:26,352][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:75
[INFO][2018-05-30 13:25:26,392][org.apache.spark.scheduler.DAGScheduler]Got job 0 (isEmpty at ReceiveKafkaData.scala:75) with 1 output partitions
[INFO][2018-05-30 13:25:26,393][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 0 (isEmpty at ReceiveKafkaData.scala:75)
[INFO][2018-05-30 13:25:26,394][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-30 13:25:26,397][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-30 13:25:26,430][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 0 (MapPartitionsRDD[1] at map at ReceiveKafkaData.scala:74), which has no missing parents
[INFO][2018-05-30 13:25:26,596][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-30 13:25:26,631][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0_piece0 stored as bytes in memory (estimated size 1972.0 B, free 912.3 MB)
[INFO][2018-05-30 13:25:26,633][org.apache.spark.storage.BlockManagerInfo]Added broadcast_0_piece0 in memory on 10.194.67.11:57228 (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-30 13:25:26,636][org.apache.spark.SparkContext]Created broadcast 0 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-30 13:25:26,654][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at map at ReceiveKafkaData.scala:74) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-30 13:25:26,655][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 0.0 with 1 tasks
[INFO][2018-05-30 13:25:26,693][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-30 13:25:26,703][org.apache.spark.executor.Executor]Running task 0.0 in stage 0.0 (TID 0)
[INFO][2018-05-30 13:25:26,730][org.apache.spark.streaming.kafka010.KafkaRDD]Beginning offset 189042 is the same as ending offset skipping seven 0
[INFO][2018-05-30 13:25:26,741][org.apache.spark.executor.Executor]Finished task 0.0 in stage 0.0 (TID 0). 708 bytes result sent to driver
[INFO][2018-05-30 13:25:26,748][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 0.0 (TID 0) in 65 ms on localhost (executor driver) (1/1)
[INFO][2018-05-30 13:25:26,752][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO][2018-05-30 13:25:26,757][org.apache.spark.scheduler.DAGScheduler]ResultStage 0 (isEmpty at ReceiveKafkaData.scala:75) finished in 0.084 s
[INFO][2018-05-30 13:25:26,764][org.apache.spark.scheduler.DAGScheduler]Job 0 finished: isEmpty at ReceiveKafkaData.scala:75, took 0.410191 s
[INFO][2018-05-30 13:25:26,773][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527657915000 ms.0 from job set of time 1527657915000 ms
[INFO][2018-05-30 13:25:26,776][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 11.771 s for time 1527657915000 ms (execution: 0.668 s)
[INFO][2018-05-30 13:25:26,776][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527657920000 ms.0 from job set of time 1527657920000 ms
[INFO][2018-05-30 13:25:26,785][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-30 13:25:26,789][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 
[INFO][2018-05-30 13:25:26,795][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:75
[INFO][2018-05-30 13:25:26,796][org.apache.spark.scheduler.DAGScheduler]Got job 1 (isEmpty at ReceiveKafkaData.scala:75) with 1 output partitions
[INFO][2018-05-30 13:25:26,796][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 1 (isEmpty at ReceiveKafkaData.scala:75)
[INFO][2018-05-30 13:25:26,796][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-30 13:25:26,796][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-30 13:25:26,797][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 1 (MapPartitionsRDD[3] at map at ReceiveKafkaData.scala:74), which has no missing parents
[INFO][2018-05-30 13:25:26,800][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-30 13:25:26,802][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1_piece0 stored as bytes in memory (estimated size 1973.0 B, free 912.3 MB)
[INFO][2018-05-30 13:25:26,803][org.apache.spark.storage.BlockManagerInfo]Added broadcast_1_piece0 in memory on 10.194.67.11:57228 (size: 1973.0 B, free: 912.3 MB)
[INFO][2018-05-30 13:25:26,804][org.apache.spark.SparkContext]Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-30 13:25:26,807][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at map at ReceiveKafkaData.scala:74) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-30 13:25:26,807][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 1.0 with 1 tasks
[INFO][2018-05-30 13:25:26,808][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-30 13:25:26,808][org.apache.spark.executor.Executor]Running task 0.0 in stage 1.0 (TID 1)
[INFO][2018-05-30 13:25:26,812][org.apache.spark.streaming.kafka010.KafkaRDD]Beginning offset 189042 is the same as ending offset skipping seven 0
[INFO][2018-05-30 13:25:26,814][org.apache.spark.executor.Executor]Finished task 0.0 in stage 1.0 (TID 1). 708 bytes result sent to driver
[INFO][2018-05-30 13:25:26,815][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 1.0 (TID 1) in 8 ms on localhost (executor driver) (1/1)
[INFO][2018-05-30 13:25:26,815][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO][2018-05-30 13:25:26,816][org.apache.spark.scheduler.DAGScheduler]ResultStage 1 (isEmpty at ReceiveKafkaData.scala:75) finished in 0.009 s
[INFO][2018-05-30 13:25:26,816][org.apache.spark.scheduler.DAGScheduler]Job 1 finished: isEmpty at ReceiveKafkaData.scala:75, took 0.021166 s
[INFO][2018-05-30 13:25:26,817][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527657920000 ms.0 from job set of time 1527657920000 ms
[INFO][2018-05-30 13:25:26,817][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 6.817 s for time 1527657920000 ms (execution: 0.041 s)
[INFO][2018-05-30 13:25:26,817][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527657925000 ms.0 from job set of time 1527657925000 ms
[INFO][2018-05-30 13:25:26,818][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 1 from persistence list
[INFO][2018-05-30 13:25:26,823][org.apache.spark.storage.BlockManager]Removing RDD 1
[INFO][2018-05-30 13:25:26,822][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 0 from persistence list
[INFO][2018-05-30 13:25:26,823][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-30 13:25:26,823][org.apache.spark.storage.BlockManager]Removing RDD 0
[INFO][2018-05-30 13:25:26,824][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 
[INFO][2018-05-30 13:25:26,828][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:75
[INFO][2018-05-30 13:25:26,829][org.apache.spark.scheduler.DAGScheduler]Got job 2 (isEmpty at ReceiveKafkaData.scala:75) with 1 output partitions
[INFO][2018-05-30 13:25:26,829][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 2 (isEmpty at ReceiveKafkaData.scala:75)
[INFO][2018-05-30 13:25:26,829][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-30 13:25:26,829][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-30 13:25:26,830][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 2 (MapPartitionsRDD[5] at map at ReceiveKafkaData.scala:74), which has no missing parents
[INFO][2018-05-30 13:25:26,834][org.apache.spark.storage.memory.MemoryStore]Block broadcast_2 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-30 13:25:26,835][org.apache.spark.storage.memory.MemoryStore]Block broadcast_2_piece0 stored as bytes in memory (estimated size 1973.0 B, free 912.3 MB)
[INFO][2018-05-30 13:25:26,836][org.apache.spark.storage.BlockManagerInfo]Added broadcast_2_piece0 in memory on 10.194.67.11:57228 (size: 1973.0 B, free: 912.3 MB)
[INFO][2018-05-30 13:25:26,837][org.apache.spark.SparkContext]Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-30 13:25:26,839][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[5] at map at ReceiveKafkaData.scala:74) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-30 13:25:26,839][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 2.0 with 1 tasks
[INFO][2018-05-30 13:25:26,840][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-30 13:25:26,840][org.apache.spark.executor.Executor]Running task 0.0 in stage 2.0 (TID 2)
[INFO][2018-05-30 13:25:26,844][org.apache.spark.streaming.kafka010.KafkaRDD]Beginning offset 189042 is the same as ending offset skipping seven 0
[INFO][2018-05-30 13:25:26,845][org.apache.spark.executor.Executor]Finished task 0.0 in stage 2.0 (TID 2). 708 bytes result sent to driver
[INFO][2018-05-30 13:25:26,847][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 2.0 (TID 2) in 7 ms on localhost (executor driver) (1/1)
[INFO][2018-05-30 13:25:26,847][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO][2018-05-30 13:25:26,848][org.apache.spark.scheduler.DAGScheduler]ResultStage 2 (isEmpty at ReceiveKafkaData.scala:75) finished in 0.008 s
[INFO][2018-05-30 13:25:26,848][org.apache.spark.scheduler.DAGScheduler]Job 2 finished: isEmpty at ReceiveKafkaData.scala:75, took 0.020195 s
[INFO][2018-05-30 13:25:26,849][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527657925000 ms.0 from job set of time 1527657925000 ms
[INFO][2018-05-30 13:25:26,849][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 3 from persistence list
[INFO][2018-05-30 13:25:26,849][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 1.849 s for time 1527657925000 ms (execution: 0.032 s)
[INFO][2018-05-30 13:25:26,850][org.apache.spark.storage.BlockManager]Removing RDD 3
[INFO][2018-05-30 13:25:26,850][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 2 from persistence list
[INFO][2018-05-30 13:25:26,850][org.apache.spark.storage.BlockManager]Removing RDD 2
[INFO][2018-05-30 13:25:26,850][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-30 13:25:26,850][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527657915000 ms
[INFO][2018-05-30 13:25:30,017][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527657930000 ms
[INFO][2018-05-30 13:25:30,017][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527657930000 ms.0 from job set of time 1527657930000 ms
[INFO][2018-05-30 13:25:30,026][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:75
[INFO][2018-05-30 13:25:30,027][org.apache.spark.scheduler.DAGScheduler]Got job 3 (isEmpty at ReceiveKafkaData.scala:75) with 1 output partitions
[INFO][2018-05-30 13:25:30,027][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 3 (isEmpty at ReceiveKafkaData.scala:75)
[INFO][2018-05-30 13:25:30,027][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-30 13:25:30,027][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-30 13:25:30,028][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 3 (MapPartitionsRDD[7] at map at ReceiveKafkaData.scala:74), which has no missing parents
[INFO][2018-05-30 13:25:30,033][org.apache.spark.storage.memory.MemoryStore]Block broadcast_3 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-30 13:25:30,035][org.apache.spark.storage.memory.MemoryStore]Block broadcast_3_piece0 stored as bytes in memory (estimated size 1971.0 B, free 912.3 MB)
[INFO][2018-05-30 13:25:30,035][org.apache.spark.storage.BlockManagerInfo]Added broadcast_3_piece0 in memory on 10.194.67.11:57228 (size: 1971.0 B, free: 912.3 MB)
[INFO][2018-05-30 13:25:30,036][org.apache.spark.SparkContext]Created broadcast 3 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-30 13:25:30,037][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[7] at map at ReceiveKafkaData.scala:74) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-30 13:25:30,037][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 3.0 with 1 tasks
[INFO][2018-05-30 13:25:30,038][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 3.0 (TID 3, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-30 13:25:30,038][org.apache.spark.executor.Executor]Running task 0.0 in stage 3.0 (TID 3)
[INFO][2018-05-30 13:25:30,043][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 189042 -> 189079
[INFO][2018-05-30 13:25:30,045][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initializing cache 16 64 0.75
[INFO][2018-05-30 13:25:30,047][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Cache miss for CacheKey(spark-executor-use_a_separate_group_id_for_each_stream,seven,0)
[INFO][2018-05-30 13:25:30,048][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	metric.reporters = []
	metadata.max.age.ms = 300000
	partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor]
	reconnect.backoff.ms = 50
	sasl.kerberos.ticket.renew.window.factor = 0.8
	max.partition.fetch.bytes = 1048576
	bootstrap.servers = [vm-xaj-bigdata-da-d01:9092, vm-xaj-bigdata-da-d02:9092, vm-xaj-bigdata-da-d03:9092, vm-xaj-bigdata-da-d04:9092, vm-xaj-bigdata-da-d05:9092, vm-xaj-bigdata-da-d06:9092, vm-xaj-bigdata-da-d07:9092]
	ssl.keystore.type = JKS
	enable.auto.commit = false
	sasl.mechanism = GSSAPI
	interceptor.classes = null
	exclude.internal.topics = true
	ssl.truststore.password = null
	client.id = 
	ssl.endpoint.identification.algorithm = null
	max.poll.records = 2147483647
	check.crcs = true
	request.timeout.ms = 40000
	heartbeat.interval.ms = 3000
	auto.commit.interval.ms = 5000
	receive.buffer.bytes = 65536
	ssl.truststore.type = JKS
	ssl.truststore.location = null
	ssl.keystore.password = null
	fetch.min.bytes = 1
	send.buffer.bytes = 131072
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	group.id = spark-executor-use_a_separate_group_id_for_each_stream
	retry.backoff.ms = 100
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	ssl.trustmanager.algorithm = PKIX
	ssl.key.password = null
	fetch.max.wait.ms = 500
	sasl.kerberos.min.time.before.relogin = 60000
	connections.max.idle.ms = 540000
	session.timeout.ms = 30000
	metrics.num.samples = 2
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	ssl.protocol = TLS
	ssl.provider = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.keystore.location = null
	ssl.cipher.suites = null
	security.protocol = PLAINTEXT
	ssl.keymanager.algorithm = SunX509
	metrics.sample.window.ms = 30000
	auto.offset.reset = none

[INFO][2018-05-30 13:25:30,050][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	metric.reporters = []
	metadata.max.age.ms = 300000
	partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor]
	reconnect.backoff.ms = 50
	sasl.kerberos.ticket.renew.window.factor = 0.8
	max.partition.fetch.bytes = 1048576
	bootstrap.servers = [vm-xaj-bigdata-da-d01:9092, vm-xaj-bigdata-da-d02:9092, vm-xaj-bigdata-da-d03:9092, vm-xaj-bigdata-da-d04:9092, vm-xaj-bigdata-da-d05:9092, vm-xaj-bigdata-da-d06:9092, vm-xaj-bigdata-da-d07:9092]
	ssl.keystore.type = JKS
	enable.auto.commit = false
	sasl.mechanism = GSSAPI
	interceptor.classes = null
	exclude.internal.topics = true
	ssl.truststore.password = null
	client.id = consumer-2
	ssl.endpoint.identification.algorithm = null
	max.poll.records = 2147483647
	check.crcs = true
	request.timeout.ms = 40000
	heartbeat.interval.ms = 3000
	auto.commit.interval.ms = 5000
	receive.buffer.bytes = 65536
	ssl.truststore.type = JKS
	ssl.truststore.location = null
	ssl.keystore.password = null
	fetch.min.bytes = 1
	send.buffer.bytes = 131072
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	group.id = spark-executor-use_a_separate_group_id_for_each_stream
	retry.backoff.ms = 100
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	ssl.trustmanager.algorithm = PKIX
	ssl.key.password = null
	fetch.max.wait.ms = 500
	sasl.kerberos.min.time.before.relogin = 60000
	connections.max.idle.ms = 540000
	session.timeout.ms = 30000
	metrics.num.samples = 2
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	ssl.protocol = TLS
	ssl.provider = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.keystore.location = null
	ssl.cipher.suites = null
	security.protocol = PLAINTEXT
	ssl.keymanager.algorithm = SunX509
	metrics.sample.window.ms = 30000
	auto.offset.reset = none

[INFO][2018-05-30 13:25:30,052][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 0.10.0.1
[INFO][2018-05-30 13:25:30,053][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a7a17cdec9eaa6c5
[INFO][2018-05-30 13:25:30,056][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 189042
[INFO][2018-05-30 13:25:30,171][org.apache.kafka.clients.consumer.internals.AbstractCoordinator]Discovered coordinator vm-xaj-bigdata-da-d06:9092 (id: 2147483530 rack: null) for group spark-executor-use_a_separate_group_id_for_each_stream.
[INFO][2018-05-30 13:25:30,214][org.apache.spark.executor.Executor]Finished task 0.0 in stage 3.0 (TID 3). 969 bytes result sent to driver
[INFO][2018-05-30 13:25:30,215][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 3.0 (TID 3) in 177 ms on localhost (executor driver) (1/1)
[INFO][2018-05-30 13:25:30,216][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 3.0, whose tasks have all completed, from pool 
[INFO][2018-05-30 13:25:30,217][org.apache.spark.scheduler.DAGScheduler]ResultStage 3 (isEmpty at ReceiveKafkaData.scala:75) finished in 0.178 s
[INFO][2018-05-30 13:25:30,217][org.apache.spark.scheduler.DAGScheduler]Job 3 finished: isEmpty at ReceiveKafkaData.scala:75, took 0.191257 s
[INFO][2018-05-30 13:25:30,230][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:76
[INFO][2018-05-30 13:25:30,230][org.apache.spark.scheduler.DAGScheduler]Got job 4 (foreachPartition at ReceiveKafkaData.scala:76) with 1 output partitions
[INFO][2018-05-30 13:25:30,230][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 4 (foreachPartition at ReceiveKafkaData.scala:76)
[INFO][2018-05-30 13:25:30,231][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-30 13:25:30,231][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-30 13:25:30,231][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 4 (MapPartitionsRDD[7] at map at ReceiveKafkaData.scala:74), which has no missing parents
[INFO][2018-05-30 13:25:30,233][org.apache.spark.storage.memory.MemoryStore]Block broadcast_4 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-30 13:25:30,234][org.apache.spark.storage.memory.MemoryStore]Block broadcast_4_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.3 MB)
[INFO][2018-05-30 13:25:30,235][org.apache.spark.storage.BlockManagerInfo]Added broadcast_4_piece0 in memory on 10.194.67.11:57228 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-30 13:25:30,235][org.apache.spark.SparkContext]Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-30 13:25:30,236][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[7] at map at ReceiveKafkaData.scala:74) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-30 13:25:30,237][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 4.0 with 1 tasks
[INFO][2018-05-30 13:25:30,238][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 4.0 (TID 4, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-30 13:25:30,238][org.apache.spark.executor.Executor]Running task 0.0 in stage 4.0 (TID 4)
[INFO][2018-05-30 13:25:30,242][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 189042 -> 189079
[INFO][2018-05-30 13:25:30,242][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 189042
[INFO][2018-05-30 13:25:30,463][org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper]Process identifier=hconnection-0x3a6f0230 connecting to ZooKeeper ensemble=vm-xaj-bigdata-da-d01:2181,vm-xaj-bigdata-da-d02:2181,vm-xaj-bigdata-da-d03:2181
[INFO][2018-05-30 13:25:30,472][org.apache.zookeeper.ZooKeeper]Client environment:zookeeper.version=3.4.5-cdh5.7.0--1, built on 04/06/2016 14:24 GMT
[INFO][2018-05-30 13:25:30,472][org.apache.zookeeper.ZooKeeper]Client environment:host.name=10.194.67.11
[INFO][2018-05-30 13:25:30,472][org.apache.zookeeper.ZooKeeper]Client environment:java.version=1.8.0_161
[INFO][2018-05-30 13:25:30,473][org.apache.zookeeper.ZooKeeper]Client environment:java.vendor=Oracle Corporation
[INFO][2018-05-30 13:25:30,473][org.apache.zookeeper.ZooKeeper]Client environment:java.home=/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre
[INFO][2018-05-30 13:25:30,473][org.apache.zookeeper.ZooKeeper]Client environment:java.class.path=/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/charsets.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/deploy.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/cldrdata.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/dnsns.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/jaccess.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/jfxrt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/localedata.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/nashorn.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/sunec.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/sunjce_provider.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/sunpkcs11.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/zipfs.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/javaws.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/jce.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/jfr.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/jfxswt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/jsse.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/management-agent.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/plugin.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/resources.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/rt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/ant-javafx.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/dt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/javafx-mx.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/jconsole.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/packager.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/sa-jdi.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/tools.jar:/Users/seven/project/github/dataMining/target/classes:/Users/seven/software/maven/repository/org/scala-lang/scala-library/2.11.12/scala-library-2.11.12.jar:/Users/seven/software/maven/repository/org/scala-lang/scala-compiler/2.11.12/scala-compiler-2.11.12.jar:/Users/seven/software/maven/repository/org/scala-lang/modules/scala-xml_2.11/1.0.5/scala-xml_2.11-1.0.5.jar:/Users/seven/software/maven/repository/org/scala-lang/modules/scala-parser-combinators_2.11/1.0.4/scala-parser-combinators_2.11-1.0.4.jar:/Users/seven/software/maven/repository/org/scala-lang/scala-reflect/2.11.12/scala-reflect-2.11.12.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-client/2.6.0-cdh5.7.0/hadoop-client-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-common/2.6.0-cdh5.7.0/hadoop-common-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/xmlenc/xmlenc/0.52/xmlenc-0.52.jar:/Users/seven/software/maven/repository/commons-net/commons-net/3.1/commons-net-3.1.jar:/Users/seven/software/maven/repository/commons-configuration/commons-configuration/1.6/commons-configuration-1.6.jar:/Users/seven/software/maven/repository/commons-digester/commons-digester/1.8/commons-digester-1.8.jar:/Users/seven/software/maven/repository/commons-beanutils/commons-beanutils/1.7.0/commons-beanutils-1.7.0.jar:/Users/seven/software/maven/repository/org/slf4j/slf4j-api/1.7.5/slf4j-api-1.7.5.jar:/Users/seven/software/maven/repository/org/slf4j/slf4j-log4j12/1.7.5/slf4j-log4j12-1.7.5.jar:/Users/seven/software/maven/repository/org/apache/avro/avro/1.7.6-cdh5.7.0/avro-1.7.6-cdh5.7.0.jar:/Users/seven/software/maven/repository/com/google/code/gson/gson/2.2.4/gson-2.2.4.jar:/Users/seven/software/maven/repository/org/apache/curator/curator-client/2.7.1/curator-client-2.7.1.jar:/Users/seven/software/maven/repository/org/apache/curator/curator-recipes/2.7.1/curator-recipes-2.7.1.jar:/Users/seven/software/maven/repository/com/google/code/findbugs/jsr305/3.0.0/jsr305-3.0.0.jar:/Users/seven/software/maven/repository/org/apache/htrace/htrace-core4/4.0.1-incubating/htrace-core4-4.0.1-incubating.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-compress/1.4.1/commons-compress-1.4.1.jar:/Users/seven/software/maven/repository/org/tukaani/xz/1.0/xz-1.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-hdfs/2.6.0-cdh5.7.0/hadoop-hdfs-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/io/netty/netty/3.6.2.Final/netty-3.6.2.Final.jar:/Users/seven/software/maven/repository/xerces/xercesImpl/2.9.1/xercesImpl-2.9.1.jar:/Users/seven/software/maven/repository/xml-apis/xml-apis/1.3.04/xml-apis-1.3.04.jar:/Users/seven/software/maven/repository/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-app/2.6.0-cdh5.7.0/hadoop-mapreduce-client-app-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-common/2.6.0-cdh5.7.0/hadoop-mapreduce-client-common-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-yarn-client/2.6.0-cdh5.7.0/hadoop-yarn-client-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-yarn-server-common/2.6.0-cdh5.7.0/hadoop-yarn-server-common-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-shuffle/2.6.0-cdh5.7.0/hadoop-mapreduce-client-shuffle-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-yarn-api/2.6.0-cdh5.7.0/hadoop-yarn-api-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-core/2.6.0-cdh5.7.0/hadoop-mapreduce-client-core-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-yarn-common/2.6.0-cdh5.7.0/hadoop-yarn-common-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/javax/xml/bind/jaxb-api/2.2.2/jaxb-api-2.2.2.jar:/Users/seven/software/maven/repository/javax/xml/stream/stax-api/1.0-2/stax-api-1.0-2.jar:/Users/seven/software/maven/repository/javax/activation/activation/1.1/activation-1.1.jar:/Users/seven/software/maven/repository/com/sun/jersey/jersey-client/1.9/jersey-client-1.9.jar:/Users/seven/software/maven/repository/org/codehaus/jackson/jackson-xc/1.8.8/jackson-xc-1.8.8.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.6.0-cdh5.7.0/hadoop-mapreduce-client-jobclient-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-aws/2.6.0-cdh5.7.0/hadoop-aws-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/com/amazonaws/aws-java-sdk-s3/1.10.6/aws-java-sdk-s3-1.10.6.jar:/Users/seven/software/maven/repository/com/amazonaws/aws-java-sdk-kms/1.10.6/aws-java-sdk-kms-1.10.6.jar:/Users/seven/software/maven/repository/com/amazonaws/aws-java-sdk-core/1.10.6/aws-java-sdk-core-1.10.6.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-annotations/2.6.0-cdh5.7.0/hadoop-annotations-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-client/1.2.0-cdh5.7.0/hbase-client-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-annotations/1.2.0-cdh5.7.0/hbase-annotations-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-common/1.2.0-cdh5.7.0/hbase-common-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-protocol/1.2.0-cdh5.7.0/hbase-protocol-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/commons-codec/commons-codec/1.9/commons-codec-1.9.jar:/Users/seven/software/maven/repository/commons-io/commons-io/2.4/commons-io-2.4.jar:/Users/seven/software/maven/repository/commons-lang/commons-lang/2.6/commons-lang-2.6.jar:/Users/seven/software/maven/repository/commons-logging/commons-logging/1.2/commons-logging-1.2.jar:/Users/seven/software/maven/repository/com/google/guava/guava/12.0.1/guava-12.0.1.jar:/Users/seven/software/maven/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/Users/seven/software/maven/repository/io/netty/netty-all/4.0.23.Final/netty-all-4.0.23.Final.jar:/Users/seven/software/maven/repository/org/apache/zookeeper/zookeeper/3.4.5-cdh5.7.0/zookeeper-3.4.5-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/htrace/htrace-core/3.2.0-incubating/htrace-core-3.2.0-incubating.jar:/Users/seven/software/maven/repository/org/codehaus/jackson/jackson-mapper-asl/1.8.8/jackson-mapper-asl-1.8.8.jar:/Users/seven/software/maven/repository/org/jruby/jcodings/jcodings/1.0.8/jcodings-1.0.8.jar:/Users/seven/software/maven/repository/org/jruby/joni/joni/2.1.2/joni-2.1.2.jar:/Users/seven/software/maven/repository/com/yammer/metrics/metrics-core/2.2.0/metrics-core-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-auth/2.6.0-cdh5.7.0/hadoop-auth-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/httpcomponents/httpclient/4.2.5/httpclient-4.2.5.jar:/Users/seven/software/maven/repository/org/apache/httpcomponents/httpcore/4.2.4/httpcore-4.2.4.jar:/Users/seven/software/maven/repository/org/apache/directory/server/apacheds-kerberos-codec/2.0.0-M15/apacheds-kerberos-codec-2.0.0-M15.jar:/Users/seven/software/maven/repository/org/apache/directory/server/apacheds-i18n/2.0.0-M15/apacheds-i18n-2.0.0-M15.jar:/Users/seven/software/maven/repository/org/apache/directory/api/api-asn1-api/1.0.0-M20/api-asn1-api-1.0.0-M20.jar:/Users/seven/software/maven/repository/org/apache/directory/api/api-util/1.0.0-M20/api-util-1.0.0-M20.jar:/Users/seven/software/maven/repository/org/apache/curator/curator-framework/2.7.1/curator-framework-2.7.1.jar:/Users/seven/software/maven/repository/com/github/stephenc/findbugs/findbugs-annotations/1.3.9-1/findbugs-annotations-1.3.9-1.jar:/Users/seven/software/maven/repository/junit/junit/4.12/junit-4.12.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-server/1.2.0-cdh5.7.0/hbase-server-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-procedure/1.2.0-cdh5.7.0/hbase-procedure-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-common/1.2.0-cdh5.7.0/hbase-common-1.2.0-cdh5.7.0-tests.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-prefix-tree/1.2.0-cdh5.7.0/hbase-prefix-tree-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar:/Users/seven/software/maven/repository/commons-collections/commons-collections/3.2.2/commons-collections-3.2.2.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-hadoop-compat/1.2.0-cdh5.7.0/hbase-hadoop-compat-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-hadoop2-compat/1.2.0-cdh5.7.0/hbase-hadoop2-compat-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/com/sun/jersey/jersey-core/1.9/jersey-core-1.9.jar:/Users/seven/software/maven/repository/com/sun/jersey/jersey-server/1.9/jersey-server-1.9.jar:/Users/seven/software/maven/repository/asm/asm/3.1/asm-3.1.jar:/Users/seven/software/maven/repository/commons-cli/commons-cli/1.2/commons-cli-1.2.jar:/Users/seven/software/maven/repository/com/github/stephenc/high-scale-lib/high-scale-lib/1.1.1/high-scale-lib-1.1.1.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-math/2.1/commons-math-2.1.jar:/Users/seven/software/maven/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jetty/6.1.26.cloudera.4/jetty-6.1.26.cloudera.4.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jetty-util/6.1.26.cloudera.4/jetty-util-6.1.26.cloudera.4.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jetty-sslengine/6.1.26.cloudera.4/jetty-sslengine-6.1.26.cloudera.4.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jsp-2.1/6.1.14/jsp-2.1-6.1.14.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jsp-api-2.1/6.1.14/jsp-api-2.1-6.1.14.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/servlet-api-2.5/6.1.14/servlet-api-2.5-6.1.14.jar:/Users/seven/software/maven/repository/org/codehaus/jackson/jackson-core-asl/1.8.8/jackson-core-asl-1.8.8.jar:/Users/seven/software/maven/repository/org/codehaus/jackson/jackson-jaxrs/1.8.8/jackson-jaxrs-1.8.8.jar:/Users/seven/software/maven/repository/tomcat/jasper-compiler/5.5.23/jasper-compiler-5.5.23.jar:/Users/seven/software/maven/repository/tomcat/jasper-runtime/5.5.23/jasper-runtime-5.5.23.jar:/Users/seven/software/maven/repository/commons-el/commons-el/1.0/commons-el-1.0.jar:/Users/seven/software/maven/repository/org/jamon/jamon-runtime/2.4.1/jamon-runtime-2.4.1.jar:/Users/seven/software/maven/repository/com/lmax/disruptor/3.3.0/disruptor-3.3.0.jar:/Users/seven/software/maven/repository/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar:/Users/seven/software/maven/repository/org/owasp/esapi/esapi/2.1.0/esapi-2.1.0.jar:/Users/seven/software/maven/repository/commons-beanutils/commons-beanutils-core/1.7.0/commons-beanutils-core-1.7.0.jar:/Users/seven/software/maven/repository/commons-fileupload/commons-fileupload/1.2/commons-fileupload-1.2.jar:/Users/seven/software/maven/repository/xom/xom/1.2.5/xom-1.2.5.jar:/Users/seven/software/maven/repository/xalan/xalan/2.7.0/xalan-2.7.0.jar:/Users/seven/software/maven/repository/org/beanshell/bsh-core/2.0b4/bsh-core-2.0b4.jar:/Users/seven/software/maven/repository/org/owasp/antisamy/antisamy/1.4.3/antisamy-1.4.3.jar:/Users/seven/software/maven/repository/org/apache/xmlgraphics/batik-css/1.7/batik-css-1.7.jar:/Users/seven/software/maven/repository/org/apache/xmlgraphics/batik-ext/1.7/batik-ext-1.7.jar:/Users/seven/software/maven/repository/org/apache/xmlgraphics/batik-util/1.7/batik-util-1.7.jar:/Users/seven/software/maven/repository/xml-apis/xml-apis-ext/1.3.04/xml-apis-ext-1.3.04.jar:/Users/seven/software/maven/repository/net/sourceforge/nekohtml/nekohtml/1.9.12/nekohtml-1.9.12.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-core/2.6.0-mr1-cdh5.7.0/hadoop-core-2.6.0-mr1-cdh5.7.0.jar:/Users/seven/software/maven/repository/javax/servlet/jsp/jsp-api/2.1/jsp-api-2.1.jar:/Users/seven/software/maven/repository/net/java/dev/jets3t/jets3t/0.6.1/jets3t-0.6.1.jar:/Users/seven/software/maven/repository/javax/servlet/servlet-api/2.5/servlet-api-2.5.jar:/Users/seven/software/maven/repository/hsqldb/hsqldb/1.8.0.10/hsqldb-1.8.0.10.jar:/Users/seven/software/maven/repository/org/eclipse/jdt/core/3.1.1/core-3.1.1.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-hdfs/2.6.0-cdh5.7.0/hadoop-hdfs-2.6.0-cdh5.7.0-tests.jar:/Users/seven/software/maven/repository/commons-daemon/commons-daemon/1.0.13/commons-daemon-1.0.13.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-lang3/3.5/commons-lang3-3.5.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-pool2/2.4.2/commons-pool2-2.4.2.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-mllib_2.11/2.2.0/spark-mllib_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-core_2.11/2.2.0/spark-core_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/avro/avro-mapred/1.7.7/avro-mapred-1.7.7-hadoop2.jar:/Users/seven/software/maven/repository/org/apache/avro/avro-ipc/1.7.7/avro-ipc-1.7.7-tests.jar:/Users/seven/software/maven/repository/com/twitter/chill_2.11/0.8.0/chill_2.11-0.8.0.jar:/Users/seven/software/maven/repository/com/esotericsoftware/kryo-shaded/3.0.3/kryo-shaded-3.0.3.jar:/Users/seven/software/maven/repository/com/esotericsoftware/minlog/1.3.0/minlog-1.3.0.jar:/Users/seven/software/maven/repository/org/objenesis/objenesis/2.1/objenesis-2.1.jar:/Users/seven/software/maven/repository/com/twitter/chill-java/0.8.0/chill-java-0.8.0.jar:/Users/seven/software/maven/repository/org/apache/xbean/xbean-asm5-shaded/4.4/xbean-asm5-shaded-4.4.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-launcher_2.11/2.2.0/spark-launcher_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-network-common_2.11/2.2.0/spark-network-common_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-network-shuffle_2.11/2.2.0/spark-network-shuffle_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-unsafe_2.11/2.2.0/spark-unsafe_2.11-2.2.0.jar:/Users/seven/software/maven/repository/javax/servlet/javax.servlet-api/3.1.0/javax.servlet-api-3.1.0.jar:/Users/seven/software/maven/repository/org/slf4j/jul-to-slf4j/1.7.16/jul-to-slf4j-1.7.16.jar:/Users/seven/software/maven/repository/org/slf4j/jcl-over-slf4j/1.7.16/jcl-over-slf4j-1.7.16.jar:/Users/seven/software/maven/repository/com/ning/compress-lzf/1.0.3/compress-lzf-1.0.3.jar:/Users/seven/software/maven/repository/org/xerial/snappy/snappy-java/1.1.2.6/snappy-java-1.1.2.6.jar:/Users/seven/software/maven/repository/org/roaringbitmap/RoaringBitmap/0.5.11/RoaringBitmap-0.5.11.jar:/Users/seven/software/maven/repository/org/json4s/json4s-jackson_2.11/3.2.11/json4s-jackson_2.11-3.2.11.jar:/Users/seven/software/maven/repository/org/json4s/json4s-core_2.11/3.2.11/json4s-core_2.11-3.2.11.jar:/Users/seven/software/maven/repository/org/json4s/json4s-ast_2.11/3.2.11/json4s-ast_2.11-3.2.11.jar:/Users/seven/software/maven/repository/org/scala-lang/scalap/2.11.0/scalap-2.11.0.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/core/jersey-client/2.22.2/jersey-client-2.22.2.jar:/Users/seven/software/maven/repository/javax/ws/rs/javax.ws.rs-api/2.0.1/javax.ws.rs-api-2.0.1.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/hk2-api/2.4.0-b34/hk2-api-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/hk2-utils/2.4.0-b34/hk2-utils-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/external/aopalliance-repackaged/2.4.0-b34/aopalliance-repackaged-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/external/javax.inject/2.4.0-b34/javax.inject-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/hk2-locator/2.4.0-b34/hk2-locator-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/javassist/javassist/3.18.1-GA/javassist-3.18.1-GA.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/core/jersey-common/2.22.2/jersey-common-2.22.2.jar:/Users/seven/software/maven/repository/javax/annotation/javax.annotation-api/1.2/javax.annotation-api-1.2.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/bundles/repackaged/jersey-guava/2.22.2/jersey-guava-2.22.2.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/osgi-resource-locator/1.0.1/osgi-resource-locator-1.0.1.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/core/jersey-server/2.22.2/jersey-server-2.22.2.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/media/jersey-media-jaxb/2.22.2/jersey-media-jaxb-2.22.2.jar:/Users/seven/software/maven/repository/javax/validation/validation-api/1.1.0.Final/validation-api-1.1.0.Final.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/containers/jersey-container-servlet/2.22.2/jersey-container-servlet-2.22.2.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/containers/jersey-container-servlet-core/2.22.2/jersey-container-servlet-core-2.22.2.jar:/Users/seven/software/maven/repository/com/clearspring/analytics/stream/2.7.0/stream-2.7.0.jar:/Users/seven/software/maven/repository/io/dropwizard/metrics/metrics-core/3.1.2/metrics-core-3.1.2.jar:/Users/seven/software/maven/repository/io/dropwizard/metrics/metrics-jvm/3.1.2/metrics-jvm-3.1.2.jar:/Users/seven/software/maven/repository/io/dropwizard/metrics/metrics-json/3.1.2/metrics-json-3.1.2.jar:/Users/seven/software/maven/repository/io/dropwizard/metrics/metrics-graphite/3.1.2/metrics-graphite-3.1.2.jar:/Users/seven/software/maven/repository/org/apache/ivy/ivy/2.4.0/ivy-2.4.0.jar:/Users/seven/software/maven/repository/oro/oro/2.0.8/oro-2.0.8.jar:/Users/seven/software/maven/repository/net/razorvine/pyrolite/4.13/pyrolite-4.13.jar:/Users/seven/software/maven/repository/net/sf/py4j/py4j/0.10.4/py4j-0.10.4.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-crypto/1.0.0/commons-crypto-1.0.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-streaming_2.11/2.2.0/spark-streaming_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-sql_2.11/2.2.0/spark-sql_2.11-2.2.0.jar:/Users/seven/software/maven/repository/com/univocity/univocity-parsers/2.2.1/univocity-parsers-2.2.1.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-sketch_2.11/2.2.0/spark-sketch_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-catalyst_2.11/2.2.0/spark-catalyst_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/codehaus/janino/janino/3.0.0/janino-3.0.0.jar:/Users/seven/software/maven/repository/org/codehaus/janino/commons-compiler/3.0.0/commons-compiler-3.0.0.jar:/Users/seven/software/maven/repository/org/antlr/antlr4-runtime/4.5.3/antlr4-runtime-4.5.3.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-column/1.8.2/parquet-column-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-common/1.8.2/parquet-common-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-encoding/1.8.2/parquet-encoding-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-hadoop/1.8.2/parquet-hadoop-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-format/2.3.1/parquet-format-2.3.1.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-jackson/1.8.2/parquet-jackson-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-graphx_2.11/2.2.0/spark-graphx_2.11-2.2.0.jar:/Users/seven/software/maven/repository/com/github/fommil/netlib/core/1.1.2/core-1.1.2.jar:/Users/seven/software/maven/repository/net/sourceforge/f2j/arpack_combined_all/0.1/arpack_combined_all-0.1.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-mllib-local_2.11/2.2.0/spark-mllib-local_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/scalanlp/breeze_2.11/0.13.1/breeze_2.11-0.13.1.jar:/Users/seven/software/maven/repository/org/scalanlp/breeze-macros_2.11/0.13.1/breeze-macros_2.11-0.13.1.jar:/Users/seven/software/maven/repository/net/sf/opencsv/opencsv/2.3/opencsv-2.3.jar:/Users/seven/software/maven/repository/com/github/rwl/jtransforms/2.4.0/jtransforms-2.4.0.jar:/Users/seven/software/maven/repository/org/spire-math/spire_2.11/0.13.0/spire_2.11-0.13.0.jar:/Users/seven/software/maven/repository/org/spire-math/spire-macros_2.11/0.13.0/spire-macros_2.11-0.13.0.jar:/Users/seven/software/maven/repository/org/typelevel/machinist_2.11/0.6.1/machinist_2.11-0.6.1.jar:/Users/seven/software/maven/repository/com/chuusai/shapeless_2.11/2.3.2/shapeless_2.11-2.3.2.jar:/Users/seven/software/maven/repository/org/typelevel/macro-compat_2.11/1.1.1/macro-compat_2.11-1.1.1.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-math3/3.4.1/commons-math3-3.4.1.jar:/Users/seven/software/maven/repository/org/jpmml/pmml-model/1.2.15/pmml-model-1.2.15.jar:/Users/seven/software/maven/repository/org/jpmml/pmml-schema/1.2.15/pmml-schema-1.2.15.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-tags_2.11/2.2.0/spark-tags_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-streaming-kafka-0-10_2.11/2.2.0/spark-streaming-kafka-0-10_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/kafka/kafka_2.11/0.10.0.1/kafka_2.11-0.10.0.1.jar:/Users/seven/software/maven/repository/com/101tec/zkclient/0.8/zkclient-0.8.jar:/Users/seven/software/maven/repository/org/apache/kafka/kafka-clients/0.10.0.1/kafka-clients-0.10.0.1.jar:/Users/seven/software/maven/repository/mysql/mysql-connector-java/5.1.31/mysql-connector-java-5.1.31.jar:/Users/seven/software/maven/repository/org/elasticsearch/client/transport/5.1.2/transport-5.1.2.jar:/Users/seven/software/maven/repository/org/elasticsearch/plugin/transport-netty3-client/5.1.2/transport-netty3-client-5.1.2.jar:/Users/seven/software/maven/repository/org/elasticsearch/plugin/transport-netty4-client/5.1.2/transport-netty4-client-5.1.2.jar:/Users/seven/software/maven/repository/io/netty/netty-buffer/4.1.6.Final/netty-buffer-4.1.6.Final.jar:/Users/seven/software/maven/repository/io/netty/netty-codec/4.1.6.Final/netty-codec-4.1.6.Final.jar:/Users/seven/software/maven/repository/io/netty/netty-codec-http/4.1.6.Final/netty-codec-http-4.1.6.Final.jar:/Users/seven/software/maven/repository/io/netty/netty-common/4.1.6.Final/netty-common-4.1.6.Final.jar:/Users/seven/software/maven/repository/io/netty/netty-handler/4.1.6.Final/netty-handler-4.1.6.Final.jar:/Users/seven/software/maven/repository/io/netty/netty-resolver/4.1.6.Final/netty-resolver-4.1.6.Final.jar:/Users/seven/software/maven/repository/io/netty/netty-transport/4.1.6.Final/netty-transport-4.1.6.Final.jar:/Users/seven/software/maven/repository/org/elasticsearch/plugin/reindex-client/5.1.2/reindex-client-5.1.2.jar:/Users/seven/software/maven/repository/org/elasticsearch/client/rest/5.1.2/rest-5.1.2.jar:/Users/seven/software/maven/repository/org/apache/httpcomponents/httpasyncclient/4.1.2/httpasyncclient-4.1.2.jar:/Users/seven/software/maven/repository/org/apache/httpcomponents/httpcore-nio/4.4.5/httpcore-nio-4.4.5.jar:/Users/seven/software/maven/repository/org/elasticsearch/plugin/lang-mustache-client/5.1.2/lang-mustache-client-5.1.2.jar:/Users/seven/software/maven/repository/com/github/spullara/mustache/java/compiler/0.9.3/compiler-0.9.3.jar:/Users/seven/software/maven/repository/org/elasticsearch/plugin/percolator-client/5.1.2/percolator-client-5.1.2.jar:/Users/seven/software/maven/repository/org/elasticsearch/elasticsearch/5.1.2/elasticsearch-5.1.2.jar:/Users/seven/software/maven/repository/org/apache/lucene/lucene-core/6.3.0/lucene-core-6.3.0.jar:/Users/seven/software/maven/repository/org/apache/lucene/lucene-analyzers-common/6.3.0/lucene-analyzers-common-6.3.0.jar:/Users/seven/software/maven/repository/org/apache/lucene/lucene-backward-codecs/6.3.0/lucene-backward-codecs-6.3.0.jar:/Users/seven/software/maven/repository/org/apache/lucene/lucene-grouping/6.3.0/lucene-grouping-6.3.0.jar:/Users/seven/software/maven/repository/org/apache/lucene/lucene-highlighter/6.3.0/lucene-highlighter-6.3.0.jar:/Users/seven/software/maven/repository/org/apache/lucene/lucene-join/6.3.0/lucene-join-6.3.0.jar:/Users/seven/software/maven/repository/org/apache/lucene/lucene-memory/6.3.0/lucene-memory-6.3.0.jar:/Users/seven/software/maven/repository/org/apache/lucene/lucene-misc/6.3.0/lucene-misc-6.3.0.jar:/Users/seven/software/maven/repository/org/apache/lucene/lucene-queries/6.3.0/lucene-queries-6.3.0.jar:/Users/seven/software/maven/repository/org/apache/lucene/lucene-queryparser/6.3.0/lucene-queryparser-6.3.0.jar:/Users/seven/software/maven/repository/org/apache/lucene/lucene-sandbox/6.3.0/lucene-sandbox-6.3.0.jar:/Users/seven/software/maven/repository/org/apache/lucene/lucene-spatial/6.3.0/lucene-spatial-6.3.0.jar:/Users/seven/software/maven/repository/org/apache/lucene/lucene-spatial-extras/6.3.0/lucene-spatial-extras-6.3.0.jar:/Users/seven/software/maven/repository/org/apache/lucene/lucene-spatial3d/6.3.0/lucene-spatial3d-6.3.0.jar:/Users/seven/software/maven/repository/org/apache/lucene/lucene-suggest/6.3.0/lucene-suggest-6.3.0.jar:/Users/seven/software/maven/repository/org/elasticsearch/securesm/1.1/securesm-1.1.jar:/Users/seven/software/maven/repository/net/sf/jopt-simple/jopt-simple/5.0.2/jopt-simple-5.0.2.jar:/Users/seven/software/maven/repository/com/carrotsearch/hppc/0.7.1/hppc-0.7.1.jar:/Users/seven/software/maven/repository/joda-time/joda-time/2.9.5/joda-time-2.9.5.jar:/Users/seven/software/maven/repository/org/yaml/snakeyaml/1.15/snakeyaml-1.15.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/core/jackson-core/2.8.1/jackson-core-2.8.1.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/dataformat/jackson-dataformat-smile/2.8.1/jackson-dataformat-smile-2.8.1.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/dataformat/jackson-dataformat-yaml/2.8.1/jackson-dataformat-yaml-2.8.1.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/dataformat/jackson-dataformat-cbor/2.8.1/jackson-dataformat-cbor-2.8.1.jar:/Users/seven/software/maven/repository/com/tdunning/t-digest/3.0/t-digest-3.0.jar:/Users/seven/software/maven/repository/org/hdrhistogram/HdrHistogram/2.1.6/HdrHistogram-2.1.6.jar:/Users/seven/software/maven/repository/net/java/dev/jna/jna/4.2.2/jna-4.2.2.jar:/Users/seven/software/maven/repository/org/apache/logging/log4j/log4j-api/2.6.2/log4j-api-2.6.2.jar:/Users/seven/software/maven/repository/org/apache/logging/log4j/log4j-core/2.6.2/log4j-core-2.6.2.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/module/jackson-module-scala_2.11/2.6.5/jackson-module-scala_2.11-2.6.5.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/core/jackson-annotations/2.6.5/jackson-annotations-2.6.5.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/core/jackson-databind/2.6.5/jackson-databind-2.6.5.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/module/jackson-module-paranamer/2.6.5/jackson-module-paranamer-2.6.5.jar:/Users/seven/software/maven/repository/com/thoughtworks/paranamer/paranamer/2.6/paranamer-2.6.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-streaming-flume_2.11/2.2.0/spark-streaming-flume_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/flume/flume-ng-core/1.6.0/flume-ng-core-1.6.0.jar:/Users/seven/software/maven/repository/org/apache/flume/flume-ng-configuration/1.6.0/flume-ng-configuration-1.6.0.jar:/Users/seven/software/maven/repository/org/apache/avro/avro-ipc/1.7.4/avro-ipc-1.7.4.jar:/Users/seven/software/maven/repository/org/apache/velocity/velocity/1.7/velocity-1.7.jar:/Users/seven/software/maven/repository/org/apache/mina/mina-core/2.0.4/mina-core-2.0.4.jar:/Users/seven/software/maven/repository/org/apache/flume/flume-ng-sdk/1.6.0/flume-ng-sdk-1.6.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-streaming-flume-sink_2.11/2.2.0/spark-streaming-flume-sink_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/flume/flume-ng-clients/flume-ng-log4jappender/1.6.0/flume-ng-log4jappender-1.6.0.jar:/Users/seven/software/maven/repository/net/jpountz/lz4/lz4/1.3.0/lz4-1.3.0.jar:/Applications/IntelliJ IDEA.app/Contents/lib/idea_rt.jar
[INFO][2018-05-30 13:25:30,474][org.apache.zookeeper.ZooKeeper]Client environment:java.library.path=/Users/seven/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:.
[INFO][2018-05-30 13:25:30,474][org.apache.zookeeper.ZooKeeper]Client environment:java.io.tmpdir=/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/
[INFO][2018-05-30 13:25:30,474][org.apache.zookeeper.ZooKeeper]Client environment:java.compiler=<NA>
[INFO][2018-05-30 13:25:30,474][org.apache.zookeeper.ZooKeeper]Client environment:os.name=Mac OS X
[INFO][2018-05-30 13:25:30,475][org.apache.zookeeper.ZooKeeper]Client environment:os.arch=x86_64
[INFO][2018-05-30 13:25:30,475][org.apache.zookeeper.ZooKeeper]Client environment:os.version=10.13.4
[INFO][2018-05-30 13:25:30,475][org.apache.zookeeper.ZooKeeper]Client environment:user.name=seven
[INFO][2018-05-30 13:25:30,475][org.apache.zookeeper.ZooKeeper]Client environment:user.home=/Users/seven
[INFO][2018-05-30 13:25:30,475][org.apache.zookeeper.ZooKeeper]Client environment:user.dir=/Users/seven/project/github/dataMining
[INFO][2018-05-30 13:25:30,475][org.apache.zookeeper.ZooKeeper]Initiating client connection, connectString=vm-xaj-bigdata-da-d01:2181,vm-xaj-bigdata-da-d02:2181,vm-xaj-bigdata-da-d03:2181 sessionTimeout=60000 watcher=hconnection-0x3a6f02300x0, quorum=vm-xaj-bigdata-da-d01:2181,vm-xaj-bigdata-da-d02:2181,vm-xaj-bigdata-da-d03:2181, baseZNode=/hbase
[INFO][2018-05-30 13:25:30,488][org.apache.zookeeper.ClientCnxn]Opening socket connection to server vm-xaj-bigdata-da-d03/10.213.4.27:2181. Will not attempt to authenticate using SASL (unknown error)
[INFO][2018-05-30 13:25:30,497][org.apache.zookeeper.ClientCnxn]Socket connection established, initiating session, client: /10.194.67.11:57258, server: vm-xaj-bigdata-da-d03/10.213.4.27:2181
[INFO][2018-05-30 13:25:30,517][org.apache.zookeeper.ClientCnxn]Session establishment complete on server vm-xaj-bigdata-da-d03/10.213.4.27:2181, sessionid = 0x362b50d75ec8e40, negotiated timeout = 60000
[WARN][2018-05-30 13:25:30,985][org.apache.hadoop.util.NativeCodeLoader]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO][2018-05-30 13:25:31,543][com.seven.spark.streaming.ReceiveKafkaData$]Inserting 37 lines of data to HBase is success . . .
[INFO][2018-05-30 13:25:31,678][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_3_piece0 on 10.194.67.11:57228 in memory (size: 1971.0 B, free: 912.3 MB)
[INFO][2018-05-30 13:25:31,681][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_1_piece0 on 10.194.67.11:57228 in memory (size: 1973.0 B, free: 912.3 MB)
[INFO][2018-05-30 13:25:31,682][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_2_piece0 on 10.194.67.11:57228 in memory (size: 1973.0 B, free: 912.3 MB)
[INFO][2018-05-30 13:25:32,515][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_0_piece0 on 10.194.67.11:57228 in memory (size: 1972.0 B, free: 912.3 MB)
[INFO][2018-05-30 13:25:33,359][com.seven.spark.streaming.ReceiveKafkaData$]Inserting 37 lines of data to ElasticSearch is success . . .
[INFO][2018-05-30 13:25:33,364][org.apache.spark.executor.Executor]Finished task 0.0 in stage 4.0 (TID 4). 794 bytes result sent to driver
[INFO][2018-05-30 13:25:33,365][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 4.0 (TID 4) in 3128 ms on localhost (executor driver) (1/1)
[INFO][2018-05-30 13:25:33,365][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 4.0, whose tasks have all completed, from pool 
[INFO][2018-05-30 13:25:33,366][org.apache.spark.scheduler.DAGScheduler]ResultStage 4 (foreachPartition at ReceiveKafkaData.scala:76) finished in 3.127 s
[INFO][2018-05-30 13:25:33,366][org.apache.spark.scheduler.DAGScheduler]Job 4 finished: foreachPartition at ReceiveKafkaData.scala:76, took 3.136403 s
[INFO][2018-05-30 13:25:33,367][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527657930000 ms.0 from job set of time 1527657930000 ms
[INFO][2018-05-30 13:25:33,367][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 3.367 s for time 1527657930000 ms (execution: 3.350 s)
[INFO][2018-05-30 13:25:33,368][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 5 from persistence list
[INFO][2018-05-30 13:25:33,381][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 4 from persistence list
[INFO][2018-05-30 13:25:33,381][org.apache.spark.storage.BlockManager]Removing RDD 5
[INFO][2018-05-30 13:25:33,383][org.apache.spark.storage.BlockManager]Removing RDD 4
[INFO][2018-05-30 13:25:33,383][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-30 13:25:33,383][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527657920000 ms
[INFO][2018-05-30 13:25:35,019][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527657935000 ms
[INFO][2018-05-30 13:25:35,020][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527657935000 ms.0 from job set of time 1527657935000 ms
[INFO][2018-05-30 13:25:35,030][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:75
[INFO][2018-05-30 13:25:35,031][org.apache.spark.scheduler.DAGScheduler]Got job 5 (isEmpty at ReceiveKafkaData.scala:75) with 1 output partitions
[INFO][2018-05-30 13:25:35,031][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 5 (isEmpty at ReceiveKafkaData.scala:75)
[INFO][2018-05-30 13:25:35,031][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-30 13:25:35,032][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-30 13:25:35,032][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 5 (MapPartitionsRDD[9] at map at ReceiveKafkaData.scala:74), which has no missing parents
[INFO][2018-05-30 13:25:35,036][org.apache.spark.storage.memory.MemoryStore]Block broadcast_5 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-30 13:25:35,038][org.apache.spark.storage.memory.MemoryStore]Block broadcast_5_piece0 stored as bytes in memory (estimated size 1973.0 B, free 912.3 MB)
[INFO][2018-05-30 13:25:35,038][org.apache.spark.storage.BlockManagerInfo]Added broadcast_5_piece0 in memory on 10.194.67.11:57228 (size: 1973.0 B, free: 912.3 MB)
[INFO][2018-05-30 13:25:35,039][org.apache.spark.SparkContext]Created broadcast 5 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-30 13:25:35,040][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[9] at map at ReceiveKafkaData.scala:74) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-30 13:25:35,040][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 5.0 with 1 tasks
[INFO][2018-05-30 13:25:35,041][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 5.0 (TID 5, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-30 13:25:35,041][org.apache.spark.executor.Executor]Running task 0.0 in stage 5.0 (TID 5)
[INFO][2018-05-30 13:25:35,070][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 189079 -> 189127
[INFO][2018-05-30 13:25:35,071][org.apache.spark.executor.Executor]Finished task 0.0 in stage 5.0 (TID 5). 974 bytes result sent to driver
[INFO][2018-05-30 13:25:35,072][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 5.0 (TID 5) in 32 ms on localhost (executor driver) (1/1)
[INFO][2018-05-30 13:25:35,072][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 5.0, whose tasks have all completed, from pool 
[INFO][2018-05-30 13:25:35,073][org.apache.spark.scheduler.DAGScheduler]ResultStage 5 (isEmpty at ReceiveKafkaData.scala:75) finished in 0.033 s
[INFO][2018-05-30 13:25:35,073][org.apache.spark.scheduler.DAGScheduler]Job 5 finished: isEmpty at ReceiveKafkaData.scala:75, took 0.043188 s
[INFO][2018-05-30 13:25:35,081][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:76
[INFO][2018-05-30 13:25:35,082][org.apache.spark.scheduler.DAGScheduler]Got job 6 (foreachPartition at ReceiveKafkaData.scala:76) with 1 output partitions
[INFO][2018-05-30 13:25:35,082][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 6 (foreachPartition at ReceiveKafkaData.scala:76)
[INFO][2018-05-30 13:25:35,082][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-30 13:25:35,083][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-30 13:25:35,084][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 6 (MapPartitionsRDD[9] at map at ReceiveKafkaData.scala:74), which has no missing parents
[INFO][2018-05-30 13:25:35,086][org.apache.spark.storage.memory.MemoryStore]Block broadcast_6 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-30 13:25:35,088][org.apache.spark.storage.memory.MemoryStore]Block broadcast_6_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.3 MB)
[INFO][2018-05-30 13:25:35,089][org.apache.spark.storage.BlockManagerInfo]Added broadcast_6_piece0 in memory on 10.194.67.11:57228 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-30 13:25:35,090][org.apache.spark.SparkContext]Created broadcast 6 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-30 13:25:35,092][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[9] at map at ReceiveKafkaData.scala:74) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-30 13:25:35,092][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 6.0 with 1 tasks
[INFO][2018-05-30 13:25:35,093][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 6.0 (TID 6, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-30 13:25:35,094][org.apache.spark.executor.Executor]Running task 0.0 in stage 6.0 (TID 6)
[INFO][2018-05-30 13:25:35,098][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 189079 -> 189127
[INFO][2018-05-30 13:25:35,098][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 189079
[INFO][2018-05-30 13:25:35,193][com.seven.spark.streaming.ReceiveKafkaData$]Inserting 48 lines of data to HBase is success . . .
[INFO][2018-05-30 13:25:35,243][com.seven.spark.streaming.ReceiveKafkaData$]Inserting 48 lines of data to ElasticSearch is success . . .
[INFO][2018-05-30 13:25:35,245][org.apache.spark.executor.Executor]Finished task 0.0 in stage 6.0 (TID 6). 708 bytes result sent to driver
[INFO][2018-05-30 13:25:35,246][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 6.0 (TID 6) in 153 ms on localhost (executor driver) (1/1)
[INFO][2018-05-30 13:25:35,247][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 6.0, whose tasks have all completed, from pool 
[INFO][2018-05-30 13:25:35,247][org.apache.spark.scheduler.DAGScheduler]ResultStage 6 (foreachPartition at ReceiveKafkaData.scala:76) finished in 0.154 s
[INFO][2018-05-30 13:25:35,248][org.apache.spark.scheduler.DAGScheduler]Job 6 finished: foreachPartition at ReceiveKafkaData.scala:76, took 0.166153 s
[INFO][2018-05-30 13:25:35,249][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527657935000 ms.0 from job set of time 1527657935000 ms
[INFO][2018-05-30 13:25:35,249][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.248 s for time 1527657935000 ms (execution: 0.228 s)
[INFO][2018-05-30 13:25:35,250][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 7 from persistence list
[INFO][2018-05-30 13:25:35,250][org.apache.spark.storage.BlockManager]Removing RDD 7
[INFO][2018-05-30 13:25:35,250][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 6 from persistence list
[INFO][2018-05-30 13:25:35,252][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-30 13:25:35,252][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527657925000 ms
[INFO][2018-05-30 13:25:35,252][org.apache.spark.storage.BlockManager]Removing RDD 6
[INFO][2018-05-30 13:25:36,577][org.apache.kafka.clients.producer.KafkaProducer]Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
[INFO][2018-05-30 13:25:36,587][org.apache.spark.util.ShutdownHookManager]Shutdown hook called
[INFO][2018-05-30 13:25:36,588][org.apache.spark.util.ShutdownHookManager]Deleting directory /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/spark-abecd9b1-f255-49d8-8fa1-e6fb39aa73cf
[INFO][2018-05-30 13:25:40,017][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527657940000 ms
[INFO][2018-05-30 13:25:40,017][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527657940000 ms.0 from job set of time 1527657940000 ms
[INFO][2018-05-30 13:25:40,024][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:75
[INFO][2018-05-30 13:25:40,024][org.apache.spark.scheduler.DAGScheduler]Got job 7 (isEmpty at ReceiveKafkaData.scala:75) with 1 output partitions
[INFO][2018-05-30 13:25:40,024][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 7 (isEmpty at ReceiveKafkaData.scala:75)
[INFO][2018-05-30 13:25:40,025][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-30 13:25:40,025][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-30 13:25:40,025][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 7 (MapPartitionsRDD[11] at map at ReceiveKafkaData.scala:74), which has no missing parents
[INFO][2018-05-30 13:25:40,030][org.apache.spark.storage.memory.MemoryStore]Block broadcast_7 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-30 13:25:40,031][org.apache.spark.storage.memory.MemoryStore]Block broadcast_7_piece0 stored as bytes in memory (estimated size 1973.0 B, free 912.3 MB)
[INFO][2018-05-30 13:25:40,033][org.apache.spark.storage.BlockManagerInfo]Added broadcast_7_piece0 in memory on 10.194.67.11:57228 (size: 1973.0 B, free: 912.3 MB)
[INFO][2018-05-30 13:25:40,033][org.apache.spark.SparkContext]Created broadcast 7 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-30 13:25:40,034][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[11] at map at ReceiveKafkaData.scala:74) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-30 13:25:40,034][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 7.0 with 1 tasks
[INFO][2018-05-30 13:25:40,035][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 7.0 (TID 7, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-30 13:25:40,036][org.apache.spark.executor.Executor]Running task 0.0 in stage 7.0 (TID 7)
[INFO][2018-05-30 13:25:40,040][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 189127 -> 189142
[INFO][2018-05-30 13:25:40,041][org.apache.spark.executor.Executor]Finished task 0.0 in stage 7.0 (TID 7). 959 bytes result sent to driver
[INFO][2018-05-30 13:25:40,042][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 7.0 (TID 7) in 7 ms on localhost (executor driver) (1/1)
[INFO][2018-05-30 13:25:40,042][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 7.0, whose tasks have all completed, from pool 
[INFO][2018-05-30 13:25:40,043][org.apache.spark.scheduler.DAGScheduler]ResultStage 7 (isEmpty at ReceiveKafkaData.scala:75) finished in 0.008 s
[INFO][2018-05-30 13:25:40,045][org.apache.spark.scheduler.DAGScheduler]Job 7 finished: isEmpty at ReceiveKafkaData.scala:75, took 0.020935 s
[INFO][2018-05-30 13:25:40,055][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:76
[INFO][2018-05-30 13:25:40,055][org.apache.spark.scheduler.DAGScheduler]Got job 8 (foreachPartition at ReceiveKafkaData.scala:76) with 1 output partitions
[INFO][2018-05-30 13:25:40,056][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 8 (foreachPartition at ReceiveKafkaData.scala:76)
[INFO][2018-05-30 13:25:40,056][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-30 13:25:40,056][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-30 13:25:40,056][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 8 (MapPartitionsRDD[11] at map at ReceiveKafkaData.scala:74), which has no missing parents
[INFO][2018-05-30 13:25:40,058][org.apache.spark.storage.memory.MemoryStore]Block broadcast_8 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-30 13:25:40,061][org.apache.spark.storage.memory.MemoryStore]Block broadcast_8_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.3 MB)
[INFO][2018-05-30 13:25:40,062][org.apache.spark.storage.BlockManagerInfo]Added broadcast_8_piece0 in memory on 10.194.67.11:57228 (size: 2.0 KB, free: 912.3 MB)
[INFO][2018-05-30 13:25:40,063][org.apache.spark.SparkContext]Created broadcast 8 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-30 13:25:40,064][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[11] at map at ReceiveKafkaData.scala:74) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-30 13:25:40,064][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 8.0 with 1 tasks
[INFO][2018-05-30 13:25:40,065][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 8.0 (TID 8, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-30 13:25:40,068][org.apache.spark.executor.Executor]Running task 0.0 in stage 8.0 (TID 8)
[INFO][2018-05-30 13:25:40,071][org.apache.spark.streaming.kafka010.KafkaRDD]Computing topic seven, partition 0 offsets 189127 -> 189142
[INFO][2018-05-30 13:25:40,071][org.apache.spark.streaming.kafka010.CachedKafkaConsumer]Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream seven 0 189127
[INFO][2018-05-30 13:25:40,110][com.seven.spark.streaming.ReceiveKafkaData$]Inserting 15 lines of data to HBase is success . . .
[INFO][2018-05-30 13:25:40,137][com.seven.spark.streaming.ReceiveKafkaData$]Inserting 15 lines of data to ElasticSearch is success . . .
[INFO][2018-05-30 13:25:40,138][org.apache.spark.executor.Executor]Finished task 0.0 in stage 8.0 (TID 8). 751 bytes result sent to driver
[INFO][2018-05-30 13:25:40,139][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 8.0 (TID 8) in 75 ms on localhost (executor driver) (1/1)
[INFO][2018-05-30 13:25:40,139][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 8.0, whose tasks have all completed, from pool 
[INFO][2018-05-30 13:25:40,144][org.apache.spark.scheduler.DAGScheduler]ResultStage 8 (foreachPartition at ReceiveKafkaData.scala:76) finished in 0.079 s
[INFO][2018-05-30 13:25:40,144][org.apache.spark.scheduler.DAGScheduler]Job 8 finished: foreachPartition at ReceiveKafkaData.scala:76, took 0.089495 s
[INFO][2018-05-30 13:25:40,145][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527657940000 ms.0 from job set of time 1527657940000 ms
[INFO][2018-05-30 13:25:40,145][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.145 s for time 1527657940000 ms (execution: 0.128 s)
[INFO][2018-05-30 13:25:40,145][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 9 from persistence list
[INFO][2018-05-30 13:25:40,146][org.apache.spark.storage.BlockManager]Removing RDD 9
[INFO][2018-05-30 13:25:40,146][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 8 from persistence list
[INFO][2018-05-30 13:25:40,146][org.apache.spark.storage.BlockManager]Removing RDD 8
[INFO][2018-05-30 13:25:40,146][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-30 13:25:40,146][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527657930000 ms
[INFO][2018-05-30 13:25:45,023][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527657945000 ms
[INFO][2018-05-30 13:25:45,023][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527657945000 ms.0 from job set of time 1527657945000 ms
[INFO][2018-05-30 13:25:45,031][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:75
[INFO][2018-05-30 13:25:45,032][org.apache.spark.scheduler.DAGScheduler]Got job 9 (isEmpty at ReceiveKafkaData.scala:75) with 1 output partitions
[INFO][2018-05-30 13:25:45,032][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 9 (isEmpty at ReceiveKafkaData.scala:75)
[INFO][2018-05-30 13:25:45,032][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-30 13:25:45,032][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-30 13:25:45,032][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 9 (MapPartitionsRDD[13] at map at ReceiveKafkaData.scala:74), which has no missing parents
[INFO][2018-05-30 13:25:45,034][org.apache.spark.storage.memory.MemoryStore]Block broadcast_9 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-30 13:25:45,038][org.apache.spark.storage.memory.MemoryStore]Block broadcast_9_piece0 stored as bytes in memory (estimated size 1973.0 B, free 912.3 MB)
[INFO][2018-05-30 13:25:45,039][org.apache.spark.storage.BlockManagerInfo]Added broadcast_9_piece0 in memory on 10.194.67.11:57228 (size: 1973.0 B, free: 912.3 MB)
[INFO][2018-05-30 13:25:45,039][org.apache.spark.SparkContext]Created broadcast 9 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-30 13:25:45,040][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[13] at map at ReceiveKafkaData.scala:74) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-30 13:25:45,040][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 9.0 with 1 tasks
[INFO][2018-05-30 13:25:45,041][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 9.0 (TID 9, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-30 13:25:45,042][org.apache.spark.executor.Executor]Running task 0.0 in stage 9.0 (TID 9)
[INFO][2018-05-30 13:25:45,044][org.apache.spark.streaming.kafka010.KafkaRDD]Beginning offset 189142 is the same as ending offset skipping seven 0
[INFO][2018-05-30 13:25:45,048][org.apache.spark.executor.Executor]Finished task 0.0 in stage 9.0 (TID 9). 708 bytes result sent to driver
[INFO][2018-05-30 13:25:45,048][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 9.0 (TID 9) in 7 ms on localhost (executor driver) (1/1)
[INFO][2018-05-30 13:25:45,049][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 9.0, whose tasks have all completed, from pool 
[INFO][2018-05-30 13:25:45,049][org.apache.spark.scheduler.DAGScheduler]ResultStage 9 (isEmpty at ReceiveKafkaData.scala:75) finished in 0.008 s
[INFO][2018-05-30 13:25:45,050][org.apache.spark.scheduler.DAGScheduler]Job 9 finished: isEmpty at ReceiveKafkaData.scala:75, took 0.018768 s
[INFO][2018-05-30 13:25:45,050][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527657945000 ms.0 from job set of time 1527657945000 ms
[INFO][2018-05-30 13:25:45,051][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 11 from persistence list
[INFO][2018-05-30 13:25:45,051][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.050 s for time 1527657945000 ms (execution: 0.027 s)
[INFO][2018-05-30 13:25:45,052][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 10 from persistence list
[INFO][2018-05-30 13:25:45,052][org.apache.spark.storage.BlockManager]Removing RDD 11
[INFO][2018-05-30 13:25:45,053][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-30 13:25:45,053][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527657935000 ms
[INFO][2018-05-30 13:25:45,053][org.apache.spark.storage.BlockManager]Removing RDD 10
[INFO][2018-05-30 13:25:50,021][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527657950000 ms
[INFO][2018-05-30 13:25:50,022][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527657950000 ms.0 from job set of time 1527657950000 ms
[INFO][2018-05-30 13:25:50,029][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:75
[INFO][2018-05-30 13:25:50,030][org.apache.spark.scheduler.DAGScheduler]Got job 10 (isEmpty at ReceiveKafkaData.scala:75) with 1 output partitions
[INFO][2018-05-30 13:25:50,030][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 10 (isEmpty at ReceiveKafkaData.scala:75)
[INFO][2018-05-30 13:25:50,030][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-30 13:25:50,030][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-30 13:25:50,031][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 10 (MapPartitionsRDD[15] at map at ReceiveKafkaData.scala:74), which has no missing parents
[INFO][2018-05-30 13:25:50,033][org.apache.spark.storage.memory.MemoryStore]Block broadcast_10 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-30 13:25:50,036][org.apache.spark.storage.memory.MemoryStore]Block broadcast_10_piece0 stored as bytes in memory (estimated size 1973.0 B, free 912.3 MB)
[INFO][2018-05-30 13:25:50,037][org.apache.spark.storage.BlockManagerInfo]Added broadcast_10_piece0 in memory on 10.194.67.11:57228 (size: 1973.0 B, free: 912.3 MB)
[INFO][2018-05-30 13:25:50,038][org.apache.spark.SparkContext]Created broadcast 10 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-30 13:25:50,039][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[15] at map at ReceiveKafkaData.scala:74) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-30 13:25:50,039][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 10.0 with 1 tasks
[INFO][2018-05-30 13:25:50,040][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 10.0 (TID 10, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-30 13:25:50,040][org.apache.spark.executor.Executor]Running task 0.0 in stage 10.0 (TID 10)
[INFO][2018-05-30 13:25:50,042][org.apache.spark.streaming.kafka010.KafkaRDD]Beginning offset 189142 is the same as ending offset skipping seven 0
[INFO][2018-05-30 13:25:50,043][org.apache.spark.executor.Executor]Finished task 0.0 in stage 10.0 (TID 10). 665 bytes result sent to driver
[INFO][2018-05-30 13:25:50,044][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 10.0 (TID 10) in 4 ms on localhost (executor driver) (1/1)
[INFO][2018-05-30 13:25:50,044][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 10.0, whose tasks have all completed, from pool 
[INFO][2018-05-30 13:25:50,045][org.apache.spark.scheduler.DAGScheduler]ResultStage 10 (isEmpty at ReceiveKafkaData.scala:75) finished in 0.005 s
[INFO][2018-05-30 13:25:50,045][org.apache.spark.scheduler.DAGScheduler]Job 10 finished: isEmpty at ReceiveKafkaData.scala:75, took 0.015868 s
[INFO][2018-05-30 13:25:50,046][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527657950000 ms.0 from job set of time 1527657950000 ms
[INFO][2018-05-30 13:25:50,046][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.046 s for time 1527657950000 ms (execution: 0.024 s)
[INFO][2018-05-30 13:25:50,046][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 13 from persistence list
[INFO][2018-05-30 13:25:50,047][org.apache.spark.storage.BlockManager]Removing RDD 13
[INFO][2018-05-30 13:25:50,047][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 12 from persistence list
[INFO][2018-05-30 13:25:50,048][org.apache.spark.storage.BlockManager]Removing RDD 12
[INFO][2018-05-30 13:25:50,048][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-30 13:25:50,048][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527657940000 ms
[INFO][2018-05-30 13:25:55,034][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527657955000 ms
[INFO][2018-05-30 13:25:55,034][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527657955000 ms.0 from job set of time 1527657955000 ms
[INFO][2018-05-30 13:25:55,042][org.apache.spark.SparkContext]Starting job: isEmpty at ReceiveKafkaData.scala:75
[INFO][2018-05-30 13:25:55,043][org.apache.spark.scheduler.DAGScheduler]Got job 11 (isEmpty at ReceiveKafkaData.scala:75) with 1 output partitions
[INFO][2018-05-30 13:25:55,043][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 11 (isEmpty at ReceiveKafkaData.scala:75)
[INFO][2018-05-30 13:25:55,043][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-30 13:25:55,043][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-30 13:25:55,044][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 11 (MapPartitionsRDD[17] at map at ReceiveKafkaData.scala:74), which has no missing parents
[INFO][2018-05-30 13:25:55,046][org.apache.spark.storage.memory.MemoryStore]Block broadcast_11 stored as values in memory (estimated size 3.1 KB, free 912.3 MB)
[INFO][2018-05-30 13:25:55,050][org.apache.spark.storage.memory.MemoryStore]Block broadcast_11_piece0 stored as bytes in memory (estimated size 1970.0 B, free 912.3 MB)
[INFO][2018-05-30 13:25:55,050][org.apache.spark.storage.BlockManagerInfo]Added broadcast_11_piece0 in memory on 10.194.67.11:57228 (size: 1970.0 B, free: 912.3 MB)
[INFO][2018-05-30 13:25:55,050][org.apache.spark.SparkContext]Created broadcast 11 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-30 13:25:55,051][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[17] at map at ReceiveKafkaData.scala:74) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-30 13:25:55,051][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 11.0 with 1 tasks
[INFO][2018-05-30 13:25:55,052][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 11.0 (TID 11, localhost, executor driver, partition 0, PROCESS_LOCAL, 4703 bytes)
[INFO][2018-05-30 13:25:55,052][org.apache.spark.executor.Executor]Running task 0.0 in stage 11.0 (TID 11)
[INFO][2018-05-30 13:25:55,053][org.apache.spark.streaming.kafka010.KafkaRDD]Beginning offset 189142 is the same as ending offset skipping seven 0
[INFO][2018-05-30 13:25:55,054][org.apache.spark.executor.Executor]Finished task 0.0 in stage 11.0 (TID 11). 665 bytes result sent to driver
[INFO][2018-05-30 13:25:55,054][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 11.0 (TID 11) in 3 ms on localhost (executor driver) (1/1)
[INFO][2018-05-30 13:25:55,054][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 11.0, whose tasks have all completed, from pool 
[INFO][2018-05-30 13:25:55,055][org.apache.spark.scheduler.DAGScheduler]ResultStage 11 (isEmpty at ReceiveKafkaData.scala:75) finished in 0.004 s
[INFO][2018-05-30 13:25:55,055][org.apache.spark.scheduler.DAGScheduler]Job 11 finished: isEmpty at ReceiveKafkaData.scala:75, took 0.012429 s
[INFO][2018-05-30 13:25:55,056][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527657955000 ms.0 from job set of time 1527657955000 ms
[INFO][2018-05-30 13:25:55,056][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 15 from persistence list
[INFO][2018-05-30 13:25:55,056][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.055 s for time 1527657955000 ms (execution: 0.021 s)
[INFO][2018-05-30 13:25:55,056][org.apache.spark.storage.BlockManager]Removing RDD 15
[INFO][2018-05-30 13:25:55,056][org.apache.spark.streaming.kafka010.KafkaRDD]Removing RDD 14 from persistence list
[INFO][2018-05-30 13:25:55,057][org.apache.spark.storage.BlockManager]Removing RDD 14
[INFO][2018-05-30 13:25:55,057][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-30 13:25:55,058][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527657945000 ms
[INFO][2018-05-30 13:25:56,357][org.apache.spark.streaming.StreamingContext]Invoking stop(stopGracefully=false) from shutdown hook
[INFO][2018-05-30 13:25:56,359][org.apache.spark.streaming.scheduler.ReceiverTracker]ReceiverTracker stopped
[INFO][2018-05-30 13:25:56,360][org.apache.spark.streaming.scheduler.JobGenerator]Stopping JobGenerator immediately
[INFO][2018-05-30 13:25:56,361][org.apache.spark.streaming.util.RecurringTimer]Stopped timer for JobGenerator after time 1527657955000
[INFO][2018-05-30 13:25:56,371][org.apache.spark.streaming.scheduler.JobGenerator]Stopped JobGenerator
[INFO][2018-05-30 13:25:56,373][org.apache.spark.streaming.scheduler.JobScheduler]Stopped JobScheduler
[INFO][2018-05-30 13:25:56,378][org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@57adfab0{/streaming,null,UNAVAILABLE,@Spark}
[INFO][2018-05-30 13:25:56,379][org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@2a22ad2b{/streaming/batch,null,UNAVAILABLE,@Spark}
[INFO][2018-05-30 13:25:56,380][org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@5dbf5634{/static/streaming,null,UNAVAILABLE,@Spark}
[INFO][2018-05-30 13:25:56,381][org.apache.spark.streaming.StreamingContext]StreamingContext stopped successfully
[INFO][2018-05-30 13:25:56,382][org.apache.spark.SparkContext]Invoking stop() from shutdown hook
[INFO][2018-05-30 13:25:56,391][org.spark_project.jetty.server.AbstractConnector]Stopped Spark@df60c0{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-05-30 13:25:56,393][org.apache.spark.ui.SparkUI]Stopped Spark web UI at http://10.194.67.11:4040
[INFO][2018-05-30 13:25:56,404][org.apache.spark.MapOutputTrackerMasterEndpoint]MapOutputTrackerMasterEndpoint stopped!
[INFO][2018-05-30 13:25:56,418][org.apache.spark.storage.memory.MemoryStore]MemoryStore cleared
[INFO][2018-05-30 13:25:56,418][org.apache.spark.storage.BlockManager]BlockManager stopped
[INFO][2018-05-30 13:25:56,419][org.apache.spark.storage.BlockManagerMaster]BlockManagerMaster stopped
[INFO][2018-05-30 13:25:56,421][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint]OutputCommitCoordinator stopped!
[INFO][2018-05-30 13:25:56,424][org.apache.spark.SparkContext]Successfully stopped SparkContext
[INFO][2018-05-30 13:25:56,425][org.apache.spark.util.ShutdownHookManager]Shutdown hook called
[INFO][2018-05-30 13:25:56,426][org.apache.spark.util.ShutdownHookManager]Deleting directory /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/spark-f58786ee-a328-4869-a2ad-df61ce72bced
[INFO][2018-05-31 14:27:59,116][org.apache.spark.SparkContext]Running Spark version 2.2.0
[INFO][2018-05-31 14:28:00,602][org.apache.spark.SparkContext]Submitted application: SparkSessionTest$
[INFO][2018-05-31 14:28:00,662][org.apache.spark.SecurityManager]Changing view acls to: seven
[INFO][2018-05-31 14:28:00,663][org.apache.spark.SecurityManager]Changing modify acls to: seven
[INFO][2018-05-31 14:28:00,664][org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO][2018-05-31 14:28:00,665][org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO][2018-05-31 14:28:00,666][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(seven); groups with view permissions: Set(); users  with modify permissions: Set(seven); groups with modify permissions: Set()
[INFO][2018-05-31 14:28:01,377][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 56456.
[INFO][2018-05-31 14:28:01,417][org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO][2018-05-31 14:28:01,472][org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO][2018-05-31 14:28:01,478][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2018-05-31 14:28:01,479][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
[INFO][2018-05-31 14:28:01,495][org.apache.spark.storage.DiskBlockManager]Created local directory at /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/blockmgr-2a17b297-9e17-41cb-b9a5-134e6a0ed0a3
[INFO][2018-05-31 14:28:01,577][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 912.3 MB
[INFO][2018-05-31 14:28:01,803][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO][2018-05-31 14:28:02,013][org.spark_project.jetty.util.log]Logging initialized @4413ms
[INFO][2018-05-31 14:28:02,187][org.spark_project.jetty.server.Server]jetty-9.3.z-SNAPSHOT
[INFO][2018-05-31 14:28:02,213][org.spark_project.jetty.server.Server]Started @4615ms
[INFO][2018-05-31 14:28:02,251][org.spark_project.jetty.server.AbstractConnector]Started ServerConnector@6141712d{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-05-31 14:28:02,252][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4040.
[INFO][2018-05-31 14:28:02,259][org.apache.spark.SparkContext]Running Spark version 2.2.0
[INFO][2018-05-31 14:28:02,289][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@8a589a2{/jobs,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:28:02,289][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7bb3a9fe{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:28:02,290][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7f811d00{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:28:02,292][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@f19c9d2{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:28:02,293][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@b91d8c4{/stages,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:28:02,294][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@a77614d{/stages/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:28:02,294][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4a067c25{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:28:02,296][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2baa8d82{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:28:02,297][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@791cbf87{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:28:02,298][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@754777cd{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:28:02,299][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@372ea2bc{/storage,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:28:02,300][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2f08c4b{/storage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:28:02,301][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7de0c6ae{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:28:02,303][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@cdc3aae{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:28:02,303][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5dcbb60{/environment,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:28:02,304][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@21526f6c{/environment/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:28:02,305][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@299266e2{/executors,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:28:02,306][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@66ea1466{/executors/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:28:02,307][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3bffddff{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:28:02,308][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@50687efb{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:28:02,322][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@142eef62{/static,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:28:02,324][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3b9d6699{/,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:28:02,337][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@21694e53{/api,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:28:02,338][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@32057e6{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:28:02,339][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6ea1bcdc{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:28:02,355][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://10.194.32.157:4040
[INFO][2018-05-31 14:28:02,589][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
[INFO][2018-05-31 14:28:02,649][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 56457.
[INFO][2018-05-31 14:28:02,650][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 10.194.32.157:56457
[INFO][2018-05-31 14:28:02,653][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2018-05-31 14:28:02,657][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 10.194.32.157, 56457, None)
[INFO][2018-05-31 14:28:02,671][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 10.194.32.157:56457 with 912.3 MB RAM, BlockManagerId(driver, 10.194.32.157, 56457, None)
[INFO][2018-05-31 14:28:02,678][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 10.194.32.157, 56457, None)
[INFO][2018-05-31 14:28:02,679][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 10.194.32.157, 56457, None)
[INFO][2018-05-31 14:28:03,190][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@238ad8c{/metrics/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:28:03,427][org.apache.spark.sql.internal.SharedState]Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/Users/seven/project/github/dataMining/spark-warehouse').
[INFO][2018-05-31 14:28:03,428][org.apache.spark.sql.internal.SharedState]Warehouse path is 'file:/Users/seven/project/github/dataMining/spark-warehouse'.
[INFO][2018-05-31 14:28:03,438][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@faa3fed{/SQL,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:28:03,439][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@988246e{/SQL/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:28:03,440][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@417ad4f3{/SQL/execution,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:28:03,441][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@58f174d9{/SQL/execution/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:28:03,443][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@e93f3d5{/static/sql,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:28:03,753][org.apache.spark.SparkContext]Submitted application: SparkSessionTest$
[INFO][2018-05-31 14:28:03,807][org.apache.spark.SecurityManager]Changing view acls to: seven
[INFO][2018-05-31 14:28:03,808][org.apache.spark.SecurityManager]Changing modify acls to: seven
[INFO][2018-05-31 14:28:03,809][org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO][2018-05-31 14:28:03,810][org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO][2018-05-31 14:28:03,811][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(seven); groups with view permissions: Set(); users  with modify permissions: Set(seven); groups with modify permissions: Set()
[INFO][2018-05-31 14:28:04,897][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 56458.
[INFO][2018-05-31 14:28:04,944][org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO][2018-05-31 14:28:04,980][org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO][2018-05-31 14:28:04,984][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2018-05-31 14:28:04,986][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
[INFO][2018-05-31 14:28:05,001][org.apache.spark.storage.DiskBlockManager]Created local directory at /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/blockmgr-177eef2c-85c0-484d-b654-938a07569eeb
[INFO][2018-05-31 14:28:05,090][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 912.3 MB
[INFO][2018-05-31 14:28:05,261][org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef]Registered StateStoreCoordinator endpoint
[INFO][2018-05-31 14:28:05,263][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO][2018-05-31 14:28:05,450][org.apache.spark.SparkContext]Invoking stop() from shutdown hook
[INFO][2018-05-31 14:28:05,469][org.spark_project.jetty.server.AbstractConnector]Stopped Spark@6141712d{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-05-31 14:28:05,482][org.apache.spark.ui.SparkUI]Stopped Spark web UI at http://10.194.32.157:4040
[INFO][2018-05-31 14:28:05,501][org.apache.spark.MapOutputTrackerMasterEndpoint]MapOutputTrackerMasterEndpoint stopped!
[INFO][2018-05-31 14:28:05,527][org.apache.spark.storage.memory.MemoryStore]MemoryStore cleared
[INFO][2018-05-31 14:28:05,528][org.apache.spark.storage.BlockManager]BlockManager stopped
[INFO][2018-05-31 14:28:05,540][org.apache.spark.storage.BlockManagerMaster]BlockManagerMaster stopped
[INFO][2018-05-31 14:28:05,541][org.spark_project.jetty.util.log]Logging initialized @5498ms
[INFO][2018-05-31 14:28:05,547][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint]OutputCommitCoordinator stopped!
[INFO][2018-05-31 14:28:05,550][org.apache.spark.SparkContext]Successfully stopped SparkContext
[INFO][2018-05-31 14:28:05,551][org.apache.spark.util.ShutdownHookManager]Shutdown hook called
[INFO][2018-05-31 14:28:05,554][org.apache.spark.util.ShutdownHookManager]Deleting directory /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/spark-4d2e8956-c59a-4267-85b4-f03a75954e76
[INFO][2018-05-31 14:28:05,641][org.spark_project.jetty.server.Server]jetty-9.3.z-SNAPSHOT
[INFO][2018-05-31 14:28:05,662][org.spark_project.jetty.server.Server]Started @5621ms
[INFO][2018-05-31 14:28:05,686][org.spark_project.jetty.server.AbstractConnector]Started ServerConnector@4492eede{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-05-31 14:28:05,688][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4040.
[INFO][2018-05-31 14:28:05,718][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@65f2f9b0{/jobs,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:28:05,719][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@35764bef{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:28:05,719][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5d5160e6{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:28:05,721][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@37af1f93{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:28:05,722][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@408e96d9{/stages,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:28:05,722][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@168cd36b{/stages/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:28:05,723][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3901f6af{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:28:05,725][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@b835727{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:28:05,726][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2c8662ac{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:28:05,726][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3724b43e{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:28:05,727][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@68e7c8c3{/storage,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:28:05,728][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@238bfd6c{/storage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:28:05,728][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@58860997{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:28:05,729][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7487b142{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:28:05,730][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@199bc830{/environment,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:28:05,731][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@27b45ea{/environment/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:28:05,731][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@790a251b{/executors,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:28:05,732][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@150ede8b{/executors/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:28:05,734][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3e15bb06{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:28:05,735][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4e1ce44{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:28:05,746][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7a7cc52c{/static,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:28:05,747][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4fc142ec{/,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:28:05,749][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@29eda4f8{/api,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:28:05,749][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@67a3bd51{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:28:05,753][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@56913163{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:28:05,757][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://10.194.32.157:4040
[INFO][2018-05-31 14:28:05,913][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
[INFO][2018-05-31 14:28:05,953][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 56459.
[INFO][2018-05-31 14:28:05,957][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 10.194.32.157:56459
[INFO][2018-05-31 14:28:05,960][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2018-05-31 14:28:05,963][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 10.194.32.157, 56459, None)
[INFO][2018-05-31 14:28:05,968][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 10.194.32.157:56459 with 912.3 MB RAM, BlockManagerId(driver, 10.194.32.157, 56459, None)
[INFO][2018-05-31 14:28:05,973][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 10.194.32.157, 56459, None)
[INFO][2018-05-31 14:28:05,974][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 10.194.32.157, 56459, None)
[INFO][2018-05-31 14:28:06,204][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@58a2b917{/metrics/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:28:06,279][org.apache.spark.sql.internal.SharedState]Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/Users/seven/project/github/dataMining/spark-warehouse/').
[INFO][2018-05-31 14:28:06,280][org.apache.spark.sql.internal.SharedState]Warehouse path is 'file:/Users/seven/project/github/dataMining/spark-warehouse/'.
[INFO][2018-05-31 14:28:06,289][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3c91530d{/SQL,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:28:06,290][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@732f6050{/SQL/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:28:06,291][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5f96f6a2{/SQL/execution,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:28:06,292][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@290e8cab{/SQL/execution/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:28:06,294][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@33feb805{/static/sql,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:28:08,244][org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef]Registered StateStoreCoordinator endpoint
[INFO][2018-05-31 14:28:49,225][org.apache.spark.SparkContext]Invoking stop() from shutdown hook
[INFO][2018-05-31 14:28:49,247][org.spark_project.jetty.server.AbstractConnector]Stopped Spark@4492eede{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-05-31 14:28:49,255][org.apache.spark.ui.SparkUI]Stopped Spark web UI at http://10.194.32.157:4040
[INFO][2018-05-31 14:28:49,273][org.apache.spark.MapOutputTrackerMasterEndpoint]MapOutputTrackerMasterEndpoint stopped!
[INFO][2018-05-31 14:28:49,297][org.apache.spark.storage.memory.MemoryStore]MemoryStore cleared
[INFO][2018-05-31 14:28:49,298][org.apache.spark.storage.BlockManager]BlockManager stopped
[INFO][2018-05-31 14:28:49,300][org.apache.spark.storage.BlockManagerMaster]BlockManagerMaster stopped
[INFO][2018-05-31 14:28:49,305][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint]OutputCommitCoordinator stopped!
[INFO][2018-05-31 14:28:49,309][org.apache.spark.SparkContext]Successfully stopped SparkContext
[INFO][2018-05-31 14:28:49,310][org.apache.spark.util.ShutdownHookManager]Shutdown hook called
[INFO][2018-05-31 14:28:49,312][org.apache.spark.util.ShutdownHookManager]Deleting directory /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/spark-29f899fa-0944-4d45-b06b-da710ecd2748
[INFO][2018-05-31 14:34:39,872][org.apache.spark.SparkContext]Running Spark version 2.2.0
[INFO][2018-05-31 14:34:40,727][org.apache.spark.SparkContext]Submitted application: SparkSessionTest$
[INFO][2018-05-31 14:34:40,763][org.apache.spark.SecurityManager]Changing view acls to: seven
[INFO][2018-05-31 14:34:40,763][org.apache.spark.SecurityManager]Changing modify acls to: seven
[INFO][2018-05-31 14:34:40,764][org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO][2018-05-31 14:34:40,765][org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO][2018-05-31 14:34:40,766][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(seven); groups with view permissions: Set(); users  with modify permissions: Set(seven); groups with modify permissions: Set()
[INFO][2018-05-31 14:34:41,174][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 56635.
[INFO][2018-05-31 14:34:41,194][org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO][2018-05-31 14:34:41,214][org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO][2018-05-31 14:34:41,217][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2018-05-31 14:34:41,217][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
[INFO][2018-05-31 14:34:41,225][org.apache.spark.storage.DiskBlockManager]Created local directory at /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/blockmgr-9fbe837a-38e2-43ca-8167-2eb9bf5f7e47
[INFO][2018-05-31 14:34:41,273][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 912.3 MB
[INFO][2018-05-31 14:34:41,324][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO][2018-05-31 14:34:41,413][org.spark_project.jetty.util.log]Logging initialized @3032ms
[INFO][2018-05-31 14:34:41,474][org.spark_project.jetty.server.Server]jetty-9.3.z-SNAPSHOT
[INFO][2018-05-31 14:34:41,487][org.spark_project.jetty.server.Server]Started @3107ms
[INFO][2018-05-31 14:34:41,518][org.spark_project.jetty.server.AbstractConnector]Started ServerConnector@42c28305{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-05-31 14:34:41,518][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4040.
[INFO][2018-05-31 14:34:41,546][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@a18649a{/jobs,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:34:41,547][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@772861aa{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:34:41,547][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@19962194{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:34:41,550][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3cbf1ba4{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:34:41,551][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7e2c64{/stages,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:34:41,552][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@383864d5{/stages/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:34:41,553][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4cb40e3b{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:34:41,555][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7efe7b87{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:34:41,556][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5a2bd7c8{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:34:41,557][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7187bac9{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:34:41,557][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6f139fc9{/storage,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:34:41,558][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@19382338{/storage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:34:41,559][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@15dc339f{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:34:41,560][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@34acbc60{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:34:41,560][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@36061cf3{/environment,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:34:41,561][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@20134094{/environment/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:34:41,562][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@706fe5c6{/executors,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:34:41,562][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@56380231{/executors/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:34:41,563][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6b3f6585{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:34:41,564][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3e104d4b{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:34:41,574][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@45e1aa48{/static,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:34:41,575][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@260a3a5e{/,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:34:41,577][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3c0bbc9f{/api,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:34:41,578][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@407873d3{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:34:41,579][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5412bfea{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:34:41,582][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://10.194.32.157:4040
[INFO][2018-05-31 14:34:41,698][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
[INFO][2018-05-31 14:34:41,719][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 56636.
[INFO][2018-05-31 14:34:41,719][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 10.194.32.157:56636
[INFO][2018-05-31 14:34:41,721][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2018-05-31 14:34:41,722][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 10.194.32.157, 56636, None)
[INFO][2018-05-31 14:34:41,727][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 10.194.32.157:56636 with 912.3 MB RAM, BlockManagerId(driver, 10.194.32.157, 56636, None)
[INFO][2018-05-31 14:34:41,731][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 10.194.32.157, 56636, None)
[INFO][2018-05-31 14:34:41,732][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 10.194.32.157, 56636, None)
[INFO][2018-05-31 14:34:41,961][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@770beef5{/metrics/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:34:42,059][org.apache.spark.sql.internal.SharedState]Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/Users/seven/project/github/dataMining/spark-warehouse/').
[INFO][2018-05-31 14:34:42,060][org.apache.spark.sql.internal.SharedState]Warehouse path is 'file:/Users/seven/project/github/dataMining/spark-warehouse/'.
[INFO][2018-05-31 14:34:42,068][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3ddeaa5f{/SQL,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:34:42,069][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7f608e21{/SQL/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:34:42,070][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@66236a0a{/SQL/execution,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:34:42,070][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@77c10a5f{/SQL/execution/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:34:42,072][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@c386958{/static/sql,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:34:43,006][org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef]Registered StateStoreCoordinator endpoint
[INFO][2018-05-31 14:34:53,549][org.apache.spark.SparkContext]Invoking stop() from shutdown hook
[INFO][2018-05-31 14:34:53,567][org.spark_project.jetty.server.AbstractConnector]Stopped Spark@42c28305{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-05-31 14:34:53,573][org.apache.spark.ui.SparkUI]Stopped Spark web UI at http://10.194.32.157:4040
[INFO][2018-05-31 14:34:53,585][org.apache.spark.MapOutputTrackerMasterEndpoint]MapOutputTrackerMasterEndpoint stopped!
[INFO][2018-05-31 14:34:53,598][org.apache.spark.storage.memory.MemoryStore]MemoryStore cleared
[INFO][2018-05-31 14:34:53,599][org.apache.spark.storage.BlockManager]BlockManager stopped
[INFO][2018-05-31 14:34:53,604][org.apache.spark.storage.BlockManagerMaster]BlockManagerMaster stopped
[INFO][2018-05-31 14:34:53,610][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint]OutputCommitCoordinator stopped!
[INFO][2018-05-31 14:34:53,615][org.apache.spark.SparkContext]Successfully stopped SparkContext
[INFO][2018-05-31 14:34:53,615][org.apache.spark.util.ShutdownHookManager]Shutdown hook called
[INFO][2018-05-31 14:34:53,616][org.apache.spark.util.ShutdownHookManager]Deleting directory /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/spark-70be5778-4b19-4698-be39-4241d710c459
[INFO][2018-05-31 14:40:12,570][org.apache.spark.SparkContext]Running Spark version 2.2.0
[INFO][2018-05-31 14:40:13,620][org.apache.spark.SparkContext]Submitted application: SparkSessionTest$
[INFO][2018-05-31 14:40:13,643][org.apache.spark.SecurityManager]Changing view acls to: seven
[INFO][2018-05-31 14:40:13,644][org.apache.spark.SecurityManager]Changing modify acls to: seven
[INFO][2018-05-31 14:40:13,644][org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO][2018-05-31 14:40:13,645][org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO][2018-05-31 14:40:13,645][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(seven); groups with view permissions: Set(); users  with modify permissions: Set(seven); groups with modify permissions: Set()
[INFO][2018-05-31 14:40:13,919][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 57052.
[INFO][2018-05-31 14:40:13,941][org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO][2018-05-31 14:40:13,972][org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO][2018-05-31 14:40:13,977][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2018-05-31 14:40:13,978][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
[INFO][2018-05-31 14:40:13,986][org.apache.spark.storage.DiskBlockManager]Created local directory at /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/blockmgr-49c00868-3ab7-4b3a-9765-60685d7d42c5
[INFO][2018-05-31 14:40:14,003][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 912.3 MB
[INFO][2018-05-31 14:40:14,075][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO][2018-05-31 14:40:14,167][org.spark_project.jetty.util.log]Logging initialized @2722ms
[INFO][2018-05-31 14:40:14,230][org.spark_project.jetty.server.Server]jetty-9.3.z-SNAPSHOT
[INFO][2018-05-31 14:40:14,241][org.spark_project.jetty.server.Server]Started @2798ms
[INFO][2018-05-31 14:40:14,258][org.spark_project.jetty.server.AbstractConnector]Started ServerConnector@60d8c0dc{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-05-31 14:40:14,259][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4040.
[INFO][2018-05-31 14:40:14,278][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2b62442c{/jobs,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:40:14,279][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@674c583e{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:40:14,279][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3f23a3a0{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:40:14,280][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@439a8f59{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:40:14,281][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@31024624{/stages,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:40:14,281][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@32cb636e{/stages/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:40:14,281][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@40dd3977{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:40:14,282][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4b6579e8{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:40:14,283][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6c6357f9{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:40:14,283][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3954d008{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:40:14,284][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@593e824f{/storage,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:40:14,284][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6d8792db{/storage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:40:14,285][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@493dfb8e{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:40:14,286][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@ce5a68e{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:40:14,286][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2f162cc0{/environment,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:40:14,287][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7c041b41{/environment/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:40:14,287][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@61078690{/executors,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:40:14,288][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@403132fc{/executors/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:40:14,289][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2cab9998{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:40:14,289][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@669513d8{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:40:14,300][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4a8a60bc{/static,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:40:14,302][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@37ebc9d8{/,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:40:14,304][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2416a51{/api,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:40:14,305][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@77307458{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:40:14,306][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@290b1b2e{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:40:14,308][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://10.194.32.157:4040
[INFO][2018-05-31 14:40:14,434][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
[INFO][2018-05-31 14:40:14,454][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 57053.
[INFO][2018-05-31 14:40:14,455][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 10.194.32.157:57053
[INFO][2018-05-31 14:40:14,457][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2018-05-31 14:40:14,458][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 10.194.32.157, 57053, None)
[INFO][2018-05-31 14:40:14,461][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 10.194.32.157:57053 with 912.3 MB RAM, BlockManagerId(driver, 10.194.32.157, 57053, None)
[INFO][2018-05-31 14:40:14,464][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 10.194.32.157, 57053, None)
[INFO][2018-05-31 14:40:14,465][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 10.194.32.157, 57053, None)
[INFO][2018-05-31 14:40:14,750][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@437ebf59{/metrics/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:40:14,858][org.apache.spark.sql.internal.SharedState]Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/Users/seven/project/github/dataMining/spark-warehouse/').
[INFO][2018-05-31 14:40:14,859][org.apache.spark.sql.internal.SharedState]Warehouse path is 'file:/Users/seven/project/github/dataMining/spark-warehouse/'.
[INFO][2018-05-31 14:40:14,868][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3cc20577{/SQL,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:40:14,869][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@775594f2{/SQL/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:40:14,869][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@48b0e701{/SQL/execution,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:40:14,870][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@547c04c4{/SQL/execution/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:40:14,871][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4bf324f9{/static/sql,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:40:15,650][org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef]Registered StateStoreCoordinator endpoint
[INFO][2018-05-31 14:40:16,186][org.apache.spark.SparkContext]Invoking stop() from shutdown hook
[INFO][2018-05-31 14:40:16,197][org.spark_project.jetty.server.AbstractConnector]Stopped Spark@60d8c0dc{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-05-31 14:40:16,201][org.apache.spark.ui.SparkUI]Stopped Spark web UI at http://10.194.32.157:4040
[INFO][2018-05-31 14:40:16,208][org.apache.spark.MapOutputTrackerMasterEndpoint]MapOutputTrackerMasterEndpoint stopped!
[INFO][2018-05-31 14:40:16,222][org.apache.spark.storage.memory.MemoryStore]MemoryStore cleared
[INFO][2018-05-31 14:40:16,223][org.apache.spark.storage.BlockManager]BlockManager stopped
[INFO][2018-05-31 14:40:16,229][org.apache.spark.storage.BlockManagerMaster]BlockManagerMaster stopped
[INFO][2018-05-31 14:40:16,234][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint]OutputCommitCoordinator stopped!
[INFO][2018-05-31 14:40:16,235][org.apache.spark.SparkContext]Successfully stopped SparkContext
[INFO][2018-05-31 14:40:16,236][org.apache.spark.util.ShutdownHookManager]Shutdown hook called
[INFO][2018-05-31 14:40:16,237][org.apache.spark.util.ShutdownHookManager]Deleting directory /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/spark-80f66746-9141-41a1-bf19-f993ac9766c9
[INFO][2018-05-31 14:40:28,988][org.apache.spark.SparkContext]Running Spark version 2.2.0
[INFO][2018-05-31 14:40:29,886][org.apache.spark.SparkContext]Submitted application: SparkSessionTest$
[INFO][2018-05-31 14:40:29,920][org.apache.spark.SecurityManager]Changing view acls to: seven
[INFO][2018-05-31 14:40:29,921][org.apache.spark.SecurityManager]Changing modify acls to: seven
[INFO][2018-05-31 14:40:29,922][org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO][2018-05-31 14:40:29,923][org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO][2018-05-31 14:40:29,924][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(seven); groups with view permissions: Set(); users  with modify permissions: Set(seven); groups with modify permissions: Set()
[INFO][2018-05-31 14:40:30,347][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 57060.
[INFO][2018-05-31 14:40:30,381][org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO][2018-05-31 14:40:30,404][org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO][2018-05-31 14:40:30,407][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2018-05-31 14:40:30,408][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
[INFO][2018-05-31 14:40:30,418][org.apache.spark.storage.DiskBlockManager]Created local directory at /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/blockmgr-39aca500-31ce-4f63-9227-ea0cc8d4b26b
[INFO][2018-05-31 14:40:30,478][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 912.3 MB
[INFO][2018-05-31 14:40:30,534][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO][2018-05-31 14:40:30,632][org.spark_project.jetty.util.log]Logging initialized @3136ms
[INFO][2018-05-31 14:40:30,696][org.spark_project.jetty.server.Server]jetty-9.3.z-SNAPSHOT
[INFO][2018-05-31 14:40:30,720][org.spark_project.jetty.server.Server]Started @3226ms
[INFO][2018-05-31 14:40:30,754][org.spark_project.jetty.server.AbstractConnector]Started ServerConnector@2314ddae{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-05-31 14:40:30,754][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4040.
[INFO][2018-05-31 14:40:30,790][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@396639b{/jobs,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:40:30,791][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@19962194{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:40:30,791][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7c8f9c2e{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:40:30,794][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7e2c64{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:40:30,794][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@383864d5{/stages,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:40:30,798][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4cb40e3b{/stages/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:40:30,799][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3a543f31{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:40:30,801][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5a2bd7c8{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:40:30,802][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7187bac9{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:40:30,802][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6f139fc9{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:40:30,803][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@19382338{/storage,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:40:30,804][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@15dc339f{/storage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:40:30,806][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@34acbc60{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:40:30,807][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@36061cf3{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:40:30,808][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@20134094{/environment,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:40:30,808][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@706fe5c6{/environment/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:40:30,810][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@56380231{/executors,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:40:30,814][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6b3f6585{/executors/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:40:30,815][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3e104d4b{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:40:30,816][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@45e1aa48{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:40:30,836][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2e807c54{/static,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:40:30,837][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3c0bbc9f{/,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:40:30,839][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6438a7fe{/api,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:40:30,840][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5412bfea{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:40:30,841][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4743a322{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:40:30,843][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://10.194.32.157:4040
[INFO][2018-05-31 14:40:30,973][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
[INFO][2018-05-31 14:40:31,004][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 57061.
[INFO][2018-05-31 14:40:31,005][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 10.194.32.157:57061
[INFO][2018-05-31 14:40:31,007][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2018-05-31 14:40:31,009][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 10.194.32.157, 57061, None)
[INFO][2018-05-31 14:40:31,013][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 10.194.32.157:57061 with 912.3 MB RAM, BlockManagerId(driver, 10.194.32.157, 57061, None)
[INFO][2018-05-31 14:40:31,023][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 10.194.32.157, 57061, None)
[INFO][2018-05-31 14:40:31,024][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 10.194.32.157, 57061, None)
[INFO][2018-05-31 14:40:31,319][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@37637a24{/metrics/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:40:31,411][org.apache.spark.sql.internal.SharedState]Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/Users/seven/project/github/dataMining/spark-warehouse/').
[INFO][2018-05-31 14:40:31,412][org.apache.spark.sql.internal.SharedState]Warehouse path is 'file:/Users/seven/project/github/dataMining/spark-warehouse/'.
[INFO][2018-05-31 14:40:31,419][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3e7b65d7{/SQL,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:40:31,419][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3ddeaa5f{/SQL/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:40:31,420][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3bec5821{/SQL/execution,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:40:31,420][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@66236a0a{/SQL/execution/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:40:31,422][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@355c94be{/static/sql,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:40:32,322][org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef]Registered StateStoreCoordinator endpoint
[INFO][2018-05-31 14:40:43,577][org.apache.spark.SparkContext]Invoking stop() from shutdown hook
[INFO][2018-05-31 14:40:43,595][org.spark_project.jetty.server.AbstractConnector]Stopped Spark@2314ddae{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-05-31 14:40:43,598][org.apache.spark.ui.SparkUI]Stopped Spark web UI at http://10.194.32.157:4040
[INFO][2018-05-31 14:40:43,610][org.apache.spark.MapOutputTrackerMasterEndpoint]MapOutputTrackerMasterEndpoint stopped!
[INFO][2018-05-31 14:40:43,630][org.apache.spark.storage.memory.MemoryStore]MemoryStore cleared
[INFO][2018-05-31 14:40:43,630][org.apache.spark.storage.BlockManager]BlockManager stopped
[INFO][2018-05-31 14:40:43,631][org.apache.spark.storage.BlockManagerMaster]BlockManagerMaster stopped
[INFO][2018-05-31 14:40:43,637][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint]OutputCommitCoordinator stopped!
[INFO][2018-05-31 14:40:43,643][org.apache.spark.SparkContext]Successfully stopped SparkContext
[INFO][2018-05-31 14:40:43,644][org.apache.spark.util.ShutdownHookManager]Shutdown hook called
[INFO][2018-05-31 14:40:43,646][org.apache.spark.util.ShutdownHookManager]Deleting directory /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/spark-9c7e269d-7a2d-475a-8172-004d4309ab13
[INFO][2018-05-31 14:43:53,159][org.apache.spark.SparkContext]Running Spark version 2.2.0
[INFO][2018-05-31 14:43:54,032][org.apache.spark.SparkContext]Submitted application: SparkSessionTest$
[INFO][2018-05-31 14:43:54,065][org.apache.spark.SecurityManager]Changing view acls to: seven
[INFO][2018-05-31 14:43:54,065][org.apache.spark.SecurityManager]Changing modify acls to: seven
[INFO][2018-05-31 14:43:54,066][org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO][2018-05-31 14:43:54,067][org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO][2018-05-31 14:43:54,068][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(seven); groups with view permissions: Set(); users  with modify permissions: Set(seven); groups with modify permissions: Set()
[INFO][2018-05-31 14:43:54,417][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 57366.
[INFO][2018-05-31 14:43:54,437][org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO][2018-05-31 14:43:54,463][org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO][2018-05-31 14:43:54,466][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2018-05-31 14:43:54,467][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
[INFO][2018-05-31 14:43:54,478][org.apache.spark.storage.DiskBlockManager]Created local directory at /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/blockmgr-63b4cc04-03f2-48f8-8033-7e378f94f905
[INFO][2018-05-31 14:43:54,529][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 912.3 MB
[INFO][2018-05-31 14:43:54,579][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO][2018-05-31 14:43:54,669][org.spark_project.jetty.util.log]Logging initialized @2886ms
[INFO][2018-05-31 14:43:54,727][org.spark_project.jetty.server.Server]jetty-9.3.z-SNAPSHOT
[INFO][2018-05-31 14:43:54,742][org.spark_project.jetty.server.Server]Started @2960ms
[INFO][2018-05-31 14:43:54,775][org.spark_project.jetty.server.AbstractConnector]Started ServerConnector@2314ddae{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-05-31 14:43:54,775][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4040.
[INFO][2018-05-31 14:43:54,803][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@702b06fb{/jobs,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:43:54,804][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@8a2a6a{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:43:54,804][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6631cb64{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:43:54,806][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@140d1230{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:43:54,807][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@10bea4{/stages,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:43:54,808][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7c1e32c9{/stages/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:43:54,808][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3dd818e8{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:43:54,810][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@65c86db8{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:43:54,811][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@ac20bb4{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:43:54,813][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7ca8d498{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:43:54,815][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6b8280e6{/storage,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:43:54,816][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@55259aa7{/storage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:43:54,817][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@66420549{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:43:54,817][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6cd56321{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:43:54,818][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@42b28ff1{/environment,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:43:54,819][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@718dbd79{/environment/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:43:54,820][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@76889e60{/executors,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:43:54,821][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6c15e8c7{/executors/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:43:54,826][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@64f1fd08{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:43:54,827][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@282ffbf5{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:43:54,840][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@55e2fe3c{/static,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:43:54,840][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5c8e67b9{/,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:43:54,842][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@49206065{/api,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:43:54,843][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@615e3f51{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:43:54,843][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@608bc8f8{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:43:54,847][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://10.194.32.157:4040
[INFO][2018-05-31 14:43:54,972][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
[INFO][2018-05-31 14:43:55,004][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 57367.
[INFO][2018-05-31 14:43:55,005][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 10.194.32.157:57367
[INFO][2018-05-31 14:43:55,007][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2018-05-31 14:43:55,009][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 10.194.32.157, 57367, None)
[INFO][2018-05-31 14:43:55,013][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 10.194.32.157:57367 with 912.3 MB RAM, BlockManagerId(driver, 10.194.32.157, 57367, None)
[INFO][2018-05-31 14:43:55,018][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 10.194.32.157, 57367, None)
[INFO][2018-05-31 14:43:55,019][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 10.194.32.157, 57367, None)
[INFO][2018-05-31 14:43:55,285][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@770beef5{/metrics/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:43:55,412][org.apache.spark.sql.internal.SharedState]Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/Users/seven/project/github/dataMining/spark-warehouse/').
[INFO][2018-05-31 14:43:55,413][org.apache.spark.sql.internal.SharedState]Warehouse path is 'file:/Users/seven/project/github/dataMining/spark-warehouse/'.
[INFO][2018-05-31 14:43:55,425][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3ddeaa5f{/SQL,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:43:55,426][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7f608e21{/SQL/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:43:55,427][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@66236a0a{/SQL/execution,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:43:55,427][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@77c10a5f{/SQL/execution/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:43:55,429][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@c386958{/static/sql,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:43:56,284][org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef]Registered StateStoreCoordinator endpoint
[INFO][2018-05-31 14:44:04,540][org.apache.spark.SparkContext]Invoking stop() from shutdown hook
[INFO][2018-05-31 14:44:04,563][org.spark_project.jetty.server.AbstractConnector]Stopped Spark@2314ddae{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-05-31 14:44:04,566][org.apache.spark.ui.SparkUI]Stopped Spark web UI at http://10.194.32.157:4040
[INFO][2018-05-31 14:44:04,578][org.apache.spark.MapOutputTrackerMasterEndpoint]MapOutputTrackerMasterEndpoint stopped!
[INFO][2018-05-31 14:44:04,599][org.apache.spark.storage.memory.MemoryStore]MemoryStore cleared
[INFO][2018-05-31 14:44:04,600][org.apache.spark.storage.BlockManager]BlockManager stopped
[INFO][2018-05-31 14:44:04,605][org.apache.spark.storage.BlockManagerMaster]BlockManagerMaster stopped
[INFO][2018-05-31 14:44:04,610][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint]OutputCommitCoordinator stopped!
[INFO][2018-05-31 14:44:04,612][org.apache.spark.SparkContext]Successfully stopped SparkContext
[INFO][2018-05-31 14:44:04,613][org.apache.spark.util.ShutdownHookManager]Shutdown hook called
[INFO][2018-05-31 14:44:04,614][org.apache.spark.util.ShutdownHookManager]Deleting directory /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/spark-4d48c4fe-c397-4c88-96dd-228c6c504979
[INFO][2018-05-31 14:46:10,826][org.apache.spark.SparkContext]Running Spark version 2.2.0
[INFO][2018-05-31 14:46:11,538][org.apache.spark.SparkContext]Submitted application: SparkSessionTest$
[INFO][2018-05-31 14:46:11,573][org.apache.spark.SecurityManager]Changing view acls to: seven
[INFO][2018-05-31 14:46:11,574][org.apache.spark.SecurityManager]Changing modify acls to: seven
[INFO][2018-05-31 14:46:11,575][org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO][2018-05-31 14:46:11,576][org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO][2018-05-31 14:46:11,577][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(seven); groups with view permissions: Set(); users  with modify permissions: Set(seven); groups with modify permissions: Set()
[INFO][2018-05-31 14:46:11,984][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 57404.
[INFO][2018-05-31 14:46:12,015][org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO][2018-05-31 14:46:12,035][org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO][2018-05-31 14:46:12,038][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2018-05-31 14:46:12,039][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
[INFO][2018-05-31 14:46:12,047][org.apache.spark.storage.DiskBlockManager]Created local directory at /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/blockmgr-f2e02516-d6b9-4c44-bffa-4f998e2111d6
[INFO][2018-05-31 14:46:12,102][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 912.3 MB
[INFO][2018-05-31 14:46:12,162][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO][2018-05-31 14:46:12,265][org.spark_project.jetty.util.log]Logging initialized @2905ms
[INFO][2018-05-31 14:46:12,320][org.spark_project.jetty.server.Server]jetty-9.3.z-SNAPSHOT
[INFO][2018-05-31 14:46:12,333][org.spark_project.jetty.server.Server]Started @2974ms
[INFO][2018-05-31 14:46:12,360][org.spark_project.jetty.server.AbstractConnector]Started ServerConnector@42c28305{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-05-31 14:46:12,360][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4040.
[INFO][2018-05-31 14:46:12,387][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@a18649a{/jobs,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:46:12,388][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@772861aa{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:46:12,389][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@19962194{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:46:12,390][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3cbf1ba4{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:46:12,391][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7e2c64{/stages,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:46:12,391][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@383864d5{/stages/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:46:12,392][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4cb40e3b{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:46:12,393][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7efe7b87{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:46:12,395][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5a2bd7c8{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:46:12,396][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7187bac9{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:46:12,396][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6f139fc9{/storage,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:46:12,397][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@19382338{/storage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:46:12,398][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@15dc339f{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:46:12,399][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@34acbc60{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:46:12,400][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@36061cf3{/environment,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:46:12,400][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@20134094{/environment/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:46:12,401][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@706fe5c6{/executors,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:46:12,403][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@56380231{/executors/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:46:12,404][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6b3f6585{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:46:12,404][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3e104d4b{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:46:12,413][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@45e1aa48{/static,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:46:12,414][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@260a3a5e{/,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:46:12,415][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3c0bbc9f{/api,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:46:12,418][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@407873d3{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:46:12,419][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5412bfea{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:46:12,422][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://10.194.32.157:4040
[INFO][2018-05-31 14:46:12,540][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
[INFO][2018-05-31 14:46:12,569][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 57405.
[INFO][2018-05-31 14:46:12,570][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 10.194.32.157:57405
[INFO][2018-05-31 14:46:12,572][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2018-05-31 14:46:12,573][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 10.194.32.157, 57405, None)
[INFO][2018-05-31 14:46:12,577][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 10.194.32.157:57405 with 912.3 MB RAM, BlockManagerId(driver, 10.194.32.157, 57405, None)
[INFO][2018-05-31 14:46:12,584][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 10.194.32.157, 57405, None)
[INFO][2018-05-31 14:46:12,584][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 10.194.32.157, 57405, None)
[INFO][2018-05-31 14:46:12,860][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2e09c51{/metrics/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:46:12,954][org.apache.spark.sql.internal.SharedState]Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/Users/seven/project/github/dataMining/spark-warehouse/').
[INFO][2018-05-31 14:46:12,954][org.apache.spark.sql.internal.SharedState]Warehouse path is 'file:/Users/seven/project/github/dataMining/spark-warehouse/'.
[INFO][2018-05-31 14:46:12,978][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@470d183{/SQL,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:46:12,979][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@ea52184{/SQL/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:46:12,982][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7f608e21{/SQL/execution,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:46:12,985][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@210d2a6c{/SQL/execution/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:46:12,990][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7c781c42{/static/sql,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:46:14,180][org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef]Registered StateStoreCoordinator endpoint
[INFO][2018-05-31 14:46:21,119][org.apache.spark.SparkContext]Invoking stop() from shutdown hook
[INFO][2018-05-31 14:46:21,132][org.spark_project.jetty.server.AbstractConnector]Stopped Spark@42c28305{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-05-31 14:46:21,136][org.apache.spark.ui.SparkUI]Stopped Spark web UI at http://10.194.32.157:4040
[INFO][2018-05-31 14:46:21,148][org.apache.spark.MapOutputTrackerMasterEndpoint]MapOutputTrackerMasterEndpoint stopped!
[INFO][2018-05-31 14:46:21,169][org.apache.spark.storage.memory.MemoryStore]MemoryStore cleared
[INFO][2018-05-31 14:46:21,170][org.apache.spark.storage.BlockManager]BlockManager stopped
[INFO][2018-05-31 14:46:21,176][org.apache.spark.storage.BlockManagerMaster]BlockManagerMaster stopped
[INFO][2018-05-31 14:46:21,181][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint]OutputCommitCoordinator stopped!
[INFO][2018-05-31 14:46:21,184][org.apache.spark.SparkContext]Successfully stopped SparkContext
[INFO][2018-05-31 14:46:21,185][org.apache.spark.util.ShutdownHookManager]Shutdown hook called
[INFO][2018-05-31 14:46:21,186][org.apache.spark.util.ShutdownHookManager]Deleting directory /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/spark-57c457eb-bc96-48b6-88b9-41d58c0898bc
[INFO][2018-05-31 14:49:13,082][org.apache.spark.SparkContext]Running Spark version 2.2.0
[INFO][2018-05-31 14:49:13,929][org.apache.spark.SparkContext]Submitted application: SparkSessionTest$
[INFO][2018-05-31 14:49:13,952][org.apache.spark.SecurityManager]Changing view acls to: seven
[INFO][2018-05-31 14:49:13,953][org.apache.spark.SecurityManager]Changing modify acls to: seven
[INFO][2018-05-31 14:49:13,953][org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO][2018-05-31 14:49:13,954][org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO][2018-05-31 14:49:13,955][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(seven); groups with view permissions: Set(); users  with modify permissions: Set(seven); groups with modify permissions: Set()
[INFO][2018-05-31 14:49:14,338][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 57444.
[INFO][2018-05-31 14:49:14,359][org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO][2018-05-31 14:49:14,378][org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO][2018-05-31 14:49:14,382][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2018-05-31 14:49:14,382][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
[INFO][2018-05-31 14:49:14,390][org.apache.spark.storage.DiskBlockManager]Created local directory at /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/blockmgr-e0133ef5-2b29-49fd-8281-79852b209b2e
[INFO][2018-05-31 14:49:14,440][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 912.3 MB
[INFO][2018-05-31 14:49:14,490][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO][2018-05-31 14:49:14,577][org.spark_project.jetty.util.log]Logging initialized @3216ms
[INFO][2018-05-31 14:49:14,639][org.spark_project.jetty.server.Server]jetty-9.3.z-SNAPSHOT
[INFO][2018-05-31 14:49:14,653][org.spark_project.jetty.server.Server]Started @3293ms
[INFO][2018-05-31 14:49:14,673][org.spark_project.jetty.server.AbstractConnector]Started ServerConnector@8d85c3e{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-05-31 14:49:14,673][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4040.
[INFO][2018-05-31 14:49:14,700][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5ff2b8ca{/jobs,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:49:14,701][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7d199c68{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:49:14,702][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@35cd68d4{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:49:14,703][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@35764bef{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:49:14,704][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5d5160e6{/stages,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:49:14,704][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2903c6ff{/stages/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:49:14,705][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@37af1f93{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:49:14,707][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3901f6af{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:49:14,708][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@10cd6753{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:49:14,709][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@47af099e{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:49:14,710][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@b835727{/storage,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:49:14,710][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2c8662ac{/storage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:49:14,711][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3724b43e{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:49:14,712][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@68e7c8c3{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:49:14,713][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@238bfd6c{/environment,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:49:14,714][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@58860997{/environment/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:49:14,715][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7487b142{/executors,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:49:14,716][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@199bc830{/executors/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:49:14,717][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@27b45ea{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:49:14,718][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@790a251b{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:49:14,727][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@150ede8b{/static,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:49:14,728][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@75361cf6{/,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:49:14,729][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6ba7383d{/api,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:49:14,730][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@29eda4f8{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:49:14,731][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5e048149{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:49:14,733][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://10.194.32.157:4040
[INFO][2018-05-31 14:49:14,878][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
[INFO][2018-05-31 14:49:14,907][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 57445.
[INFO][2018-05-31 14:49:14,908][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 10.194.32.157:57445
[INFO][2018-05-31 14:49:14,910][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2018-05-31 14:49:14,912][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 10.194.32.157, 57445, None)
[INFO][2018-05-31 14:49:14,921][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 10.194.32.157:57445 with 912.3 MB RAM, BlockManagerId(driver, 10.194.32.157, 57445, None)
[INFO][2018-05-31 14:49:14,927][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 10.194.32.157, 57445, None)
[INFO][2018-05-31 14:49:14,927][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 10.194.32.157, 57445, None)
[INFO][2018-05-31 14:49:15,285][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6a6f6c7e{/metrics/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:49:15,380][org.apache.spark.sql.internal.SharedState]Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/Users/seven/project/github/dataMining/spark-warehouse/').
[INFO][2018-05-31 14:49:15,381][org.apache.spark.sql.internal.SharedState]Warehouse path is 'file:/Users/seven/project/github/dataMining/spark-warehouse/'.
[INFO][2018-05-31 14:49:15,390][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1a0c5e9{/SQL,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:49:15,394][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1d247525{/SQL/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:49:15,395][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3289079a{/SQL/execution,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:49:15,395][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@788ba63e{/SQL/execution/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:49:15,397][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@11eed657{/static/sql,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:49:16,235][org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef]Registered StateStoreCoordinator endpoint
[INFO][2018-05-31 14:49:29,218][org.apache.spark.SparkContext]Invoking stop() from shutdown hook
[INFO][2018-05-31 14:49:29,235][org.spark_project.jetty.server.AbstractConnector]Stopped Spark@8d85c3e{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-05-31 14:49:29,237][org.apache.spark.ui.SparkUI]Stopped Spark web UI at http://10.194.32.157:4040
[INFO][2018-05-31 14:49:29,249][org.apache.spark.MapOutputTrackerMasterEndpoint]MapOutputTrackerMasterEndpoint stopped!
[INFO][2018-05-31 14:49:29,265][org.apache.spark.storage.memory.MemoryStore]MemoryStore cleared
[INFO][2018-05-31 14:49:29,266][org.apache.spark.storage.BlockManager]BlockManager stopped
[INFO][2018-05-31 14:49:29,266][org.apache.spark.storage.BlockManagerMaster]BlockManagerMaster stopped
[INFO][2018-05-31 14:49:29,271][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint]OutputCommitCoordinator stopped!
[INFO][2018-05-31 14:49:29,274][org.apache.spark.SparkContext]Successfully stopped SparkContext
[INFO][2018-05-31 14:49:29,275][org.apache.spark.util.ShutdownHookManager]Shutdown hook called
[INFO][2018-05-31 14:49:29,276][org.apache.spark.util.ShutdownHookManager]Deleting directory /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/spark-e4dc7053-be26-4c90-8579-835bea1d14d3
[INFO][2018-05-31 14:50:35,830][org.apache.spark.SparkContext]Running Spark version 2.2.0
[INFO][2018-05-31 14:50:36,534][org.apache.spark.SparkContext]Submitted application: SparkSessionTest$
[INFO][2018-05-31 14:50:36,570][org.apache.spark.SecurityManager]Changing view acls to: seven
[INFO][2018-05-31 14:50:36,571][org.apache.spark.SecurityManager]Changing modify acls to: seven
[INFO][2018-05-31 14:50:36,572][org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO][2018-05-31 14:50:36,573][org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO][2018-05-31 14:50:36,574][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(seven); groups with view permissions: Set(); users  with modify permissions: Set(seven); groups with modify permissions: Set()
[INFO][2018-05-31 14:50:36,925][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 57476.
[INFO][2018-05-31 14:50:36,950][org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO][2018-05-31 14:50:36,970][org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO][2018-05-31 14:50:36,973][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2018-05-31 14:50:36,973][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
[INFO][2018-05-31 14:50:36,981][org.apache.spark.storage.DiskBlockManager]Created local directory at /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/blockmgr-cd58f943-0254-4fc5-93c2-4c952e327fcc
[INFO][2018-05-31 14:50:37,031][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 912.3 MB
[INFO][2018-05-31 14:50:37,084][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO][2018-05-31 14:50:37,170][org.spark_project.jetty.util.log]Logging initialized @2894ms
[INFO][2018-05-31 14:50:37,225][org.spark_project.jetty.server.Server]jetty-9.3.z-SNAPSHOT
[INFO][2018-05-31 14:50:37,244][org.spark_project.jetty.server.Server]Started @2969ms
[INFO][2018-05-31 14:50:37,264][org.spark_project.jetty.server.AbstractConnector]Started ServerConnector@8d85c3e{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-05-31 14:50:37,264][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4040.
[INFO][2018-05-31 14:50:37,288][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1aa6e3c0{/jobs,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:50:37,289][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@35cd68d4{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:50:37,290][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@216914{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:50:37,291][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5d5160e6{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:50:37,292][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2903c6ff{/stages,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:50:37,293][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@37af1f93{/stages/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:50:37,293][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@408e96d9{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:50:37,296][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@10cd6753{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:50:37,297][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@47af099e{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:50:37,298][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@b835727{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:50:37,299][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2c8662ac{/storage,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:50:37,299][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3724b43e{/storage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:50:37,300][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@68e7c8c3{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:50:37,301][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@238bfd6c{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:50:37,302][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@58860997{/environment,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:50:37,303][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7487b142{/environment/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:50:37,304][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@199bc830{/executors,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:50:37,304][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@27b45ea{/executors/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:50:37,305][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@790a251b{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:50:37,306][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@150ede8b{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:50:37,314][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3e15bb06{/static,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:50:37,315][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6ba7383d{/,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:50:37,316][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@710d89e2{/api,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:50:37,317][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5e048149{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:50:37,318][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3d5790ea{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:50:37,320][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://10.194.32.157:4040
[INFO][2018-05-31 14:50:37,444][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
[INFO][2018-05-31 14:50:37,483][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 57477.
[INFO][2018-05-31 14:50:37,484][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 10.194.32.157:57477
[INFO][2018-05-31 14:50:37,487][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2018-05-31 14:50:37,489][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 10.194.32.157, 57477, None)
[INFO][2018-05-31 14:50:37,493][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 10.194.32.157:57477 with 912.3 MB RAM, BlockManagerId(driver, 10.194.32.157, 57477, None)
[INFO][2018-05-31 14:50:37,497][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 10.194.32.157, 57477, None)
[INFO][2018-05-31 14:50:37,498][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 10.194.32.157, 57477, None)
[INFO][2018-05-31 14:50:37,711][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1abebef3{/metrics/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:50:37,831][org.apache.spark.sql.internal.SharedState]Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/Users/seven/project/github/dataMining/spark-warehouse/').
[INFO][2018-05-31 14:50:37,831][org.apache.spark.sql.internal.SharedState]Warehouse path is 'file:/Users/seven/project/github/dataMining/spark-warehouse/'.
[INFO][2018-05-31 14:50:37,838][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@68a78f3c{/SQL,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:50:37,839][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3481ff98{/SQL/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:50:37,840][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@732f6050{/SQL/execution,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:50:37,841][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@30c4e352{/SQL/execution/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:50:37,843][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@67507df{/static/sql,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:50:38,640][org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef]Registered StateStoreCoordinator endpoint
[INFO][2018-05-31 14:50:45,486][org.apache.spark.SparkContext]Invoking stop() from shutdown hook
[INFO][2018-05-31 14:50:45,501][org.spark_project.jetty.server.AbstractConnector]Stopped Spark@8d85c3e{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-05-31 14:50:45,505][org.apache.spark.ui.SparkUI]Stopped Spark web UI at http://10.194.32.157:4040
[INFO][2018-05-31 14:50:45,525][org.apache.spark.MapOutputTrackerMasterEndpoint]MapOutputTrackerMasterEndpoint stopped!
[INFO][2018-05-31 14:50:45,545][org.apache.spark.storage.memory.MemoryStore]MemoryStore cleared
[INFO][2018-05-31 14:50:45,545][org.apache.spark.storage.BlockManager]BlockManager stopped
[INFO][2018-05-31 14:50:45,550][org.apache.spark.storage.BlockManagerMaster]BlockManagerMaster stopped
[INFO][2018-05-31 14:50:45,555][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint]OutputCommitCoordinator stopped!
[INFO][2018-05-31 14:50:45,557][org.apache.spark.SparkContext]Successfully stopped SparkContext
[INFO][2018-05-31 14:50:45,557][org.apache.spark.util.ShutdownHookManager]Shutdown hook called
[INFO][2018-05-31 14:50:45,558][org.apache.spark.util.ShutdownHookManager]Deleting directory /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/spark-3ae8abc4-a71b-4cbb-b9f9-e8cab8798034
[INFO][2018-05-31 14:53:09,606][org.apache.spark.SparkContext]Running Spark version 2.2.0
[INFO][2018-05-31 14:53:10,418][org.apache.spark.SparkContext]Submitted application: SparkSessionTest$
[INFO][2018-05-31 14:53:10,458][org.apache.spark.SecurityManager]Changing view acls to: seven
[INFO][2018-05-31 14:53:10,459][org.apache.spark.SecurityManager]Changing modify acls to: seven
[INFO][2018-05-31 14:53:10,462][org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO][2018-05-31 14:53:10,462][org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO][2018-05-31 14:53:10,464][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(seven); groups with view permissions: Set(); users  with modify permissions: Set(seven); groups with modify permissions: Set()
[INFO][2018-05-31 14:53:10,876][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 57582.
[INFO][2018-05-31 14:53:10,899][org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO][2018-05-31 14:53:10,920][org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO][2018-05-31 14:53:10,923][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2018-05-31 14:53:10,923][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
[INFO][2018-05-31 14:53:10,933][org.apache.spark.storage.DiskBlockManager]Created local directory at /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/blockmgr-dcd3c9d6-b08f-420c-b27b-35599673156f
[INFO][2018-05-31 14:53:10,984][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 912.3 MB
[INFO][2018-05-31 14:53:11,038][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO][2018-05-31 14:53:11,140][org.spark_project.jetty.util.log]Logging initialized @3218ms
[INFO][2018-05-31 14:53:11,204][org.spark_project.jetty.server.Server]jetty-9.3.z-SNAPSHOT
[INFO][2018-05-31 14:53:11,219][org.spark_project.jetty.server.Server]Started @3298ms
[INFO][2018-05-31 14:53:11,249][org.spark_project.jetty.server.AbstractConnector]Started ServerConnector@2b9e241b{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-05-31 14:53:11,250][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4040.
[INFO][2018-05-31 14:53:11,277][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@702b06fb{/jobs,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:53:11,278][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@8a2a6a{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:53:11,279][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6631cb64{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:53:11,280][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@140d1230{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:53:11,281][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@10bea4{/stages,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:53:11,282][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7c1e32c9{/stages/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:53:11,283][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3dd818e8{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:53:11,285][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@65c86db8{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:53:11,286][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@ac20bb4{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:53:11,287][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7ca8d498{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:53:11,288][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6b8280e6{/storage,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:53:11,289][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@55259aa7{/storage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:53:11,290][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@66420549{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:53:11,291][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6cd56321{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:53:11,292][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@42b28ff1{/environment,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:53:11,293][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@718dbd79{/environment/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:53:11,294][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@76889e60{/executors,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:53:11,295][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6c15e8c7{/executors/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:53:11,296][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@64f1fd08{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:53:11,297][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@282ffbf5{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:53:11,307][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@55e2fe3c{/static,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:53:11,307][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5c8e67b9{/,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:53:11,309][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@49206065{/api,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:53:11,310][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@615e3f51{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:53:11,311][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@608bc8f8{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:53:11,314][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://10.194.32.157:4040
[INFO][2018-05-31 14:53:11,473][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
[INFO][2018-05-31 14:53:11,527][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 57584.
[INFO][2018-05-31 14:53:11,528][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 10.194.32.157:57584
[INFO][2018-05-31 14:53:11,531][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2018-05-31 14:53:11,533][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 10.194.32.157, 57584, None)
[INFO][2018-05-31 14:53:11,540][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 10.194.32.157:57584 with 912.3 MB RAM, BlockManagerId(driver, 10.194.32.157, 57584, None)
[INFO][2018-05-31 14:53:11,547][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 10.194.32.157, 57584, None)
[INFO][2018-05-31 14:53:11,548][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 10.194.32.157, 57584, None)
[INFO][2018-05-31 14:53:11,754][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2e09c51{/metrics/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:53:11,842][org.apache.spark.sql.internal.SharedState]Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/Users/seven/project/github/dataMining/spark-warehouse/').
[INFO][2018-05-31 14:53:11,843][org.apache.spark.sql.internal.SharedState]Warehouse path is 'file:/Users/seven/project/github/dataMining/spark-warehouse/'.
[INFO][2018-05-31 14:53:11,853][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@470d183{/SQL,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:53:11,853][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@ea52184{/SQL/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:53:11,854][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7f608e21{/SQL/execution,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:53:11,855][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@210d2a6c{/SQL/execution/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:53:11,856][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7c781c42{/static/sql,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:53:12,801][org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef]Registered StateStoreCoordinator endpoint
[INFO][2018-05-31 14:53:19,308][org.apache.spark.SparkContext]Invoking stop() from shutdown hook
[INFO][2018-05-31 14:53:19,321][org.spark_project.jetty.server.AbstractConnector]Stopped Spark@2b9e241b{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-05-31 14:53:19,323][org.apache.spark.ui.SparkUI]Stopped Spark web UI at http://10.194.32.157:4040
[INFO][2018-05-31 14:53:19,335][org.apache.spark.MapOutputTrackerMasterEndpoint]MapOutputTrackerMasterEndpoint stopped!
[INFO][2018-05-31 14:53:19,353][org.apache.spark.storage.memory.MemoryStore]MemoryStore cleared
[INFO][2018-05-31 14:53:19,354][org.apache.spark.storage.BlockManager]BlockManager stopped
[INFO][2018-05-31 14:53:19,359][org.apache.spark.storage.BlockManagerMaster]BlockManagerMaster stopped
[INFO][2018-05-31 14:53:19,365][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint]OutputCommitCoordinator stopped!
[INFO][2018-05-31 14:53:19,367][org.apache.spark.SparkContext]Successfully stopped SparkContext
[INFO][2018-05-31 14:53:19,368][org.apache.spark.util.ShutdownHookManager]Shutdown hook called
[INFO][2018-05-31 14:53:19,369][org.apache.spark.util.ShutdownHookManager]Deleting directory /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/spark-0b75d7f3-2dab-4cc6-a58f-ce9a25023240
[INFO][2018-05-31 14:54:14,334][org.apache.spark.SparkContext]Running Spark version 2.2.0
[INFO][2018-05-31 14:54:15,145][org.apache.spark.SparkContext]Submitted application: SparkSessionTest$
[INFO][2018-05-31 14:54:15,167][org.apache.spark.SecurityManager]Changing view acls to: seven
[INFO][2018-05-31 14:54:15,167][org.apache.spark.SecurityManager]Changing modify acls to: seven
[INFO][2018-05-31 14:54:15,168][org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO][2018-05-31 14:54:15,168][org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO][2018-05-31 14:54:15,169][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(seven); groups with view permissions: Set(); users  with modify permissions: Set(seven); groups with modify permissions: Set()
[INFO][2018-05-31 14:54:15,444][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 57608.
[INFO][2018-05-31 14:54:15,470][org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO][2018-05-31 14:54:15,493][org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO][2018-05-31 14:54:15,496][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2018-05-31 14:54:15,496][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
[INFO][2018-05-31 14:54:15,503][org.apache.spark.storage.DiskBlockManager]Created local directory at /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/blockmgr-1b7a0033-ae91-45ba-9798-a6edb4832d99
[INFO][2018-05-31 14:54:15,519][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 912.3 MB
[INFO][2018-05-31 14:54:15,586][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO][2018-05-31 14:54:15,679][org.spark_project.jetty.util.log]Logging initialized @2299ms
[INFO][2018-05-31 14:54:15,731][org.spark_project.jetty.server.Server]jetty-9.3.z-SNAPSHOT
[INFO][2018-05-31 14:54:15,743][org.spark_project.jetty.server.Server]Started @2364ms
[INFO][2018-05-31 14:54:15,760][org.spark_project.jetty.server.AbstractConnector]Started ServerConnector@1275ad70{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-05-31 14:54:15,760][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4040.
[INFO][2018-05-31 14:54:15,781][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@66629f63{/jobs,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:54:15,782][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@25f7391e{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:54:15,782][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5ab14cb9{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:54:15,783][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@61861a29{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:54:15,783][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@25bcd0c7{/stages,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:54:15,784][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@63cd604c{/stages/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:54:15,784][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3a4e343{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:54:15,786][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6fff253c{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:54:15,786][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@591e58fa{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:54:15,787][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2f94c4db{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:54:15,788][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@72ccd81a{/storage,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:54:15,788][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@64bc21ac{/storage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:54:15,789][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5d25e6bb{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:54:15,789][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@9d157ff{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:54:15,790][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5df417a7{/environment,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:54:15,791][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7f69d591{/environment/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:54:15,792][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1cb3ec38{/executors,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:54:15,793][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@71c5b236{/executors/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:54:15,793][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2f7a7219{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:54:15,793][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3a1d593e{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:54:15,807][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@361c294e{/static,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:54:15,808][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@293bb8a5{/,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:54:15,810][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6fa590ba{/api,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:54:15,810][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1fc0053e{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:54:15,811][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@47874b25{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:54:15,814][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://10.194.32.157:4040
[INFO][2018-05-31 14:54:15,915][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
[INFO][2018-05-31 14:54:15,955][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 57609.
[INFO][2018-05-31 14:54:15,956][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 10.194.32.157:57609
[INFO][2018-05-31 14:54:15,958][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2018-05-31 14:54:15,960][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 10.194.32.157, 57609, None)
[INFO][2018-05-31 14:54:15,963][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 10.194.32.157:57609 with 912.3 MB RAM, BlockManagerId(driver, 10.194.32.157, 57609, None)
[INFO][2018-05-31 14:54:15,973][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 10.194.32.157, 57609, None)
[INFO][2018-05-31 14:54:15,975][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 10.194.32.157, 57609, None)
[INFO][2018-05-31 14:54:16,239][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@41c89d2f{/metrics/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:54:16,307][org.apache.spark.sql.internal.SharedState]Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/Users/seven/project/github/dataMining/spark-warehouse/').
[INFO][2018-05-31 14:54:16,308][org.apache.spark.sql.internal.SharedState]Warehouse path is 'file:/Users/seven/project/github/dataMining/spark-warehouse/'.
[INFO][2018-05-31 14:54:16,314][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@33a630fa{/SQL,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:54:16,315][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5767b2af{/SQL/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:54:16,316][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@241a0c3a{/SQL/execution,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:54:16,316][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@30e92cb9{/SQL/execution/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:54:16,319][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4f7c0be3{/static/sql,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:54:16,943][org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef]Registered StateStoreCoordinator endpoint
[INFO][2018-05-31 14:54:17,351][org.apache.spark.SparkContext]Invoking stop() from shutdown hook
[INFO][2018-05-31 14:54:17,362][org.spark_project.jetty.server.AbstractConnector]Stopped Spark@1275ad70{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-05-31 14:54:17,364][org.apache.spark.ui.SparkUI]Stopped Spark web UI at http://10.194.32.157:4040
[INFO][2018-05-31 14:54:17,371][org.apache.spark.MapOutputTrackerMasterEndpoint]MapOutputTrackerMasterEndpoint stopped!
[INFO][2018-05-31 14:54:17,381][org.apache.spark.storage.memory.MemoryStore]MemoryStore cleared
[INFO][2018-05-31 14:54:17,382][org.apache.spark.storage.BlockManager]BlockManager stopped
[INFO][2018-05-31 14:54:17,386][org.apache.spark.storage.BlockManagerMaster]BlockManagerMaster stopped
[INFO][2018-05-31 14:54:17,389][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint]OutputCommitCoordinator stopped!
[INFO][2018-05-31 14:54:17,390][org.apache.spark.SparkContext]Successfully stopped SparkContext
[INFO][2018-05-31 14:54:17,390][org.apache.spark.util.ShutdownHookManager]Shutdown hook called
[INFO][2018-05-31 14:54:17,391][org.apache.spark.util.ShutdownHookManager]Deleting directory /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/spark-95883200-a7a1-4287-9998-0b3cf4b67887
[INFO][2018-05-31 14:54:28,373][org.apache.spark.SparkContext]Running Spark version 2.2.0
[INFO][2018-05-31 14:54:29,088][org.apache.spark.SparkContext]Submitted application: SparkSessionTest$
[INFO][2018-05-31 14:54:29,115][org.apache.spark.SecurityManager]Changing view acls to: seven
[INFO][2018-05-31 14:54:29,116][org.apache.spark.SecurityManager]Changing modify acls to: seven
[INFO][2018-05-31 14:54:29,116][org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO][2018-05-31 14:54:29,117][org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO][2018-05-31 14:54:29,118][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(seven); groups with view permissions: Set(); users  with modify permissions: Set(seven); groups with modify permissions: Set()
[INFO][2018-05-31 14:54:29,436][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 57614.
[INFO][2018-05-31 14:54:29,458][org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO][2018-05-31 14:54:29,484][org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO][2018-05-31 14:54:29,487][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2018-05-31 14:54:29,488][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
[INFO][2018-05-31 14:54:29,496][org.apache.spark.storage.DiskBlockManager]Created local directory at /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/blockmgr-46c2c2d4-1861-4491-96f9-63c236b80f4f
[INFO][2018-05-31 14:54:29,556][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 912.3 MB
[INFO][2018-05-31 14:54:29,624][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO][2018-05-31 14:54:29,711][org.spark_project.jetty.util.log]Logging initialized @2829ms
[INFO][2018-05-31 14:54:29,772][org.spark_project.jetty.server.Server]jetty-9.3.z-SNAPSHOT
[INFO][2018-05-31 14:54:29,786][org.spark_project.jetty.server.Server]Started @2905ms
[INFO][2018-05-31 14:54:29,816][org.spark_project.jetty.server.AbstractConnector]Started ServerConnector@2314ddae{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-05-31 14:54:29,856][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4040.
[INFO][2018-05-31 14:54:29,891][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@a18649a{/jobs,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:54:29,892][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@772861aa{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:54:29,892][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@19962194{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:54:29,896][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3cbf1ba4{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:54:29,897][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7e2c64{/stages,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:54:29,897][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@383864d5{/stages/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:54:29,898][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4cb40e3b{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:54:29,900][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7efe7b87{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:54:29,902][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5a2bd7c8{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:54:29,903][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7187bac9{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:54:29,904][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6f139fc9{/storage,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:54:29,905][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@19382338{/storage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:54:29,905][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@15dc339f{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:54:29,907][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@34acbc60{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:54:29,910][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@36061cf3{/environment,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:54:29,911][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@20134094{/environment/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:54:29,911][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@706fe5c6{/executors,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:54:29,912][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@56380231{/executors/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:54:29,913][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6b3f6585{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:54:29,916][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3e104d4b{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:54:29,928][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@45e1aa48{/static,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:54:29,929][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@260a3a5e{/,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:54:29,930][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3c0bbc9f{/api,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:54:29,931][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@407873d3{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:54:29,932][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5412bfea{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:54:29,935][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://10.194.32.157:4040
[INFO][2018-05-31 14:54:30,056][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
[INFO][2018-05-31 14:54:30,080][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 57615.
[INFO][2018-05-31 14:54:30,081][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 10.194.32.157:57615
[INFO][2018-05-31 14:54:30,083][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2018-05-31 14:54:30,084][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 10.194.32.157, 57615, None)
[INFO][2018-05-31 14:54:30,088][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 10.194.32.157:57615 with 912.3 MB RAM, BlockManagerId(driver, 10.194.32.157, 57615, None)
[INFO][2018-05-31 14:54:30,092][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 10.194.32.157, 57615, None)
[INFO][2018-05-31 14:54:30,093][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 10.194.32.157, 57615, None)
[INFO][2018-05-31 14:54:30,310][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@770beef5{/metrics/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:54:30,391][org.apache.spark.sql.internal.SharedState]Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/Users/seven/project/github/dataMining/spark-warehouse/').
[INFO][2018-05-31 14:54:30,394][org.apache.spark.sql.internal.SharedState]Warehouse path is 'file:/Users/seven/project/github/dataMining/spark-warehouse/'.
[INFO][2018-05-31 14:54:30,406][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3ddeaa5f{/SQL,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:54:30,412][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7f608e21{/SQL/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:54:30,413][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@66236a0a{/SQL/execution,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:54:30,415][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@77c10a5f{/SQL/execution/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:54:30,419][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@c386958{/static/sql,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:54:31,428][org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef]Registered StateStoreCoordinator endpoint
[INFO][2018-05-31 14:54:40,554][org.apache.spark.SparkContext]Invoking stop() from shutdown hook
[INFO][2018-05-31 14:54:40,568][org.spark_project.jetty.server.AbstractConnector]Stopped Spark@2314ddae{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-05-31 14:54:40,570][org.apache.spark.ui.SparkUI]Stopped Spark web UI at http://10.194.32.157:4040
[INFO][2018-05-31 14:54:40,580][org.apache.spark.MapOutputTrackerMasterEndpoint]MapOutputTrackerMasterEndpoint stopped!
[INFO][2018-05-31 14:54:40,619][org.apache.spark.storage.memory.MemoryStore]MemoryStore cleared
[INFO][2018-05-31 14:54:40,620][org.apache.spark.storage.BlockManager]BlockManager stopped
[INFO][2018-05-31 14:54:40,625][org.apache.spark.storage.BlockManagerMaster]BlockManagerMaster stopped
[INFO][2018-05-31 14:54:40,629][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint]OutputCommitCoordinator stopped!
[INFO][2018-05-31 14:54:40,631][org.apache.spark.SparkContext]Successfully stopped SparkContext
[INFO][2018-05-31 14:54:40,632][org.apache.spark.util.ShutdownHookManager]Shutdown hook called
[INFO][2018-05-31 14:54:40,633][org.apache.spark.util.ShutdownHookManager]Deleting directory /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/spark-2a6aeb3f-1f21-41b4-8807-6728844f8f54
[INFO][2018-05-31 14:57:26,941][org.apache.spark.SparkContext]Running Spark version 2.2.0
[INFO][2018-05-31 14:57:27,662][org.apache.spark.SparkContext]Submitted application: SparkSessionTest$
[INFO][2018-05-31 14:57:27,697][org.apache.spark.SecurityManager]Changing view acls to: seven
[INFO][2018-05-31 14:57:27,698][org.apache.spark.SecurityManager]Changing modify acls to: seven
[INFO][2018-05-31 14:57:27,700][org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO][2018-05-31 14:57:27,702][org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO][2018-05-31 14:57:27,702][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(seven); groups with view permissions: Set(); users  with modify permissions: Set(seven); groups with modify permissions: Set()
[INFO][2018-05-31 14:57:28,100][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 57659.
[INFO][2018-05-31 14:57:28,121][org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO][2018-05-31 14:57:28,146][org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO][2018-05-31 14:57:28,149][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2018-05-31 14:57:28,149][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
[INFO][2018-05-31 14:57:28,158][org.apache.spark.storage.DiskBlockManager]Created local directory at /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/blockmgr-ea3ab52b-9455-4b7e-a5f5-a9fb58f9072b
[INFO][2018-05-31 14:57:28,205][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 912.3 MB
[INFO][2018-05-31 14:57:28,261][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO][2018-05-31 14:57:28,346][org.spark_project.jetty.util.log]Logging initialized @2977ms
[INFO][2018-05-31 14:57:28,407][org.spark_project.jetty.server.Server]jetty-9.3.z-SNAPSHOT
[INFO][2018-05-31 14:57:28,419][org.spark_project.jetty.server.Server]Started @3051ms
[INFO][2018-05-31 14:57:28,445][org.spark_project.jetty.server.AbstractConnector]Started ServerConnector@42c28305{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-05-31 14:57:28,446][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4040.
[INFO][2018-05-31 14:57:28,469][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@a18649a{/jobs,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:57:28,470][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@772861aa{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:57:28,470][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@19962194{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:57:28,471][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3cbf1ba4{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:57:28,472][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7e2c64{/stages,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:57:28,473][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@383864d5{/stages/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:57:28,474][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4cb40e3b{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:57:28,475][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7efe7b87{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:57:28,476][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5a2bd7c8{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:57:28,477][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7187bac9{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:57:28,478][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6f139fc9{/storage,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:57:28,478][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@19382338{/storage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:57:28,479][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@15dc339f{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:57:28,480][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@34acbc60{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:57:28,481][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@36061cf3{/environment,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:57:28,481][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@20134094{/environment/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:57:28,482][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@706fe5c6{/executors,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:57:28,483][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@56380231{/executors/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:57:28,484][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6b3f6585{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:57:28,485][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3e104d4b{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:57:28,494][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@45e1aa48{/static,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:57:28,495][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@260a3a5e{/,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:57:28,496][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3c0bbc9f{/api,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:57:28,497][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@407873d3{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:57:28,498][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5412bfea{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:57:28,501][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://10.194.32.157:4040
[INFO][2018-05-31 14:57:28,595][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
[INFO][2018-05-31 14:57:28,622][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 57660.
[INFO][2018-05-31 14:57:28,623][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 10.194.32.157:57660
[INFO][2018-05-31 14:57:28,629][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2018-05-31 14:57:28,633][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 10.194.32.157, 57660, None)
[INFO][2018-05-31 14:57:28,636][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 10.194.32.157:57660 with 912.3 MB RAM, BlockManagerId(driver, 10.194.32.157, 57660, None)
[INFO][2018-05-31 14:57:28,639][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 10.194.32.157, 57660, None)
[INFO][2018-05-31 14:57:28,640][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 10.194.32.157, 57660, None)
[INFO][2018-05-31 14:57:29,033][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@770beef5{/metrics/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:57:29,102][org.apache.spark.sql.internal.SharedState]Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/Users/seven/project/github/dataMining/spark-warehouse/').
[INFO][2018-05-31 14:57:29,104][org.apache.spark.sql.internal.SharedState]Warehouse path is 'file:/Users/seven/project/github/dataMining/spark-warehouse/'.
[INFO][2018-05-31 14:57:29,110][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3ddeaa5f{/SQL,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:57:29,111][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7f608e21{/SQL/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:57:29,112][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@66236a0a{/SQL/execution,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:57:29,112][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@77c10a5f{/SQL/execution/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:57:29,114][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@c386958{/static/sql,null,AVAILABLE,@Spark}
[INFO][2018-05-31 14:57:30,005][org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef]Registered StateStoreCoordinator endpoint
[INFO][2018-05-31 14:57:35,196][org.apache.spark.SparkContext]Invoking stop() from shutdown hook
[INFO][2018-05-31 14:57:35,210][org.spark_project.jetty.server.AbstractConnector]Stopped Spark@42c28305{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-05-31 14:57:35,211][org.apache.spark.ui.SparkUI]Stopped Spark web UI at http://10.194.32.157:4040
[INFO][2018-05-31 14:57:35,222][org.apache.spark.MapOutputTrackerMasterEndpoint]MapOutputTrackerMasterEndpoint stopped!
[INFO][2018-05-31 14:57:35,238][org.apache.spark.storage.memory.MemoryStore]MemoryStore cleared
[INFO][2018-05-31 14:57:35,238][org.apache.spark.storage.BlockManager]BlockManager stopped
[INFO][2018-05-31 14:57:35,243][org.apache.spark.storage.BlockManagerMaster]BlockManagerMaster stopped
[INFO][2018-05-31 14:57:35,247][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint]OutputCommitCoordinator stopped!
[INFO][2018-05-31 14:57:35,250][org.apache.spark.SparkContext]Successfully stopped SparkContext
[INFO][2018-05-31 14:57:35,251][org.apache.spark.util.ShutdownHookManager]Shutdown hook called
[INFO][2018-05-31 14:57:35,252][org.apache.spark.util.ShutdownHookManager]Deleting directory /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/spark-4105b292-20cc-4be8-90d0-d011a13d3ed6
[INFO][2018-05-31 15:01:03,055][org.apache.spark.SparkContext]Running Spark version 2.2.0
[INFO][2018-05-31 15:01:03,960][org.apache.spark.SparkContext]Submitted application: SparkSessionTest$
[INFO][2018-05-31 15:01:03,996][org.apache.spark.SecurityManager]Changing view acls to: seven
[INFO][2018-05-31 15:01:03,997][org.apache.spark.SecurityManager]Changing modify acls to: seven
[INFO][2018-05-31 15:01:03,998][org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO][2018-05-31 15:01:03,999][org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO][2018-05-31 15:01:04,001][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(seven); groups with view permissions: Set(); users  with modify permissions: Set(seven); groups with modify permissions: Set()
[INFO][2018-05-31 15:01:04,391][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 57710.
[INFO][2018-05-31 15:01:04,440][org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO][2018-05-31 15:01:04,474][org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO][2018-05-31 15:01:04,477][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2018-05-31 15:01:04,478][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
[INFO][2018-05-31 15:01:04,487][org.apache.spark.storage.DiskBlockManager]Created local directory at /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/blockmgr-6fd6e862-48ea-47fc-b9f0-b98ad7b10202
[INFO][2018-05-31 15:01:04,538][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 912.3 MB
[INFO][2018-05-31 15:01:04,595][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO][2018-05-31 15:01:04,704][org.spark_project.jetty.util.log]Logging initialized @3131ms
[INFO][2018-05-31 15:01:04,771][org.spark_project.jetty.server.Server]jetty-9.3.z-SNAPSHOT
[INFO][2018-05-31 15:01:04,785][org.spark_project.jetty.server.Server]Started @3213ms
[INFO][2018-05-31 15:01:04,817][org.spark_project.jetty.server.AbstractConnector]Started ServerConnector@2314ddae{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-05-31 15:01:04,817][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4040.
[INFO][2018-05-31 15:01:04,850][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@702b06fb{/jobs,null,AVAILABLE,@Spark}
[INFO][2018-05-31 15:01:04,851][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@8a2a6a{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 15:01:04,851][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6631cb64{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2018-05-31 15:01:04,852][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@140d1230{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 15:01:04,853][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@10bea4{/stages,null,AVAILABLE,@Spark}
[INFO][2018-05-31 15:01:04,853][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7c1e32c9{/stages/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 15:01:04,854][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3dd818e8{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2018-05-31 15:01:04,857][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@65c86db8{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 15:01:04,858][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@ac20bb4{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2018-05-31 15:01:04,859][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7ca8d498{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 15:01:04,859][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6b8280e6{/storage,null,AVAILABLE,@Spark}
[INFO][2018-05-31 15:01:04,860][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@55259aa7{/storage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 15:01:04,861][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@66420549{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2018-05-31 15:01:04,862][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6cd56321{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 15:01:04,862][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@42b28ff1{/environment,null,AVAILABLE,@Spark}
[INFO][2018-05-31 15:01:04,864][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@718dbd79{/environment/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 15:01:04,865][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@76889e60{/executors,null,AVAILABLE,@Spark}
[INFO][2018-05-31 15:01:04,866][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6c15e8c7{/executors/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 15:01:04,868][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@64f1fd08{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2018-05-31 15:01:04,869][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@282ffbf5{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 15:01:04,882][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@55e2fe3c{/static,null,AVAILABLE,@Spark}
[INFO][2018-05-31 15:01:04,883][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5c8e67b9{/,null,AVAILABLE,@Spark}
[INFO][2018-05-31 15:01:04,884][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@49206065{/api,null,AVAILABLE,@Spark}
[INFO][2018-05-31 15:01:04,885][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@615e3f51{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-31 15:01:04,886][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@608bc8f8{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-31 15:01:04,890][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://10.194.32.157:4040
[INFO][2018-05-31 15:01:05,001][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
[INFO][2018-05-31 15:01:05,028][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 57711.
[INFO][2018-05-31 15:01:05,029][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 10.194.32.157:57711
[INFO][2018-05-31 15:01:05,033][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2018-05-31 15:01:05,036][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 10.194.32.157, 57711, None)
[INFO][2018-05-31 15:01:05,040][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 10.194.32.157:57711 with 912.3 MB RAM, BlockManagerId(driver, 10.194.32.157, 57711, None)
[INFO][2018-05-31 15:01:05,048][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 10.194.32.157, 57711, None)
[INFO][2018-05-31 15:01:05,050][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 10.194.32.157, 57711, None)
[INFO][2018-05-31 15:01:05,353][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2e09c51{/metrics/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 15:01:05,447][org.apache.spark.sql.internal.SharedState]Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/Users/seven/project/github/dataMining/spark-warehouse/').
[INFO][2018-05-31 15:01:05,448][org.apache.spark.sql.internal.SharedState]Warehouse path is 'file:/Users/seven/project/github/dataMining/spark-warehouse/'.
[INFO][2018-05-31 15:01:05,455][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@470d183{/SQL,null,AVAILABLE,@Spark}
[INFO][2018-05-31 15:01:05,456][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@ea52184{/SQL/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 15:01:05,457][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7f608e21{/SQL/execution,null,AVAILABLE,@Spark}
[INFO][2018-05-31 15:01:05,457][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@210d2a6c{/SQL/execution/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 15:01:05,459][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7c781c42{/static/sql,null,AVAILABLE,@Spark}
[INFO][2018-05-31 15:01:06,291][org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef]Registered StateStoreCoordinator endpoint
[INFO][2018-05-31 15:01:28,587][org.apache.spark.SparkContext]Invoking stop() from shutdown hook
[INFO][2018-05-31 15:01:28,599][org.spark_project.jetty.server.AbstractConnector]Stopped Spark@2314ddae{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-05-31 15:01:28,601][org.apache.spark.ui.SparkUI]Stopped Spark web UI at http://10.194.32.157:4040
[INFO][2018-05-31 15:01:28,612][org.apache.spark.MapOutputTrackerMasterEndpoint]MapOutputTrackerMasterEndpoint stopped!
[INFO][2018-05-31 15:01:28,626][org.apache.spark.storage.memory.MemoryStore]MemoryStore cleared
[INFO][2018-05-31 15:01:28,627][org.apache.spark.storage.BlockManager]BlockManager stopped
[INFO][2018-05-31 15:01:28,628][org.apache.spark.storage.BlockManagerMaster]BlockManagerMaster stopped
[INFO][2018-05-31 15:01:28,635][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint]OutputCommitCoordinator stopped!
[INFO][2018-05-31 15:01:28,640][org.apache.spark.SparkContext]Successfully stopped SparkContext
[INFO][2018-05-31 15:01:28,641][org.apache.spark.util.ShutdownHookManager]Shutdown hook called
[INFO][2018-05-31 15:01:28,644][org.apache.spark.util.ShutdownHookManager]Deleting directory /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/spark-1d29bd8c-acbc-497b-831d-9b6dd77a1d45
[INFO][2018-05-31 16:56:05,142][org.apache.spark.SparkContext]Running Spark version 2.2.0
[INFO][2018-05-31 16:56:06,071][org.apache.spark.SparkContext]Submitted application: NetType$
[INFO][2018-05-31 16:56:06,096][org.apache.spark.SecurityManager]Changing view acls to: seven
[INFO][2018-05-31 16:56:06,096][org.apache.spark.SecurityManager]Changing modify acls to: seven
[INFO][2018-05-31 16:56:06,097][org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO][2018-05-31 16:56:06,098][org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO][2018-05-31 16:56:06,099][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(seven); groups with view permissions: Set(); users  with modify permissions: Set(seven); groups with modify permissions: Set()
[INFO][2018-05-31 16:56:06,391][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 61800.
[INFO][2018-05-31 16:56:06,408][org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO][2018-05-31 16:56:06,427][org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO][2018-05-31 16:56:06,430][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2018-05-31 16:56:06,431][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
[INFO][2018-05-31 16:56:06,439][org.apache.spark.storage.DiskBlockManager]Created local directory at /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/blockmgr-d02fbd11-ca09-4bc4-ba16-05bddfde596f
[INFO][2018-05-31 16:56:06,456][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 912.3 MB
[INFO][2018-05-31 16:56:06,544][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO][2018-05-31 16:56:06,645][org.spark_project.jetty.util.log]Logging initialized @3068ms
[INFO][2018-05-31 16:56:06,707][org.spark_project.jetty.server.Server]jetty-9.3.z-SNAPSHOT
[INFO][2018-05-31 16:56:06,720][org.spark_project.jetty.server.Server]Started @3144ms
[INFO][2018-05-31 16:56:06,740][org.spark_project.jetty.server.AbstractConnector]Started ServerConnector@6cab17fd{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-05-31 16:56:06,740][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4040.
[INFO][2018-05-31 16:56:06,766][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@66629f63{/jobs,null,AVAILABLE,@Spark}
[INFO][2018-05-31 16:56:06,767][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@25f7391e{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 16:56:06,767][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5ab14cb9{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2018-05-31 16:56:06,768][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@61861a29{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 16:56:06,773][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@25bcd0c7{/stages,null,AVAILABLE,@Spark}
[INFO][2018-05-31 16:56:06,774][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@63cd604c{/stages/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 16:56:06,776][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3a4e343{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2018-05-31 16:56:06,783][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6fff253c{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 16:56:06,788][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@591e58fa{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2018-05-31 16:56:06,789][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2f94c4db{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 16:56:06,798][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@72ccd81a{/storage,null,AVAILABLE,@Spark}
[INFO][2018-05-31 16:56:06,799][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@64bc21ac{/storage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 16:56:06,804][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5d25e6bb{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2018-05-31 16:56:06,805][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@9d157ff{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 16:56:06,807][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5df417a7{/environment,null,AVAILABLE,@Spark}
[INFO][2018-05-31 16:56:06,808][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7f69d591{/environment/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 16:56:06,808][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1cb3ec38{/executors,null,AVAILABLE,@Spark}
[INFO][2018-05-31 16:56:06,810][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@71c5b236{/executors/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 16:56:06,811][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2f7a7219{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2018-05-31 16:56:06,812][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3a1d593e{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 16:56:06,821][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@361c294e{/static,null,AVAILABLE,@Spark}
[INFO][2018-05-31 16:56:06,822][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@293bb8a5{/,null,AVAILABLE,@Spark}
[INFO][2018-05-31 16:56:06,824][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6fa590ba{/api,null,AVAILABLE,@Spark}
[INFO][2018-05-31 16:56:06,824][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1fc0053e{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-31 16:56:06,825][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@47874b25{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-31 16:56:06,827][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://10.194.32.157:4040
[INFO][2018-05-31 16:56:06,945][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
[INFO][2018-05-31 16:56:06,968][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 61802.
[INFO][2018-05-31 16:56:06,970][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 10.194.32.157:61802
[INFO][2018-05-31 16:56:06,972][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2018-05-31 16:56:06,974][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 10.194.32.157, 61802, None)
[INFO][2018-05-31 16:56:06,979][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 10.194.32.157:61802 with 912.3 MB RAM, BlockManagerId(driver, 10.194.32.157, 61802, None)
[INFO][2018-05-31 16:56:06,984][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 10.194.32.157, 61802, None)
[INFO][2018-05-31 16:56:06,984][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 10.194.32.157, 61802, None)
[INFO][2018-05-31 16:56:07,253][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@41c89d2f{/metrics/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 16:56:07,422][org.apache.spark.sql.internal.SharedState]Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('target/spark-warehouse').
[INFO][2018-05-31 16:56:07,423][org.apache.spark.sql.internal.SharedState]Warehouse path is 'target/spark-warehouse'.
[INFO][2018-05-31 16:56:07,449][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@33a630fa{/SQL,null,AVAILABLE,@Spark}
[INFO][2018-05-31 16:56:07,450][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5767b2af{/SQL/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 16:56:07,450][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@241a0c3a{/SQL/execution,null,AVAILABLE,@Spark}
[INFO][2018-05-31 16:56:07,455][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@30e92cb9{/SQL/execution/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 16:56:07,459][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4f7c0be3{/static/sql,null,AVAILABLE,@Spark}
[WARN][2018-05-31 16:56:09,696][org.apache.hadoop.util.NativeCodeLoader]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO][2018-05-31 16:56:09,962][org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef]Registered StateStoreCoordinator endpoint
[INFO][2018-05-31 16:56:09,967][com.seven.spark.sparksql.NetType$]NetType$ is start ! ! !
[INFO][2018-05-31 16:56:09,967][com.seven.spark.sparksql.NetType$]load vem_nettype is start
[INFO][2018-05-31 16:56:12,474][org.apache.spark.sql.execution.datasources.FileSourceStrategy]Pruning directories with: 
[INFO][2018-05-31 16:56:12,480][org.apache.spark.sql.execution.datasources.FileSourceStrategy]Post-Scan Filters: (length(trim(value#0)) > 0)
[INFO][2018-05-31 16:56:12,484][org.apache.spark.sql.execution.datasources.FileSourceStrategy]Output Data Schema: struct<value: string>
[INFO][2018-05-31 16:56:12,500][org.apache.spark.sql.execution.FileSourceScanExec]Pushed Filters: 
[INFO][2018-05-31 16:56:13,231][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator]Code generated in 207.789434 ms
[INFO][2018-05-31 16:56:13,307][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0 stored as values in memory (estimated size 261.0 KB, free 912.0 MB)
[INFO][2018-05-31 16:56:13,553][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0_piece0 stored as bytes in memory (estimated size 22.7 KB, free 912.0 MB)
[INFO][2018-05-31 16:56:13,555][org.apache.spark.storage.BlockManagerInfo]Added broadcast_0_piece0 in memory on 10.194.32.157:61802 (size: 22.7 KB, free: 912.3 MB)
[INFO][2018-05-31 16:56:13,561][org.apache.spark.SparkContext]Created broadcast 0 from csv at NetType.scala:83
[INFO][2018-05-31 16:56:13,575][org.apache.spark.sql.execution.FileSourceScanExec]Planning scan with bin packing, max size: 8072675 bytes, open cost is considered as scanning 4194304 bytes.
[INFO][2018-05-31 16:56:13,845][org.apache.spark.SparkContext]Starting job: csv at NetType.scala:83
[INFO][2018-05-31 16:56:13,871][org.apache.spark.scheduler.DAGScheduler]Got job 0 (csv at NetType.scala:83) with 1 output partitions
[INFO][2018-05-31 16:56:13,872][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 0 (csv at NetType.scala:83)
[INFO][2018-05-31 16:56:13,873][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-31 16:56:13,875][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-31 16:56:13,884][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 0 (MapPartitionsRDD[2] at csv at NetType.scala:83), which has no missing parents
[INFO][2018-05-31 16:56:13,939][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1 stored as values in memory (estimated size 8.2 KB, free 912.0 MB)
[INFO][2018-05-31 16:56:13,947][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.3 KB, free 912.0 MB)
[INFO][2018-05-31 16:56:13,948][org.apache.spark.storage.BlockManagerInfo]Added broadcast_1_piece0 in memory on 10.194.32.157:61802 (size: 4.3 KB, free: 912.3 MB)
[INFO][2018-05-31 16:56:13,949][org.apache.spark.SparkContext]Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-31 16:56:13,962][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at csv at NetType.scala:83) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-31 16:56:13,963][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 0.0 with 1 tasks
[INFO][2018-05-31 16:56:14,020][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 5344 bytes)
[INFO][2018-05-31 16:56:14,030][org.apache.spark.executor.Executor]Running task 0.0 in stage 0.0 (TID 0)
[INFO][2018-05-31 16:56:14,077][org.apache.spark.sql.execution.datasources.FileScanRDD]Reading File path: hdfs://vm-xaj-bigdata-da-d01:8020/yst/sta_vem/vem_nettype/vem_nettype__1254c28d_5ec2_43ea_9639_3bb39ccd093b, range: 0-8072675, partition values: [empty row]
[INFO][2018-05-31 16:56:14,095][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator]Code generated in 12.753394 ms
[INFO][2018-05-31 16:56:14,347][org.apache.spark.executor.Executor]Finished task 0.0 in stage 0.0 (TID 0). 1414 bytes result sent to driver
[INFO][2018-05-31 16:56:14,361][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 0.0 (TID 0) in 358 ms on localhost (executor driver) (1/1)
[INFO][2018-05-31 16:56:14,364][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO][2018-05-31 16:56:14,379][org.apache.spark.scheduler.DAGScheduler]ResultStage 0 (csv at NetType.scala:83) finished in 0.386 s
[INFO][2018-05-31 16:56:14,389][org.apache.spark.scheduler.DAGScheduler]Job 0 finished: csv at NetType.scala:83, took 0.543677 s
[INFO][2018-05-31 16:56:14,417][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator]Code generated in 14.184377 ms
[INFO][2018-05-31 16:56:14,498][org.apache.spark.sql.execution.datasources.FileSourceStrategy]Pruning directories with: 
[INFO][2018-05-31 16:56:14,498][org.apache.spark.sql.execution.datasources.FileSourceStrategy]Post-Scan Filters: 
[INFO][2018-05-31 16:56:14,499][org.apache.spark.sql.execution.datasources.FileSourceStrategy]Output Data Schema: struct<value: string>
[INFO][2018-05-31 16:56:14,499][org.apache.spark.sql.execution.FileSourceScanExec]Pushed Filters: 
[INFO][2018-05-31 16:56:14,512][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator]Code generated in 7.849741 ms
[INFO][2018-05-31 16:56:14,521][org.apache.spark.storage.memory.MemoryStore]Block broadcast_2 stored as values in memory (estimated size 261.0 KB, free 911.8 MB)
[INFO][2018-05-31 16:56:14,548][org.apache.spark.storage.memory.MemoryStore]Block broadcast_2_piece0 stored as bytes in memory (estimated size 22.7 KB, free 911.7 MB)
[INFO][2018-05-31 16:56:14,561][org.apache.spark.storage.BlockManagerInfo]Added broadcast_2_piece0 in memory on 10.194.32.157:61802 (size: 22.7 KB, free: 912.3 MB)
[INFO][2018-05-31 16:56:14,562][org.apache.spark.SparkContext]Created broadcast 2 from csv at NetType.scala:83
[INFO][2018-05-31 16:56:14,563][org.apache.spark.sql.execution.FileSourceScanExec]Planning scan with bin packing, max size: 8072675 bytes, open cost is considered as scanning 4194304 bytes.
[INFO][2018-05-31 16:56:14,705][org.apache.spark.sql.execution.datasources.FileSourceStrategy]Pruning directories with: 
[INFO][2018-05-31 16:56:14,707][org.apache.spark.sql.execution.datasources.FileSourceStrategy]Post-Scan Filters: 
[INFO][2018-05-31 16:56:14,708][org.apache.spark.sql.execution.datasources.FileSourceStrategy]Output Data Schema: struct<_c0: string, _c1: string, _c2: string, _c3: string, _c4: string ... 28 more fields>
[INFO][2018-05-31 16:56:14,708][org.apache.spark.sql.execution.FileSourceScanExec]Pushed Filters: 
[INFO][2018-05-31 16:56:14,737][org.apache.spark.storage.memory.MemoryStore]Block broadcast_3 stored as values in memory (estimated size 261.1 KB, free 911.5 MB)
[INFO][2018-05-31 16:56:14,762][org.apache.spark.storage.memory.MemoryStore]Block broadcast_3_piece0 stored as bytes in memory (estimated size 22.7 KB, free 911.5 MB)
[INFO][2018-05-31 16:56:14,763][org.apache.spark.storage.BlockManagerInfo]Added broadcast_3_piece0 in memory on 10.194.32.157:61802 (size: 22.7 KB, free: 912.2 MB)
[INFO][2018-05-31 16:56:14,764][org.apache.spark.SparkContext]Created broadcast 3 from rdd at NetType.scala:84
[INFO][2018-05-31 16:56:14,767][org.apache.spark.sql.execution.FileSourceScanExec]Planning scan with bin packing, max size: 8072675 bytes, open cost is considered as scanning 4194304 bytes.
[INFO][2018-05-31 16:56:15,539][org.apache.spark.sql.execution.SparkSqlParser]Parsing command: isDelete = '0'
[INFO][2018-05-31 16:56:15,693][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_1_piece0 on 10.194.32.157:61802 in memory (size: 4.3 KB, free: 912.2 MB)
[INFO][2018-05-31 16:56:16,260][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator]Code generated in 34.501572 ms
[WARN][2018-05-31 16:56:16,275][org.apache.spark.util.Utils]Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.
[INFO][2018-05-31 16:56:16,278][org.apache.spark.sql.execution.SparkSqlParser]Parsing command: vem_nettype
[INFO][2018-05-31 16:56:16,313][com.seven.spark.sparksql.NetType$]load vem_nettype is success timeout is 0:00:06.344
[INFO][2018-05-31 16:56:16,314][com.seven.spark.sparksql.NetType$]save point data is start
[INFO][2018-05-31 16:56:16,489][org.apache.spark.sql.execution.datasources.FileSourceStrategy]Pruning directories with: 
[INFO][2018-05-31 16:56:16,491][org.apache.spark.sql.execution.datasources.FileSourceStrategy]Post-Scan Filters: (length(trim(value#1147)) > 0)
[INFO][2018-05-31 16:56:16,492][org.apache.spark.sql.execution.datasources.FileSourceStrategy]Output Data Schema: struct<value: string>
[INFO][2018-05-31 16:56:16,493][org.apache.spark.sql.execution.FileSourceScanExec]Pushed Filters: 
[INFO][2018-05-31 16:56:16,506][org.apache.spark.storage.memory.MemoryStore]Block broadcast_4 stored as values in memory (estimated size 261.0 KB, free 911.2 MB)
[INFO][2018-05-31 16:56:16,538][org.apache.spark.storage.memory.MemoryStore]Block broadcast_4_piece0 stored as bytes in memory (estimated size 22.7 KB, free 911.2 MB)
[INFO][2018-05-31 16:56:16,540][org.apache.spark.storage.BlockManagerInfo]Added broadcast_4_piece0 in memory on 10.194.32.157:61802 (size: 22.7 KB, free: 912.2 MB)
[INFO][2018-05-31 16:56:16,542][org.apache.spark.SparkContext]Created broadcast 4 from csv at NetType.scala:123
[INFO][2018-05-31 16:56:16,543][org.apache.spark.sql.execution.FileSourceScanExec]Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[INFO][2018-05-31 16:56:16,554][org.apache.spark.SparkContext]Starting job: csv at NetType.scala:123
[INFO][2018-05-31 16:56:16,555][org.apache.spark.scheduler.DAGScheduler]Got job 1 (csv at NetType.scala:123) with 1 output partitions
[INFO][2018-05-31 16:56:16,555][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 1 (csv at NetType.scala:123)
[INFO][2018-05-31 16:56:16,555][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-31 16:56:16,555][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-31 16:56:16,556][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 1 (MapPartitionsRDD[18] at csv at NetType.scala:123), which has no missing parents
[INFO][2018-05-31 16:56:16,559][org.apache.spark.storage.memory.MemoryStore]Block broadcast_5 stored as values in memory (estimated size 8.2 KB, free 911.2 MB)
[INFO][2018-05-31 16:56:16,567][org.apache.spark.storage.memory.MemoryStore]Block broadcast_5_piece0 stored as bytes in memory (estimated size 4.3 KB, free 911.2 MB)
[INFO][2018-05-31 16:56:16,569][org.apache.spark.storage.BlockManagerInfo]Added broadcast_5_piece0 in memory on 10.194.32.157:61802 (size: 4.3 KB, free: 912.2 MB)
[INFO][2018-05-31 16:56:16,569][org.apache.spark.SparkContext]Created broadcast 5 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-31 16:56:16,570][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[18] at csv at NetType.scala:123) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-31 16:56:16,570][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 1.0 with 1 tasks
[INFO][2018-05-31 16:56:16,571][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, ANY, 5352 bytes)
[INFO][2018-05-31 16:56:16,572][org.apache.spark.executor.Executor]Running task 0.0 in stage 1.0 (TID 1)
[INFO][2018-05-31 16:56:16,577][org.apache.spark.sql.execution.datasources.FileScanRDD]Reading File path: hdfs://vm-xaj-bigdata-da-d01:8020/yst/sta_vem/point_community/point_community__13b71ebe_9d3e_4f9d_ab4c_36e1387f238e, range: 0-1458329, partition values: [empty row]
[INFO][2018-05-31 16:56:16,637][org.apache.spark.executor.Executor]Finished task 0.0 in stage 1.0 (TID 1). 1389 bytes result sent to driver
[INFO][2018-05-31 16:56:16,642][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 1.0 (TID 1) in 71 ms on localhost (executor driver) (1/1)
[INFO][2018-05-31 16:56:16,642][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO][2018-05-31 16:56:16,643][org.apache.spark.scheduler.DAGScheduler]ResultStage 1 (csv at NetType.scala:123) finished in 0.071 s
[INFO][2018-05-31 16:56:16,643][org.apache.spark.scheduler.DAGScheduler]Job 1 finished: csv at NetType.scala:123, took 0.088849 s
[INFO][2018-05-31 16:56:16,649][org.apache.spark.sql.execution.datasources.FileSourceStrategy]Pruning directories with: 
[INFO][2018-05-31 16:56:16,649][org.apache.spark.sql.execution.datasources.FileSourceStrategy]Post-Scan Filters: 
[INFO][2018-05-31 16:56:16,649][org.apache.spark.sql.execution.datasources.FileSourceStrategy]Output Data Schema: struct<value: string>
[INFO][2018-05-31 16:56:16,649][org.apache.spark.sql.execution.FileSourceScanExec]Pushed Filters: 
[INFO][2018-05-31 16:56:16,656][org.apache.spark.storage.memory.MemoryStore]Block broadcast_6 stored as values in memory (estimated size 261.0 KB, free 910.9 MB)
[INFO][2018-05-31 16:56:16,685][org.apache.spark.storage.memory.MemoryStore]Block broadcast_6_piece0 stored as bytes in memory (estimated size 22.7 KB, free 910.9 MB)
[INFO][2018-05-31 16:56:16,688][org.apache.spark.storage.BlockManagerInfo]Added broadcast_6_piece0 in memory on 10.194.32.157:61802 (size: 22.7 KB, free: 912.2 MB)
[INFO][2018-05-31 16:56:16,689][org.apache.spark.SparkContext]Created broadcast 6 from csv at NetType.scala:123
[INFO][2018-05-31 16:56:16,689][org.apache.spark.sql.execution.FileSourceScanExec]Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[INFO][2018-05-31 16:56:16,718][org.apache.spark.sql.execution.datasources.FileSourceStrategy]Pruning directories with: 
[INFO][2018-05-31 16:56:16,719][org.apache.spark.sql.execution.datasources.FileSourceStrategy]Post-Scan Filters: 
[INFO][2018-05-31 16:56:16,720][org.apache.spark.sql.execution.datasources.FileSourceStrategy]Output Data Schema: struct<_c0: string, _c1: string, _c2: string, _c3: string, _c4: string ... 20 more fields>
[INFO][2018-05-31 16:56:16,720][org.apache.spark.sql.execution.FileSourceScanExec]Pushed Filters: 
[INFO][2018-05-31 16:56:16,728][org.apache.spark.storage.memory.MemoryStore]Block broadcast_7 stored as values in memory (estimated size 261.1 KB, free 910.6 MB)
[INFO][2018-05-31 16:56:16,764][org.apache.spark.storage.memory.MemoryStore]Block broadcast_7_piece0 stored as bytes in memory (estimated size 22.7 KB, free 910.6 MB)
[INFO][2018-05-31 16:56:16,765][org.apache.spark.storage.BlockManagerInfo]Added broadcast_7_piece0 in memory on 10.194.32.157:61802 (size: 22.7 KB, free: 912.2 MB)
[INFO][2018-05-31 16:56:16,766][org.apache.spark.SparkContext]Created broadcast 7 from rdd at NetType.scala:124
[INFO][2018-05-31 16:56:16,767][org.apache.spark.sql.execution.FileSourceScanExec]Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[INFO][2018-05-31 16:56:16,925][org.apache.spark.sql.execution.SparkSqlParser]Parsing command: isDelete = '0'
[INFO][2018-05-31 16:56:17,000][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator]Code generated in 33.038912 ms
[INFO][2018-05-31 16:56:17,012][org.apache.spark.sql.execution.SparkSqlParser]Parsing command: point_community
[INFO][2018-05-31 16:56:17,018][org.apache.spark.sql.execution.SparkSqlParser]Parsing command: 
select
v.id,p.pointName,v.parentId,v.createDate,v.isDelete,v.type,v.province,v.city,v.county,
v.address,v.operators,v.lat,v.lng,p.sort,p.pointType,p.householdCoverageNum,p.householdOccupancyNum,
p.ctcSignalStrength,p.cmccSignalStrength,p.cuccSignalStrength,p.signalOperator,p.signalSolution
from point_community p
left join vem_nettype v
on v.communityId = p.id
where v.type = 'point' and v.parentId <> '101' and v.parentId <> '620' and v.parentId <> '7667'
    
[INFO][2018-05-31 16:56:17,277][org.apache.spark.sql.execution.columnar.InMemoryTableScanExec]Predicate isnotnull(id#1294) generates partition filter: ((id.count#1840 - id.nullCount#1839) > 0)
[INFO][2018-05-31 16:56:17,279][org.apache.spark.sql.execution.columnar.InMemoryTableScanExec]Predicate isnotnull(type#451) generates partition filter: ((type.count#1990 - type.nullCount#1989) > 0)
[INFO][2018-05-31 16:56:17,280][org.apache.spark.sql.execution.columnar.InMemoryTableScanExec]Predicate isnotnull(parentId#259) generates partition filter: ((parentId.count#1960 - parentId.nullCount#1959) > 0)
[INFO][2018-05-31 16:56:17,280][org.apache.spark.sql.execution.columnar.InMemoryTableScanExec]Predicate (type#451 = point) generates partition filter: ((type.lowerBound#1988 <= point) && (point <= type.upperBound#1987))
[INFO][2018-05-31 16:56:17,281][org.apache.spark.sql.execution.columnar.InMemoryTableScanExec]Predicate isnotnull(communityId#835) generates partition filter: ((communityId.count#2050 - communityId.nullCount#2049) > 0)
[INFO][2018-05-31 16:56:17,351][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator]Code generated in 32.433415 ms
[INFO][2018-05-31 16:56:17,374][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator]Code generated in 17.758742 ms
[INFO][2018-05-31 16:56:17,390][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator]Code generated in 13.219615 ms
[INFO][2018-05-31 16:56:17,451][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator]Code generated in 13.59357 ms
[INFO][2018-05-31 16:56:17,686][org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter]File Output Committer Algorithm version is 1
[INFO][2018-05-31 16:56:17,688][org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol]Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[INFO][2018-05-31 16:56:17,756][org.apache.spark.SparkContext]Starting job: csv at NetType.scala:169
[INFO][2018-05-31 16:56:17,760][org.apache.spark.scheduler.DAGScheduler]Registering RDD 34 (cache at NetType.scala:161)
[INFO][2018-05-31 16:56:17,761][org.apache.spark.scheduler.DAGScheduler]Registering RDD 39 (cache at NetType.scala:161)
[INFO][2018-05-31 16:56:17,761][org.apache.spark.scheduler.DAGScheduler]Registering RDD 46 (csv at NetType.scala:169)
[INFO][2018-05-31 16:56:17,761][org.apache.spark.scheduler.DAGScheduler]Got job 2 (csv at NetType.scala:169) with 1 output partitions
[INFO][2018-05-31 16:56:17,761][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 5 (csv at NetType.scala:169)
[INFO][2018-05-31 16:56:17,761][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 4)
[INFO][2018-05-31 16:56:17,761][org.apache.spark.scheduler.DAGScheduler]Missing parents: List(ShuffleMapStage 4)
[INFO][2018-05-31 16:56:17,767][org.apache.spark.scheduler.DAGScheduler]Submitting ShuffleMapStage 2 (MapPartitionsRDD[34] at cache at NetType.scala:161), which has no missing parents
[INFO][2018-05-31 16:56:17,806][org.apache.spark.storage.memory.MemoryStore]Block broadcast_8 stored as values in memory (estimated size 58.7 KB, free 910.6 MB)
[INFO][2018-05-31 16:56:17,811][org.apache.spark.storage.memory.MemoryStore]Block broadcast_8_piece0 stored as bytes in memory (estimated size 19.8 KB, free 910.5 MB)
[INFO][2018-05-31 16:56:17,812][org.apache.spark.storage.BlockManagerInfo]Added broadcast_8_piece0 in memory on 10.194.32.157:61802 (size: 19.8 KB, free: 912.1 MB)
[INFO][2018-05-31 16:56:17,812][org.apache.spark.SparkContext]Created broadcast 8 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-31 16:56:17,816][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[34] at cache at NetType.scala:161) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-31 16:56:17,817][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 2.0 with 1 tasks
[INFO][2018-05-31 16:56:17,818][org.apache.spark.scheduler.DAGScheduler]Submitting ShuffleMapStage 3 (MapPartitionsRDD[39] at cache at NetType.scala:161), which has no missing parents
[INFO][2018-05-31 16:56:17,818][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, ANY, 5341 bytes)
[INFO][2018-05-31 16:56:17,818][org.apache.spark.executor.Executor]Running task 0.0 in stage 2.0 (TID 2)
[INFO][2018-05-31 16:56:17,825][org.apache.spark.storage.memory.MemoryStore]Block broadcast_9 stored as values in memory (estimated size 72.9 KB, free 910.5 MB)
[INFO][2018-05-31 16:56:17,829][org.apache.spark.storage.memory.MemoryStore]Block broadcast_9_piece0 stored as bytes in memory (estimated size 23.7 KB, free 910.5 MB)
[INFO][2018-05-31 16:56:17,831][org.apache.spark.storage.BlockManagerInfo]Added broadcast_9_piece0 in memory on 10.194.32.157:61802 (size: 23.7 KB, free: 912.1 MB)
[INFO][2018-05-31 16:56:17,831][org.apache.spark.SparkContext]Created broadcast 9 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-31 16:56:17,832][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[39] at cache at NetType.scala:161) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2018-05-31 16:56:17,832][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 3.0 with 2 tasks
[INFO][2018-05-31 16:56:17,833][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 3.0 (TID 3, localhost, executor driver, partition 0, ANY, 5333 bytes)
[INFO][2018-05-31 16:56:17,833][org.apache.spark.executor.Executor]Running task 0.0 in stage 3.0 (TID 3)
[INFO][2018-05-31 16:56:17,883][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator]Code generated in 22.560694 ms
[INFO][2018-05-31 16:56:17,887][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator]Code generated in 26.139872 ms
[INFO][2018-05-31 16:56:17,904][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator]Code generated in 13.160179 ms
[INFO][2018-05-31 16:56:17,911][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator]Code generated in 20.413705 ms
[INFO][2018-05-31 16:56:17,915][org.apache.spark.sql.execution.datasources.FileScanRDD]Reading File path: hdfs://vm-xaj-bigdata-da-d01:8020/yst/sta_vem/point_community/point_community__13b71ebe_9d3e_4f9d_ab4c_36e1387f238e, range: 0-1458329, partition values: [empty row]
[INFO][2018-05-31 16:56:17,915][org.apache.spark.sql.execution.datasources.FileScanRDD]Reading File path: hdfs://vm-xaj-bigdata-da-d01:8020/yst/sta_vem/vem_nettype/vem_nettype__1254c28d_5ec2_43ea_9639_3bb39ccd093b, range: 0-8072675, partition values: [empty row]
[INFO][2018-05-31 16:56:18,131][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator]Code generated in 95.701872 ms
[INFO][2018-05-31 16:56:18,161][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator]Code generated in 122.53793 ms
[INFO][2018-05-31 16:56:18,213][org.apache.spark.ContextCleaner]Cleaned accumulator 102
[INFO][2018-05-31 16:56:18,215][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_5_piece0 on 10.194.32.157:61802 in memory (size: 4.3 KB, free: 912.1 MB)
[INFO][2018-05-31 16:56:18,216][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_6_piece0 on 10.194.32.157:61802 in memory (size: 22.7 KB, free: 912.1 MB)
[INFO][2018-05-31 16:56:18,216][org.apache.spark.ContextCleaner]Cleaned accumulator 98
[INFO][2018-05-31 16:56:18,216][org.apache.spark.ContextCleaner]Cleaned accumulator 99
[INFO][2018-05-31 16:56:18,216][org.apache.spark.ContextCleaner]Cleaned accumulator 113
[INFO][2018-05-31 16:56:18,216][org.apache.spark.ContextCleaner]Cleaned accumulator 100
[INFO][2018-05-31 16:56:18,216][org.apache.spark.ContextCleaner]Cleaned accumulator 70
[INFO][2018-05-31 16:56:18,217][org.apache.spark.ContextCleaner]Cleaned accumulator 71
[INFO][2018-05-31 16:56:18,217][org.apache.spark.ContextCleaner]Cleaned accumulator 133
[INFO][2018-05-31 16:56:18,217][org.apache.spark.ContextCleaner]Cleaned accumulator 101
[INFO][2018-05-31 16:56:18,218][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_4_piece0 on 10.194.32.157:61802 in memory (size: 22.7 KB, free: 912.2 MB)
[INFO][2018-05-31 16:56:18,220][org.apache.spark.ContextCleaner]Cleaned accumulator 72
[INFO][2018-05-31 16:56:18,220][org.apache.spark.ContextCleaner]Cleaned accumulator 112
[INFO][2018-05-31 16:56:18,220][org.apache.spark.ContextCleaner]Cleaned accumulator 73
[INFO][2018-05-31 16:56:18,220][org.apache.spark.ContextCleaner]Cleaned accumulator 69
[INFO][2018-05-31 16:56:18,220][org.apache.spark.ContextCleaner]Cleaned accumulator 68
[INFO][2018-05-31 16:56:19,420][org.apache.spark.storage.memory.MemoryStore]Block rdd_31_0 stored as values in memory (estimated size 770.7 KB, free 910.3 MB)
[INFO][2018-05-31 16:56:19,422][org.apache.spark.storage.BlockManagerInfo]Added rdd_31_0 in memory on 10.194.32.157:61802 (size: 770.7 KB, free: 911.4 MB)
[INFO][2018-05-31 16:56:19,441][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator]Code generated in 5.237439 ms
[INFO][2018-05-31 16:56:19,581][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator]Code generated in 113.909097 ms
[INFO][2018-05-31 16:56:19,604][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator]Code generated in 13.10985 ms
[INFO][2018-05-31 16:56:20,312][org.apache.spark.executor.Executor]Finished task 0.0 in stage 2.0 (TID 2). 3001 bytes result sent to driver
[INFO][2018-05-31 16:56:20,313][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 3.0 (TID 4, localhost, executor driver, partition 1, ANY, 5333 bytes)
[INFO][2018-05-31 16:56:20,313][org.apache.spark.executor.Executor]Running task 1.0 in stage 3.0 (TID 4)
[INFO][2018-05-31 16:56:20,332][org.apache.spark.sql.execution.datasources.FileScanRDD]Reading File path: hdfs://vm-xaj-bigdata-da-d01:8020/yst/sta_vem/vem_nettype/vem_nettype__1254c28d_5ec2_43ea_9639_3bb39ccd093b, range: 8072675-11951047, partition values: [empty row]
[INFO][2018-05-31 16:56:20,341][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 2.0 (TID 2) in 2524 ms on localhost (executor driver) (1/1)
[INFO][2018-05-31 16:56:20,341][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO][2018-05-31 16:56:20,342][org.apache.spark.scheduler.DAGScheduler]ShuffleMapStage 2 (cache at NetType.scala:161) finished in 2.525 s
[INFO][2018-05-31 16:56:20,343][org.apache.spark.scheduler.DAGScheduler]looking for newly runnable stages
[INFO][2018-05-31 16:56:20,343][org.apache.spark.scheduler.DAGScheduler]running: Set(ShuffleMapStage 3)
[INFO][2018-05-31 16:56:20,344][org.apache.spark.scheduler.DAGScheduler]waiting: Set(ResultStage 5, ShuffleMapStage 4)
[INFO][2018-05-31 16:56:20,345][org.apache.spark.scheduler.DAGScheduler]failed: Set()
[INFO][2018-05-31 16:56:21,250][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_8_piece0 on 10.194.32.157:61802 in memory (size: 19.8 KB, free: 911.4 MB)
[INFO][2018-05-31 16:56:21,381][org.apache.spark.storage.memory.MemoryStore]Block rdd_15_0 stored as values in memory (estimated size 3.7 MB, free 906.7 MB)
[INFO][2018-05-31 16:56:21,382][org.apache.spark.storage.BlockManagerInfo]Added rdd_15_0 in memory on 10.194.32.157:61802 (size: 3.7 MB, free: 907.8 MB)
[INFO][2018-05-31 16:56:21,396][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator]Code generated in 9.019987 ms
[INFO][2018-05-31 16:56:21,412][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator]Code generated in 14.80999 ms
[INFO][2018-05-31 16:56:21,428][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator]Code generated in 11.410308 ms
[INFO][2018-05-31 16:56:21,500][org.apache.spark.sql.execution.columnar.InMemoryTableScanExec]Skipping partition based on stats id.lowerBound: 44609, id.upperBound: 54608, id.nullCount: 0, id.count: 10000, id.sizeInBytes: 90000, nettypeName.lowerBound:  五中学校商店, nettypeName.upperBound: （自贩机）含山县西门车站, nettypeName.nullCount: 0, nettypeName.count: 10000, nettypeName.sizeInBytes: 249832, parentId.lowerBound: 1, parentId.upperBound: 9, parentId.nullCount: 0, parentId.count: 10000, parentId.sizeInBytes: 56954, createPerson.lowerBound: initial_zy, createPerson.upperBound: initial_zy, createPerson.nullCount: 0, createPerson.count: 10000, createPerson.sizeInBytes: 140000, createDate.lowerBound: 2018-05-11 19:47:40.0, createDate.upperBound: 2018-05-11 19:47:40.0, createDate.nullCount: 0, createDate.count: 10000, createDate.sizeInBytes: 250000, updatePerson.lowerBound: amei1, updatePerson.upperBound: yjmin1, updatePerson.nullCount: 0, updatePerson.count: 10000, updatePerson.sizeInBytes: 139984, updateDate.lowerBound: 2018-05-11 21:43:46.0, updateDate.upperBound: 2018-05-25 09:44:13.0, updateDate.nullCount: 0, updateDate.count: 10000, updateDate.sizeInBytes: 250000, isDelete.lowerBound: 0, isDelete.upperBound: 0, isDelete.nullCount: 0, isDelete.count: 10000, isDelete.sizeInBytes: 50000, type.lowerBound: net, type.upperBound: net, type.nullCount: 0, type.count: 10000, type.sizeInBytes: 70000, office.lowerBound: 1001, office.upperBound: 982, office.nullCount: 206, office.count: 10000, office.sizeInBytes: 77187, operatorid.lowerBound: 1003, operatorid.upperBound: 8438, operatorid.nullCount: 0, operatorid.count: 10000, operatorid.sizeInBytes: 80000, province.lowerBound: 江苏省, province.upperBound: 辽宁省, province.nullCount: 9996, province.count: 10000, province.sizeInBytes: 40036, city.lowerBound: 温州市, city.upperBound: 阜新市, city.nullCount: 9996, city.count: 10000, city.sizeInBytes: 40039, county.lowerBound: 乐清市, county.upperBound: 藁城区, county.nullCount: 9996, county.count: 10000, county.sizeInBytes: 40039, contactMan.lowerBound:  陆梅花/刘琪, contactMan.upperBound: 龚恒东, contactMan.nullCount: 4236, contactMan.count: 10000, contactMan.sizeInBytes: 87026, contactPhone.lowerBound:  15530955333, contactPhone.upperBound: 无, contactPhone.nullCount: 4485, contactPhone.count: 10000, contactPhone.sizeInBytes: 100814, address.lowerBound: 张家港市华达路康得新公司内, address.upperBound: 张家港市华达路康得新公司内, address.nullCount: 9999, address.count: 10000, address.sizeInBytes: 40039, operators.lowerBound:   王甫涛, operators.upperBound: （鄂州四中）-明华康校园超市, operators.nullCount: 1, operators.count: 10000, operators.sizeInBytes: 257110, channelTypeCode.lowerBound:  , channelTypeCode.upperBound:  , channelTypeCode.nullCount: 0, channelTypeCode.count: 10000, channelTypeCode.sizeInBytes: 50000, checkStatusCode.lowerBound:  , checkStatusCode.upperBound:  , checkStatusCode.nullCount: 0, checkStatusCode.count: 10000, checkStatusCode.sizeInBytes: 50000, communityId.lowerBound: null, communityId.upperBound: null, communityId.nullCount: 10000, communityId.count: 10000, communityId.sizeInBytes: 40000, lat.lowerBound: 28.261839, lat.upperBound: 41.990902, lat.nullCount: 9996, lat.count: 10000, lat.sizeInBytes: 40035, lng.lowerBound: 114.828096, lng.upperBound: 121.652705, lng.nullCount: 9996, lng.count: 10000, lng.sizeInBytes: 40040, auditor.lowerBound: initial_zy, auditor.upperBound: initial_zy, auditor.nullCount: 0, auditor.count: 10000, auditor.sizeInBytes: 140000, auditDate.lowerBound: null, auditDate.upperBound: null, auditDate.nullCount: 10000, auditDate.count: 10000, auditDate.sizeInBytes: 40000, a25.lowerBound: DS_ZFJVEM_PRD, a25.upperBound: DS_ZFJVEM_PRD, a25.nullCount: 0, a25.count: 10000, a25.sizeInBytes: 170000, a26.lowerBound: job_hsta_vem_nettype, a26.upperBound: job_hsta_vem_nettype, a26.nullCount: 0, a26.count: 10000, a26.sizeInBytes: 240000, a27.lowerBound: vem_nettype, a27.upperBound: vem_nettype, a27.nullCount: 0, a27.count: 10000, a27.sizeInBytes: 150000, a28.lowerBound: 20180530, a28.upperBound: 20180530, a28.nullCount: 0, a28.count: 10000, a28.sizeInBytes: 120000, a29.lowerBound: 2018-05-31 07:30:33.0, a29.upperBound: 2018-05-31 07:30:35.0, a29.nullCount: 0, a29.count: 10000, a29.sizeInBytes: 250000
[INFO][2018-05-31 16:56:21,502][org.apache.spark.sql.execution.columnar.InMemoryTableScanExec]Skipping partition based on stats id.lowerBound: 54609, id.upperBound: 66121, id.nullCount: 0, id.count: 10000, id.sizeInBytes: 90000, nettypeName.lowerBound:              童话城堡亲子乐园门口, nettypeName.upperBound: �恒大影院, nettypeName.nullCount: 0, nettypeName.count: 10000, nettypeName.sizeInBytes: 284752, parentId.lowerBound: 1, parentId.upperBound: 9, parentId.nullCount: 0, parentId.count: 10000, parentId.sizeInBytes: 77463, createPerson.lowerBound: initial_zy, createPerson.upperBound: initial_zy, createPerson.nullCount: 0, createPerson.count: 10000, createPerson.sizeInBytes: 140000, createDate.lowerBound: 2018-05-11 19:47:40.0, createDate.upperBound: 2018-05-11 19:56:26.0, createDate.nullCount: 0, createDate.count: 10000, createDate.sizeInBytes: 250000, updatePerson.lowerBound: 110008, updatePerson.upperBound: ywwu, updatePerson.nullCount: 0, updatePerson.count: 10000, updatePerson.sizeInBytes: 139922, updateDate.lowerBound: 2018-05-11 21:43:46.0, updateDate.upperBound: 2018-05-30 15:05:36.0, updateDate.nullCount: 0, updateDate.count: 10000, updateDate.sizeInBytes: 250000, isDelete.lowerBound: 0, isDelete.upperBound: 0, isDelete.nullCount: 0, isDelete.count: 10000, isDelete.sizeInBytes: 50000, type.lowerBound: net, type.upperBound: point, type.nullCount: 0, type.count: 10000, type.sizeInBytes: 82228, office.lowerBound: 1001, office.upperBound: 982, office.nullCount: 292, office.count: 10000, office.sizeInBytes: 76009, operatorid.lowerBound: 1000, operatorid.upperBound: 8437, operatorid.nullCount: 0, operatorid.count: 10000, operatorid.sizeInBytes: 80000, province.lowerBound: 广西壮族自治区, province.upperBound: 辽宁省, province.nullCount: 9995, province.count: 10000, province.sizeInBytes: 40057, city.lowerBound: 大连市, city.upperBound: 苏州市, city.nullCount: 9995, city.count: 10000, city.sizeInBytes: 40045, county.lowerBound: 吴中区, county.upperBound: 金州区, county.nullCount: 9995, county.count: 10000, county.sizeInBytes: 40045, contactMan.lowerBound:  吉日格勒, contactMan.upperBound: 龚江平, contactMan.nullCount: 5064, contactMan.count: 10000, contactMan.sizeInBytes: 80672, contactPhone.lowerBound: 0, contactPhone.upperBound: 无, contactPhone.nullCount: 5276, contactPhone.count: 10000, contactPhone.sizeInBytes: 91883, address.lowerBound: 太湖湿地公园, address.upperBound: 常州高铁北, address.nullCount: 9997, address.count: 10000, address.sizeInBytes: 40048, operators.lowerBound:   王甫涛, operators.upperBound: （黄冈中学）武汉鑫安大校园后勤服务有限公司, operators.nullCount: 0, operators.count: 10000, operators.sizeInBytes: 260650, channelTypeCode.lowerBound:  , channelTypeCode.upperBound:  , channelTypeCode.nullCount: 0, channelTypeCode.count: 10000, channelTypeCode.sizeInBytes: 50000, checkStatusCode.lowerBound:  , checkStatusCode.upperBound:  , checkStatusCode.nullCount: 0, checkStatusCode.count: 10000, checkStatusCode.sizeInBytes: 50000, communityId.lowerBound: null, communityId.upperBound: null, communityId.nullCount: 10000, communityId.count: 10000, communityId.sizeInBytes: 40000, lat.lowerBound: 18.229251627917, lat.upperBound: 50.250842029062, lat.nullCount: 3881, lat.count: 10000, lat.sizeInBytes: 131007, lng.lowerBound: 100.08157011278, lng.upperBound: 99.233024150017, lng.nullCount: 3881, lng.count: 10000, lng.sizeInBytes: 130977, auditor.lowerBound: initial_zy, auditor.upperBound: initial_zy, auditor.nullCount: 0, auditor.count: 10000, auditor.sizeInBytes: 140000, auditDate.lowerBound: null, auditDate.upperBound: null, auditDate.nullCount: 10000, auditDate.count: 10000, auditDate.sizeInBytes: 40000, a25.lowerBound: DS_ZFJVEM_PRD, a25.upperBound: DS_ZFJVEM_PRD, a25.nullCount: 0, a25.count: 10000, a25.sizeInBytes: 170000, a26.lowerBound: job_hsta_vem_nettype, a26.upperBound: job_hsta_vem_nettype, a26.nullCount: 0, a26.count: 10000, a26.sizeInBytes: 240000, a27.lowerBound: vem_nettype, a27.upperBound: vem_nettype, a27.nullCount: 0, a27.count: 10000, a27.sizeInBytes: 150000, a28.lowerBound: 20180530, a28.upperBound: 20180530, a28.nullCount: 0, a28.count: 10000, a28.sizeInBytes: 120000, a29.lowerBound: 2018-05-31 07:30:35.0, a29.upperBound: 2018-05-31 07:30:35.0, a29.nullCount: 0, a29.count: 10000, a29.sizeInBytes: 250000
[INFO][2018-05-31 16:56:21,504][org.apache.spark.sql.execution.columnar.InMemoryTableScanExec]Skipping partition based on stats id.lowerBound: 66122, id.upperBound: 67457, id.nullCount: 0, id.count: 1336, id.sizeInBytes: 12024, nettypeName.lowerBound:  火箭健身广场, nettypeName.upperBound: （未运营）竹溪一中食堂门口, nettypeName.nullCount: 0, nettypeName.count: 1336, nettypeName.sizeInBytes: 44629, parentId.lowerBound: 47737, parentId.upperBound: 49271, parentId.nullCount: 0, parentId.count: 1336, parentId.sizeInBytes: 12024, createPerson.lowerBound: initial_zy, createPerson.upperBound: initial_zy, createPerson.nullCount: 0, createPerson.count: 1336, createPerson.sizeInBytes: 18704, createDate.lowerBound: 2018-05-11 19:56:26.0, createDate.upperBound: 2018-05-11 19:56:26.0, createDate.nullCount: 0, createDate.count: 1336, createDate.sizeInBytes: 33400, updatePerson.lowerBound: 4B0238, updatePerson.upperBound: initial_zy, updatePerson.nullCount: 0, updatePerson.count: 1336, updatePerson.sizeInBytes: 18632, updateDate.lowerBound: 2018-05-11 21:47:36.0, updateDate.upperBound: 2018-05-27 00:05:02.0, updateDate.nullCount: 0, updateDate.count: 1336, updateDate.sizeInBytes: 33400, isDelete.lowerBound: 0, isDelete.upperBound: 0, isDelete.nullCount: 0, isDelete.count: 1336, isDelete.sizeInBytes: 6680, type.lowerBound: point, type.upperBound: point, type.nullCount: 0, type.count: 1336, type.sizeInBytes: 12024, office.lowerBound: 1044, office.upperBound: 980, office.nullCount: 0, office.count: 1336, office.sizeInBytes: 10520, operatorid.lowerBound: 1014, operatorid.upperBound: 8419, operatorid.nullCount: 0, operatorid.count: 1336, operatorid.sizeInBytes: 10688, province.lowerBound: null, province.upperBound: null, province.nullCount: 1336, province.count: 1336, province.sizeInBytes: 5344, city.lowerBound: null, city.upperBound: null, city.nullCount: 1336, city.count: 1336, city.sizeInBytes: 5344, county.lowerBound: null, county.upperBound: null, county.nullCount: 1336, county.count: 1336, county.sizeInBytes: 5344, contactMan.lowerBound: 丁宏峰, contactMan.upperBound: 龙艳, contactMan.nullCount: 169, contactMan.count: 1336, contactMan.sizeInBytes: 14770, contactPhone.lowerBound: 0459-5872494, contactPhone.upperBound: 81717020, contactPhone.nullCount: 169, contactPhone.count: 1336, contactPhone.sizeInBytes: 18221, address.lowerBound: null, address.upperBound: null, address.nullCount: 1336, address.count: 1336, address.sizeInBytes: 5344, operators.lowerBound: 丁宏峰, operators.upperBound: （鄂州四中）-明华康校园超市, operators.nullCount: 0, operators.count: 1336, operators.sizeInBytes: 37411, channelTypeCode.lowerBound:  , channelTypeCode.upperBound:  , channelTypeCode.nullCount: 0, channelTypeCode.count: 1336, channelTypeCode.sizeInBytes: 6680, checkStatusCode.lowerBound:  , checkStatusCode.upperBound:  , checkStatusCode.nullCount: 0, checkStatusCode.count: 1336, checkStatusCode.sizeInBytes: 6680, communityId.lowerBound: null, communityId.upperBound: null, communityId.nullCount: 1336, communityId.count: 1336, communityId.sizeInBytes: 5344, lat.lowerBound: 22.008755288623, lat.upperBound: 47.318250128944, lat.nullCount: 0, lat.count: 1336, lat.sizeInBytes: 25230, lng.lowerBound: 100.2095886143, lng.upperBound: 99.711413976028, lng.nullCount: 0, lng.count: 1336, lng.sizeInBytes: 25248, auditor.lowerBound: initial_zy, auditor.upperBound: initial_zy, auditor.nullCount: 0, auditor.count: 1336, auditor.sizeInBytes: 18704, auditDate.lowerBound: null, auditDate.upperBound: null, auditDate.nullCount: 1336, auditDate.count: 1336, auditDate.sizeInBytes: 5344, a25.lowerBound: DS_ZFJVEM_PRD, a25.upperBound: DS_ZFJVEM_PRD, a25.nullCount: 0, a25.count: 1336, a25.sizeInBytes: 22712, a26.lowerBound: job_hsta_vem_nettype, a26.upperBound: job_hsta_vem_nettype, a26.nullCount: 0, a26.count: 1336, a26.sizeInBytes: 32064, a27.lowerBound: vem_nettype, a27.upperBound: vem_nettype, a27.nullCount: 0, a27.count: 1336, a27.sizeInBytes: 20040, a28.lowerBound: 20180530, a28.upperBound: 20180530, a28.nullCount: 0, a28.count: 1336, a28.sizeInBytes: 16032, a29.lowerBound: 2018-05-31 07:30:35.0, a29.upperBound: 2018-05-31 07:30:35.0, a29.nullCount: 0, a29.count: 1336, a29.sizeInBytes: 33400
[INFO][2018-05-31 16:56:21,635][org.apache.spark.executor.Executor]Finished task 0.0 in stage 3.0 (TID 3). 3001 bytes result sent to driver
[INFO][2018-05-31 16:56:21,639][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 3.0 (TID 3) in 3806 ms on localhost (executor driver) (1/2)
[INFO][2018-05-31 16:56:23,696][org.apache.spark.storage.memory.MemoryStore]Block rdd_15_1 stored as values in memory (estimated size 2002.5 KB, free 904.7 MB)
[INFO][2018-05-31 16:56:23,696][org.apache.spark.storage.BlockManagerInfo]Added rdd_15_1 in memory on 10.194.32.157:61802 (size: 2002.5 KB, free: 905.8 MB)
[INFO][2018-05-31 16:56:23,698][org.apache.spark.sql.execution.columnar.InMemoryTableScanExec]Skipping partition based on stats id.lowerBound: 67458, id.upperBound: 80077, id.nullCount: 0, id.count: 10000, id.sizeInBytes: 90000, nettypeName.lowerBound:   信丰雷登工业园11号机, nettypeName.upperBound: （香洲）银桦无人店, nettypeName.nullCount: 0, nettypeName.count: 10000, nettypeName.sizeInBytes: 271894, parentId.lowerBound: 40163, parentId.upperBound: 58494, parentId.nullCount: 0, parentId.count: 10000, parentId.sizeInBytes: 90000, createPerson.lowerBound: initial_zy, createPerson.upperBound: initial_zy, createPerson.nullCount: 0, createPerson.count: 10000, createPerson.sizeInBytes: 140000, createDate.lowerBound: 2018-05-11 19:56:26.0, createDate.upperBound: 2018-05-11 20:03:35.0, createDate.nullCount: 0, createDate.count: 10000, createDate.sizeInBytes: 250000, updatePerson.lowerBound: 1000168, updatePerson.upperBound: ywwu, updatePerson.nullCount: 0, updatePerson.count: 10000, updatePerson.sizeInBytes: 139877, updateDate.lowerBound: 2018-05-11 21:47:36.0, updateDate.upperBound: 2018-05-30 14:49:13.0, updateDate.nullCount: 0, updateDate.count: 10000, updateDate.sizeInBytes: 250000, isDelete.lowerBound: 0, isDelete.upperBound: 0, isDelete.nullCount: 0, isDelete.count: 10000, isDelete.sizeInBytes: 50000, type.lowerBound: point, type.upperBound: point, type.nullCount: 0, type.count: 10000, type.sizeInBytes: 90000, office.lowerBound: 1001, office.upperBound: 982, office.nullCount: 200, office.count: 10000, office.sizeInBytes: 76671, operatorid.lowerBound: 1032, operatorid.upperBound: 8438, operatorid.nullCount: 0, operatorid.count: 10000, operatorid.sizeInBytes: 80000, province.lowerBound: null, province.upperBound: null, province.nullCount: 10000, province.count: 10000, province.sizeInBytes: 40000, city.lowerBound: null, city.upperBound: null, city.nullCount: 10000, city.count: 10000, city.sizeInBytes: 40000, county.lowerBound: null, county.upperBound: null, county.nullCount: 10000, county.count: 10000, county.sizeInBytes: 40000, contactMan.lowerBound: 丁丸洪, contactMan.upperBound: 龙兴元, contactMan.nullCount: 7474, contactMan.count: 10000, contactMan.sizeInBytes: 60559, contactPhone.lowerBound:  15530955333, contactPhone.upperBound: 8860906, contactPhone.nullCount: 7487, contactPhone.count: 10000, contactPhone.sizeInBytes: 67719, address.lowerBound: null, address.upperBound: null, address.nullCount: 10000, address.count: 10000, address.sizeInBytes: 40000, operators.lowerBound:   王甫涛, operators.upperBound: （黄冈中学）武汉鑫安大校园后勤服务有限公司, operators.nullCount: 0, operators.count: 10000, operators.sizeInBytes: 252135, channelTypeCode.lowerBound:  , channelTypeCode.upperBound:  , channelTypeCode.nullCount: 0, channelTypeCode.count: 10000, channelTypeCode.sizeInBytes: 50000, checkStatusCode.lowerBound:  , checkStatusCode.upperBound:  , checkStatusCode.nullCount: 0, checkStatusCode.count: 10000, checkStatusCode.sizeInBytes: 50000, communityId.lowerBound: null, communityId.upperBound: null, communityId.nullCount: 10000, communityId.count: 10000, communityId.sizeInBytes: 40000, lat.lowerBound: 18.215776311561, lat.upperBound: 50.250437650987, lat.nullCount: 78, lat.count: 10000, lat.sizeInBytes: 187551, lng.lowerBound: 100.09117991355, lng.upperBound: 99.17205635701, lng.nullCount: 78, lng.count: 10000, lng.sizeInBytes: 187647, auditor.lowerBound: initial_zy, auditor.upperBound: initial_zy, auditor.nullCount: 0, auditor.count: 10000, auditor.sizeInBytes: 140000, auditDate.lowerBound: null, auditDate.upperBound: null, auditDate.nullCount: 10000, auditDate.count: 10000, auditDate.sizeInBytes: 40000, a25.lowerBound: DS_ZFJVEM_PRD, a25.upperBound: DS_ZFJVEM_PRD, a25.nullCount: 0, a25.count: 10000, a25.sizeInBytes: 170000, a26.lowerBound: job_hsta_vem_nettype, a26.upperBound: job_hsta_vem_nettype, a26.nullCount: 0, a26.count: 10000, a26.sizeInBytes: 240000, a27.lowerBound: vem_nettype, a27.upperBound: vem_nettype, a27.nullCount: 0, a27.count: 10000, a27.sizeInBytes: 150000, a28.lowerBound: 20180530, a28.upperBound: 20180530, a28.nullCount: 0, a28.count: 10000, a28.sizeInBytes: 120000, a29.lowerBound: 2018-05-31 07:30:35.0, a29.upperBound: 2018-05-31 07:30:36.0, a29.nullCount: 0, a29.count: 10000, a29.sizeInBytes: 250000
[INFO][2018-05-31 16:56:23,807][org.apache.spark.executor.Executor]Finished task 1.0 in stage 3.0 (TID 4). 3001 bytes result sent to driver
[INFO][2018-05-31 16:56:23,808][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 3.0 (TID 4) in 3496 ms on localhost (executor driver) (2/2)
[INFO][2018-05-31 16:56:23,809][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 3.0, whose tasks have all completed, from pool 
[INFO][2018-05-31 16:56:23,810][org.apache.spark.scheduler.DAGScheduler]ShuffleMapStage 3 (cache at NetType.scala:161) finished in 5.978 s
[INFO][2018-05-31 16:56:23,810][org.apache.spark.scheduler.DAGScheduler]looking for newly runnable stages
[INFO][2018-05-31 16:56:23,810][org.apache.spark.scheduler.DAGScheduler]running: Set()
[INFO][2018-05-31 16:56:23,810][org.apache.spark.scheduler.DAGScheduler]waiting: Set(ResultStage 5, ShuffleMapStage 4)
[INFO][2018-05-31 16:56:23,810][org.apache.spark.scheduler.DAGScheduler]failed: Set()
[INFO][2018-05-31 16:56:23,811][org.apache.spark.scheduler.DAGScheduler]Submitting ShuffleMapStage 4 (MapPartitionsRDD[46] at csv at NetType.scala:169), which has no missing parents
[INFO][2018-05-31 16:56:23,839][org.apache.spark.storage.memory.MemoryStore]Block broadcast_10 stored as values in memory (estimated size 41.7 KB, free 904.7 MB)
[INFO][2018-05-31 16:56:23,842][org.apache.spark.storage.memory.MemoryStore]Block broadcast_10_piece0 stored as bytes in memory (estimated size 15.1 KB, free 904.7 MB)
[INFO][2018-05-31 16:56:23,843][org.apache.spark.storage.BlockManagerInfo]Added broadcast_10_piece0 in memory on 10.194.32.157:61802 (size: 15.1 KB, free: 905.8 MB)
[INFO][2018-05-31 16:56:23,843][org.apache.spark.SparkContext]Created broadcast 10 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-31 16:56:23,845][org.apache.spark.scheduler.DAGScheduler]Submitting 200 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[46] at csv at NetType.scala:169) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[INFO][2018-05-31 16:56:23,845][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 4.0 with 200 tasks
[INFO][2018-05-31 16:56:23,850][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 4.0 (TID 5, localhost, executor driver, partition 0, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:23,850][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 4.0 (TID 6, localhost, executor driver, partition 1, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:23,850][org.apache.spark.executor.Executor]Running task 0.0 in stage 4.0 (TID 5)
[INFO][2018-05-31 16:56:23,850][org.apache.spark.executor.Executor]Running task 1.0 in stage 4.0 (TID 6)
[INFO][2018-05-31 16:56:23,872][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:23,872][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:23,875][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 7 ms
[INFO][2018-05-31 16:56:23,875][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 7 ms
[INFO][2018-05-31 16:56:23,900][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator]Code generated in 8.474224 ms
[INFO][2018-05-31 16:56:23,933][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator]Code generated in 12.300771 ms
[INFO][2018-05-31 16:56:23,946][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:23,946][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:23,948][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:23,949][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO][2018-05-31 16:56:23,953][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator]Code generated in 5.73274 ms
[INFO][2018-05-31 16:56:23,970][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator]Code generated in 15.342917 ms
[INFO][2018-05-31 16:56:24,012][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_1 stored as values in memory (estimated size 6.8 KB, free 872.5 MB)
[INFO][2018-05-31 16:56:24,013][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_1 in memory on 10.194.32.157:61802 (size: 6.8 KB, free: 905.8 MB)
[INFO][2018-05-31 16:56:24,018][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_0 stored as values in memory (estimated size 8.1 KB, free 904.6 MB)
[INFO][2018-05-31 16:56:24,018][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_0 in memory on 10.194.32.157:61802 (size: 8.1 KB, free: 905.8 MB)
[INFO][2018-05-31 16:56:24,020][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator]Code generated in 4.767565 ms
[INFO][2018-05-31 16:56:24,037][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator]Code generated in 16.383166 ms
[INFO][2018-05-31 16:56:24,046][org.apache.spark.executor.Executor]Finished task 1.0 in stage 4.0 (TID 6). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:24,048][org.apache.spark.scheduler.TaskSetManager]Starting task 2.0 in stage 4.0 (TID 7, localhost, executor driver, partition 2, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:24,049][org.apache.spark.executor.Executor]Running task 2.0 in stage 4.0 (TID 7)
[INFO][2018-05-31 16:56:24,049][org.apache.spark.executor.Executor]Finished task 0.0 in stage 4.0 (TID 5). 3748 bytes result sent to driver
[INFO][2018-05-31 16:56:24,052][org.apache.spark.scheduler.TaskSetManager]Starting task 3.0 in stage 4.0 (TID 8, localhost, executor driver, partition 3, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:24,053][org.apache.spark.executor.Executor]Running task 3.0 in stage 4.0 (TID 8)
[INFO][2018-05-31 16:56:24,055][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 4.0 (TID 6) in 205 ms on localhost (executor driver) (1/200)
[INFO][2018-05-31 16:56:24,055][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 4.0 (TID 5) in 208 ms on localhost (executor driver) (2/200)
[INFO][2018-05-31 16:56:24,057][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:24,057][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:24,058][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:24,058][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:24,059][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:24,059][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:24,059][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:24,059][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:24,065][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_2 stored as values in memory (estimated size 8.8 KB, free 872.5 MB)
[INFO][2018-05-31 16:56:24,066][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_2 in memory on 10.194.32.157:61802 (size: 8.8 KB, free: 905.8 MB)
[INFO][2018-05-31 16:56:24,070][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_3 stored as values in memory (estimated size 7.4 KB, free 904.6 MB)
[INFO][2018-05-31 16:56:24,073][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_3 in memory on 10.194.32.157:61802 (size: 7.4 KB, free: 905.8 MB)
[INFO][2018-05-31 16:56:24,076][org.apache.spark.executor.Executor]Finished task 2.0 in stage 4.0 (TID 7). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:24,077][org.apache.spark.scheduler.TaskSetManager]Starting task 4.0 in stage 4.0 (TID 9, localhost, executor driver, partition 4, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:24,077][org.apache.spark.executor.Executor]Running task 4.0 in stage 4.0 (TID 9)
[INFO][2018-05-31 16:56:24,077][org.apache.spark.scheduler.TaskSetManager]Finished task 2.0 in stage 4.0 (TID 7) in 30 ms on localhost (executor driver) (3/200)
[INFO][2018-05-31 16:56:24,080][org.apache.spark.executor.Executor]Finished task 3.0 in stage 4.0 (TID 8). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:24,080][org.apache.spark.scheduler.TaskSetManager]Starting task 5.0 in stage 4.0 (TID 10, localhost, executor driver, partition 5, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:24,081][org.apache.spark.executor.Executor]Running task 5.0 in stage 4.0 (TID 10)
[INFO][2018-05-31 16:56:24,081][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:24,081][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:24,081][org.apache.spark.scheduler.TaskSetManager]Finished task 3.0 in stage 4.0 (TID 8) in 31 ms on localhost (executor driver) (4/200)
[INFO][2018-05-31 16:56:24,082][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:24,083][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO][2018-05-31 16:56:24,085][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:24,085][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:24,087][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:24,087][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:24,091][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_4 stored as values in memory (estimated size 9.2 KB, free 888.5 MB)
[INFO][2018-05-31 16:56:24,091][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_4 in memory on 10.194.32.157:61802 (size: 9.2 KB, free: 905.8 MB)
[INFO][2018-05-31 16:56:24,096][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_5 stored as values in memory (estimated size 7.5 KB, free 904.6 MB)
[INFO][2018-05-31 16:56:24,096][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_5 in memory on 10.194.32.157:61802 (size: 7.5 KB, free: 905.7 MB)
[INFO][2018-05-31 16:56:24,097][org.apache.spark.executor.Executor]Finished task 4.0 in stage 4.0 (TID 9). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:24,098][org.apache.spark.scheduler.TaskSetManager]Starting task 6.0 in stage 4.0 (TID 11, localhost, executor driver, partition 6, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:24,098][org.apache.spark.executor.Executor]Running task 6.0 in stage 4.0 (TID 11)
[INFO][2018-05-31 16:56:24,098][org.apache.spark.scheduler.TaskSetManager]Finished task 4.0 in stage 4.0 (TID 9) in 22 ms on localhost (executor driver) (5/200)
[INFO][2018-05-31 16:56:24,103][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:24,103][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:24,104][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:24,105][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO][2018-05-31 16:56:24,109][org.apache.spark.executor.Executor]Finished task 5.0 in stage 4.0 (TID 10). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:24,109][org.apache.spark.scheduler.TaskSetManager]Starting task 7.0 in stage 4.0 (TID 12, localhost, executor driver, partition 7, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:24,110][org.apache.spark.scheduler.TaskSetManager]Finished task 5.0 in stage 4.0 (TID 10) in 30 ms on localhost (executor driver) (6/200)
[INFO][2018-05-31 16:56:24,112][org.apache.spark.executor.Executor]Running task 7.0 in stage 4.0 (TID 12)
[INFO][2018-05-31 16:56:24,115][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_6 stored as values in memory (estimated size 9.8 KB, free 904.6 MB)
[INFO][2018-05-31 16:56:24,115][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_6 in memory on 10.194.32.157:61802 (size: 9.8 KB, free: 905.7 MB)
[INFO][2018-05-31 16:56:24,123][org.apache.spark.executor.Executor]Finished task 6.0 in stage 4.0 (TID 11). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:24,123][org.apache.spark.scheduler.TaskSetManager]Starting task 8.0 in stage 4.0 (TID 13, localhost, executor driver, partition 8, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:24,124][org.apache.spark.scheduler.TaskSetManager]Finished task 6.0 in stage 4.0 (TID 11) in 27 ms on localhost (executor driver) (7/200)
[INFO][2018-05-31 16:56:24,124][org.apache.spark.executor.Executor]Running task 8.0 in stage 4.0 (TID 13)
[INFO][2018-05-31 16:56:24,127][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:24,127][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:24,129][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:24,129][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO][2018-05-31 16:56:24,129][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:24,129][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO][2018-05-31 16:56:24,132][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:24,133][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO][2018-05-31 16:56:24,142][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_8 stored as values in memory (estimated size 9.7 KB, free 904.6 MB)
[INFO][2018-05-31 16:56:24,143][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_8 in memory on 10.194.32.157:61802 (size: 9.7 KB, free: 905.7 MB)
[INFO][2018-05-31 16:56:24,151][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_7 stored as values in memory (estimated size 8.3 KB, free 904.6 MB)
[INFO][2018-05-31 16:56:24,152][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_7 in memory on 10.194.32.157:61802 (size: 8.3 KB, free: 905.7 MB)
[INFO][2018-05-31 16:56:24,156][org.apache.spark.executor.Executor]Finished task 8.0 in stage 4.0 (TID 13). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:24,156][org.apache.spark.scheduler.TaskSetManager]Starting task 9.0 in stage 4.0 (TID 14, localhost, executor driver, partition 9, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:24,157][org.apache.spark.executor.Executor]Running task 9.0 in stage 4.0 (TID 14)
[INFO][2018-05-31 16:56:24,157][org.apache.spark.scheduler.TaskSetManager]Finished task 8.0 in stage 4.0 (TID 13) in 34 ms on localhost (executor driver) (8/200)
[INFO][2018-05-31 16:56:24,162][org.apache.spark.executor.Executor]Finished task 7.0 in stage 4.0 (TID 12). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:24,164][org.apache.spark.scheduler.TaskSetManager]Starting task 10.0 in stage 4.0 (TID 15, localhost, executor driver, partition 10, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:24,164][org.apache.spark.scheduler.TaskSetManager]Finished task 7.0 in stage 4.0 (TID 12) in 55 ms on localhost (executor driver) (9/200)
[INFO][2018-05-31 16:56:24,165][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:24,165][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO][2018-05-31 16:56:24,164][org.apache.spark.executor.Executor]Running task 10.0 in stage 4.0 (TID 15)
[INFO][2018-05-31 16:56:24,166][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:24,166][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:24,178][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:24,180][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 2 ms
[INFO][2018-05-31 16:56:24,179][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_9 stored as values in memory (estimated size 7.8 KB, free 904.6 MB)
[INFO][2018-05-31 16:56:24,181][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_9 in memory on 10.194.32.157:61802 (size: 7.8 KB, free: 905.7 MB)
[INFO][2018-05-31 16:56:24,184][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:24,184][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO][2018-05-31 16:56:24,201][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_10 stored as values in memory (estimated size 6.9 KB, free 904.6 MB)
[INFO][2018-05-31 16:56:24,206][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_10 in memory on 10.194.32.157:61802 (size: 6.9 KB, free: 905.7 MB)
[INFO][2018-05-31 16:56:24,237][org.apache.spark.executor.Executor]Finished task 10.0 in stage 4.0 (TID 15). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:24,238][org.apache.spark.executor.Executor]Finished task 9.0 in stage 4.0 (TID 14). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:24,239][org.apache.spark.scheduler.TaskSetManager]Starting task 11.0 in stage 4.0 (TID 16, localhost, executor driver, partition 11, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:24,239][org.apache.spark.scheduler.TaskSetManager]Starting task 12.0 in stage 4.0 (TID 17, localhost, executor driver, partition 12, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:24,239][org.apache.spark.scheduler.TaskSetManager]Finished task 10.0 in stage 4.0 (TID 15) in 75 ms on localhost (executor driver) (10/200)
[INFO][2018-05-31 16:56:24,240][org.apache.spark.executor.Executor]Running task 12.0 in stage 4.0 (TID 17)
[INFO][2018-05-31 16:56:24,240][org.apache.spark.executor.Executor]Running task 11.0 in stage 4.0 (TID 16)
[INFO][2018-05-31 16:56:24,242][org.apache.spark.scheduler.TaskSetManager]Finished task 9.0 in stage 4.0 (TID 14) in 86 ms on localhost (executor driver) (11/200)
[INFO][2018-05-31 16:56:24,256][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:24,256][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:24,258][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:24,258][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:24,261][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:24,261][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:24,262][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:24,262][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:24,265][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_11 stored as values in memory (estimated size 7.5 KB, free 872.4 MB)
[INFO][2018-05-31 16:56:24,266][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_11 in memory on 10.194.32.157:61802 (size: 7.5 KB, free: 905.7 MB)
[INFO][2018-05-31 16:56:24,269][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_12 stored as values in memory (estimated size 7.5 KB, free 904.6 MB)
[INFO][2018-05-31 16:56:24,269][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_12 in memory on 10.194.32.157:61802 (size: 7.5 KB, free: 905.7 MB)
[INFO][2018-05-31 16:56:24,277][org.apache.spark.executor.Executor]Finished task 12.0 in stage 4.0 (TID 17). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:24,277][org.apache.spark.scheduler.TaskSetManager]Starting task 13.0 in stage 4.0 (TID 18, localhost, executor driver, partition 13, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:24,278][org.apache.spark.scheduler.TaskSetManager]Finished task 12.0 in stage 4.0 (TID 17) in 39 ms on localhost (executor driver) (12/200)
[INFO][2018-05-31 16:56:24,278][org.apache.spark.executor.Executor]Running task 13.0 in stage 4.0 (TID 18)
[INFO][2018-05-31 16:56:24,282][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:24,282][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:24,284][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:24,284][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:24,289][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_13 stored as values in memory (estimated size 6.7 KB, free 904.6 MB)
[INFO][2018-05-31 16:56:24,290][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_13 in memory on 10.194.32.157:61802 (size: 6.7 KB, free: 905.7 MB)
[INFO][2018-05-31 16:56:24,293][org.apache.spark.executor.Executor]Finished task 11.0 in stage 4.0 (TID 16). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:24,293][org.apache.spark.scheduler.TaskSetManager]Starting task 14.0 in stage 4.0 (TID 19, localhost, executor driver, partition 14, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:24,293][org.apache.spark.scheduler.TaskSetManager]Finished task 11.0 in stage 4.0 (TID 16) in 55 ms on localhost (executor driver) (13/200)
[INFO][2018-05-31 16:56:24,294][org.apache.spark.executor.Executor]Running task 14.0 in stage 4.0 (TID 19)
[INFO][2018-05-31 16:56:24,296][org.apache.spark.executor.Executor]Finished task 13.0 in stage 4.0 (TID 18). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:24,296][org.apache.spark.scheduler.TaskSetManager]Starting task 15.0 in stage 4.0 (TID 20, localhost, executor driver, partition 15, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:24,297][org.apache.spark.executor.Executor]Running task 15.0 in stage 4.0 (TID 20)
[INFO][2018-05-31 16:56:24,297][org.apache.spark.scheduler.TaskSetManager]Finished task 13.0 in stage 4.0 (TID 18) in 20 ms on localhost (executor driver) (14/200)
[INFO][2018-05-31 16:56:24,301][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:24,301][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:24,302][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:24,303][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO][2018-05-31 16:56:24,309][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_15 stored as values in memory (estimated size 7.4 KB, free 904.5 MB)
[INFO][2018-05-31 16:56:24,309][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_15 in memory on 10.194.32.157:61802 (size: 7.4 KB, free: 905.7 MB)
[INFO][2018-05-31 16:56:24,310][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:24,310][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO][2018-05-31 16:56:24,311][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:24,311][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:24,316][org.apache.spark.executor.Executor]Finished task 15.0 in stage 4.0 (TID 20). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:24,318][org.apache.spark.scheduler.TaskSetManager]Starting task 16.0 in stage 4.0 (TID 21, localhost, executor driver, partition 16, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:24,318][org.apache.spark.executor.Executor]Running task 16.0 in stage 4.0 (TID 21)
[INFO][2018-05-31 16:56:24,318][org.apache.spark.scheduler.TaskSetManager]Finished task 15.0 in stage 4.0 (TID 20) in 22 ms on localhost (executor driver) (15/200)
[INFO][2018-05-31 16:56:24,323][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:24,323][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:24,327][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:24,327][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:24,332][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_14 stored as values in memory (estimated size 8.7 KB, free 904.5 MB)
[INFO][2018-05-31 16:56:24,332][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_14 in memory on 10.194.32.157:61802 (size: 8.7 KB, free: 905.7 MB)
[INFO][2018-05-31 16:56:24,345][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_16 stored as values in memory (estimated size 8.7 KB, free 904.5 MB)
[INFO][2018-05-31 16:56:24,345][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_16 in memory on 10.194.32.157:61802 (size: 8.7 KB, free: 905.7 MB)
[INFO][2018-05-31 16:56:24,357][org.apache.spark.executor.Executor]Finished task 14.0 in stage 4.0 (TID 19). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:24,357][org.apache.spark.scheduler.TaskSetManager]Starting task 17.0 in stage 4.0 (TID 22, localhost, executor driver, partition 17, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:24,357][org.apache.spark.scheduler.TaskSetManager]Finished task 14.0 in stage 4.0 (TID 19) in 64 ms on localhost (executor driver) (16/200)
[INFO][2018-05-31 16:56:24,358][org.apache.spark.executor.Executor]Running task 17.0 in stage 4.0 (TID 22)
[INFO][2018-05-31 16:56:24,369][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:24,370][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO][2018-05-31 16:56:24,374][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:24,374][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:24,378][org.apache.spark.executor.Executor]Finished task 16.0 in stage 4.0 (TID 21). 3748 bytes result sent to driver
[INFO][2018-05-31 16:56:24,378][org.apache.spark.scheduler.TaskSetManager]Starting task 18.0 in stage 4.0 (TID 23, localhost, executor driver, partition 18, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:24,380][org.apache.spark.scheduler.TaskSetManager]Finished task 16.0 in stage 4.0 (TID 21) in 62 ms on localhost (executor driver) (17/200)
[INFO][2018-05-31 16:56:24,385][org.apache.spark.executor.Executor]Running task 18.0 in stage 4.0 (TID 23)
[INFO][2018-05-31 16:56:24,388][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_17 stored as values in memory (estimated size 9.4 KB, free 904.5 MB)
[INFO][2018-05-31 16:56:24,388][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_17 in memory on 10.194.32.157:61802 (size: 9.4 KB, free: 905.6 MB)
[INFO][2018-05-31 16:56:24,391][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:24,391][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:24,393][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:24,393][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:24,405][org.apache.spark.executor.Executor]Finished task 17.0 in stage 4.0 (TID 22). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:24,406][org.apache.spark.scheduler.TaskSetManager]Starting task 19.0 in stage 4.0 (TID 24, localhost, executor driver, partition 19, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:24,406][org.apache.spark.executor.Executor]Running task 19.0 in stage 4.0 (TID 24)
[INFO][2018-05-31 16:56:24,406][org.apache.spark.scheduler.TaskSetManager]Finished task 17.0 in stage 4.0 (TID 22) in 49 ms on localhost (executor driver) (18/200)
[INFO][2018-05-31 16:56:24,418][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:24,418][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:24,431][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:24,432][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO][2018-05-31 16:56:24,432][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_18 stored as values in memory (estimated size 8.6 KB, free 904.4 MB)
[INFO][2018-05-31 16:56:24,433][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_18 in memory on 10.194.32.157:61802 (size: 8.6 KB, free: 905.6 MB)
[INFO][2018-05-31 16:56:24,451][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_19 stored as values in memory (estimated size 8.4 KB, free 904.5 MB)
[INFO][2018-05-31 16:56:24,455][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_19 in memory on 10.194.32.157:61802 (size: 8.4 KB, free: 905.6 MB)
[INFO][2018-05-31 16:56:24,481][org.apache.spark.executor.Executor]Finished task 18.0 in stage 4.0 (TID 23). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:24,481][org.apache.spark.scheduler.TaskSetManager]Starting task 20.0 in stage 4.0 (TID 25, localhost, executor driver, partition 20, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:24,481][org.apache.spark.executor.Executor]Running task 20.0 in stage 4.0 (TID 25)
[INFO][2018-05-31 16:56:24,481][org.apache.spark.scheduler.TaskSetManager]Finished task 18.0 in stage 4.0 (TID 23) in 103 ms on localhost (executor driver) (19/200)
[INFO][2018-05-31 16:56:24,486][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:24,486][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:24,488][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:24,488][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:24,514][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_20 stored as values in memory (estimated size 9.6 KB, free 904.5 MB)
[INFO][2018-05-31 16:56:24,514][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_20 in memory on 10.194.32.157:61802 (size: 9.6 KB, free: 905.6 MB)
[INFO][2018-05-31 16:56:24,524][org.apache.spark.executor.Executor]Finished task 20.0 in stage 4.0 (TID 25). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:24,525][org.apache.spark.scheduler.TaskSetManager]Starting task 21.0 in stage 4.0 (TID 26, localhost, executor driver, partition 21, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:24,525][org.apache.spark.scheduler.TaskSetManager]Finished task 20.0 in stage 4.0 (TID 25) in 44 ms on localhost (executor driver) (20/200)
[INFO][2018-05-31 16:56:24,525][org.apache.spark.executor.Executor]Running task 21.0 in stage 4.0 (TID 26)
[INFO][2018-05-31 16:56:24,538][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:24,538][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:24,542][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:24,542][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:24,542][org.apache.spark.executor.Executor]Finished task 19.0 in stage 4.0 (TID 24). 3748 bytes result sent to driver
[INFO][2018-05-31 16:56:24,543][org.apache.spark.scheduler.TaskSetManager]Starting task 22.0 in stage 4.0 (TID 27, localhost, executor driver, partition 22, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:24,543][org.apache.spark.scheduler.TaskSetManager]Finished task 19.0 in stage 4.0 (TID 24) in 138 ms on localhost (executor driver) (21/200)
[INFO][2018-05-31 16:56:24,543][org.apache.spark.executor.Executor]Running task 22.0 in stage 4.0 (TID 27)
[INFO][2018-05-31 16:56:24,553][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_21 stored as values in memory (estimated size 8.3 KB, free 904.5 MB)
[INFO][2018-05-31 16:56:24,557][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_21 in memory on 10.194.32.157:61802 (size: 8.3 KB, free: 905.6 MB)
[INFO][2018-05-31 16:56:24,564][org.apache.spark.executor.Executor]Finished task 21.0 in stage 4.0 (TID 26). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:24,564][org.apache.spark.scheduler.TaskSetManager]Starting task 23.0 in stage 4.0 (TID 28, localhost, executor driver, partition 23, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:24,565][org.apache.spark.executor.Executor]Running task 23.0 in stage 4.0 (TID 28)
[INFO][2018-05-31 16:56:24,565][org.apache.spark.scheduler.TaskSetManager]Finished task 21.0 in stage 4.0 (TID 26) in 41 ms on localhost (executor driver) (22/200)
[INFO][2018-05-31 16:56:24,566][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:24,566][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO][2018-05-31 16:56:24,568][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:24,569][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO][2018-05-31 16:56:24,572][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:24,572][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:24,573][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:24,573][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:24,590][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_23 stored as values in memory (estimated size 8.9 KB, free 872.3 MB)
[INFO][2018-05-31 16:56:24,591][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_23 in memory on 10.194.32.157:61802 (size: 8.9 KB, free: 905.6 MB)
[INFO][2018-05-31 16:56:24,602][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_22 stored as values in memory (estimated size 7.5 KB, free 904.5 MB)
[INFO][2018-05-31 16:56:24,603][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_22 in memory on 10.194.32.157:61802 (size: 7.5 KB, free: 905.6 MB)
[INFO][2018-05-31 16:56:24,609][org.apache.spark.executor.Executor]Finished task 22.0 in stage 4.0 (TID 27). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:24,609][org.apache.spark.scheduler.TaskSetManager]Starting task 24.0 in stage 4.0 (TID 29, localhost, executor driver, partition 24, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:24,609][org.apache.spark.executor.Executor]Running task 24.0 in stage 4.0 (TID 29)
[INFO][2018-05-31 16:56:24,609][org.apache.spark.scheduler.TaskSetManager]Finished task 22.0 in stage 4.0 (TID 27) in 67 ms on localhost (executor driver) (23/200)
[INFO][2018-05-31 16:56:24,618][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:24,619][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO][2018-05-31 16:56:24,620][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:24,620][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:24,923][org.apache.spark.ContextCleaner]Cleaned accumulator 56
[INFO][2018-05-31 16:56:24,923][org.apache.spark.ContextCleaner]Cleaned accumulator 58
[INFO][2018-05-31 16:56:24,924][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_2_piece0 on 10.194.32.157:61802 in memory (size: 22.7 KB, free: 905.6 MB)
[INFO][2018-05-31 16:56:24,924][org.apache.spark.ContextCleaner]Cleaned accumulator 5
[INFO][2018-05-31 16:56:24,925][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_0_piece0 on 10.194.32.157:61802 in memory (size: 22.7 KB, free: 905.6 MB)
[INFO][2018-05-31 16:56:24,925][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_24 stored as values in memory (estimated size 8.6 KB, free 904.8 MB)
[INFO][2018-05-31 16:56:24,925][org.apache.spark.ContextCleaner]Cleaned accumulator 54
[INFO][2018-05-31 16:56:24,925][org.apache.spark.ContextCleaner]Cleaned accumulator 0
[INFO][2018-05-31 16:56:24,925][org.apache.spark.ContextCleaner]Cleaned accumulator 3
[INFO][2018-05-31 16:56:24,925][org.apache.spark.ContextCleaner]Cleaned accumulator 4
[INFO][2018-05-31 16:56:24,926][org.apache.spark.ContextCleaner]Cleaned accumulator 2
[INFO][2018-05-31 16:56:24,926][org.apache.spark.ContextCleaner]Cleaned accumulator 1
[INFO][2018-05-31 16:56:24,926][org.apache.spark.ContextCleaner]Cleaned accumulator 57
[INFO][2018-05-31 16:56:24,926][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_24 in memory on 10.194.32.157:61802 (size: 8.6 KB, free: 905.6 MB)
[INFO][2018-05-31 16:56:24,933][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_9_piece0 on 10.194.32.157:61802 in memory (size: 23.7 KB, free: 905.7 MB)
[INFO][2018-05-31 16:56:24,936][org.apache.spark.ContextCleaner]Cleaned accumulator 55
[INFO][2018-05-31 16:56:24,941][org.apache.spark.executor.Executor]Finished task 24.0 in stage 4.0 (TID 29). 3791 bytes result sent to driver
[INFO][2018-05-31 16:56:24,941][org.apache.spark.scheduler.TaskSetManager]Starting task 25.0 in stage 4.0 (TID 30, localhost, executor driver, partition 25, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:24,942][org.apache.spark.scheduler.TaskSetManager]Finished task 24.0 in stage 4.0 (TID 29) in 333 ms on localhost (executor driver) (24/200)
[INFO][2018-05-31 16:56:24,942][org.apache.spark.executor.Executor]Running task 25.0 in stage 4.0 (TID 30)
[INFO][2018-05-31 16:56:24,945][org.apache.spark.executor.Executor]Finished task 23.0 in stage 4.0 (TID 28). 3791 bytes result sent to driver
[INFO][2018-05-31 16:56:24,945][org.apache.spark.scheduler.TaskSetManager]Starting task 26.0 in stage 4.0 (TID 31, localhost, executor driver, partition 26, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:24,946][org.apache.spark.executor.Executor]Running task 26.0 in stage 4.0 (TID 31)
[INFO][2018-05-31 16:56:24,946][org.apache.spark.scheduler.TaskSetManager]Finished task 23.0 in stage 4.0 (TID 28) in 382 ms on localhost (executor driver) (25/200)
[INFO][2018-05-31 16:56:24,947][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:24,947][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:24,948][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:24,948][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:24,949][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:24,949][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:24,950][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:24,950][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:24,954][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_26 stored as values in memory (estimated size 7.4 KB, free 889.0 MB)
[INFO][2018-05-31 16:56:24,954][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_26 in memory on 10.194.32.157:61802 (size: 7.4 KB, free: 905.7 MB)
[INFO][2018-05-31 16:56:24,957][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_25 stored as values in memory (estimated size 8.8 KB, free 905.1 MB)
[INFO][2018-05-31 16:56:24,957][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_25 in memory on 10.194.32.157:61802 (size: 8.8 KB, free: 905.6 MB)
[INFO][2018-05-31 16:56:24,959][org.apache.spark.executor.Executor]Finished task 26.0 in stage 4.0 (TID 31). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:24,960][org.apache.spark.scheduler.TaskSetManager]Starting task 27.0 in stage 4.0 (TID 32, localhost, executor driver, partition 27, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:24,960][org.apache.spark.executor.Executor]Running task 27.0 in stage 4.0 (TID 32)
[INFO][2018-05-31 16:56:24,961][org.apache.spark.scheduler.TaskSetManager]Finished task 26.0 in stage 4.0 (TID 31) in 15 ms on localhost (executor driver) (26/200)
[INFO][2018-05-31 16:56:24,963][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:24,963][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:24,965][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:24,965][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO][2018-05-31 16:56:24,965][org.apache.spark.executor.Executor]Finished task 25.0 in stage 4.0 (TID 30). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:24,965][org.apache.spark.scheduler.TaskSetManager]Starting task 28.0 in stage 4.0 (TID 33, localhost, executor driver, partition 28, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:24,966][org.apache.spark.scheduler.TaskSetManager]Finished task 25.0 in stage 4.0 (TID 30) in 25 ms on localhost (executor driver) (27/200)
[INFO][2018-05-31 16:56:24,969][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_27 stored as values in memory (estimated size 9.2 KB, free 905.1 MB)
[INFO][2018-05-31 16:56:24,970][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_27 in memory on 10.194.32.157:61802 (size: 9.2 KB, free: 905.6 MB)
[INFO][2018-05-31 16:56:24,980][org.apache.spark.executor.Executor]Running task 28.0 in stage 4.0 (TID 33)
[INFO][2018-05-31 16:56:24,985][org.apache.spark.executor.Executor]Finished task 27.0 in stage 4.0 (TID 32). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:24,985][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:24,985][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:24,985][org.apache.spark.scheduler.TaskSetManager]Starting task 29.0 in stage 4.0 (TID 34, localhost, executor driver, partition 29, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:24,985][org.apache.spark.executor.Executor]Running task 29.0 in stage 4.0 (TID 34)
[INFO][2018-05-31 16:56:24,985][org.apache.spark.scheduler.TaskSetManager]Finished task 27.0 in stage 4.0 (TID 32) in 25 ms on localhost (executor driver) (28/200)
[INFO][2018-05-31 16:56:24,986][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:24,986][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:24,990][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_28 stored as values in memory (estimated size 8.6 KB, free 905.1 MB)
[INFO][2018-05-31 16:56:24,990][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_28 in memory on 10.194.32.157:61802 (size: 8.6 KB, free: 905.6 MB)
[INFO][2018-05-31 16:56:24,991][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:24,991][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:24,993][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:24,993][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:24,997][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_29 stored as values in memory (estimated size 8.6 KB, free 905.1 MB)
[INFO][2018-05-31 16:56:24,998][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_29 in memory on 10.194.32.157:61802 (size: 8.6 KB, free: 905.6 MB)
[INFO][2018-05-31 16:56:25,010][org.apache.spark.executor.Executor]Finished task 29.0 in stage 4.0 (TID 34). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:25,011][org.apache.spark.executor.Executor]Finished task 28.0 in stage 4.0 (TID 33). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:25,011][org.apache.spark.scheduler.TaskSetManager]Starting task 30.0 in stage 4.0 (TID 35, localhost, executor driver, partition 30, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:25,011][org.apache.spark.executor.Executor]Running task 30.0 in stage 4.0 (TID 35)
[INFO][2018-05-31 16:56:25,011][org.apache.spark.scheduler.TaskSetManager]Starting task 31.0 in stage 4.0 (TID 36, localhost, executor driver, partition 31, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:25,012][org.apache.spark.scheduler.TaskSetManager]Finished task 29.0 in stage 4.0 (TID 34) in 26 ms on localhost (executor driver) (29/200)
[INFO][2018-05-31 16:56:25,012][org.apache.spark.scheduler.TaskSetManager]Finished task 28.0 in stage 4.0 (TID 33) in 47 ms on localhost (executor driver) (30/200)
[INFO][2018-05-31 16:56:25,012][org.apache.spark.executor.Executor]Running task 31.0 in stage 4.0 (TID 36)
[INFO][2018-05-31 16:56:25,015][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:25,015][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,015][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:25,015][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,016][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:25,016][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,017][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:25,017][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,020][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_31 stored as values in memory (estimated size 8.4 KB, free 872.9 MB)
[INFO][2018-05-31 16:56:25,020][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_31 in memory on 10.194.32.157:61802 (size: 8.4 KB, free: 905.6 MB)
[INFO][2018-05-31 16:56:25,022][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_30 stored as values in memory (estimated size 7.1 KB, free 905.1 MB)
[INFO][2018-05-31 16:56:25,022][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_30 in memory on 10.194.32.157:61802 (size: 7.1 KB, free: 905.6 MB)
[INFO][2018-05-31 16:56:25,025][org.apache.spark.executor.Executor]Finished task 31.0 in stage 4.0 (TID 36). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:25,026][org.apache.spark.scheduler.TaskSetManager]Starting task 32.0 in stage 4.0 (TID 37, localhost, executor driver, partition 32, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:25,026][org.apache.spark.executor.Executor]Running task 32.0 in stage 4.0 (TID 37)
[INFO][2018-05-31 16:56:25,026][org.apache.spark.scheduler.TaskSetManager]Finished task 31.0 in stage 4.0 (TID 36) in 15 ms on localhost (executor driver) (31/200)
[INFO][2018-05-31 16:56:25,030][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:25,030][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,031][org.apache.spark.executor.Executor]Finished task 30.0 in stage 4.0 (TID 35). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:25,031][org.apache.spark.scheduler.TaskSetManager]Starting task 33.0 in stage 4.0 (TID 38, localhost, executor driver, partition 33, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:25,031][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:25,032][org.apache.spark.scheduler.TaskSetManager]Finished task 30.0 in stage 4.0 (TID 35) in 20 ms on localhost (executor driver) (32/200)
[INFO][2018-05-31 16:56:25,032][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO][2018-05-31 16:56:25,033][org.apache.spark.executor.Executor]Running task 33.0 in stage 4.0 (TID 38)
[INFO][2018-05-31 16:56:25,036][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_32 stored as values in memory (estimated size 8.3 KB, free 905.0 MB)
[INFO][2018-05-31 16:56:25,036][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_32 in memory on 10.194.32.157:61802 (size: 8.3 KB, free: 905.6 MB)
[INFO][2018-05-31 16:56:25,036][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:25,037][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO][2018-05-31 16:56:25,038][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:25,038][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO][2018-05-31 16:56:25,041][org.apache.spark.executor.Executor]Finished task 32.0 in stage 4.0 (TID 37). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:25,042][org.apache.spark.scheduler.TaskSetManager]Starting task 34.0 in stage 4.0 (TID 39, localhost, executor driver, partition 34, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:25,042][org.apache.spark.scheduler.TaskSetManager]Finished task 32.0 in stage 4.0 (TID 37) in 16 ms on localhost (executor driver) (33/200)
[INFO][2018-05-31 16:56:25,042][org.apache.spark.executor.Executor]Running task 34.0 in stage 4.0 (TID 39)
[INFO][2018-05-31 16:56:25,043][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_33 stored as values in memory (estimated size 8.8 KB, free 905.0 MB)
[INFO][2018-05-31 16:56:25,043][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_33 in memory on 10.194.32.157:61802 (size: 8.8 KB, free: 905.6 MB)
[INFO][2018-05-31 16:56:25,045][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:25,045][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,046][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:25,046][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,051][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_34 stored as values in memory (estimated size 7.8 KB, free 905.0 MB)
[INFO][2018-05-31 16:56:25,051][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_34 in memory on 10.194.32.157:61802 (size: 7.8 KB, free: 905.6 MB)
[INFO][2018-05-31 16:56:25,052][org.apache.spark.executor.Executor]Finished task 33.0 in stage 4.0 (TID 38). 3748 bytes result sent to driver
[INFO][2018-05-31 16:56:25,052][org.apache.spark.scheduler.TaskSetManager]Starting task 35.0 in stage 4.0 (TID 40, localhost, executor driver, partition 35, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:25,052][org.apache.spark.executor.Executor]Running task 35.0 in stage 4.0 (TID 40)
[INFO][2018-05-31 16:56:25,053][org.apache.spark.scheduler.TaskSetManager]Finished task 33.0 in stage 4.0 (TID 38) in 22 ms on localhost (executor driver) (34/200)
[INFO][2018-05-31 16:56:25,057][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:25,057][org.apache.spark.executor.Executor]Finished task 34.0 in stage 4.0 (TID 39). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:25,057][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,058][org.apache.spark.scheduler.TaskSetManager]Starting task 36.0 in stage 4.0 (TID 41, localhost, executor driver, partition 36, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:25,059][org.apache.spark.executor.Executor]Running task 36.0 in stage 4.0 (TID 41)
[INFO][2018-05-31 16:56:25,059][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:25,059][org.apache.spark.scheduler.TaskSetManager]Finished task 34.0 in stage 4.0 (TID 39) in 17 ms on localhost (executor driver) (35/200)
[INFO][2018-05-31 16:56:25,059][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,061][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:25,061][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,063][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_35 stored as values in memory (estimated size 6.5 KB, free 905.0 MB)
[INFO][2018-05-31 16:56:25,063][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:25,063][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,063][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_35 in memory on 10.194.32.157:61802 (size: 6.5 KB, free: 905.6 MB)
[INFO][2018-05-31 16:56:25,067][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_36 stored as values in memory (estimated size 9.0 KB, free 905.0 MB)
[INFO][2018-05-31 16:56:25,067][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_36 in memory on 10.194.32.157:61802 (size: 9.0 KB, free: 905.6 MB)
[INFO][2018-05-31 16:56:25,069][org.apache.spark.executor.Executor]Finished task 35.0 in stage 4.0 (TID 40). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:25,070][org.apache.spark.scheduler.TaskSetManager]Starting task 37.0 in stage 4.0 (TID 42, localhost, executor driver, partition 37, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:25,070][org.apache.spark.scheduler.TaskSetManager]Finished task 35.0 in stage 4.0 (TID 40) in 18 ms on localhost (executor driver) (36/200)
[INFO][2018-05-31 16:56:25,070][org.apache.spark.executor.Executor]Running task 37.0 in stage 4.0 (TID 42)
[INFO][2018-05-31 16:56:25,071][org.apache.spark.executor.Executor]Finished task 36.0 in stage 4.0 (TID 41). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:25,072][org.apache.spark.scheduler.TaskSetManager]Starting task 38.0 in stage 4.0 (TID 43, localhost, executor driver, partition 38, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:25,072][org.apache.spark.scheduler.TaskSetManager]Finished task 36.0 in stage 4.0 (TID 41) in 14 ms on localhost (executor driver) (37/200)
[INFO][2018-05-31 16:56:25,072][org.apache.spark.executor.Executor]Running task 38.0 in stage 4.0 (TID 43)
[INFO][2018-05-31 16:56:25,074][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:25,074][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,075][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:25,075][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:25,075][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,075][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,076][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:25,076][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,078][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_37 stored as values in memory (estimated size 8.6 KB, free 872.9 MB)
[INFO][2018-05-31 16:56:25,079][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_37 in memory on 10.194.32.157:61802 (size: 8.6 KB, free: 905.6 MB)
[INFO][2018-05-31 16:56:25,079][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_38 stored as values in memory (estimated size 8.4 KB, free 905.0 MB)
[INFO][2018-05-31 16:56:25,079][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_38 in memory on 10.194.32.157:61802 (size: 8.4 KB, free: 905.5 MB)
[INFO][2018-05-31 16:56:25,085][org.apache.spark.executor.Executor]Finished task 37.0 in stage 4.0 (TID 42). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:25,086][org.apache.spark.scheduler.TaskSetManager]Starting task 39.0 in stage 4.0 (TID 44, localhost, executor driver, partition 39, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:25,086][org.apache.spark.scheduler.TaskSetManager]Finished task 37.0 in stage 4.0 (TID 42) in 17 ms on localhost (executor driver) (38/200)
[INFO][2018-05-31 16:56:25,086][org.apache.spark.executor.Executor]Finished task 38.0 in stage 4.0 (TID 43). 3748 bytes result sent to driver
[INFO][2018-05-31 16:56:25,086][org.apache.spark.executor.Executor]Running task 39.0 in stage 4.0 (TID 44)
[INFO][2018-05-31 16:56:25,086][org.apache.spark.scheduler.TaskSetManager]Starting task 40.0 in stage 4.0 (TID 45, localhost, executor driver, partition 40, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:25,087][org.apache.spark.scheduler.TaskSetManager]Finished task 38.0 in stage 4.0 (TID 43) in 16 ms on localhost (executor driver) (39/200)
[INFO][2018-05-31 16:56:25,087][org.apache.spark.executor.Executor]Running task 40.0 in stage 4.0 (TID 45)
[INFO][2018-05-31 16:56:25,091][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:25,091][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,091][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:25,092][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO][2018-05-31 16:56:25,092][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:25,092][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,092][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:25,092][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,095][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_40 stored as values in memory (estimated size 8.5 KB, free 905.0 MB)
[INFO][2018-05-31 16:56:25,095][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_39 stored as values in memory (estimated size 8.5 KB, free 905.0 MB)
[INFO][2018-05-31 16:56:25,095][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_40 in memory on 10.194.32.157:61802 (size: 8.5 KB, free: 905.5 MB)
[INFO][2018-05-31 16:56:25,096][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_39 in memory on 10.194.32.157:61802 (size: 8.5 KB, free: 905.5 MB)
[INFO][2018-05-31 16:56:25,101][org.apache.spark.executor.Executor]Finished task 39.0 in stage 4.0 (TID 44). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:25,102][org.apache.spark.scheduler.TaskSetManager]Starting task 41.0 in stage 4.0 (TID 46, localhost, executor driver, partition 41, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:25,102][org.apache.spark.scheduler.TaskSetManager]Finished task 39.0 in stage 4.0 (TID 44) in 16 ms on localhost (executor driver) (40/200)
[INFO][2018-05-31 16:56:25,102][org.apache.spark.executor.Executor]Running task 41.0 in stage 4.0 (TID 46)
[INFO][2018-05-31 16:56:25,108][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:25,108][org.apache.spark.executor.Executor]Finished task 40.0 in stage 4.0 (TID 45). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:25,108][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,109][org.apache.spark.scheduler.TaskSetManager]Starting task 42.0 in stage 4.0 (TID 47, localhost, executor driver, partition 42, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:25,109][org.apache.spark.scheduler.TaskSetManager]Finished task 40.0 in stage 4.0 (TID 45) in 23 ms on localhost (executor driver) (41/200)
[INFO][2018-05-31 16:56:25,110][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:25,110][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,114][org.apache.spark.executor.Executor]Running task 42.0 in stage 4.0 (TID 47)
[INFO][2018-05-31 16:56:25,114][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_41 stored as values in memory (estimated size 8.6 KB, free 905.0 MB)
[INFO][2018-05-31 16:56:25,114][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_41 in memory on 10.194.32.157:61802 (size: 8.6 KB, free: 905.5 MB)
[INFO][2018-05-31 16:56:25,119][org.apache.spark.executor.Executor]Finished task 41.0 in stage 4.0 (TID 46). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:25,120][org.apache.spark.scheduler.TaskSetManager]Starting task 43.0 in stage 4.0 (TID 48, localhost, executor driver, partition 43, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:25,120][org.apache.spark.executor.Executor]Running task 43.0 in stage 4.0 (TID 48)
[INFO][2018-05-31 16:56:25,120][org.apache.spark.scheduler.TaskSetManager]Finished task 41.0 in stage 4.0 (TID 46) in 19 ms on localhost (executor driver) (42/200)
[INFO][2018-05-31 16:56:25,122][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:25,122][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,123][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:25,124][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO][2018-05-31 16:56:25,124][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:25,124][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,125][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:25,125][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,127][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_42 stored as values in memory (estimated size 7.2 KB, free 872.8 MB)
[INFO][2018-05-31 16:56:25,127][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_42 in memory on 10.194.32.157:61802 (size: 7.2 KB, free: 905.5 MB)
[INFO][2018-05-31 16:56:25,129][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_43 stored as values in memory (estimated size 8.4 KB, free 905.0 MB)
[INFO][2018-05-31 16:56:25,129][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_43 in memory on 10.194.32.157:61802 (size: 8.4 KB, free: 905.5 MB)
[INFO][2018-05-31 16:56:25,134][org.apache.spark.executor.Executor]Finished task 43.0 in stage 4.0 (TID 48). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:25,134][org.apache.spark.scheduler.TaskSetManager]Starting task 44.0 in stage 4.0 (TID 49, localhost, executor driver, partition 44, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:25,135][org.apache.spark.scheduler.TaskSetManager]Finished task 43.0 in stage 4.0 (TID 48) in 16 ms on localhost (executor driver) (43/200)
[INFO][2018-05-31 16:56:25,135][org.apache.spark.executor.Executor]Running task 44.0 in stage 4.0 (TID 49)
[INFO][2018-05-31 16:56:25,138][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:25,138][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,139][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:25,139][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,140][org.apache.spark.executor.Executor]Finished task 42.0 in stage 4.0 (TID 47). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:25,141][org.apache.spark.scheduler.TaskSetManager]Starting task 45.0 in stage 4.0 (TID 50, localhost, executor driver, partition 45, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:25,141][org.apache.spark.scheduler.TaskSetManager]Finished task 42.0 in stage 4.0 (TID 47) in 32 ms on localhost (executor driver) (44/200)
[INFO][2018-05-31 16:56:25,141][org.apache.spark.executor.Executor]Running task 45.0 in stage 4.0 (TID 50)
[INFO][2018-05-31 16:56:25,144][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_44 stored as values in memory (estimated size 8.3 KB, free 904.9 MB)
[INFO][2018-05-31 16:56:25,144][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_44 in memory on 10.194.32.157:61802 (size: 8.3 KB, free: 905.5 MB)
[INFO][2018-05-31 16:56:25,147][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:25,147][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,151][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:25,151][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,154][org.apache.spark.executor.Executor]Finished task 44.0 in stage 4.0 (TID 49). 3748 bytes result sent to driver
[INFO][2018-05-31 16:56:25,154][org.apache.spark.scheduler.TaskSetManager]Starting task 46.0 in stage 4.0 (TID 51, localhost, executor driver, partition 46, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:25,155][org.apache.spark.executor.Executor]Running task 46.0 in stage 4.0 (TID 51)
[INFO][2018-05-31 16:56:25,155][org.apache.spark.scheduler.TaskSetManager]Finished task 44.0 in stage 4.0 (TID 49) in 21 ms on localhost (executor driver) (45/200)
[INFO][2018-05-31 16:56:25,157][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_45 stored as values in memory (estimated size 8.4 KB, free 904.9 MB)
[INFO][2018-05-31 16:56:25,157][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_45 in memory on 10.194.32.157:61802 (size: 8.4 KB, free: 905.5 MB)
[INFO][2018-05-31 16:56:25,158][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:25,158][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,159][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:25,159][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,169][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_46 stored as values in memory (estimated size 7.4 KB, free 904.9 MB)
[INFO][2018-05-31 16:56:25,170][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_46 in memory on 10.194.32.157:61802 (size: 7.4 KB, free: 905.5 MB)
[INFO][2018-05-31 16:56:25,170][org.apache.spark.executor.Executor]Finished task 45.0 in stage 4.0 (TID 50). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:25,171][org.apache.spark.scheduler.TaskSetManager]Starting task 47.0 in stage 4.0 (TID 52, localhost, executor driver, partition 47, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:25,171][org.apache.spark.executor.Executor]Running task 47.0 in stage 4.0 (TID 52)
[INFO][2018-05-31 16:56:25,171][org.apache.spark.scheduler.TaskSetManager]Finished task 45.0 in stage 4.0 (TID 50) in 30 ms on localhost (executor driver) (46/200)
[INFO][2018-05-31 16:56:25,173][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:25,174][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO][2018-05-31 16:56:25,176][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:25,176][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,178][org.apache.spark.executor.Executor]Finished task 46.0 in stage 4.0 (TID 51). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:25,181][org.apache.spark.scheduler.TaskSetManager]Starting task 48.0 in stage 4.0 (TID 53, localhost, executor driver, partition 48, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:25,183][org.apache.spark.scheduler.TaskSetManager]Finished task 46.0 in stage 4.0 (TID 51) in 29 ms on localhost (executor driver) (47/200)
[INFO][2018-05-31 16:56:25,183][org.apache.spark.executor.Executor]Running task 48.0 in stage 4.0 (TID 53)
[INFO][2018-05-31 16:56:25,187][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:25,187][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,187][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_47 stored as values in memory (estimated size 7.7 KB, free 904.9 MB)
[INFO][2018-05-31 16:56:25,188][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_47 in memory on 10.194.32.157:61802 (size: 7.7 KB, free: 905.5 MB)
[INFO][2018-05-31 16:56:25,190][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:25,190][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO][2018-05-31 16:56:25,197][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_48 stored as values in memory (estimated size 8.7 KB, free 904.9 MB)
[INFO][2018-05-31 16:56:25,197][org.apache.spark.executor.Executor]Finished task 47.0 in stage 4.0 (TID 52). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:25,197][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_48 in memory on 10.194.32.157:61802 (size: 8.7 KB, free: 905.5 MB)
[INFO][2018-05-31 16:56:25,197][org.apache.spark.scheduler.TaskSetManager]Starting task 49.0 in stage 4.0 (TID 54, localhost, executor driver, partition 49, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:25,197][org.apache.spark.executor.Executor]Running task 49.0 in stage 4.0 (TID 54)
[INFO][2018-05-31 16:56:25,198][org.apache.spark.scheduler.TaskSetManager]Finished task 47.0 in stage 4.0 (TID 52) in 27 ms on localhost (executor driver) (48/200)
[INFO][2018-05-31 16:56:25,203][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:25,203][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,204][org.apache.spark.executor.Executor]Finished task 48.0 in stage 4.0 (TID 53). 3748 bytes result sent to driver
[INFO][2018-05-31 16:56:25,205][org.apache.spark.scheduler.TaskSetManager]Starting task 50.0 in stage 4.0 (TID 55, localhost, executor driver, partition 50, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:25,206][org.apache.spark.scheduler.TaskSetManager]Finished task 48.0 in stage 4.0 (TID 53) in 25 ms on localhost (executor driver) (49/200)
[INFO][2018-05-31 16:56:25,206][org.apache.spark.executor.Executor]Running task 50.0 in stage 4.0 (TID 55)
[INFO][2018-05-31 16:56:25,207][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:25,207][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,209][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:25,209][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,211][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:25,211][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,213][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_49 stored as values in memory (estimated size 7.8 KB, free 888.8 MB)
[INFO][2018-05-31 16:56:25,213][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_49 in memory on 10.194.32.157:61802 (size: 7.8 KB, free: 905.5 MB)
[INFO][2018-05-31 16:56:25,214][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_50 stored as values in memory (estimated size 8.6 KB, free 904.9 MB)
[INFO][2018-05-31 16:56:25,214][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_50 in memory on 10.194.32.157:61802 (size: 8.6 KB, free: 905.4 MB)
[INFO][2018-05-31 16:56:25,225][org.apache.spark.executor.Executor]Finished task 50.0 in stage 4.0 (TID 55). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:25,226][org.apache.spark.scheduler.TaskSetManager]Starting task 51.0 in stage 4.0 (TID 56, localhost, executor driver, partition 51, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:25,227][org.apache.spark.scheduler.TaskSetManager]Finished task 50.0 in stage 4.0 (TID 55) in 23 ms on localhost (executor driver) (50/200)
[INFO][2018-05-31 16:56:25,227][org.apache.spark.executor.Executor]Running task 51.0 in stage 4.0 (TID 56)
[INFO][2018-05-31 16:56:25,230][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:25,230][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,231][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:25,231][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,235][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_51 stored as values in memory (estimated size 8.1 KB, free 904.9 MB)
[INFO][2018-05-31 16:56:25,236][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_51 in memory on 10.194.32.157:61802 (size: 8.1 KB, free: 905.4 MB)
[INFO][2018-05-31 16:56:25,238][org.apache.spark.executor.Executor]Finished task 49.0 in stage 4.0 (TID 54). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:25,239][org.apache.spark.scheduler.TaskSetManager]Starting task 52.0 in stage 4.0 (TID 57, localhost, executor driver, partition 52, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:25,240][org.apache.spark.scheduler.TaskSetManager]Finished task 49.0 in stage 4.0 (TID 54) in 43 ms on localhost (executor driver) (51/200)
[INFO][2018-05-31 16:56:25,241][org.apache.spark.executor.Executor]Running task 52.0 in stage 4.0 (TID 57)
[INFO][2018-05-31 16:56:25,242][org.apache.spark.executor.Executor]Finished task 51.0 in stage 4.0 (TID 56). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:25,243][org.apache.spark.scheduler.TaskSetManager]Starting task 53.0 in stage 4.0 (TID 58, localhost, executor driver, partition 53, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:25,243][org.apache.spark.scheduler.TaskSetManager]Finished task 51.0 in stage 4.0 (TID 56) in 17 ms on localhost (executor driver) (52/200)
[INFO][2018-05-31 16:56:25,243][org.apache.spark.executor.Executor]Running task 53.0 in stage 4.0 (TID 58)
[INFO][2018-05-31 16:56:25,247][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:25,251][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:25,252][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 2 ms
[INFO][2018-05-31 16:56:25,253][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 7 ms
[INFO][2018-05-31 16:56:25,254][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:25,254][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,254][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:25,254][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,257][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_52 stored as values in memory (estimated size 7.7 KB, free 904.9 MB)
[INFO][2018-05-31 16:56:25,257][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_53 stored as values in memory (estimated size 8.2 KB, free 904.9 MB)
[INFO][2018-05-31 16:56:25,258][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_52 in memory on 10.194.32.157:61802 (size: 7.7 KB, free: 905.4 MB)
[INFO][2018-05-31 16:56:25,258][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_53 in memory on 10.194.32.157:61802 (size: 8.2 KB, free: 905.4 MB)
[INFO][2018-05-31 16:56:25,267][org.apache.spark.executor.Executor]Finished task 53.0 in stage 4.0 (TID 58). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:25,267][org.apache.spark.scheduler.TaskSetManager]Starting task 54.0 in stage 4.0 (TID 59, localhost, executor driver, partition 54, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:25,268][org.apache.spark.executor.Executor]Running task 54.0 in stage 4.0 (TID 59)
[INFO][2018-05-31 16:56:25,268][org.apache.spark.scheduler.TaskSetManager]Finished task 53.0 in stage 4.0 (TID 58) in 25 ms on localhost (executor driver) (53/200)
[INFO][2018-05-31 16:56:25,268][org.apache.spark.executor.Executor]Finished task 52.0 in stage 4.0 (TID 57). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:25,269][org.apache.spark.scheduler.TaskSetManager]Starting task 55.0 in stage 4.0 (TID 60, localhost, executor driver, partition 55, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:25,269][org.apache.spark.scheduler.TaskSetManager]Finished task 52.0 in stage 4.0 (TID 57) in 30 ms on localhost (executor driver) (54/200)
[INFO][2018-05-31 16:56:25,269][org.apache.spark.executor.Executor]Running task 55.0 in stage 4.0 (TID 60)
[INFO][2018-05-31 16:56:25,270][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:25,271][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO][2018-05-31 16:56:25,272][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:25,272][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,272][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:25,272][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,273][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:25,273][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,275][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_54 stored as values in memory (estimated size 8.7 KB, free 872.7 MB)
[INFO][2018-05-31 16:56:25,276][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_54 in memory on 10.194.32.157:61802 (size: 8.7 KB, free: 905.4 MB)
[INFO][2018-05-31 16:56:25,276][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_55 stored as values in memory (estimated size 7.2 KB, free 904.9 MB)
[INFO][2018-05-31 16:56:25,276][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_55 in memory on 10.194.32.157:61802 (size: 7.2 KB, free: 905.4 MB)
[INFO][2018-05-31 16:56:25,281][org.apache.spark.executor.Executor]Finished task 54.0 in stage 4.0 (TID 59). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:25,281][org.apache.spark.executor.Executor]Finished task 55.0 in stage 4.0 (TID 60). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:25,281][org.apache.spark.scheduler.TaskSetManager]Starting task 56.0 in stage 4.0 (TID 61, localhost, executor driver, partition 56, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:25,281][org.apache.spark.scheduler.TaskSetManager]Starting task 57.0 in stage 4.0 (TID 62, localhost, executor driver, partition 57, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:25,282][org.apache.spark.executor.Executor]Running task 56.0 in stage 4.0 (TID 61)
[INFO][2018-05-31 16:56:25,282][org.apache.spark.scheduler.TaskSetManager]Finished task 54.0 in stage 4.0 (TID 59) in 15 ms on localhost (executor driver) (55/200)
[INFO][2018-05-31 16:56:25,282][org.apache.spark.scheduler.TaskSetManager]Finished task 55.0 in stage 4.0 (TID 60) in 13 ms on localhost (executor driver) (56/200)
[INFO][2018-05-31 16:56:25,282][org.apache.spark.executor.Executor]Running task 57.0 in stage 4.0 (TID 62)
[INFO][2018-05-31 16:56:25,284][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:25,284][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,285][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:25,285][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,285][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:25,285][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,285][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:25,286][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO][2018-05-31 16:56:25,288][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_56 stored as values in memory (estimated size 7.1 KB, free 904.9 MB)
[INFO][2018-05-31 16:56:25,288][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_57 stored as values in memory (estimated size 8.5 KB, free 904.8 MB)
[INFO][2018-05-31 16:56:25,289][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_56 in memory on 10.194.32.157:61802 (size: 7.1 KB, free: 905.4 MB)
[INFO][2018-05-31 16:56:25,289][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_57 in memory on 10.194.32.157:61802 (size: 8.5 KB, free: 905.4 MB)
[INFO][2018-05-31 16:56:25,294][org.apache.spark.executor.Executor]Finished task 56.0 in stage 4.0 (TID 61). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:25,294][org.apache.spark.executor.Executor]Finished task 57.0 in stage 4.0 (TID 62). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:25,294][org.apache.spark.scheduler.TaskSetManager]Starting task 58.0 in stage 4.0 (TID 63, localhost, executor driver, partition 58, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:25,294][org.apache.spark.scheduler.TaskSetManager]Starting task 59.0 in stage 4.0 (TID 64, localhost, executor driver, partition 59, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:25,294][org.apache.spark.executor.Executor]Running task 58.0 in stage 4.0 (TID 63)
[INFO][2018-05-31 16:56:25,295][org.apache.spark.executor.Executor]Running task 59.0 in stage 4.0 (TID 64)
[INFO][2018-05-31 16:56:25,294][org.apache.spark.scheduler.TaskSetManager]Finished task 56.0 in stage 4.0 (TID 61) in 13 ms on localhost (executor driver) (57/200)
[INFO][2018-05-31 16:56:25,295][org.apache.spark.scheduler.TaskSetManager]Finished task 57.0 in stage 4.0 (TID 62) in 14 ms on localhost (executor driver) (58/200)
[INFO][2018-05-31 16:56:25,297][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:25,297][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:25,297][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,297][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,298][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:25,298][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:25,298][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,298][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,302][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_59 stored as values in memory (estimated size 8.9 KB, free 872.7 MB)
[INFO][2018-05-31 16:56:25,302][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_59 in memory on 10.194.32.157:61802 (size: 8.9 KB, free: 905.4 MB)
[INFO][2018-05-31 16:56:25,302][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_58 stored as values in memory (estimated size 7.8 KB, free 904.8 MB)
[INFO][2018-05-31 16:56:25,303][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_58 in memory on 10.194.32.157:61802 (size: 7.8 KB, free: 905.4 MB)
[INFO][2018-05-31 16:56:25,307][org.apache.spark.executor.Executor]Finished task 58.0 in stage 4.0 (TID 63). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:25,308][org.apache.spark.scheduler.TaskSetManager]Starting task 60.0 in stage 4.0 (TID 65, localhost, executor driver, partition 60, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:25,308][org.apache.spark.scheduler.TaskSetManager]Finished task 58.0 in stage 4.0 (TID 63) in 14 ms on localhost (executor driver) (59/200)
[INFO][2018-05-31 16:56:25,308][org.apache.spark.executor.Executor]Running task 60.0 in stage 4.0 (TID 65)
[INFO][2018-05-31 16:56:25,307][org.apache.spark.executor.Executor]Finished task 59.0 in stage 4.0 (TID 64). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:25,311][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:25,311][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,311][org.apache.spark.scheduler.TaskSetManager]Starting task 61.0 in stage 4.0 (TID 66, localhost, executor driver, partition 61, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:25,311][org.apache.spark.scheduler.TaskSetManager]Finished task 59.0 in stage 4.0 (TID 64) in 17 ms on localhost (executor driver) (60/200)
[INFO][2018-05-31 16:56:25,312][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:25,312][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,312][org.apache.spark.executor.Executor]Running task 61.0 in stage 4.0 (TID 66)
[INFO][2018-05-31 16:56:25,315][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_60 stored as values in memory (estimated size 8.1 KB, free 904.8 MB)
[INFO][2018-05-31 16:56:25,315][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_60 in memory on 10.194.32.157:61802 (size: 8.1 KB, free: 905.4 MB)
[INFO][2018-05-31 16:56:25,318][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:25,318][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,319][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:25,319][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,321][org.apache.spark.executor.Executor]Finished task 60.0 in stage 4.0 (TID 65). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:25,322][org.apache.spark.scheduler.TaskSetManager]Starting task 62.0 in stage 4.0 (TID 67, localhost, executor driver, partition 62, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:25,322][org.apache.spark.scheduler.TaskSetManager]Finished task 60.0 in stage 4.0 (TID 65) in 14 ms on localhost (executor driver) (61/200)
[INFO][2018-05-31 16:56:25,323][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_61 stored as values in memory (estimated size 8.3 KB, free 904.8 MB)
[INFO][2018-05-31 16:56:25,324][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_61 in memory on 10.194.32.157:61802 (size: 8.3 KB, free: 905.4 MB)
[INFO][2018-05-31 16:56:25,325][org.apache.spark.executor.Executor]Running task 62.0 in stage 4.0 (TID 67)
[INFO][2018-05-31 16:56:25,328][org.apache.spark.executor.Executor]Finished task 61.0 in stage 4.0 (TID 66). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:25,329][org.apache.spark.scheduler.TaskSetManager]Starting task 63.0 in stage 4.0 (TID 68, localhost, executor driver, partition 63, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:25,329][org.apache.spark.scheduler.TaskSetManager]Finished task 61.0 in stage 4.0 (TID 66) in 18 ms on localhost (executor driver) (62/200)
[INFO][2018-05-31 16:56:25,330][org.apache.spark.executor.Executor]Running task 63.0 in stage 4.0 (TID 68)
[INFO][2018-05-31 16:56:25,333][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:25,333][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,335][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:25,335][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,336][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:25,347][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 11 ms
[INFO][2018-05-31 16:56:25,347][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_63 stored as values in memory (estimated size 7.8 KB, free 904.8 MB)
[INFO][2018-05-31 16:56:25,349][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_63 in memory on 10.194.32.157:61802 (size: 7.8 KB, free: 905.4 MB)
[INFO][2018-05-31 16:56:25,349][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:25,349][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,354][org.apache.spark.executor.Executor]Finished task 63.0 in stage 4.0 (TID 68). 3748 bytes result sent to driver
[INFO][2018-05-31 16:56:25,356][org.apache.spark.scheduler.TaskSetManager]Starting task 64.0 in stage 4.0 (TID 69, localhost, executor driver, partition 64, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:25,357][org.apache.spark.scheduler.TaskSetManager]Finished task 63.0 in stage 4.0 (TID 68) in 28 ms on localhost (executor driver) (63/200)
[INFO][2018-05-31 16:56:25,358][org.apache.spark.executor.Executor]Running task 64.0 in stage 4.0 (TID 69)
[INFO][2018-05-31 16:56:25,362][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:25,362][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,364][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:25,364][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,370][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_64 stored as values in memory (estimated size 7.7 KB, free 904.7 MB)
[INFO][2018-05-31 16:56:25,370][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_64 in memory on 10.194.32.157:61802 (size: 7.7 KB, free: 905.3 MB)
[INFO][2018-05-31 16:56:25,373][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_62 stored as values in memory (estimated size 9.3 KB, free 904.8 MB)
[INFO][2018-05-31 16:56:25,373][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_62 in memory on 10.194.32.157:61802 (size: 9.3 KB, free: 905.3 MB)
[INFO][2018-05-31 16:56:25,383][org.apache.spark.executor.Executor]Finished task 64.0 in stage 4.0 (TID 69). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:25,384][org.apache.spark.scheduler.TaskSetManager]Starting task 65.0 in stage 4.0 (TID 70, localhost, executor driver, partition 65, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:25,384][org.apache.spark.scheduler.TaskSetManager]Finished task 64.0 in stage 4.0 (TID 69) in 29 ms on localhost (executor driver) (64/200)
[INFO][2018-05-31 16:56:25,385][org.apache.spark.executor.Executor]Running task 65.0 in stage 4.0 (TID 70)
[INFO][2018-05-31 16:56:25,388][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:25,388][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,389][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:25,389][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,398][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_65 stored as values in memory (estimated size 9.1 KB, free 904.8 MB)
[INFO][2018-05-31 16:56:25,398][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_65 in memory on 10.194.32.157:61802 (size: 9.1 KB, free: 905.3 MB)
[INFO][2018-05-31 16:56:25,401][org.apache.spark.executor.Executor]Finished task 62.0 in stage 4.0 (TID 67). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:25,404][org.apache.spark.scheduler.TaskSetManager]Starting task 66.0 in stage 4.0 (TID 71, localhost, executor driver, partition 66, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:25,404][org.apache.spark.scheduler.TaskSetManager]Finished task 62.0 in stage 4.0 (TID 67) in 83 ms on localhost (executor driver) (65/200)
[INFO][2018-05-31 16:56:25,406][org.apache.spark.executor.Executor]Finished task 65.0 in stage 4.0 (TID 70). 3748 bytes result sent to driver
[INFO][2018-05-31 16:56:25,408][org.apache.spark.scheduler.TaskSetManager]Starting task 67.0 in stage 4.0 (TID 72, localhost, executor driver, partition 67, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:25,408][org.apache.spark.scheduler.TaskSetManager]Finished task 65.0 in stage 4.0 (TID 70) in 24 ms on localhost (executor driver) (66/200)
[INFO][2018-05-31 16:56:25,408][org.apache.spark.executor.Executor]Running task 67.0 in stage 4.0 (TID 72)
[INFO][2018-05-31 16:56:25,412][org.apache.spark.executor.Executor]Running task 66.0 in stage 4.0 (TID 71)
[INFO][2018-05-31 16:56:25,413][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:25,413][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,414][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:25,414][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,423][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:25,425][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 2 ms
[INFO][2018-05-31 16:56:25,424][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_67 stored as values in memory (estimated size 7.4 KB, free 904.8 MB)
[INFO][2018-05-31 16:56:25,425][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_67 in memory on 10.194.32.157:61802 (size: 7.4 KB, free: 905.3 MB)
[INFO][2018-05-31 16:56:25,428][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:25,429][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO][2018-05-31 16:56:25,437][org.apache.spark.executor.Executor]Finished task 67.0 in stage 4.0 (TID 72). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:25,438][org.apache.spark.scheduler.TaskSetManager]Starting task 68.0 in stage 4.0 (TID 73, localhost, executor driver, partition 68, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:25,438][org.apache.spark.scheduler.TaskSetManager]Finished task 67.0 in stage 4.0 (TID 72) in 31 ms on localhost (executor driver) (67/200)
[INFO][2018-05-31 16:56:25,438][org.apache.spark.executor.Executor]Running task 68.0 in stage 4.0 (TID 73)
[INFO][2018-05-31 16:56:25,440][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_66 stored as values in memory (estimated size 8.4 KB, free 904.8 MB)
[INFO][2018-05-31 16:56:25,440][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_66 in memory on 10.194.32.157:61802 (size: 8.4 KB, free: 905.3 MB)
[INFO][2018-05-31 16:56:25,441][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:25,441][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,442][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:25,442][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,449][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_68 stored as values in memory (estimated size 7.6 KB, free 904.8 MB)
[INFO][2018-05-31 16:56:25,449][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_68 in memory on 10.194.32.157:61802 (size: 7.6 KB, free: 905.3 MB)
[INFO][2018-05-31 16:56:25,455][org.apache.spark.executor.Executor]Finished task 68.0 in stage 4.0 (TID 73). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:25,457][org.apache.spark.scheduler.TaskSetManager]Starting task 69.0 in stage 4.0 (TID 74, localhost, executor driver, partition 69, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:25,458][org.apache.spark.scheduler.TaskSetManager]Finished task 68.0 in stage 4.0 (TID 73) in 20 ms on localhost (executor driver) (68/200)
[INFO][2018-05-31 16:56:25,459][org.apache.spark.executor.Executor]Running task 69.0 in stage 4.0 (TID 74)
[INFO][2018-05-31 16:56:25,462][org.apache.spark.executor.Executor]Finished task 66.0 in stage 4.0 (TID 71). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:25,463][org.apache.spark.scheduler.TaskSetManager]Starting task 70.0 in stage 4.0 (TID 75, localhost, executor driver, partition 70, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:25,463][org.apache.spark.scheduler.TaskSetManager]Finished task 66.0 in stage 4.0 (TID 71) in 59 ms on localhost (executor driver) (69/200)
[INFO][2018-05-31 16:56:25,464][org.apache.spark.executor.Executor]Running task 70.0 in stage 4.0 (TID 75)
[INFO][2018-05-31 16:56:25,464][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:25,464][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,465][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:25,465][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,474][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_69 stored as values in memory (estimated size 8.2 KB, free 904.7 MB)
[INFO][2018-05-31 16:56:25,477][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:25,477][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,478][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:25,479][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO][2018-05-31 16:56:25,481][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_70 stored as values in memory (estimated size 7.2 KB, free 904.7 MB)
[INFO][2018-05-31 16:56:25,482][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_70 in memory on 10.194.32.157:61802 (size: 7.2 KB, free: 905.3 MB)
[INFO][2018-05-31 16:56:25,492][org.apache.spark.executor.Executor]Finished task 70.0 in stage 4.0 (TID 75). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:25,492][org.apache.spark.scheduler.TaskSetManager]Starting task 71.0 in stage 4.0 (TID 76, localhost, executor driver, partition 71, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:25,492][org.apache.spark.scheduler.TaskSetManager]Finished task 70.0 in stage 4.0 (TID 75) in 29 ms on localhost (executor driver) (70/200)
[INFO][2018-05-31 16:56:25,493][org.apache.spark.executor.Executor]Running task 71.0 in stage 4.0 (TID 76)
[INFO][2018-05-31 16:56:25,496][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:25,496][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,497][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:25,497][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,500][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_71 stored as values in memory (estimated size 7.8 KB, free 904.7 MB)
[INFO][2018-05-31 16:56:25,501][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_71 in memory on 10.194.32.157:61802 (size: 7.8 KB, free: 905.3 MB)
[INFO][2018-05-31 16:56:25,506][org.apache.spark.executor.Executor]Finished task 71.0 in stage 4.0 (TID 76). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:25,507][org.apache.spark.scheduler.TaskSetManager]Starting task 72.0 in stage 4.0 (TID 77, localhost, executor driver, partition 72, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:25,507][org.apache.spark.scheduler.TaskSetManager]Finished task 71.0 in stage 4.0 (TID 76) in 15 ms on localhost (executor driver) (71/200)
[INFO][2018-05-31 16:56:25,508][org.apache.spark.executor.Executor]Running task 72.0 in stage 4.0 (TID 77)
[INFO][2018-05-31 16:56:25,513][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_69 in memory on 10.194.32.157:61802 (size: 8.2 KB, free: 905.3 MB)
[INFO][2018-05-31 16:56:25,513][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:25,513][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,514][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:25,514][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,518][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_72 stored as values in memory (estimated size 7.3 KB, free 904.7 MB)
[INFO][2018-05-31 16:56:25,518][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_72 in memory on 10.194.32.157:61802 (size: 7.3 KB, free: 905.3 MB)
[INFO][2018-05-31 16:56:25,524][org.apache.spark.executor.Executor]Finished task 72.0 in stage 4.0 (TID 77). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:25,525][org.apache.spark.scheduler.TaskSetManager]Starting task 73.0 in stage 4.0 (TID 78, localhost, executor driver, partition 73, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:25,525][org.apache.spark.executor.Executor]Finished task 69.0 in stage 4.0 (TID 74). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:25,525][org.apache.spark.scheduler.TaskSetManager]Finished task 72.0 in stage 4.0 (TID 77) in 19 ms on localhost (executor driver) (72/200)
[INFO][2018-05-31 16:56:25,525][org.apache.spark.executor.Executor]Running task 73.0 in stage 4.0 (TID 78)
[INFO][2018-05-31 16:56:25,528][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:25,528][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,529][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:25,529][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,532][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_73 stored as values in memory (estimated size 8.3 KB, free 904.7 MB)
[INFO][2018-05-31 16:56:25,532][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_73 in memory on 10.194.32.157:61802 (size: 8.3 KB, free: 905.3 MB)
[INFO][2018-05-31 16:56:25,536][org.apache.spark.scheduler.TaskSetManager]Starting task 74.0 in stage 4.0 (TID 79, localhost, executor driver, partition 74, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:25,536][org.apache.spark.scheduler.TaskSetManager]Finished task 69.0 in stage 4.0 (TID 74) in 79 ms on localhost (executor driver) (73/200)
[INFO][2018-05-31 16:56:25,537][org.apache.spark.executor.Executor]Running task 74.0 in stage 4.0 (TID 79)
[INFO][2018-05-31 16:56:25,537][org.apache.spark.executor.Executor]Finished task 73.0 in stage 4.0 (TID 78). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:25,538][org.apache.spark.scheduler.TaskSetManager]Starting task 75.0 in stage 4.0 (TID 80, localhost, executor driver, partition 75, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:25,538][org.apache.spark.scheduler.TaskSetManager]Finished task 73.0 in stage 4.0 (TID 78) in 14 ms on localhost (executor driver) (74/200)
[INFO][2018-05-31 16:56:25,538][org.apache.spark.executor.Executor]Running task 75.0 in stage 4.0 (TID 80)
[INFO][2018-05-31 16:56:25,541][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:25,541][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,542][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:25,542][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,544][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_75 stored as values in memory (estimated size 7.7 KB, free 904.7 MB)
[INFO][2018-05-31 16:56:25,544][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_75 in memory on 10.194.32.157:61802 (size: 7.7 KB, free: 905.3 MB)
[INFO][2018-05-31 16:56:25,550][org.apache.spark.executor.Executor]Finished task 75.0 in stage 4.0 (TID 80). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:25,550][org.apache.spark.scheduler.TaskSetManager]Starting task 76.0 in stage 4.0 (TID 81, localhost, executor driver, partition 76, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:25,551][org.apache.spark.scheduler.TaskSetManager]Finished task 75.0 in stage 4.0 (TID 80) in 13 ms on localhost (executor driver) (75/200)
[INFO][2018-05-31 16:56:25,551][org.apache.spark.executor.Executor]Running task 76.0 in stage 4.0 (TID 81)
[INFO][2018-05-31 16:56:25,553][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:25,553][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,554][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:25,554][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,561][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_76 stored as values in memory (estimated size 8.5 KB, free 904.7 MB)
[INFO][2018-05-31 16:56:25,561][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_76 in memory on 10.194.32.157:61802 (size: 8.5 KB, free: 905.3 MB)
[INFO][2018-05-31 16:56:25,567][org.apache.spark.executor.Executor]Finished task 76.0 in stage 4.0 (TID 81). 3748 bytes result sent to driver
[INFO][2018-05-31 16:56:25,567][org.apache.spark.scheduler.TaskSetManager]Starting task 77.0 in stage 4.0 (TID 82, localhost, executor driver, partition 77, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:25,568][org.apache.spark.scheduler.TaskSetManager]Finished task 76.0 in stage 4.0 (TID 81) in 18 ms on localhost (executor driver) (76/200)
[INFO][2018-05-31 16:56:25,568][org.apache.spark.executor.Executor]Running task 77.0 in stage 4.0 (TID 82)
[INFO][2018-05-31 16:56:25,575][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:25,575][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,579][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:25,580][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO][2018-05-31 16:56:25,584][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_77 stored as values in memory (estimated size 6.9 KB, free 904.7 MB)
[INFO][2018-05-31 16:56:25,585][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_77 in memory on 10.194.32.157:61802 (size: 6.9 KB, free: 905.2 MB)
[INFO][2018-05-31 16:56:25,601][org.apache.spark.executor.Executor]Finished task 77.0 in stage 4.0 (TID 82). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:25,601][org.apache.spark.scheduler.TaskSetManager]Starting task 78.0 in stage 4.0 (TID 83, localhost, executor driver, partition 78, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:25,601][org.apache.spark.scheduler.TaskSetManager]Finished task 77.0 in stage 4.0 (TID 82) in 34 ms on localhost (executor driver) (77/200)
[INFO][2018-05-31 16:56:25,602][org.apache.spark.executor.Executor]Running task 78.0 in stage 4.0 (TID 83)
[INFO][2018-05-31 16:56:25,618][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:25,618][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,618][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:25,619][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO][2018-05-31 16:56:25,624][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:25,624][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO][2018-05-31 16:56:25,625][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:25,625][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO][2018-05-31 16:56:25,628][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_78 stored as values in memory (estimated size 8.3 KB, free 888.6 MB)
[INFO][2018-05-31 16:56:25,629][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_78 in memory on 10.194.32.157:61802 (size: 8.3 KB, free: 905.2 MB)
[INFO][2018-05-31 16:56:25,633][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_74 stored as values in memory (estimated size 9.9 KB, free 904.7 MB)
[INFO][2018-05-31 16:56:25,634][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_74 in memory on 10.194.32.157:61802 (size: 9.9 KB, free: 905.2 MB)
[INFO][2018-05-31 16:56:25,643][org.apache.spark.executor.Executor]Finished task 78.0 in stage 4.0 (TID 83). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:25,644][org.apache.spark.scheduler.TaskSetManager]Starting task 79.0 in stage 4.0 (TID 84, localhost, executor driver, partition 79, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:25,644][org.apache.spark.scheduler.TaskSetManager]Finished task 78.0 in stage 4.0 (TID 83) in 43 ms on localhost (executor driver) (78/200)
[INFO][2018-05-31 16:56:25,644][org.apache.spark.executor.Executor]Running task 79.0 in stage 4.0 (TID 84)
[INFO][2018-05-31 16:56:25,647][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:25,647][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,647][org.apache.spark.executor.Executor]Finished task 74.0 in stage 4.0 (TID 79). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:25,648][org.apache.spark.scheduler.TaskSetManager]Starting task 80.0 in stage 4.0 (TID 85, localhost, executor driver, partition 80, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:25,648][org.apache.spark.scheduler.TaskSetManager]Finished task 74.0 in stage 4.0 (TID 79) in 112 ms on localhost (executor driver) (79/200)
[INFO][2018-05-31 16:56:25,649][org.apache.spark.executor.Executor]Running task 80.0 in stage 4.0 (TID 85)
[INFO][2018-05-31 16:56:25,649][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:25,649][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,655][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:25,655][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,656][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:25,656][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,660][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_80 stored as values in memory (estimated size 8.9 KB, free 872.5 MB)
[INFO][2018-05-31 16:56:25,660][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_80 in memory on 10.194.32.157:61802 (size: 8.9 KB, free: 905.2 MB)
[INFO][2018-05-31 16:56:25,667][org.apache.spark.executor.Executor]Finished task 80.0 in stage 4.0 (TID 85). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:25,668][org.apache.spark.scheduler.TaskSetManager]Starting task 81.0 in stage 4.0 (TID 86, localhost, executor driver, partition 81, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:25,668][org.apache.spark.scheduler.TaskSetManager]Finished task 80.0 in stage 4.0 (TID 85) in 20 ms on localhost (executor driver) (80/200)
[INFO][2018-05-31 16:56:25,668][org.apache.spark.executor.Executor]Running task 81.0 in stage 4.0 (TID 86)
[INFO][2018-05-31 16:56:25,670][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_79 stored as values in memory (estimated size 7.7 KB, free 904.7 MB)
[INFO][2018-05-31 16:56:25,670][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_79 in memory on 10.194.32.157:61802 (size: 7.7 KB, free: 905.2 MB)
[INFO][2018-05-31 16:56:25,671][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:25,671][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,675][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:25,675][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,683][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_81 stored as values in memory (estimated size 9.3 KB, free 904.7 MB)
[INFO][2018-05-31 16:56:25,683][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_81 in memory on 10.194.32.157:61802 (size: 9.3 KB, free: 905.2 MB)
[INFO][2018-05-31 16:56:25,685][org.apache.spark.executor.Executor]Finished task 79.0 in stage 4.0 (TID 84). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:25,685][org.apache.spark.scheduler.TaskSetManager]Starting task 82.0 in stage 4.0 (TID 87, localhost, executor driver, partition 82, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:25,686][org.apache.spark.executor.Executor]Running task 82.0 in stage 4.0 (TID 87)
[INFO][2018-05-31 16:56:25,686][org.apache.spark.scheduler.TaskSetManager]Finished task 79.0 in stage 4.0 (TID 84) in 42 ms on localhost (executor driver) (81/200)
[INFO][2018-05-31 16:56:25,688][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:25,688][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,689][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:25,689][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,690][org.apache.spark.executor.Executor]Finished task 81.0 in stage 4.0 (TID 86). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:25,691][org.apache.spark.scheduler.TaskSetManager]Starting task 83.0 in stage 4.0 (TID 88, localhost, executor driver, partition 83, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:25,691][org.apache.spark.scheduler.TaskSetManager]Finished task 81.0 in stage 4.0 (TID 86) in 24 ms on localhost (executor driver) (82/200)
[INFO][2018-05-31 16:56:25,691][org.apache.spark.executor.Executor]Running task 83.0 in stage 4.0 (TID 88)
[INFO][2018-05-31 16:56:25,692][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_82 stored as values in memory (estimated size 8.2 KB, free 904.6 MB)
[INFO][2018-05-31 16:56:25,692][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_82 in memory on 10.194.32.157:61802 (size: 8.2 KB, free: 905.2 MB)
[INFO][2018-05-31 16:56:25,693][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:25,694][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO][2018-05-31 16:56:25,694][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:25,694][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,696][org.apache.spark.executor.Executor]Finished task 82.0 in stage 4.0 (TID 87). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:25,697][org.apache.spark.scheduler.TaskSetManager]Starting task 84.0 in stage 4.0 (TID 89, localhost, executor driver, partition 84, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:25,697][org.apache.spark.executor.Executor]Running task 84.0 in stage 4.0 (TID 89)
[INFO][2018-05-31 16:56:25,697][org.apache.spark.scheduler.TaskSetManager]Finished task 82.0 in stage 4.0 (TID 87) in 12 ms on localhost (executor driver) (83/200)
[INFO][2018-05-31 16:56:25,699][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:25,699][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,700][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:25,700][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,703][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_84 stored as values in memory (estimated size 8.7 KB, free 904.6 MB)
[INFO][2018-05-31 16:56:25,704][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_84 in memory on 10.194.32.157:61802 (size: 8.7 KB, free: 905.2 MB)
[INFO][2018-05-31 16:56:25,710][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_83 stored as values in memory (estimated size 8.9 KB, free 904.6 MB)
[INFO][2018-05-31 16:56:25,711][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_83 in memory on 10.194.32.157:61802 (size: 8.9 KB, free: 905.2 MB)
[INFO][2018-05-31 16:56:25,712][org.apache.spark.executor.Executor]Finished task 84.0 in stage 4.0 (TID 89). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:25,712][org.apache.spark.scheduler.TaskSetManager]Starting task 85.0 in stage 4.0 (TID 90, localhost, executor driver, partition 85, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:25,712][org.apache.spark.scheduler.TaskSetManager]Finished task 84.0 in stage 4.0 (TID 89) in 15 ms on localhost (executor driver) (84/200)
[INFO][2018-05-31 16:56:25,713][org.apache.spark.executor.Executor]Running task 85.0 in stage 4.0 (TID 90)
[INFO][2018-05-31 16:56:25,716][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:25,717][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO][2018-05-31 16:56:25,717][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:25,717][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,720][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_85 stored as values in memory (estimated size 8.4 KB, free 904.6 MB)
[INFO][2018-05-31 16:56:25,720][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_85 in memory on 10.194.32.157:61802 (size: 8.4 KB, free: 905.2 MB)
[INFO][2018-05-31 16:56:25,722][org.apache.spark.executor.Executor]Finished task 83.0 in stage 4.0 (TID 88). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:25,724][org.apache.spark.scheduler.TaskSetManager]Starting task 86.0 in stage 4.0 (TID 91, localhost, executor driver, partition 86, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:25,724][org.apache.spark.executor.Executor]Running task 86.0 in stage 4.0 (TID 91)
[INFO][2018-05-31 16:56:25,724][org.apache.spark.scheduler.TaskSetManager]Finished task 83.0 in stage 4.0 (TID 88) in 34 ms on localhost (executor driver) (85/200)
[INFO][2018-05-31 16:56:25,726][org.apache.spark.executor.Executor]Finished task 85.0 in stage 4.0 (TID 90). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:25,726][org.apache.spark.scheduler.TaskSetManager]Starting task 87.0 in stage 4.0 (TID 92, localhost, executor driver, partition 87, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:25,726][org.apache.spark.scheduler.TaskSetManager]Finished task 85.0 in stage 4.0 (TID 90) in 14 ms on localhost (executor driver) (86/200)
[INFO][2018-05-31 16:56:25,727][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:25,727][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,728][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:25,728][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,731][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_86 stored as values in memory (estimated size 6.8 KB, free 904.6 MB)
[INFO][2018-05-31 16:56:25,731][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_86 in memory on 10.194.32.157:61802 (size: 6.8 KB, free: 905.2 MB)
[INFO][2018-05-31 16:56:25,735][org.apache.spark.executor.Executor]Finished task 86.0 in stage 4.0 (TID 91). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:25,736][org.apache.spark.scheduler.TaskSetManager]Starting task 88.0 in stage 4.0 (TID 93, localhost, executor driver, partition 88, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:25,736][org.apache.spark.scheduler.TaskSetManager]Finished task 86.0 in stage 4.0 (TID 91) in 12 ms on localhost (executor driver) (87/200)
[INFO][2018-05-31 16:56:25,736][org.apache.spark.executor.Executor]Running task 88.0 in stage 4.0 (TID 93)
[INFO][2018-05-31 16:56:25,738][org.apache.spark.executor.Executor]Running task 87.0 in stage 4.0 (TID 92)
[INFO][2018-05-31 16:56:25,739][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:25,739][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,740][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:25,740][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,742][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:25,742][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO][2018-05-31 16:56:25,742][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:25,742][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,743][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_88 stored as values in memory (estimated size 7.5 KB, free 904.5 MB)
[INFO][2018-05-31 16:56:25,743][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_88 in memory on 10.194.32.157:61802 (size: 7.5 KB, free: 905.2 MB)
[INFO][2018-05-31 16:56:25,748][org.apache.spark.executor.Executor]Finished task 88.0 in stage 4.0 (TID 93). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:25,748][org.apache.spark.scheduler.TaskSetManager]Starting task 89.0 in stage 4.0 (TID 94, localhost, executor driver, partition 89, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:25,749][org.apache.spark.scheduler.TaskSetManager]Finished task 88.0 in stage 4.0 (TID 93) in 13 ms on localhost (executor driver) (88/200)
[INFO][2018-05-31 16:56:25,749][org.apache.spark.executor.Executor]Running task 89.0 in stage 4.0 (TID 94)
[INFO][2018-05-31 16:56:25,751][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_87 stored as values in memory (estimated size 9.1 KB, free 904.6 MB)
[INFO][2018-05-31 16:56:25,751][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_87 in memory on 10.194.32.157:61802 (size: 9.1 KB, free: 905.1 MB)
[INFO][2018-05-31 16:56:25,751][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:25,751][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,752][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:25,752][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,756][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_89 stored as values in memory (estimated size 7.5 KB, free 904.6 MB)
[INFO][2018-05-31 16:56:25,757][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_89 in memory on 10.194.32.157:61802 (size: 7.5 KB, free: 905.1 MB)
[INFO][2018-05-31 16:56:25,764][org.apache.spark.executor.Executor]Finished task 89.0 in stage 4.0 (TID 94). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:25,764][org.apache.spark.scheduler.TaskSetManager]Starting task 90.0 in stage 4.0 (TID 95, localhost, executor driver, partition 90, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:25,764][org.apache.spark.scheduler.TaskSetManager]Finished task 89.0 in stage 4.0 (TID 94) in 16 ms on localhost (executor driver) (89/200)
[INFO][2018-05-31 16:56:25,764][org.apache.spark.executor.Executor]Running task 90.0 in stage 4.0 (TID 95)
[INFO][2018-05-31 16:56:25,769][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:25,769][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,771][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:25,771][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,771][org.apache.spark.executor.Executor]Finished task 87.0 in stage 4.0 (TID 92). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:25,772][org.apache.spark.scheduler.TaskSetManager]Starting task 91.0 in stage 4.0 (TID 96, localhost, executor driver, partition 91, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:25,772][org.apache.spark.scheduler.TaskSetManager]Finished task 87.0 in stage 4.0 (TID 92) in 46 ms on localhost (executor driver) (90/200)
[INFO][2018-05-31 16:56:25,773][org.apache.spark.executor.Executor]Running task 91.0 in stage 4.0 (TID 96)
[INFO][2018-05-31 16:56:25,774][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_90 stored as values in memory (estimated size 5.4 KB, free 904.6 MB)
[INFO][2018-05-31 16:56:25,774][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_90 in memory on 10.194.32.157:61802 (size: 5.4 KB, free: 905.1 MB)
[INFO][2018-05-31 16:56:25,777][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:25,777][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,778][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:25,779][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO][2018-05-31 16:56:25,787][org.apache.spark.executor.Executor]Finished task 90.0 in stage 4.0 (TID 95). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:25,787][org.apache.spark.scheduler.TaskSetManager]Starting task 92.0 in stage 4.0 (TID 97, localhost, executor driver, partition 92, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:25,787][org.apache.spark.scheduler.TaskSetManager]Finished task 90.0 in stage 4.0 (TID 95) in 23 ms on localhost (executor driver) (91/200)
[INFO][2018-05-31 16:56:25,787][org.apache.spark.executor.Executor]Running task 92.0 in stage 4.0 (TID 97)
[INFO][2018-05-31 16:56:25,790][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:25,790][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,791][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:25,791][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,794][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_92 stored as values in memory (estimated size 7.1 KB, free 872.5 MB)
[INFO][2018-05-31 16:56:25,794][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_92 in memory on 10.194.32.157:61802 (size: 7.1 KB, free: 905.1 MB)
[INFO][2018-05-31 16:56:25,795][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_91 stored as values in memory (estimated size 6.8 KB, free 904.6 MB)
[INFO][2018-05-31 16:56:25,795][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_91 in memory on 10.194.32.157:61802 (size: 6.8 KB, free: 905.1 MB)
[INFO][2018-05-31 16:56:25,806][org.apache.spark.executor.Executor]Finished task 92.0 in stage 4.0 (TID 97). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:25,806][org.apache.spark.scheduler.TaskSetManager]Starting task 93.0 in stage 4.0 (TID 98, localhost, executor driver, partition 93, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:25,807][org.apache.spark.scheduler.TaskSetManager]Finished task 92.0 in stage 4.0 (TID 97) in 20 ms on localhost (executor driver) (92/200)
[INFO][2018-05-31 16:56:25,807][org.apache.spark.executor.Executor]Running task 93.0 in stage 4.0 (TID 98)
[INFO][2018-05-31 16:56:25,807][org.apache.spark.executor.Executor]Finished task 91.0 in stage 4.0 (TID 96). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:25,807][org.apache.spark.scheduler.TaskSetManager]Starting task 94.0 in stage 4.0 (TID 99, localhost, executor driver, partition 94, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:25,808][org.apache.spark.scheduler.TaskSetManager]Finished task 91.0 in stage 4.0 (TID 96) in 36 ms on localhost (executor driver) (93/200)
[INFO][2018-05-31 16:56:25,808][org.apache.spark.executor.Executor]Running task 94.0 in stage 4.0 (TID 99)
[INFO][2018-05-31 16:56:25,810][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:25,810][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,811][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:25,811][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,811][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:25,812][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO][2018-05-31 16:56:25,816][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_93 stored as values in memory (estimated size 7.9 KB, free 904.5 MB)
[INFO][2018-05-31 16:56:25,816][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_93 in memory on 10.194.32.157:61802 (size: 7.9 KB, free: 905.1 MB)
[INFO][2018-05-31 16:56:25,816][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:25,817][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO][2018-05-31 16:56:25,820][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_94 stored as values in memory (estimated size 9.4 KB, free 904.6 MB)
[INFO][2018-05-31 16:56:25,821][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_94 in memory on 10.194.32.157:61802 (size: 9.4 KB, free: 905.1 MB)
[INFO][2018-05-31 16:56:25,821][org.apache.spark.executor.Executor]Finished task 93.0 in stage 4.0 (TID 98). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:25,822][org.apache.spark.scheduler.TaskSetManager]Starting task 95.0 in stage 4.0 (TID 100, localhost, executor driver, partition 95, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:25,822][org.apache.spark.scheduler.TaskSetManager]Finished task 93.0 in stage 4.0 (TID 98) in 16 ms on localhost (executor driver) (94/200)
[INFO][2018-05-31 16:56:25,823][org.apache.spark.executor.Executor]Running task 95.0 in stage 4.0 (TID 100)
[INFO][2018-05-31 16:56:25,828][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:25,828][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,829][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:25,830][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO][2018-05-31 16:56:25,832][org.apache.spark.executor.Executor]Finished task 94.0 in stage 4.0 (TID 99). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:25,832][org.apache.spark.scheduler.TaskSetManager]Starting task 96.0 in stage 4.0 (TID 101, localhost, executor driver, partition 96, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:25,832][org.apache.spark.scheduler.TaskSetManager]Finished task 94.0 in stage 4.0 (TID 99) in 25 ms on localhost (executor driver) (95/200)
[INFO][2018-05-31 16:56:25,832][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_95 stored as values in memory (estimated size 7.6 KB, free 904.5 MB)
[INFO][2018-05-31 16:56:25,833][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_95 in memory on 10.194.32.157:61802 (size: 7.6 KB, free: 905.1 MB)
[INFO][2018-05-31 16:56:25,833][org.apache.spark.executor.Executor]Running task 96.0 in stage 4.0 (TID 101)
[INFO][2018-05-31 16:56:25,845][org.apache.spark.executor.Executor]Finished task 95.0 in stage 4.0 (TID 100). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:25,845][org.apache.spark.scheduler.TaskSetManager]Starting task 97.0 in stage 4.0 (TID 102, localhost, executor driver, partition 97, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:25,846][org.apache.spark.scheduler.TaskSetManager]Finished task 95.0 in stage 4.0 (TID 100) in 24 ms on localhost (executor driver) (96/200)
[INFO][2018-05-31 16:56:25,846][org.apache.spark.executor.Executor]Running task 97.0 in stage 4.0 (TID 102)
[INFO][2018-05-31 16:56:25,846][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:25,847][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO][2018-05-31 16:56:25,848][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:25,848][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:25,848][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,848][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,849][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:25,849][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,853][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_96 stored as values in memory (estimated size 7.4 KB, free 904.5 MB)
[INFO][2018-05-31 16:56:25,859][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_96 in memory on 10.194.32.157:61802 (size: 7.4 KB, free: 905.1 MB)
[INFO][2018-05-31 16:56:25,859][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_97 stored as values in memory (estimated size 8.4 KB, free 904.5 MB)
[INFO][2018-05-31 16:56:25,860][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_97 in memory on 10.194.32.157:61802 (size: 8.4 KB, free: 905.1 MB)
[INFO][2018-05-31 16:56:25,870][org.apache.spark.executor.Executor]Finished task 97.0 in stage 4.0 (TID 102). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:25,870][org.apache.spark.executor.Executor]Finished task 96.0 in stage 4.0 (TID 101). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:25,870][org.apache.spark.scheduler.TaskSetManager]Starting task 98.0 in stage 4.0 (TID 103, localhost, executor driver, partition 98, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:25,871][org.apache.spark.scheduler.TaskSetManager]Finished task 97.0 in stage 4.0 (TID 102) in 26 ms on localhost (executor driver) (97/200)
[INFO][2018-05-31 16:56:25,871][org.apache.spark.scheduler.TaskSetManager]Starting task 99.0 in stage 4.0 (TID 104, localhost, executor driver, partition 99, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:25,871][org.apache.spark.scheduler.TaskSetManager]Finished task 96.0 in stage 4.0 (TID 101) in 39 ms on localhost (executor driver) (98/200)
[INFO][2018-05-31 16:56:25,872][org.apache.spark.executor.Executor]Running task 99.0 in stage 4.0 (TID 104)
[INFO][2018-05-31 16:56:25,873][org.apache.spark.executor.Executor]Running task 98.0 in stage 4.0 (TID 103)
[INFO][2018-05-31 16:56:25,880][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:25,880][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,881][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:25,881][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,884][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_99 stored as values in memory (estimated size 8.9 KB, free 904.5 MB)
[INFO][2018-05-31 16:56:25,884][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_99 in memory on 10.194.32.157:61802 (size: 8.9 KB, free: 905.1 MB)
[INFO][2018-05-31 16:56:25,888][org.apache.spark.executor.Executor]Finished task 99.0 in stage 4.0 (TID 104). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:25,889][org.apache.spark.scheduler.TaskSetManager]Starting task 100.0 in stage 4.0 (TID 105, localhost, executor driver, partition 100, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:25,889][org.apache.spark.scheduler.TaskSetManager]Finished task 99.0 in stage 4.0 (TID 104) in 18 ms on localhost (executor driver) (99/200)
[INFO][2018-05-31 16:56:25,889][org.apache.spark.executor.Executor]Running task 100.0 in stage 4.0 (TID 105)
[INFO][2018-05-31 16:56:25,891][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:25,892][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO][2018-05-31 16:56:25,892][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:25,892][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,895][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_100 stored as values in memory (estimated size 7.5 KB, free 904.5 MB)
[INFO][2018-05-31 16:56:25,895][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_100 in memory on 10.194.32.157:61802 (size: 7.5 KB, free: 905.1 MB)
[INFO][2018-05-31 16:56:25,897][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:25,897][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,899][org.apache.spark.executor.Executor]Finished task 100.0 in stage 4.0 (TID 105). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:25,900][org.apache.spark.scheduler.TaskSetManager]Starting task 101.0 in stage 4.0 (TID 106, localhost, executor driver, partition 101, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:25,900][org.apache.spark.scheduler.TaskSetManager]Finished task 100.0 in stage 4.0 (TID 105) in 11 ms on localhost (executor driver) (100/200)
[INFO][2018-05-31 16:56:25,900][org.apache.spark.executor.Executor]Running task 101.0 in stage 4.0 (TID 106)
[INFO][2018-05-31 16:56:25,901][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:25,903][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:25,903][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,903][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,904][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:25,905][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO][2018-05-31 16:56:25,905][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_98 stored as values in memory (estimated size 7.6 KB, free 904.4 MB)
[INFO][2018-05-31 16:56:25,907][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_101 stored as values in memory (estimated size 8.5 KB, free 904.5 MB)
[INFO][2018-05-31 16:56:25,908][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_98 in memory on 10.194.32.157:61802 (size: 7.6 KB, free: 905.1 MB)
[INFO][2018-05-31 16:56:25,908][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_101 in memory on 10.194.32.157:61802 (size: 8.5 KB, free: 905.0 MB)
[INFO][2018-05-31 16:56:25,913][org.apache.spark.executor.Executor]Finished task 101.0 in stage 4.0 (TID 106). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:25,913][org.apache.spark.scheduler.TaskSetManager]Starting task 102.0 in stage 4.0 (TID 107, localhost, executor driver, partition 102, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:25,914][org.apache.spark.scheduler.TaskSetManager]Finished task 101.0 in stage 4.0 (TID 106) in 14 ms on localhost (executor driver) (101/200)
[INFO][2018-05-31 16:56:25,914][org.apache.spark.executor.Executor]Running task 102.0 in stage 4.0 (TID 107)
[INFO][2018-05-31 16:56:25,913][org.apache.spark.executor.Executor]Finished task 98.0 in stage 4.0 (TID 103). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:25,915][org.apache.spark.scheduler.TaskSetManager]Starting task 103.0 in stage 4.0 (TID 108, localhost, executor driver, partition 103, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:25,915][org.apache.spark.scheduler.TaskSetManager]Finished task 98.0 in stage 4.0 (TID 103) in 45 ms on localhost (executor driver) (102/200)
[INFO][2018-05-31 16:56:25,916][org.apache.spark.executor.Executor]Running task 103.0 in stage 4.0 (TID 108)
[INFO][2018-05-31 16:56:25,916][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:25,916][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,917][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:25,917][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,919][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:25,919][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,919][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:25,919][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,921][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_102 stored as values in memory (estimated size 8.3 KB, free 904.4 MB)
[INFO][2018-05-31 16:56:25,921][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_102 in memory on 10.194.32.157:61802 (size: 8.3 KB, free: 905.0 MB)
[INFO][2018-05-31 16:56:25,922][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_103 stored as values in memory (estimated size 7.8 KB, free 904.5 MB)
[INFO][2018-05-31 16:56:25,922][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_103 in memory on 10.194.32.157:61802 (size: 7.8 KB, free: 905.0 MB)
[INFO][2018-05-31 16:56:25,925][org.apache.spark.executor.Executor]Finished task 102.0 in stage 4.0 (TID 107). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:25,926][org.apache.spark.scheduler.TaskSetManager]Starting task 104.0 in stage 4.0 (TID 109, localhost, executor driver, partition 104, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:25,926][org.apache.spark.scheduler.TaskSetManager]Finished task 102.0 in stage 4.0 (TID 107) in 13 ms on localhost (executor driver) (103/200)
[INFO][2018-05-31 16:56:25,926][org.apache.spark.executor.Executor]Running task 104.0 in stage 4.0 (TID 109)
[INFO][2018-05-31 16:56:25,929][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:25,929][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO][2018-05-31 16:56:25,929][org.apache.spark.executor.Executor]Finished task 103.0 in stage 4.0 (TID 108). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:25,929][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:25,930][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO][2018-05-31 16:56:25,931][org.apache.spark.scheduler.TaskSetManager]Starting task 105.0 in stage 4.0 (TID 110, localhost, executor driver, partition 105, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:25,931][org.apache.spark.scheduler.TaskSetManager]Finished task 103.0 in stage 4.0 (TID 108) in 16 ms on localhost (executor driver) (104/200)
[INFO][2018-05-31 16:56:25,932][org.apache.spark.executor.Executor]Running task 105.0 in stage 4.0 (TID 110)
[INFO][2018-05-31 16:56:25,932][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_104 stored as values in memory (estimated size 7.5 KB, free 904.5 MB)
[INFO][2018-05-31 16:56:25,932][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_104 in memory on 10.194.32.157:61802 (size: 7.5 KB, free: 905.0 MB)
[INFO][2018-05-31 16:56:25,935][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:25,935][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,936][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:25,936][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,937][org.apache.spark.executor.Executor]Finished task 104.0 in stage 4.0 (TID 109). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:25,938][org.apache.spark.scheduler.TaskSetManager]Starting task 106.0 in stage 4.0 (TID 111, localhost, executor driver, partition 106, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:25,938][org.apache.spark.scheduler.TaskSetManager]Finished task 104.0 in stage 4.0 (TID 109) in 13 ms on localhost (executor driver) (105/200)
[INFO][2018-05-31 16:56:25,938][org.apache.spark.executor.Executor]Running task 106.0 in stage 4.0 (TID 111)
[INFO][2018-05-31 16:56:25,941][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:25,941][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,941][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_105 stored as values in memory (estimated size 7.3 KB, free 904.5 MB)
[INFO][2018-05-31 16:56:25,942][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:25,942][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,942][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_105 in memory on 10.194.32.157:61802 (size: 7.3 KB, free: 905.0 MB)
[INFO][2018-05-31 16:56:25,944][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_106 stored as values in memory (estimated size 7.7 KB, free 904.5 MB)
[INFO][2018-05-31 16:56:25,945][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_106 in memory on 10.194.32.157:61802 (size: 7.7 KB, free: 905.0 MB)
[INFO][2018-05-31 16:56:25,947][org.apache.spark.executor.Executor]Finished task 105.0 in stage 4.0 (TID 110). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:25,947][org.apache.spark.scheduler.TaskSetManager]Starting task 107.0 in stage 4.0 (TID 112, localhost, executor driver, partition 107, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:25,948][org.apache.spark.scheduler.TaskSetManager]Finished task 105.0 in stage 4.0 (TID 110) in 18 ms on localhost (executor driver) (106/200)
[INFO][2018-05-31 16:56:25,949][org.apache.spark.executor.Executor]Running task 107.0 in stage 4.0 (TID 112)
[INFO][2018-05-31 16:56:25,950][org.apache.spark.executor.Executor]Finished task 106.0 in stage 4.0 (TID 111). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:25,950][org.apache.spark.scheduler.TaskSetManager]Starting task 108.0 in stage 4.0 (TID 113, localhost, executor driver, partition 108, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:25,951][org.apache.spark.executor.Executor]Running task 108.0 in stage 4.0 (TID 113)
[INFO][2018-05-31 16:56:25,951][org.apache.spark.scheduler.TaskSetManager]Finished task 106.0 in stage 4.0 (TID 111) in 13 ms on localhost (executor driver) (107/200)
[INFO][2018-05-31 16:56:25,952][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:25,952][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,953][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:25,953][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,953][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:25,954][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO][2018-05-31 16:56:25,954][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:25,954][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,956][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_108 stored as values in memory (estimated size 8.5 KB, free 904.4 MB)
[INFO][2018-05-31 16:56:25,957][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_108 in memory on 10.194.32.157:61802 (size: 8.5 KB, free: 905.0 MB)
[INFO][2018-05-31 16:56:25,962][org.apache.spark.executor.Executor]Finished task 108.0 in stage 4.0 (TID 113). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:25,962][org.apache.spark.scheduler.TaskSetManager]Starting task 109.0 in stage 4.0 (TID 114, localhost, executor driver, partition 109, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:25,962][org.apache.spark.scheduler.TaskSetManager]Finished task 108.0 in stage 4.0 (TID 113) in 12 ms on localhost (executor driver) (108/200)
[INFO][2018-05-31 16:56:25,963][org.apache.spark.executor.Executor]Running task 109.0 in stage 4.0 (TID 114)
[INFO][2018-05-31 16:56:25,965][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:25,965][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,966][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:25,966][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,969][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_109 stored as values in memory (estimated size 9.1 KB, free 904.3 MB)
[INFO][2018-05-31 16:56:25,969][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_109 in memory on 10.194.32.157:61802 (size: 9.1 KB, free: 905.0 MB)
[INFO][2018-05-31 16:56:25,975][org.apache.spark.executor.Executor]Finished task 109.0 in stage 4.0 (TID 114). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:25,975][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_107 stored as values in memory (estimated size 7.9 KB, free 904.4 MB)
[INFO][2018-05-31 16:56:25,977][org.apache.spark.scheduler.TaskSetManager]Starting task 110.0 in stage 4.0 (TID 115, localhost, executor driver, partition 110, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:25,977][org.apache.spark.scheduler.TaskSetManager]Finished task 109.0 in stage 4.0 (TID 114) in 15 ms on localhost (executor driver) (109/200)
[INFO][2018-05-31 16:56:25,978][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_107 in memory on 10.194.32.157:61802 (size: 7.9 KB, free: 905.0 MB)
[INFO][2018-05-31 16:56:25,978][org.apache.spark.executor.Executor]Running task 110.0 in stage 4.0 (TID 115)
[INFO][2018-05-31 16:56:25,984][org.apache.spark.executor.Executor]Finished task 107.0 in stage 4.0 (TID 112). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:25,984][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:25,984][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO][2018-05-31 16:56:25,984][org.apache.spark.scheduler.TaskSetManager]Starting task 111.0 in stage 4.0 (TID 116, localhost, executor driver, partition 111, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:25,985][org.apache.spark.scheduler.TaskSetManager]Finished task 107.0 in stage 4.0 (TID 112) in 38 ms on localhost (executor driver) (110/200)
[INFO][2018-05-31 16:56:25,985][org.apache.spark.executor.Executor]Running task 111.0 in stage 4.0 (TID 116)
[INFO][2018-05-31 16:56:25,986][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:25,986][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:25,992][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:25,993][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO][2018-05-31 16:56:25,996][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:25,996][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO][2018-05-31 16:56:25,998][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_111 stored as values in memory (estimated size 8.3 KB, free 872.3 MB)
[INFO][2018-05-31 16:56:25,998][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_111 in memory on 10.194.32.157:61802 (size: 8.3 KB, free: 905.0 MB)
[INFO][2018-05-31 16:56:26,001][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_110 stored as values in memory (estimated size 8.4 KB, free 904.4 MB)
[INFO][2018-05-31 16:56:26,005][org.apache.spark.executor.Executor]Finished task 111.0 in stage 4.0 (TID 116). 3748 bytes result sent to driver
[INFO][2018-05-31 16:56:26,005][org.apache.spark.scheduler.TaskSetManager]Starting task 112.0 in stage 4.0 (TID 117, localhost, executor driver, partition 112, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:26,006][org.apache.spark.scheduler.TaskSetManager]Finished task 111.0 in stage 4.0 (TID 116) in 22 ms on localhost (executor driver) (111/200)
[INFO][2018-05-31 16:56:26,007][org.apache.spark.executor.Executor]Running task 112.0 in stage 4.0 (TID 117)
[INFO][2018-05-31 16:56:26,017][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_110 in memory on 10.194.32.157:61802 (size: 8.4 KB, free: 905.0 MB)
[INFO][2018-05-31 16:56:26,018][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:26,018][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,019][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:26,019][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,022][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_112 stored as values in memory (estimated size 8.5 KB, free 904.4 MB)
[INFO][2018-05-31 16:56:26,023][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_112 in memory on 10.194.32.157:61802 (size: 8.5 KB, free: 905.0 MB)
[INFO][2018-05-31 16:56:26,035][org.apache.spark.executor.Executor]Finished task 112.0 in stage 4.0 (TID 117). 3748 bytes result sent to driver
[INFO][2018-05-31 16:56:26,036][org.apache.spark.scheduler.TaskSetManager]Starting task 113.0 in stage 4.0 (TID 118, localhost, executor driver, partition 113, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:26,036][org.apache.spark.scheduler.TaskSetManager]Finished task 112.0 in stage 4.0 (TID 117) in 31 ms on localhost (executor driver) (112/200)
[INFO][2018-05-31 16:56:26,036][org.apache.spark.executor.Executor]Running task 113.0 in stage 4.0 (TID 118)
[INFO][2018-05-31 16:56:26,037][org.apache.spark.executor.Executor]Finished task 110.0 in stage 4.0 (TID 115). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:26,037][org.apache.spark.scheduler.TaskSetManager]Starting task 114.0 in stage 4.0 (TID 119, localhost, executor driver, partition 114, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:26,037][org.apache.spark.scheduler.TaskSetManager]Finished task 110.0 in stage 4.0 (TID 115) in 61 ms on localhost (executor driver) (113/200)
[INFO][2018-05-31 16:56:26,037][org.apache.spark.executor.Executor]Running task 114.0 in stage 4.0 (TID 119)
[INFO][2018-05-31 16:56:26,041][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:26,041][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,042][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:26,043][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO][2018-05-31 16:56:26,048][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_113 stored as values in memory (estimated size 7.7 KB, free 904.4 MB)
[INFO][2018-05-31 16:56:26,048][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_113 in memory on 10.194.32.157:61802 (size: 7.7 KB, free: 905.0 MB)
[INFO][2018-05-31 16:56:26,051][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:26,051][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,052][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:26,052][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,054][org.apache.spark.executor.Executor]Finished task 113.0 in stage 4.0 (TID 118). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:26,056][org.apache.spark.scheduler.TaskSetManager]Starting task 115.0 in stage 4.0 (TID 120, localhost, executor driver, partition 115, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:26,057][org.apache.spark.scheduler.TaskSetManager]Finished task 113.0 in stage 4.0 (TID 118) in 22 ms on localhost (executor driver) (114/200)
[INFO][2018-05-31 16:56:26,057][org.apache.spark.executor.Executor]Running task 115.0 in stage 4.0 (TID 120)
[INFO][2018-05-31 16:56:26,061][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:26,061][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,062][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:26,062][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,063][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_114 stored as values in memory (estimated size 8.5 KB, free 904.3 MB)
[INFO][2018-05-31 16:56:26,063][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_114 in memory on 10.194.32.157:61802 (size: 8.5 KB, free: 904.9 MB)
[INFO][2018-05-31 16:56:26,066][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_115 stored as values in memory (estimated size 8.1 KB, free 904.4 MB)
[INFO][2018-05-31 16:56:26,066][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_115 in memory on 10.194.32.157:61802 (size: 8.1 KB, free: 904.9 MB)
[INFO][2018-05-31 16:56:26,072][org.apache.spark.executor.Executor]Finished task 115.0 in stage 4.0 (TID 120). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:26,073][org.apache.spark.executor.Executor]Finished task 114.0 in stage 4.0 (TID 119). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:26,073][org.apache.spark.scheduler.TaskSetManager]Starting task 116.0 in stage 4.0 (TID 121, localhost, executor driver, partition 116, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:26,073][org.apache.spark.executor.Executor]Running task 116.0 in stage 4.0 (TID 121)
[INFO][2018-05-31 16:56:26,073][org.apache.spark.scheduler.TaskSetManager]Starting task 117.0 in stage 4.0 (TID 122, localhost, executor driver, partition 117, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:26,073][org.apache.spark.scheduler.TaskSetManager]Finished task 115.0 in stage 4.0 (TID 120) in 18 ms on localhost (executor driver) (115/200)
[INFO][2018-05-31 16:56:26,073][org.apache.spark.scheduler.TaskSetManager]Finished task 114.0 in stage 4.0 (TID 119) in 36 ms on localhost (executor driver) (116/200)
[INFO][2018-05-31 16:56:26,073][org.apache.spark.executor.Executor]Running task 117.0 in stage 4.0 (TID 122)
[INFO][2018-05-31 16:56:26,076][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:26,076][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,076][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:26,077][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO][2018-05-31 16:56:26,077][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:26,077][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,082][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:26,083][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,086][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_117 stored as values in memory (estimated size 8.4 KB, free 872.3 MB)
[INFO][2018-05-31 16:56:26,087][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_117 in memory on 10.194.32.157:61802 (size: 8.4 KB, free: 904.9 MB)
[INFO][2018-05-31 16:56:26,088][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_116 stored as values in memory (estimated size 7.8 KB, free 904.4 MB)
[INFO][2018-05-31 16:56:26,088][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_116 in memory on 10.194.32.157:61802 (size: 7.8 KB, free: 904.9 MB)
[INFO][2018-05-31 16:56:26,093][org.apache.spark.executor.Executor]Finished task 117.0 in stage 4.0 (TID 122). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:26,093][org.apache.spark.executor.Executor]Finished task 116.0 in stage 4.0 (TID 121). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:26,093][org.apache.spark.scheduler.TaskSetManager]Starting task 118.0 in stage 4.0 (TID 123, localhost, executor driver, partition 118, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:26,093][org.apache.spark.executor.Executor]Running task 118.0 in stage 4.0 (TID 123)
[INFO][2018-05-31 16:56:26,093][org.apache.spark.scheduler.TaskSetManager]Starting task 119.0 in stage 4.0 (TID 124, localhost, executor driver, partition 119, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:26,094][org.apache.spark.scheduler.TaskSetManager]Finished task 117.0 in stage 4.0 (TID 122) in 21 ms on localhost (executor driver) (117/200)
[INFO][2018-05-31 16:56:26,094][org.apache.spark.scheduler.TaskSetManager]Finished task 116.0 in stage 4.0 (TID 121) in 22 ms on localhost (executor driver) (118/200)
[INFO][2018-05-31 16:56:26,094][org.apache.spark.executor.Executor]Running task 119.0 in stage 4.0 (TID 124)
[INFO][2018-05-31 16:56:26,097][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:26,097][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,097][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:26,098][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:26,098][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,098][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO][2018-05-31 16:56:26,098][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:26,098][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,101][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_118 stored as values in memory (estimated size 7.8 KB, free 888.3 MB)
[INFO][2018-05-31 16:56:26,101][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_118 in memory on 10.194.32.157:61802 (size: 7.8 KB, free: 904.9 MB)
[INFO][2018-05-31 16:56:26,101][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_119 stored as values in memory (estimated size 8.3 KB, free 904.4 MB)
[INFO][2018-05-31 16:56:26,101][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_119 in memory on 10.194.32.157:61802 (size: 8.3 KB, free: 904.9 MB)
[INFO][2018-05-31 16:56:26,105][org.apache.spark.executor.Executor]Finished task 119.0 in stage 4.0 (TID 124). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:26,106][org.apache.spark.scheduler.TaskSetManager]Starting task 120.0 in stage 4.0 (TID 125, localhost, executor driver, partition 120, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:26,106][org.apache.spark.scheduler.TaskSetManager]Finished task 119.0 in stage 4.0 (TID 124) in 13 ms on localhost (executor driver) (119/200)
[INFO][2018-05-31 16:56:26,106][org.apache.spark.executor.Executor]Running task 120.0 in stage 4.0 (TID 125)
[INFO][2018-05-31 16:56:26,109][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:26,109][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,110][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:26,110][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,114][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_120 stored as values in memory (estimated size 7.2 KB, free 904.3 MB)
[INFO][2018-05-31 16:56:26,114][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_120 in memory on 10.194.32.157:61802 (size: 7.2 KB, free: 904.9 MB)
[INFO][2018-05-31 16:56:26,115][org.apache.spark.executor.Executor]Finished task 118.0 in stage 4.0 (TID 123). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:26,115][org.apache.spark.scheduler.TaskSetManager]Starting task 121.0 in stage 4.0 (TID 126, localhost, executor driver, partition 121, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:26,115][org.apache.spark.scheduler.TaskSetManager]Finished task 118.0 in stage 4.0 (TID 123) in 22 ms on localhost (executor driver) (120/200)
[INFO][2018-05-31 16:56:26,116][org.apache.spark.executor.Executor]Running task 121.0 in stage 4.0 (TID 126)
[INFO][2018-05-31 16:56:26,119][org.apache.spark.executor.Executor]Finished task 120.0 in stage 4.0 (TID 125). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:26,120][org.apache.spark.scheduler.TaskSetManager]Starting task 122.0 in stage 4.0 (TID 127, localhost, executor driver, partition 122, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:26,120][org.apache.spark.scheduler.TaskSetManager]Finished task 120.0 in stage 4.0 (TID 125) in 14 ms on localhost (executor driver) (121/200)
[INFO][2018-05-31 16:56:26,120][org.apache.spark.executor.Executor]Running task 122.0 in stage 4.0 (TID 127)
[INFO][2018-05-31 16:56:26,122][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:26,122][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,123][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:26,123][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:26,123][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,123][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,125][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:26,125][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,128][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_122 stored as values in memory (estimated size 7.7 KB, free 872.2 MB)
[INFO][2018-05-31 16:56:26,128][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_122 in memory on 10.194.32.157:61802 (size: 7.7 KB, free: 904.9 MB)
[INFO][2018-05-31 16:56:26,132][org.apache.spark.executor.Executor]Finished task 122.0 in stage 4.0 (TID 127). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:26,133][org.apache.spark.scheduler.TaskSetManager]Starting task 123.0 in stage 4.0 (TID 128, localhost, executor driver, partition 123, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:26,133][org.apache.spark.scheduler.TaskSetManager]Finished task 122.0 in stage 4.0 (TID 127) in 13 ms on localhost (executor driver) (122/200)
[INFO][2018-05-31 16:56:26,133][org.apache.spark.executor.Executor]Running task 123.0 in stage 4.0 (TID 128)
[INFO][2018-05-31 16:56:26,133][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_121 stored as values in memory (estimated size 6.3 KB, free 904.3 MB)
[INFO][2018-05-31 16:56:26,134][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_121 in memory on 10.194.32.157:61802 (size: 6.3 KB, free: 904.9 MB)
[INFO][2018-05-31 16:56:26,136][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:26,136][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,137][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:26,137][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,140][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_123 stored as values in memory (estimated size 8.6 KB, free 904.3 MB)
[INFO][2018-05-31 16:56:26,140][org.apache.spark.executor.Executor]Finished task 121.0 in stage 4.0 (TID 126). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:26,140][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_123 in memory on 10.194.32.157:61802 (size: 8.6 KB, free: 904.9 MB)
[INFO][2018-05-31 16:56:26,140][org.apache.spark.scheduler.TaskSetManager]Starting task 124.0 in stage 4.0 (TID 129, localhost, executor driver, partition 124, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:26,141][org.apache.spark.scheduler.TaskSetManager]Finished task 121.0 in stage 4.0 (TID 126) in 26 ms on localhost (executor driver) (123/200)
[INFO][2018-05-31 16:56:26,141][org.apache.spark.executor.Executor]Running task 124.0 in stage 4.0 (TID 129)
[INFO][2018-05-31 16:56:26,144][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:26,144][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,145][org.apache.spark.executor.Executor]Finished task 123.0 in stage 4.0 (TID 128). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:26,145][org.apache.spark.scheduler.TaskSetManager]Starting task 125.0 in stage 4.0 (TID 130, localhost, executor driver, partition 125, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:26,145][org.apache.spark.scheduler.TaskSetManager]Finished task 123.0 in stage 4.0 (TID 128) in 12 ms on localhost (executor driver) (124/200)
[INFO][2018-05-31 16:56:26,145][org.apache.spark.executor.Executor]Running task 125.0 in stage 4.0 (TID 130)
[INFO][2018-05-31 16:56:26,146][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:26,146][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,148][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:26,148][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,149][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_124 stored as values in memory (estimated size 8.6 KB, free 904.3 MB)
[INFO][2018-05-31 16:56:26,149][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:26,149][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,149][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_124 in memory on 10.194.32.157:61802 (size: 8.6 KB, free: 904.9 MB)
[INFO][2018-05-31 16:56:26,152][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_125 stored as values in memory (estimated size 9.0 KB, free 904.3 MB)
[INFO][2018-05-31 16:56:26,152][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_125 in memory on 10.194.32.157:61802 (size: 9.0 KB, free: 904.9 MB)
[INFO][2018-05-31 16:56:26,156][org.apache.spark.executor.Executor]Finished task 125.0 in stage 4.0 (TID 130). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:26,157][org.apache.spark.scheduler.TaskSetManager]Starting task 126.0 in stage 4.0 (TID 131, localhost, executor driver, partition 126, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:26,157][org.apache.spark.scheduler.TaskSetManager]Finished task 125.0 in stage 4.0 (TID 130) in 12 ms on localhost (executor driver) (125/200)
[INFO][2018-05-31 16:56:26,157][org.apache.spark.executor.Executor]Running task 126.0 in stage 4.0 (TID 131)
[INFO][2018-05-31 16:56:26,157][org.apache.spark.executor.Executor]Finished task 124.0 in stage 4.0 (TID 129). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:26,158][org.apache.spark.scheduler.TaskSetManager]Starting task 127.0 in stage 4.0 (TID 132, localhost, executor driver, partition 127, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:26,159][org.apache.spark.scheduler.TaskSetManager]Finished task 124.0 in stage 4.0 (TID 129) in 19 ms on localhost (executor driver) (126/200)
[INFO][2018-05-31 16:56:26,160][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:26,161][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO][2018-05-31 16:56:26,161][org.apache.spark.executor.Executor]Running task 127.0 in stage 4.0 (TID 132)
[INFO][2018-05-31 16:56:26,161][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:26,161][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,164][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_126 stored as values in memory (estimated size 7.8 KB, free 904.3 MB)
[INFO][2018-05-31 16:56:26,164][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_126 in memory on 10.194.32.157:61802 (size: 7.8 KB, free: 904.9 MB)
[INFO][2018-05-31 16:56:26,169][org.apache.spark.executor.Executor]Finished task 126.0 in stage 4.0 (TID 131). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:26,169][org.apache.spark.scheduler.TaskSetManager]Starting task 128.0 in stage 4.0 (TID 133, localhost, executor driver, partition 128, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:26,170][org.apache.spark.scheduler.TaskSetManager]Finished task 126.0 in stage 4.0 (TID 131) in 14 ms on localhost (executor driver) (127/200)
[INFO][2018-05-31 16:56:26,170][org.apache.spark.executor.Executor]Running task 128.0 in stage 4.0 (TID 133)
[INFO][2018-05-31 16:56:26,172][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:26,172][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,173][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:26,173][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,176][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_128 stored as values in memory (estimated size 7.2 KB, free 904.3 MB)
[INFO][2018-05-31 16:56:26,177][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_128 in memory on 10.194.32.157:61802 (size: 7.2 KB, free: 904.8 MB)
[INFO][2018-05-31 16:56:26,177][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:26,177][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,178][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:26,178][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,183][org.apache.spark.executor.Executor]Finished task 128.0 in stage 4.0 (TID 133). 3748 bytes result sent to driver
[INFO][2018-05-31 16:56:26,183][org.apache.spark.scheduler.TaskSetManager]Starting task 129.0 in stage 4.0 (TID 134, localhost, executor driver, partition 129, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:26,184][org.apache.spark.executor.Executor]Running task 129.0 in stage 4.0 (TID 134)
[INFO][2018-05-31 16:56:26,184][org.apache.spark.scheduler.TaskSetManager]Finished task 128.0 in stage 4.0 (TID 133) in 15 ms on localhost (executor driver) (128/200)
[INFO][2018-05-31 16:56:26,186][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_127 stored as values in memory (estimated size 6.7 KB, free 904.3 MB)
[INFO][2018-05-31 16:56:26,186][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_127 in memory on 10.194.32.157:61802 (size: 6.7 KB, free: 904.8 MB)
[INFO][2018-05-31 16:56:26,187][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:26,187][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,190][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:26,190][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,193][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_129 stored as values in memory (estimated size 9.7 KB, free 904.3 MB)
[INFO][2018-05-31 16:56:26,193][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_129 in memory on 10.194.32.157:61802 (size: 9.7 KB, free: 904.8 MB)
[INFO][2018-05-31 16:56:26,197][org.apache.spark.executor.Executor]Finished task 127.0 in stage 4.0 (TID 132). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:26,198][org.apache.spark.scheduler.TaskSetManager]Starting task 130.0 in stage 4.0 (TID 135, localhost, executor driver, partition 130, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:26,199][org.apache.spark.scheduler.TaskSetManager]Finished task 127.0 in stage 4.0 (TID 132) in 41 ms on localhost (executor driver) (129/200)
[INFO][2018-05-31 16:56:26,201][org.apache.spark.executor.Executor]Running task 130.0 in stage 4.0 (TID 135)
[INFO][2018-05-31 16:56:26,201][org.apache.spark.executor.Executor]Finished task 129.0 in stage 4.0 (TID 134). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:26,203][org.apache.spark.scheduler.TaskSetManager]Starting task 131.0 in stage 4.0 (TID 136, localhost, executor driver, partition 131, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:26,204][org.apache.spark.scheduler.TaskSetManager]Finished task 129.0 in stage 4.0 (TID 134) in 21 ms on localhost (executor driver) (130/200)
[INFO][2018-05-31 16:56:26,204][org.apache.spark.executor.Executor]Running task 131.0 in stage 4.0 (TID 136)
[INFO][2018-05-31 16:56:26,206][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:26,206][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,207][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:26,207][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,209][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:26,209][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,210][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:26,210][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,210][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_131 stored as values in memory (estimated size 8.4 KB, free 904.2 MB)
[INFO][2018-05-31 16:56:26,211][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_131 in memory on 10.194.32.157:61802 (size: 8.4 KB, free: 904.8 MB)
[INFO][2018-05-31 16:56:26,212][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_130 stored as values in memory (estimated size 8.6 KB, free 904.3 MB)
[INFO][2018-05-31 16:56:26,213][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_130 in memory on 10.194.32.157:61802 (size: 8.6 KB, free: 904.8 MB)
[INFO][2018-05-31 16:56:26,222][org.apache.spark.executor.Executor]Finished task 131.0 in stage 4.0 (TID 136). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:26,223][org.apache.spark.scheduler.TaskSetManager]Starting task 132.0 in stage 4.0 (TID 137, localhost, executor driver, partition 132, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:26,223][org.apache.spark.scheduler.TaskSetManager]Finished task 131.0 in stage 4.0 (TID 136) in 20 ms on localhost (executor driver) (131/200)
[INFO][2018-05-31 16:56:26,225][org.apache.spark.executor.Executor]Finished task 130.0 in stage 4.0 (TID 135). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:26,226][org.apache.spark.scheduler.TaskSetManager]Starting task 133.0 in stage 4.0 (TID 138, localhost, executor driver, partition 133, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:26,226][org.apache.spark.executor.Executor]Running task 132.0 in stage 4.0 (TID 137)
[INFO][2018-05-31 16:56:26,226][org.apache.spark.scheduler.TaskSetManager]Finished task 130.0 in stage 4.0 (TID 135) in 28 ms on localhost (executor driver) (132/200)
[INFO][2018-05-31 16:56:26,226][org.apache.spark.executor.Executor]Running task 133.0 in stage 4.0 (TID 138)
[INFO][2018-05-31 16:56:26,230][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:26,231][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO][2018-05-31 16:56:26,233][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:26,234][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO][2018-05-31 16:56:26,235][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:26,235][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,235][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:26,235][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,238][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_133 stored as values in memory (estimated size 8.6 KB, free 872.1 MB)
[INFO][2018-05-31 16:56:26,239][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_133 in memory on 10.194.32.157:61802 (size: 8.6 KB, free: 904.8 MB)
[INFO][2018-05-31 16:56:26,242][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_132 stored as values in memory (estimated size 6.9 KB, free 904.2 MB)
[INFO][2018-05-31 16:56:26,243][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_132 in memory on 10.194.32.157:61802 (size: 6.9 KB, free: 904.8 MB)
[INFO][2018-05-31 16:56:26,251][org.apache.spark.executor.Executor]Finished task 133.0 in stage 4.0 (TID 138). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:26,251][org.apache.spark.scheduler.TaskSetManager]Starting task 134.0 in stage 4.0 (TID 139, localhost, executor driver, partition 134, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:26,252][org.apache.spark.scheduler.TaskSetManager]Finished task 133.0 in stage 4.0 (TID 138) in 26 ms on localhost (executor driver) (133/200)
[INFO][2018-05-31 16:56:26,252][org.apache.spark.executor.Executor]Running task 134.0 in stage 4.0 (TID 139)
[INFO][2018-05-31 16:56:26,256][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:26,256][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,258][org.apache.spark.executor.Executor]Finished task 132.0 in stage 4.0 (TID 137). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:26,259][org.apache.spark.scheduler.TaskSetManager]Starting task 135.0 in stage 4.0 (TID 140, localhost, executor driver, partition 135, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:26,260][org.apache.spark.scheduler.TaskSetManager]Finished task 132.0 in stage 4.0 (TID 137) in 37 ms on localhost (executor driver) (134/200)
[INFO][2018-05-31 16:56:26,261][org.apache.spark.executor.Executor]Running task 135.0 in stage 4.0 (TID 140)
[INFO][2018-05-31 16:56:26,261][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:26,261][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,264][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:26,264][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_134 stored as values in memory (estimated size 7.1 KB, free 904.2 MB)
[INFO][2018-05-31 16:56:26,265][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO][2018-05-31 16:56:26,265][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_134 in memory on 10.194.32.157:61802 (size: 7.1 KB, free: 904.8 MB)
[INFO][2018-05-31 16:56:26,266][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:26,266][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,273][org.apache.spark.executor.Executor]Finished task 134.0 in stage 4.0 (TID 139). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:26,274][org.apache.spark.scheduler.TaskSetManager]Starting task 136.0 in stage 4.0 (TID 141, localhost, executor driver, partition 136, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:26,274][org.apache.spark.executor.Executor]Running task 136.0 in stage 4.0 (TID 141)
[INFO][2018-05-31 16:56:26,274][org.apache.spark.scheduler.TaskSetManager]Finished task 134.0 in stage 4.0 (TID 139) in 23 ms on localhost (executor driver) (135/200)
[INFO][2018-05-31 16:56:26,275][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_135 stored as values in memory (estimated size 8.9 KB, free 904.2 MB)
[INFO][2018-05-31 16:56:26,275][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_135 in memory on 10.194.32.157:61802 (size: 8.9 KB, free: 904.8 MB)
[INFO][2018-05-31 16:56:26,276][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:26,277][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO][2018-05-31 16:56:26,278][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:26,278][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO][2018-05-31 16:56:26,286][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_136 stored as values in memory (estimated size 8.0 KB, free 904.2 MB)
[INFO][2018-05-31 16:56:26,287][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_136 in memory on 10.194.32.157:61802 (size: 8.0 KB, free: 904.8 MB)
[INFO][2018-05-31 16:56:26,290][org.apache.spark.executor.Executor]Finished task 135.0 in stage 4.0 (TID 140). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:26,290][org.apache.spark.scheduler.TaskSetManager]Starting task 137.0 in stage 4.0 (TID 142, localhost, executor driver, partition 137, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:26,290][org.apache.spark.scheduler.TaskSetManager]Finished task 135.0 in stage 4.0 (TID 140) in 31 ms on localhost (executor driver) (136/200)
[INFO][2018-05-31 16:56:26,291][org.apache.spark.executor.Executor]Running task 137.0 in stage 4.0 (TID 142)
[INFO][2018-05-31 16:56:26,292][org.apache.spark.executor.Executor]Finished task 136.0 in stage 4.0 (TID 141). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:26,293][org.apache.spark.scheduler.TaskSetManager]Starting task 138.0 in stage 4.0 (TID 143, localhost, executor driver, partition 138, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:26,293][org.apache.spark.scheduler.TaskSetManager]Finished task 136.0 in stage 4.0 (TID 141) in 19 ms on localhost (executor driver) (137/200)
[INFO][2018-05-31 16:56:26,294][org.apache.spark.executor.Executor]Running task 138.0 in stage 4.0 (TID 143)
[INFO][2018-05-31 16:56:26,295][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:26,295][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO][2018-05-31 16:56:26,301][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:26,301][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,302][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:26,302][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,302][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:26,302][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,304][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_137 stored as values in memory (estimated size 8.5 KB, free 872.1 MB)
[INFO][2018-05-31 16:56:26,304][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_137 in memory on 10.194.32.157:61802 (size: 8.5 KB, free: 904.8 MB)
[INFO][2018-05-31 16:56:26,305][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_138 stored as values in memory (estimated size 7.5 KB, free 904.2 MB)
[INFO][2018-05-31 16:56:26,305][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_138 in memory on 10.194.32.157:61802 (size: 7.5 KB, free: 904.8 MB)
[INFO][2018-05-31 16:56:26,310][org.apache.spark.executor.Executor]Finished task 138.0 in stage 4.0 (TID 143). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:26,311][org.apache.spark.scheduler.TaskSetManager]Starting task 139.0 in stage 4.0 (TID 144, localhost, executor driver, partition 139, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:26,311][org.apache.spark.executor.Executor]Running task 139.0 in stage 4.0 (TID 144)
[INFO][2018-05-31 16:56:26,310][org.apache.spark.executor.Executor]Finished task 137.0 in stage 4.0 (TID 142). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:26,311][org.apache.spark.scheduler.TaskSetManager]Finished task 138.0 in stage 4.0 (TID 143) in 19 ms on localhost (executor driver) (138/200)
[INFO][2018-05-31 16:56:26,312][org.apache.spark.scheduler.TaskSetManager]Starting task 140.0 in stage 4.0 (TID 145, localhost, executor driver, partition 140, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:26,312][org.apache.spark.scheduler.TaskSetManager]Finished task 137.0 in stage 4.0 (TID 142) in 22 ms on localhost (executor driver) (139/200)
[INFO][2018-05-31 16:56:26,312][org.apache.spark.executor.Executor]Running task 140.0 in stage 4.0 (TID 145)
[INFO][2018-05-31 16:56:26,314][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:26,314][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,315][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:26,315][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,315][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:26,315][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,316][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:26,316][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,317][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_139 stored as values in memory (estimated size 8.3 KB, free 872.1 MB)
[INFO][2018-05-31 16:56:26,318][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_139 in memory on 10.194.32.157:61802 (size: 8.3 KB, free: 904.8 MB)
[INFO][2018-05-31 16:56:26,318][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_140 stored as values in memory (estimated size 6.6 KB, free 904.2 MB)
[INFO][2018-05-31 16:56:26,318][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_140 in memory on 10.194.32.157:61802 (size: 6.6 KB, free: 904.7 MB)
[INFO][2018-05-31 16:56:26,322][org.apache.spark.executor.Executor]Finished task 139.0 in stage 4.0 (TID 144). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:26,322][org.apache.spark.scheduler.TaskSetManager]Starting task 141.0 in stage 4.0 (TID 146, localhost, executor driver, partition 141, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:26,323][org.apache.spark.scheduler.TaskSetManager]Finished task 139.0 in stage 4.0 (TID 144) in 13 ms on localhost (executor driver) (140/200)
[INFO][2018-05-31 16:56:26,323][org.apache.spark.executor.Executor]Running task 141.0 in stage 4.0 (TID 146)
[INFO][2018-05-31 16:56:26,324][org.apache.spark.executor.Executor]Finished task 140.0 in stage 4.0 (TID 145). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:26,324][org.apache.spark.scheduler.TaskSetManager]Starting task 142.0 in stage 4.0 (TID 147, localhost, executor driver, partition 142, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:26,325][org.apache.spark.scheduler.TaskSetManager]Finished task 140.0 in stage 4.0 (TID 145) in 14 ms on localhost (executor driver) (141/200)
[INFO][2018-05-31 16:56:26,325][org.apache.spark.executor.Executor]Running task 142.0 in stage 4.0 (TID 147)
[INFO][2018-05-31 16:56:26,325][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:26,325][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,326][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:26,326][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,327][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:26,327][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,328][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:26,328][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,329][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_141 stored as values in memory (estimated size 7.4 KB, free 904.1 MB)
[INFO][2018-05-31 16:56:26,330][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_141 in memory on 10.194.32.157:61802 (size: 7.4 KB, free: 904.7 MB)
[INFO][2018-05-31 16:56:26,332][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_142 stored as values in memory (estimated size 7.5 KB, free 904.2 MB)
[INFO][2018-05-31 16:56:26,332][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_142 in memory on 10.194.32.157:61802 (size: 7.5 KB, free: 904.7 MB)
[INFO][2018-05-31 16:56:26,334][org.apache.spark.executor.Executor]Finished task 141.0 in stage 4.0 (TID 146). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:26,335][org.apache.spark.scheduler.TaskSetManager]Starting task 143.0 in stage 4.0 (TID 148, localhost, executor driver, partition 143, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:26,335][org.apache.spark.scheduler.TaskSetManager]Finished task 141.0 in stage 4.0 (TID 146) in 13 ms on localhost (executor driver) (142/200)
[INFO][2018-05-31 16:56:26,335][org.apache.spark.executor.Executor]Running task 143.0 in stage 4.0 (TID 148)
[INFO][2018-05-31 16:56:26,338][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:26,338][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,339][org.apache.spark.executor.Executor]Finished task 142.0 in stage 4.0 (TID 147). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:26,339][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:26,339][org.apache.spark.scheduler.TaskSetManager]Starting task 144.0 in stage 4.0 (TID 149, localhost, executor driver, partition 144, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:26,339][org.apache.spark.scheduler.TaskSetManager]Finished task 142.0 in stage 4.0 (TID 147) in 15 ms on localhost (executor driver) (143/200)
[INFO][2018-05-31 16:56:26,339][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,340][org.apache.spark.executor.Executor]Running task 144.0 in stage 4.0 (TID 149)
[INFO][2018-05-31 16:56:26,342][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:26,342][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,343][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_143 stored as values in memory (estimated size 8.7 KB, free 904.2 MB)
[INFO][2018-05-31 16:56:26,343][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_143 in memory on 10.194.32.157:61802 (size: 8.7 KB, free: 904.7 MB)
[INFO][2018-05-31 16:56:26,343][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:26,343][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,347][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_144 stored as values in memory (estimated size 10.0 KB, free 904.2 MB)
[INFO][2018-05-31 16:56:26,347][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_144 in memory on 10.194.32.157:61802 (size: 10.0 KB, free: 904.7 MB)
[INFO][2018-05-31 16:56:26,348][org.apache.spark.executor.Executor]Finished task 143.0 in stage 4.0 (TID 148). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:26,348][org.apache.spark.scheduler.TaskSetManager]Starting task 145.0 in stage 4.0 (TID 150, localhost, executor driver, partition 145, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:26,349][org.apache.spark.executor.Executor]Running task 145.0 in stage 4.0 (TID 150)
[INFO][2018-05-31 16:56:26,349][org.apache.spark.scheduler.TaskSetManager]Finished task 143.0 in stage 4.0 (TID 148) in 14 ms on localhost (executor driver) (144/200)
[INFO][2018-05-31 16:56:26,351][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:26,351][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,352][org.apache.spark.executor.Executor]Finished task 144.0 in stage 4.0 (TID 149). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:26,352][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:26,352][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,352][org.apache.spark.scheduler.TaskSetManager]Starting task 146.0 in stage 4.0 (TID 151, localhost, executor driver, partition 146, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:26,352][org.apache.spark.scheduler.TaskSetManager]Finished task 144.0 in stage 4.0 (TID 149) in 13 ms on localhost (executor driver) (145/200)
[INFO][2018-05-31 16:56:26,352][org.apache.spark.executor.Executor]Running task 146.0 in stage 4.0 (TID 151)
[INFO][2018-05-31 16:56:26,356][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:26,356][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO][2018-05-31 16:56:26,356][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_145 stored as values in memory (estimated size 9.6 KB, free 904.2 MB)
[INFO][2018-05-31 16:56:26,356][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_145 in memory on 10.194.32.157:61802 (size: 9.6 KB, free: 904.7 MB)
[INFO][2018-05-31 16:56:26,356][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:26,356][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,359][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_146 stored as values in memory (estimated size 7.5 KB, free 904.1 MB)
[INFO][2018-05-31 16:56:26,359][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_146 in memory on 10.194.32.157:61802 (size: 7.5 KB, free: 904.7 MB)
[INFO][2018-05-31 16:56:26,360][org.apache.spark.executor.Executor]Finished task 145.0 in stage 4.0 (TID 150). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:26,361][org.apache.spark.scheduler.TaskSetManager]Starting task 147.0 in stage 4.0 (TID 152, localhost, executor driver, partition 147, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:26,361][org.apache.spark.scheduler.TaskSetManager]Finished task 145.0 in stage 4.0 (TID 150) in 13 ms on localhost (executor driver) (146/200)
[INFO][2018-05-31 16:56:26,361][org.apache.spark.executor.Executor]Running task 147.0 in stage 4.0 (TID 152)
[INFO][2018-05-31 16:56:26,364][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:26,364][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,365][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:26,365][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,367][org.apache.spark.executor.Executor]Finished task 146.0 in stage 4.0 (TID 151). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:26,367][org.apache.spark.scheduler.TaskSetManager]Starting task 148.0 in stage 4.0 (TID 153, localhost, executor driver, partition 148, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:26,368][org.apache.spark.scheduler.TaskSetManager]Finished task 146.0 in stage 4.0 (TID 151) in 16 ms on localhost (executor driver) (147/200)
[INFO][2018-05-31 16:56:26,368][org.apache.spark.executor.Executor]Running task 148.0 in stage 4.0 (TID 153)
[INFO][2018-05-31 16:56:26,369][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_147 stored as values in memory (estimated size 7.9 KB, free 904.1 MB)
[INFO][2018-05-31 16:56:26,369][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_147 in memory on 10.194.32.157:61802 (size: 7.9 KB, free: 904.7 MB)
[INFO][2018-05-31 16:56:26,372][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:26,372][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,375][org.apache.spark.executor.Executor]Finished task 147.0 in stage 4.0 (TID 152). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:26,375][org.apache.spark.scheduler.TaskSetManager]Starting task 149.0 in stage 4.0 (TID 154, localhost, executor driver, partition 149, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:26,375][org.apache.spark.scheduler.TaskSetManager]Finished task 147.0 in stage 4.0 (TID 152) in 14 ms on localhost (executor driver) (148/200)
[INFO][2018-05-31 16:56:26,375][org.apache.spark.executor.Executor]Running task 149.0 in stage 4.0 (TID 154)
[INFO][2018-05-31 16:56:26,376][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:26,376][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,378][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:26,379][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO][2018-05-31 16:56:26,381][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:26,381][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,383][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_148 stored as values in memory (estimated size 7.8 KB, free 872.0 MB)
[INFO][2018-05-31 16:56:26,383][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_148 in memory on 10.194.32.157:61802 (size: 7.8 KB, free: 904.7 MB)
[INFO][2018-05-31 16:56:26,384][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_149 stored as values in memory (estimated size 9.1 KB, free 904.1 MB)
[INFO][2018-05-31 16:56:26,384][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_149 in memory on 10.194.32.157:61802 (size: 9.1 KB, free: 904.7 MB)
[INFO][2018-05-31 16:56:26,390][org.apache.spark.executor.Executor]Finished task 148.0 in stage 4.0 (TID 153). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:26,390][org.apache.spark.executor.Executor]Finished task 149.0 in stage 4.0 (TID 154). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:26,390][org.apache.spark.scheduler.TaskSetManager]Starting task 150.0 in stage 4.0 (TID 155, localhost, executor driver, partition 150, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:26,390][org.apache.spark.executor.Executor]Running task 150.0 in stage 4.0 (TID 155)
[INFO][2018-05-31 16:56:26,390][org.apache.spark.scheduler.TaskSetManager]Starting task 151.0 in stage 4.0 (TID 156, localhost, executor driver, partition 151, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:26,390][org.apache.spark.scheduler.TaskSetManager]Finished task 148.0 in stage 4.0 (TID 153) in 23 ms on localhost (executor driver) (149/200)
[INFO][2018-05-31 16:56:26,390][org.apache.spark.scheduler.TaskSetManager]Finished task 149.0 in stage 4.0 (TID 154) in 15 ms on localhost (executor driver) (150/200)
[INFO][2018-05-31 16:56:26,390][org.apache.spark.executor.Executor]Running task 151.0 in stage 4.0 (TID 156)
[INFO][2018-05-31 16:56:26,392][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:26,393][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO][2018-05-31 16:56:26,403][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:26,403][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:26,403][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,403][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,406][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:26,406][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,408][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_150 stored as values in memory (estimated size 9.1 KB, free 888.0 MB)
[INFO][2018-05-31 16:56:26,408][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_150 in memory on 10.194.32.157:61802 (size: 9.1 KB, free: 904.7 MB)
[INFO][2018-05-31 16:56:26,410][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_151 stored as values in memory (estimated size 10.3 KB, free 904.1 MB)
[INFO][2018-05-31 16:56:26,410][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_151 in memory on 10.194.32.157:61802 (size: 10.3 KB, free: 904.7 MB)
[INFO][2018-05-31 16:56:26,414][org.apache.spark.executor.Executor]Finished task 150.0 in stage 4.0 (TID 155). 3748 bytes result sent to driver
[INFO][2018-05-31 16:56:26,414][org.apache.spark.scheduler.TaskSetManager]Starting task 152.0 in stage 4.0 (TID 157, localhost, executor driver, partition 152, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:26,414][org.apache.spark.scheduler.TaskSetManager]Finished task 150.0 in stage 4.0 (TID 155) in 24 ms on localhost (executor driver) (151/200)
[INFO][2018-05-31 16:56:26,414][org.apache.spark.executor.Executor]Running task 152.0 in stage 4.0 (TID 157)
[INFO][2018-05-31 16:56:26,415][org.apache.spark.executor.Executor]Finished task 151.0 in stage 4.0 (TID 156). 3791 bytes result sent to driver
[INFO][2018-05-31 16:56:26,416][org.apache.spark.scheduler.TaskSetManager]Starting task 153.0 in stage 4.0 (TID 158, localhost, executor driver, partition 153, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:26,416][org.apache.spark.scheduler.TaskSetManager]Finished task 151.0 in stage 4.0 (TID 156) in 26 ms on localhost (executor driver) (152/200)
[INFO][2018-05-31 16:56:26,416][org.apache.spark.executor.Executor]Running task 153.0 in stage 4.0 (TID 158)
[INFO][2018-05-31 16:56:26,423][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:26,423][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,425][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:26,425][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,426][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:26,426][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,428][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:26,428][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,429][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_153 stored as values in memory (estimated size 8.7 KB, free 904.0 MB)
[INFO][2018-05-31 16:56:26,429][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_153 in memory on 10.194.32.157:61802 (size: 8.7 KB, free: 904.6 MB)
[INFO][2018-05-31 16:56:26,439][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_152 stored as values in memory (estimated size 8.4 KB, free 904.1 MB)
[INFO][2018-05-31 16:56:26,439][org.apache.spark.executor.Executor]Finished task 153.0 in stage 4.0 (TID 158). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:26,439][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_152 in memory on 10.194.32.157:61802 (size: 8.4 KB, free: 904.6 MB)
[INFO][2018-05-31 16:56:26,440][org.apache.spark.scheduler.TaskSetManager]Starting task 154.0 in stage 4.0 (TID 159, localhost, executor driver, partition 154, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:26,440][org.apache.spark.executor.Executor]Running task 154.0 in stage 4.0 (TID 159)
[INFO][2018-05-31 16:56:26,440][org.apache.spark.scheduler.TaskSetManager]Finished task 153.0 in stage 4.0 (TID 158) in 25 ms on localhost (executor driver) (153/200)
[INFO][2018-05-31 16:56:26,443][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:26,443][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,446][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:26,446][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO][2018-05-31 16:56:26,453][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_154 stored as values in memory (estimated size 7.5 KB, free 904.1 MB)
[INFO][2018-05-31 16:56:26,454][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_154 in memory on 10.194.32.157:61802 (size: 7.5 KB, free: 904.6 MB)
[INFO][2018-05-31 16:56:26,454][org.apache.spark.executor.Executor]Finished task 152.0 in stage 4.0 (TID 157). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:26,455][org.apache.spark.scheduler.TaskSetManager]Starting task 155.0 in stage 4.0 (TID 160, localhost, executor driver, partition 155, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:26,455][org.apache.spark.scheduler.TaskSetManager]Finished task 152.0 in stage 4.0 (TID 157) in 41 ms on localhost (executor driver) (154/200)
[INFO][2018-05-31 16:56:26,455][org.apache.spark.executor.Executor]Running task 155.0 in stage 4.0 (TID 160)
[INFO][2018-05-31 16:56:26,457][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:26,457][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,458][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:26,458][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,463][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_155 stored as values in memory (estimated size 6.9 KB, free 904.1 MB)
[INFO][2018-05-31 16:56:26,464][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_155 in memory on 10.194.32.157:61802 (size: 6.9 KB, free: 904.6 MB)
[INFO][2018-05-31 16:56:26,465][org.apache.spark.executor.Executor]Finished task 154.0 in stage 4.0 (TID 159). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:26,465][org.apache.spark.scheduler.TaskSetManager]Starting task 156.0 in stage 4.0 (TID 161, localhost, executor driver, partition 156, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:26,466][org.apache.spark.scheduler.TaskSetManager]Finished task 154.0 in stage 4.0 (TID 159) in 27 ms on localhost (executor driver) (155/200)
[INFO][2018-05-31 16:56:26,468][org.apache.spark.executor.Executor]Running task 156.0 in stage 4.0 (TID 161)
[INFO][2018-05-31 16:56:26,476][org.apache.spark.executor.Executor]Finished task 155.0 in stage 4.0 (TID 160). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:26,476][org.apache.spark.scheduler.TaskSetManager]Starting task 157.0 in stage 4.0 (TID 162, localhost, executor driver, partition 157, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:26,476][org.apache.spark.scheduler.TaskSetManager]Finished task 155.0 in stage 4.0 (TID 160) in 22 ms on localhost (executor driver) (156/200)
[INFO][2018-05-31 16:56:26,477][org.apache.spark.executor.Executor]Running task 157.0 in stage 4.0 (TID 162)
[INFO][2018-05-31 16:56:26,478][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:26,478][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,479][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:26,479][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,480][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:26,480][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,480][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:26,480][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,485][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_157 stored as values in memory (estimated size 8.4 KB, free 887.9 MB)
[INFO][2018-05-31 16:56:26,486][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_157 in memory on 10.194.32.157:61802 (size: 8.4 KB, free: 904.6 MB)
[INFO][2018-05-31 16:56:26,495][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_156 stored as values in memory (estimated size 9.4 KB, free 904.1 MB)
[INFO][2018-05-31 16:56:26,495][org.apache.spark.executor.Executor]Finished task 157.0 in stage 4.0 (TID 162). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:26,495][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_156 in memory on 10.194.32.157:61802 (size: 9.4 KB, free: 904.6 MB)
[INFO][2018-05-31 16:56:26,495][org.apache.spark.scheduler.TaskSetManager]Starting task 158.0 in stage 4.0 (TID 163, localhost, executor driver, partition 158, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:26,495][org.apache.spark.scheduler.TaskSetManager]Finished task 157.0 in stage 4.0 (TID 162) in 19 ms on localhost (executor driver) (157/200)
[INFO][2018-05-31 16:56:26,495][org.apache.spark.executor.Executor]Running task 158.0 in stage 4.0 (TID 163)
[INFO][2018-05-31 16:56:26,501][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:26,501][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,504][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:26,505][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,508][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_158 stored as values in memory (estimated size 7.4 KB, free 904.0 MB)
[INFO][2018-05-31 16:56:26,508][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_158 in memory on 10.194.32.157:61802 (size: 7.4 KB, free: 904.6 MB)
[INFO][2018-05-31 16:56:26,509][org.apache.spark.executor.Executor]Finished task 156.0 in stage 4.0 (TID 161). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:26,510][org.apache.spark.scheduler.TaskSetManager]Starting task 159.0 in stage 4.0 (TID 164, localhost, executor driver, partition 159, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:26,510][org.apache.spark.scheduler.TaskSetManager]Finished task 156.0 in stage 4.0 (TID 161) in 45 ms on localhost (executor driver) (158/200)
[INFO][2018-05-31 16:56:26,510][org.apache.spark.executor.Executor]Running task 159.0 in stage 4.0 (TID 164)
[INFO][2018-05-31 16:56:26,516][org.apache.spark.executor.Executor]Finished task 158.0 in stage 4.0 (TID 163). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:26,517][org.apache.spark.scheduler.TaskSetManager]Starting task 160.0 in stage 4.0 (TID 165, localhost, executor driver, partition 160, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:26,517][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:26,517][org.apache.spark.scheduler.TaskSetManager]Finished task 158.0 in stage 4.0 (TID 163) in 22 ms on localhost (executor driver) (159/200)
[INFO][2018-05-31 16:56:26,518][org.apache.spark.executor.Executor]Running task 160.0 in stage 4.0 (TID 165)
[INFO][2018-05-31 16:56:26,518][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO][2018-05-31 16:56:26,519][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:26,519][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,520][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:26,520][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,521][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:26,521][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,521][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_159 stored as values in memory (estimated size 6.5 KB, free 903.9 MB)
[INFO][2018-05-31 16:56:26,522][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_159 in memory on 10.194.32.157:61802 (size: 6.5 KB, free: 904.6 MB)
[INFO][2018-05-31 16:56:26,523][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_160 stored as values in memory (estimated size 7.2 KB, free 904.0 MB)
[INFO][2018-05-31 16:56:26,523][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_160 in memory on 10.194.32.157:61802 (size: 7.2 KB, free: 904.6 MB)
[INFO][2018-05-31 16:56:26,526][org.apache.spark.executor.Executor]Finished task 159.0 in stage 4.0 (TID 164). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:26,527][org.apache.spark.executor.Executor]Finished task 160.0 in stage 4.0 (TID 165). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:26,527][org.apache.spark.scheduler.TaskSetManager]Starting task 161.0 in stage 4.0 (TID 166, localhost, executor driver, partition 161, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:26,527][org.apache.spark.scheduler.TaskSetManager]Starting task 162.0 in stage 4.0 (TID 167, localhost, executor driver, partition 162, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:26,527][org.apache.spark.scheduler.TaskSetManager]Finished task 159.0 in stage 4.0 (TID 164) in 17 ms on localhost (executor driver) (160/200)
[INFO][2018-05-31 16:56:26,527][org.apache.spark.executor.Executor]Running task 161.0 in stage 4.0 (TID 166)
[INFO][2018-05-31 16:56:26,527][org.apache.spark.scheduler.TaskSetManager]Finished task 160.0 in stage 4.0 (TID 165) in 10 ms on localhost (executor driver) (161/200)
[INFO][2018-05-31 16:56:26,527][org.apache.spark.executor.Executor]Running task 162.0 in stage 4.0 (TID 167)
[INFO][2018-05-31 16:56:26,530][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:26,530][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,530][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:26,531][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO][2018-05-31 16:56:26,531][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:26,531][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,532][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:26,532][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,534][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_162 stored as values in memory (estimated size 7.7 KB, free 871.9 MB)
[INFO][2018-05-31 16:56:26,534][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_162 in memory on 10.194.32.157:61802 (size: 7.7 KB, free: 904.6 MB)
[INFO][2018-05-31 16:56:26,536][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_161 stored as values in memory (estimated size 7.7 KB, free 904.0 MB)
[INFO][2018-05-31 16:56:26,536][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_161 in memory on 10.194.32.157:61802 (size: 7.7 KB, free: 904.6 MB)
[INFO][2018-05-31 16:56:26,539][org.apache.spark.executor.Executor]Finished task 162.0 in stage 4.0 (TID 167). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:26,539][org.apache.spark.scheduler.TaskSetManager]Starting task 163.0 in stage 4.0 (TID 168, localhost, executor driver, partition 163, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:26,539][org.apache.spark.scheduler.TaskSetManager]Finished task 162.0 in stage 4.0 (TID 167) in 12 ms on localhost (executor driver) (162/200)
[INFO][2018-05-31 16:56:26,539][org.apache.spark.executor.Executor]Running task 163.0 in stage 4.0 (TID 168)
[INFO][2018-05-31 16:56:26,540][org.apache.spark.executor.Executor]Finished task 161.0 in stage 4.0 (TID 166). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:26,541][org.apache.spark.scheduler.TaskSetManager]Starting task 164.0 in stage 4.0 (TID 169, localhost, executor driver, partition 164, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:26,541][org.apache.spark.scheduler.TaskSetManager]Finished task 161.0 in stage 4.0 (TID 166) in 14 ms on localhost (executor driver) (163/200)
[INFO][2018-05-31 16:56:26,541][org.apache.spark.executor.Executor]Running task 164.0 in stage 4.0 (TID 169)
[INFO][2018-05-31 16:56:26,543][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:26,543][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO][2018-05-31 16:56:26,543][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:26,543][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,543][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:26,543][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,544][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:26,544][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO][2018-05-31 16:56:26,546][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_164 stored as values in memory (estimated size 7.5 KB, free 904.0 MB)
[INFO][2018-05-31 16:56:26,546][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_163 stored as values in memory (estimated size 9.1 KB, free 904.0 MB)
[INFO][2018-05-31 16:56:26,546][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_164 in memory on 10.194.32.157:61802 (size: 7.5 KB, free: 904.6 MB)
[INFO][2018-05-31 16:56:26,547][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_163 in memory on 10.194.32.157:61802 (size: 9.1 KB, free: 904.6 MB)
[INFO][2018-05-31 16:56:26,551][org.apache.spark.executor.Executor]Finished task 164.0 in stage 4.0 (TID 169). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:26,551][org.apache.spark.scheduler.TaskSetManager]Starting task 165.0 in stage 4.0 (TID 170, localhost, executor driver, partition 165, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:26,552][org.apache.spark.scheduler.TaskSetManager]Finished task 164.0 in stage 4.0 (TID 169) in 11 ms on localhost (executor driver) (164/200)
[INFO][2018-05-31 16:56:26,552][org.apache.spark.executor.Executor]Running task 165.0 in stage 4.0 (TID 170)
[INFO][2018-05-31 16:56:26,552][org.apache.spark.executor.Executor]Finished task 163.0 in stage 4.0 (TID 168). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:26,553][org.apache.spark.scheduler.TaskSetManager]Starting task 166.0 in stage 4.0 (TID 171, localhost, executor driver, partition 166, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:26,553][org.apache.spark.scheduler.TaskSetManager]Finished task 163.0 in stage 4.0 (TID 168) in 14 ms on localhost (executor driver) (165/200)
[INFO][2018-05-31 16:56:26,553][org.apache.spark.executor.Executor]Running task 166.0 in stage 4.0 (TID 171)
[INFO][2018-05-31 16:56:26,554][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:26,554][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,555][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:26,555][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,557][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:26,558][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO][2018-05-31 16:56:26,558][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_165 stored as values in memory (estimated size 9.3 KB, free 904.0 MB)
[INFO][2018-05-31 16:56:26,558][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_165 in memory on 10.194.32.157:61802 (size: 9.3 KB, free: 904.5 MB)
[INFO][2018-05-31 16:56:26,558][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:26,558][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,561][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_166 stored as values in memory (estimated size 8.5 KB, free 904.0 MB)
[INFO][2018-05-31 16:56:26,561][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_166 in memory on 10.194.32.157:61802 (size: 8.5 KB, free: 904.5 MB)
[INFO][2018-05-31 16:56:26,563][org.apache.spark.executor.Executor]Finished task 165.0 in stage 4.0 (TID 170). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:26,564][org.apache.spark.scheduler.TaskSetManager]Starting task 167.0 in stage 4.0 (TID 172, localhost, executor driver, partition 167, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:26,564][org.apache.spark.scheduler.TaskSetManager]Finished task 165.0 in stage 4.0 (TID 170) in 13 ms on localhost (executor driver) (166/200)
[INFO][2018-05-31 16:56:26,564][org.apache.spark.executor.Executor]Running task 167.0 in stage 4.0 (TID 172)
[INFO][2018-05-31 16:56:26,566][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:26,566][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,567][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:26,567][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,570][org.apache.spark.executor.Executor]Finished task 166.0 in stage 4.0 (TID 171). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:26,571][org.apache.spark.scheduler.TaskSetManager]Starting task 168.0 in stage 4.0 (TID 173, localhost, executor driver, partition 168, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:26,571][org.apache.spark.scheduler.TaskSetManager]Finished task 166.0 in stage 4.0 (TID 171) in 18 ms on localhost (executor driver) (167/200)
[INFO][2018-05-31 16:56:26,571][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_167 stored as values in memory (estimated size 8.9 KB, free 904.0 MB)
[INFO][2018-05-31 16:56:26,571][org.apache.spark.executor.Executor]Running task 168.0 in stage 4.0 (TID 173)
[INFO][2018-05-31 16:56:26,572][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_167 in memory on 10.194.32.157:61802 (size: 8.9 KB, free: 904.5 MB)
[INFO][2018-05-31 16:56:26,578][org.apache.spark.executor.Executor]Finished task 167.0 in stage 4.0 (TID 172). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:26,578][org.apache.spark.scheduler.TaskSetManager]Starting task 169.0 in stage 4.0 (TID 174, localhost, executor driver, partition 169, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:26,578][org.apache.spark.scheduler.TaskSetManager]Finished task 167.0 in stage 4.0 (TID 172) in 15 ms on localhost (executor driver) (168/200)
[INFO][2018-05-31 16:56:26,579][org.apache.spark.executor.Executor]Running task 169.0 in stage 4.0 (TID 174)
[INFO][2018-05-31 16:56:26,579][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:26,580][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO][2018-05-31 16:56:26,582][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:26,582][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,583][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:26,583][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,584][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:26,584][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,588][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_169 stored as values in memory (estimated size 7.3 KB, free 903.8 MB)
[INFO][2018-05-31 16:56:26,588][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_169 in memory on 10.194.32.157:61802 (size: 7.3 KB, free: 904.5 MB)
[INFO][2018-05-31 16:56:26,590][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_168 stored as values in memory (estimated size 8.7 KB, free 904.0 MB)
[INFO][2018-05-31 16:56:26,591][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_168 in memory on 10.194.32.157:61802 (size: 8.7 KB, free: 904.5 MB)
[INFO][2018-05-31 16:56:26,598][org.apache.spark.executor.Executor]Finished task 169.0 in stage 4.0 (TID 174). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:26,598][org.apache.spark.scheduler.TaskSetManager]Starting task 170.0 in stage 4.0 (TID 175, localhost, executor driver, partition 170, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:26,599][org.apache.spark.scheduler.TaskSetManager]Finished task 169.0 in stage 4.0 (TID 174) in 21 ms on localhost (executor driver) (169/200)
[INFO][2018-05-31 16:56:26,599][org.apache.spark.executor.Executor]Running task 170.0 in stage 4.0 (TID 175)
[INFO][2018-05-31 16:56:26,601][org.apache.spark.executor.Executor]Finished task 168.0 in stage 4.0 (TID 173). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:26,601][org.apache.spark.scheduler.TaskSetManager]Starting task 171.0 in stage 4.0 (TID 176, localhost, executor driver, partition 171, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:26,601][org.apache.spark.scheduler.TaskSetManager]Finished task 168.0 in stage 4.0 (TID 173) in 30 ms on localhost (executor driver) (170/200)
[INFO][2018-05-31 16:56:26,601][org.apache.spark.executor.Executor]Running task 171.0 in stage 4.0 (TID 176)
[INFO][2018-05-31 16:56:26,601][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:26,602][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO][2018-05-31 16:56:26,612][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:26,613][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO][2018-05-31 16:56:26,613][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:26,614][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO][2018-05-31 16:56:26,618][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_171 stored as values in memory (estimated size 8.5 KB, free 904.0 MB)
[INFO][2018-05-31 16:56:26,619][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_171 in memory on 10.194.32.157:61802 (size: 8.5 KB, free: 904.5 MB)
[INFO][2018-05-31 16:56:26,632][org.apache.spark.executor.Executor]Finished task 171.0 in stage 4.0 (TID 176). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:26,632][org.apache.spark.scheduler.TaskSetManager]Starting task 172.0 in stage 4.0 (TID 177, localhost, executor driver, partition 172, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:26,632][org.apache.spark.scheduler.TaskSetManager]Finished task 171.0 in stage 4.0 (TID 176) in 31 ms on localhost (executor driver) (171/200)
[INFO][2018-05-31 16:56:26,632][org.apache.spark.executor.Executor]Running task 172.0 in stage 4.0 (TID 177)
[INFO][2018-05-31 16:56:26,635][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:26,635][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,636][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:26,636][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,640][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:26,640][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,644][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_170 stored as values in memory (estimated size 7.1 KB, free 903.9 MB)
[INFO][2018-05-31 16:56:26,646][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_170 in memory on 10.194.32.157:61802 (size: 7.1 KB, free: 904.5 MB)
[INFO][2018-05-31 16:56:26,646][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_172 stored as values in memory (estimated size 8.3 KB, free 903.9 MB)
[INFO][2018-05-31 16:56:26,647][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_172 in memory on 10.194.32.157:61802 (size: 8.3 KB, free: 904.5 MB)
[INFO][2018-05-31 16:56:26,656][org.apache.spark.executor.Executor]Finished task 170.0 in stage 4.0 (TID 175). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:26,656][org.apache.spark.scheduler.TaskSetManager]Starting task 173.0 in stage 4.0 (TID 178, localhost, executor driver, partition 173, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:26,656][org.apache.spark.scheduler.TaskSetManager]Finished task 170.0 in stage 4.0 (TID 175) in 58 ms on localhost (executor driver) (172/200)
[INFO][2018-05-31 16:56:26,657][org.apache.spark.executor.Executor]Running task 173.0 in stage 4.0 (TID 178)
[INFO][2018-05-31 16:56:26,658][org.apache.spark.executor.Executor]Finished task 172.0 in stage 4.0 (TID 177). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:26,658][org.apache.spark.scheduler.TaskSetManager]Starting task 174.0 in stage 4.0 (TID 179, localhost, executor driver, partition 174, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:26,658][org.apache.spark.scheduler.TaskSetManager]Finished task 172.0 in stage 4.0 (TID 177) in 26 ms on localhost (executor driver) (173/200)
[INFO][2018-05-31 16:56:26,658][org.apache.spark.executor.Executor]Running task 174.0 in stage 4.0 (TID 179)
[INFO][2018-05-31 16:56:26,660][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:26,660][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,661][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:26,661][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,662][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:26,662][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,664][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:26,664][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,665][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_173 stored as values in memory (estimated size 8.0 KB, free 871.8 MB)
[INFO][2018-05-31 16:56:26,666][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_173 in memory on 10.194.32.157:61802 (size: 8.0 KB, free: 904.5 MB)
[INFO][2018-05-31 16:56:26,667][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_174 stored as values in memory (estimated size 8.5 KB, free 903.9 MB)
[INFO][2018-05-31 16:56:26,667][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_174 in memory on 10.194.32.157:61802 (size: 8.5 KB, free: 904.5 MB)
[INFO][2018-05-31 16:56:26,673][org.apache.spark.executor.Executor]Finished task 174.0 in stage 4.0 (TID 179). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:26,675][org.apache.spark.executor.Executor]Finished task 173.0 in stage 4.0 (TID 178). 3748 bytes result sent to driver
[INFO][2018-05-31 16:56:26,676][org.apache.spark.scheduler.TaskSetManager]Starting task 175.0 in stage 4.0 (TID 180, localhost, executor driver, partition 175, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:26,677][org.apache.spark.scheduler.TaskSetManager]Finished task 174.0 in stage 4.0 (TID 179) in 19 ms on localhost (executor driver) (174/200)
[INFO][2018-05-31 16:56:26,677][org.apache.spark.executor.Executor]Running task 175.0 in stage 4.0 (TID 180)
[INFO][2018-05-31 16:56:26,678][org.apache.spark.scheduler.TaskSetManager]Starting task 176.0 in stage 4.0 (TID 181, localhost, executor driver, partition 176, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:26,678][org.apache.spark.scheduler.TaskSetManager]Finished task 173.0 in stage 4.0 (TID 178) in 22 ms on localhost (executor driver) (175/200)
[INFO][2018-05-31 16:56:26,679][org.apache.spark.executor.Executor]Running task 176.0 in stage 4.0 (TID 181)
[INFO][2018-05-31 16:56:26,681][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:26,681][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,681][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:26,681][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,682][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:26,682][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,682][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:26,682][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,685][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_175 stored as values in memory (estimated size 7.7 KB, free 871.8 MB)
[INFO][2018-05-31 16:56:26,686][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_175 in memory on 10.194.32.157:61802 (size: 7.7 KB, free: 904.5 MB)
[INFO][2018-05-31 16:56:26,687][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_176 stored as values in memory (estimated size 7.9 KB, free 903.9 MB)
[INFO][2018-05-31 16:56:26,688][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_176 in memory on 10.194.32.157:61802 (size: 7.9 KB, free: 904.5 MB)
[INFO][2018-05-31 16:56:26,692][org.apache.spark.executor.Executor]Finished task 175.0 in stage 4.0 (TID 180). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:26,693][org.apache.spark.scheduler.TaskSetManager]Starting task 177.0 in stage 4.0 (TID 182, localhost, executor driver, partition 177, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:26,693][org.apache.spark.scheduler.TaskSetManager]Finished task 175.0 in stage 4.0 (TID 180) in 17 ms on localhost (executor driver) (176/200)
[INFO][2018-05-31 16:56:26,693][org.apache.spark.executor.Executor]Running task 177.0 in stage 4.0 (TID 182)
[INFO][2018-05-31 16:56:26,693][org.apache.spark.executor.Executor]Finished task 176.0 in stage 4.0 (TID 181). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:26,694][org.apache.spark.scheduler.TaskSetManager]Starting task 178.0 in stage 4.0 (TID 183, localhost, executor driver, partition 178, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:26,694][org.apache.spark.scheduler.TaskSetManager]Finished task 176.0 in stage 4.0 (TID 181) in 16 ms on localhost (executor driver) (177/200)
[INFO][2018-05-31 16:56:26,694][org.apache.spark.executor.Executor]Running task 178.0 in stage 4.0 (TID 183)
[INFO][2018-05-31 16:56:26,695][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:26,695][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,697][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:26,698][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO][2018-05-31 16:56:26,697][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:26,699][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 2 ms
[INFO][2018-05-31 16:56:26,701][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:26,701][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO][2018-05-31 16:56:26,705][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_178 stored as values in memory (estimated size 7.0 KB, free 903.9 MB)
[INFO][2018-05-31 16:56:26,705][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_177 stored as values in memory (estimated size 8.5 KB, free 903.9 MB)
[INFO][2018-05-31 16:56:26,706][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_178 in memory on 10.194.32.157:61802 (size: 7.0 KB, free: 904.4 MB)
[INFO][2018-05-31 16:56:26,706][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_177 in memory on 10.194.32.157:61802 (size: 8.5 KB, free: 904.4 MB)
[INFO][2018-05-31 16:56:26,711][org.apache.spark.executor.Executor]Finished task 178.0 in stage 4.0 (TID 183). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:26,711][org.apache.spark.executor.Executor]Finished task 177.0 in stage 4.0 (TID 182). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:26,711][org.apache.spark.scheduler.TaskSetManager]Starting task 179.0 in stage 4.0 (TID 184, localhost, executor driver, partition 179, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:26,712][org.apache.spark.scheduler.TaskSetManager]Starting task 180.0 in stage 4.0 (TID 185, localhost, executor driver, partition 180, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:26,712][org.apache.spark.executor.Executor]Running task 179.0 in stage 4.0 (TID 184)
[INFO][2018-05-31 16:56:26,712][org.apache.spark.scheduler.TaskSetManager]Finished task 178.0 in stage 4.0 (TID 183) in 19 ms on localhost (executor driver) (178/200)
[INFO][2018-05-31 16:56:26,712][org.apache.spark.scheduler.TaskSetManager]Finished task 177.0 in stage 4.0 (TID 182) in 20 ms on localhost (executor driver) (179/200)
[INFO][2018-05-31 16:56:26,712][org.apache.spark.executor.Executor]Running task 180.0 in stage 4.0 (TID 185)
[INFO][2018-05-31 16:56:26,714][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:26,714][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,715][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:26,715][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,714][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:26,715][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO][2018-05-31 16:56:26,716][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:26,716][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,718][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_179 stored as values in memory (estimated size 8.2 KB, free 871.8 MB)
[INFO][2018-05-31 16:56:26,718][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_179 in memory on 10.194.32.157:61802 (size: 8.2 KB, free: 904.4 MB)
[INFO][2018-05-31 16:56:26,719][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_180 stored as values in memory (estimated size 6.8 KB, free 903.9 MB)
[INFO][2018-05-31 16:56:26,719][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_180 in memory on 10.194.32.157:61802 (size: 6.8 KB, free: 904.4 MB)
[INFO][2018-05-31 16:56:26,722][org.apache.spark.executor.Executor]Finished task 179.0 in stage 4.0 (TID 184). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:26,722][org.apache.spark.scheduler.TaskSetManager]Starting task 181.0 in stage 4.0 (TID 186, localhost, executor driver, partition 181, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:26,723][org.apache.spark.scheduler.TaskSetManager]Finished task 179.0 in stage 4.0 (TID 184) in 12 ms on localhost (executor driver) (180/200)
[INFO][2018-05-31 16:56:26,723][org.apache.spark.executor.Executor]Running task 181.0 in stage 4.0 (TID 186)
[INFO][2018-05-31 16:56:26,724][org.apache.spark.executor.Executor]Finished task 180.0 in stage 4.0 (TID 185). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:26,725][org.apache.spark.scheduler.TaskSetManager]Starting task 182.0 in stage 4.0 (TID 187, localhost, executor driver, partition 182, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:26,725][org.apache.spark.scheduler.TaskSetManager]Finished task 180.0 in stage 4.0 (TID 185) in 13 ms on localhost (executor driver) (181/200)
[INFO][2018-05-31 16:56:26,725][org.apache.spark.executor.Executor]Running task 182.0 in stage 4.0 (TID 187)
[INFO][2018-05-31 16:56:26,726][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:26,726][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,727][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:26,727][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,729][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:26,729][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,730][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_181 stored as values in memory (estimated size 6.7 KB, free 903.8 MB)
[INFO][2018-05-31 16:56:26,730][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_181 in memory on 10.194.32.157:61802 (size: 6.7 KB, free: 904.4 MB)
[INFO][2018-05-31 16:56:26,730][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:26,731][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO][2018-05-31 16:56:26,734][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_182 stored as values in memory (estimated size 8.5 KB, free 903.9 MB)
[INFO][2018-05-31 16:56:26,734][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_182 in memory on 10.194.32.157:61802 (size: 8.5 KB, free: 904.4 MB)
[INFO][2018-05-31 16:56:26,735][org.apache.spark.executor.Executor]Finished task 181.0 in stage 4.0 (TID 186). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:26,735][org.apache.spark.scheduler.TaskSetManager]Starting task 183.0 in stage 4.0 (TID 188, localhost, executor driver, partition 183, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:26,735][org.apache.spark.executor.Executor]Running task 183.0 in stage 4.0 (TID 188)
[INFO][2018-05-31 16:56:26,736][org.apache.spark.scheduler.TaskSetManager]Finished task 181.0 in stage 4.0 (TID 186) in 14 ms on localhost (executor driver) (182/200)
[INFO][2018-05-31 16:56:26,738][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:26,738][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,739][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:26,739][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,742][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_183 stored as values in memory (estimated size 8.5 KB, free 903.9 MB)
[INFO][2018-05-31 16:56:26,742][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_183 in memory on 10.194.32.157:61802 (size: 8.5 KB, free: 904.4 MB)
[INFO][2018-05-31 16:56:26,742][org.apache.spark.executor.Executor]Finished task 182.0 in stage 4.0 (TID 187). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:26,743][org.apache.spark.scheduler.TaskSetManager]Starting task 184.0 in stage 4.0 (TID 189, localhost, executor driver, partition 184, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:26,743][org.apache.spark.scheduler.TaskSetManager]Finished task 182.0 in stage 4.0 (TID 187) in 18 ms on localhost (executor driver) (183/200)
[INFO][2018-05-31 16:56:26,743][org.apache.spark.executor.Executor]Running task 184.0 in stage 4.0 (TID 189)
[INFO][2018-05-31 16:56:26,747][org.apache.spark.executor.Executor]Finished task 183.0 in stage 4.0 (TID 188). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:26,747][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:26,747][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO][2018-05-31 16:56:26,747][org.apache.spark.scheduler.TaskSetManager]Starting task 185.0 in stage 4.0 (TID 190, localhost, executor driver, partition 185, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:26,747][org.apache.spark.executor.Executor]Running task 185.0 in stage 4.0 (TID 190)
[INFO][2018-05-31 16:56:26,747][org.apache.spark.scheduler.TaskSetManager]Finished task 183.0 in stage 4.0 (TID 188) in 12 ms on localhost (executor driver) (184/200)
[INFO][2018-05-31 16:56:26,748][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:26,749][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO][2018-05-31 16:56:26,750][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:26,750][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,751][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:26,751][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,751][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_184 stored as values in memory (estimated size 8.3 KB, free 903.8 MB)
[INFO][2018-05-31 16:56:26,751][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_184 in memory on 10.194.32.157:61802 (size: 8.3 KB, free: 904.4 MB)
[INFO][2018-05-31 16:56:26,753][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_185 stored as values in memory (estimated size 8.7 KB, free 903.8 MB)
[INFO][2018-05-31 16:56:26,753][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_185 in memory on 10.194.32.157:61802 (size: 8.7 KB, free: 904.4 MB)
[INFO][2018-05-31 16:56:26,756][org.apache.spark.executor.Executor]Finished task 184.0 in stage 4.0 (TID 189). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:26,757][org.apache.spark.scheduler.TaskSetManager]Starting task 186.0 in stage 4.0 (TID 191, localhost, executor driver, partition 186, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:26,757][org.apache.spark.executor.Executor]Running task 186.0 in stage 4.0 (TID 191)
[INFO][2018-05-31 16:56:26,757][org.apache.spark.scheduler.TaskSetManager]Finished task 184.0 in stage 4.0 (TID 189) in 14 ms on localhost (executor driver) (185/200)
[INFO][2018-05-31 16:56:26,758][org.apache.spark.executor.Executor]Finished task 185.0 in stage 4.0 (TID 190). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:26,758][org.apache.spark.scheduler.TaskSetManager]Starting task 187.0 in stage 4.0 (TID 192, localhost, executor driver, partition 187, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:26,759][org.apache.spark.executor.Executor]Running task 187.0 in stage 4.0 (TID 192)
[INFO][2018-05-31 16:56:26,759][org.apache.spark.scheduler.TaskSetManager]Finished task 185.0 in stage 4.0 (TID 190) in 12 ms on localhost (executor driver) (186/200)
[INFO][2018-05-31 16:56:26,759][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:26,759][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,760][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:26,761][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO][2018-05-31 16:56:26,761][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:26,761][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,762][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:26,762][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,763][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_186 stored as values in memory (estimated size 7.8 KB, free 887.7 MB)
[INFO][2018-05-31 16:56:26,763][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_186 in memory on 10.194.32.157:61802 (size: 7.8 KB, free: 904.4 MB)
[INFO][2018-05-31 16:56:26,764][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_187 stored as values in memory (estimated size 9.3 KB, free 903.8 MB)
[INFO][2018-05-31 16:56:26,764][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_187 in memory on 10.194.32.157:61802 (size: 9.3 KB, free: 904.4 MB)
[INFO][2018-05-31 16:56:26,769][org.apache.spark.executor.Executor]Finished task 187.0 in stage 4.0 (TID 192). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:26,769][org.apache.spark.executor.Executor]Finished task 186.0 in stage 4.0 (TID 191). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:26,771][org.apache.spark.scheduler.TaskSetManager]Starting task 188.0 in stage 4.0 (TID 193, localhost, executor driver, partition 188, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:26,771][org.apache.spark.scheduler.TaskSetManager]Starting task 189.0 in stage 4.0 (TID 194, localhost, executor driver, partition 189, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:26,772][org.apache.spark.scheduler.TaskSetManager]Finished task 186.0 in stage 4.0 (TID 191) in 15 ms on localhost (executor driver) (187/200)
[INFO][2018-05-31 16:56:26,772][org.apache.spark.scheduler.TaskSetManager]Finished task 187.0 in stage 4.0 (TID 192) in 14 ms on localhost (executor driver) (188/200)
[INFO][2018-05-31 16:56:26,772][org.apache.spark.executor.Executor]Running task 189.0 in stage 4.0 (TID 194)
[INFO][2018-05-31 16:56:26,774][org.apache.spark.executor.Executor]Running task 188.0 in stage 4.0 (TID 193)
[INFO][2018-05-31 16:56:26,774][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:26,775][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO][2018-05-31 16:56:26,775][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:26,775][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,778][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_189 stored as values in memory (estimated size 7.5 KB, free 903.8 MB)
[INFO][2018-05-31 16:56:26,778][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_189 in memory on 10.194.32.157:61802 (size: 7.5 KB, free: 904.4 MB)
[INFO][2018-05-31 16:56:26,781][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:26,781][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,782][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:26,782][org.apache.spark.executor.Executor]Finished task 189.0 in stage 4.0 (TID 194). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:26,783][org.apache.spark.scheduler.TaskSetManager]Starting task 190.0 in stage 4.0 (TID 195, localhost, executor driver, partition 190, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:26,782][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,783][org.apache.spark.scheduler.TaskSetManager]Finished task 189.0 in stage 4.0 (TID 194) in 12 ms on localhost (executor driver) (189/200)
[INFO][2018-05-31 16:56:26,783][org.apache.spark.executor.Executor]Running task 190.0 in stage 4.0 (TID 195)
[INFO][2018-05-31 16:56:26,785][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:26,785][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,789][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:26,789][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,792][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_190 stored as values in memory (estimated size 9.0 KB, free 871.7 MB)
[INFO][2018-05-31 16:56:26,793][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_190 in memory on 10.194.32.157:61802 (size: 9.0 KB, free: 904.4 MB)
[INFO][2018-05-31 16:56:26,797][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_188 stored as values in memory (estimated size 9.1 KB, free 903.8 MB)
[INFO][2018-05-31 16:56:26,797][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_188 in memory on 10.194.32.157:61802 (size: 9.1 KB, free: 904.3 MB)
[INFO][2018-05-31 16:56:26,805][org.apache.spark.executor.Executor]Finished task 188.0 in stage 4.0 (TID 193). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:26,805][org.apache.spark.scheduler.TaskSetManager]Starting task 191.0 in stage 4.0 (TID 196, localhost, executor driver, partition 191, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:26,805][org.apache.spark.scheduler.TaskSetManager]Finished task 188.0 in stage 4.0 (TID 193) in 34 ms on localhost (executor driver) (190/200)
[INFO][2018-05-31 16:56:26,807][org.apache.spark.executor.Executor]Running task 191.0 in stage 4.0 (TID 196)
[INFO][2018-05-31 16:56:26,813][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:26,813][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,816][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:26,817][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO][2018-05-31 16:56:26,821][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_191 stored as values in memory (estimated size 9.5 KB, free 903.8 MB)
[INFO][2018-05-31 16:56:26,822][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_191 in memory on 10.194.32.157:61802 (size: 9.5 KB, free: 904.3 MB)
[INFO][2018-05-31 16:56:26,831][org.apache.spark.executor.Executor]Finished task 191.0 in stage 4.0 (TID 196). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:26,832][org.apache.spark.executor.Executor]Finished task 190.0 in stage 4.0 (TID 195). 3748 bytes result sent to driver
[INFO][2018-05-31 16:56:26,832][org.apache.spark.scheduler.TaskSetManager]Starting task 192.0 in stage 4.0 (TID 197, localhost, executor driver, partition 192, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:26,832][org.apache.spark.scheduler.TaskSetManager]Finished task 191.0 in stage 4.0 (TID 196) in 27 ms on localhost (executor driver) (191/200)
[INFO][2018-05-31 16:56:26,832][org.apache.spark.executor.Executor]Running task 192.0 in stage 4.0 (TID 197)
[INFO][2018-05-31 16:56:26,832][org.apache.spark.scheduler.TaskSetManager]Starting task 193.0 in stage 4.0 (TID 198, localhost, executor driver, partition 193, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:26,832][org.apache.spark.scheduler.TaskSetManager]Finished task 190.0 in stage 4.0 (TID 195) in 50 ms on localhost (executor driver) (192/200)
[INFO][2018-05-31 16:56:26,833][org.apache.spark.executor.Executor]Running task 193.0 in stage 4.0 (TID 198)
[INFO][2018-05-31 16:56:26,835][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:26,835][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,835][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:26,835][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,836][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:26,836][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,836][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:26,841][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 5 ms
[INFO][2018-05-31 16:56:26,847][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_192 stored as values in memory (estimated size 8.2 KB, free 903.8 MB)
[INFO][2018-05-31 16:56:26,847][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_193 stored as values in memory (estimated size 8.0 KB, free 903.8 MB)
[INFO][2018-05-31 16:56:26,847][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_192 in memory on 10.194.32.157:61802 (size: 8.2 KB, free: 904.3 MB)
[INFO][2018-05-31 16:56:26,847][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_193 in memory on 10.194.32.157:61802 (size: 8.0 KB, free: 904.3 MB)
[INFO][2018-05-31 16:56:26,854][org.apache.spark.executor.Executor]Finished task 193.0 in stage 4.0 (TID 198). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:26,855][org.apache.spark.scheduler.TaskSetManager]Starting task 194.0 in stage 4.0 (TID 199, localhost, executor driver, partition 194, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:26,855][org.apache.spark.scheduler.TaskSetManager]Finished task 193.0 in stage 4.0 (TID 198) in 23 ms on localhost (executor driver) (193/200)
[INFO][2018-05-31 16:56:26,855][org.apache.spark.executor.Executor]Running task 194.0 in stage 4.0 (TID 199)
[INFO][2018-05-31 16:56:26,857][org.apache.spark.executor.Executor]Finished task 192.0 in stage 4.0 (TID 197). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:26,857][org.apache.spark.scheduler.TaskSetManager]Starting task 195.0 in stage 4.0 (TID 200, localhost, executor driver, partition 195, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:26,857][org.apache.spark.scheduler.TaskSetManager]Finished task 192.0 in stage 4.0 (TID 197) in 26 ms on localhost (executor driver) (194/200)
[INFO][2018-05-31 16:56:26,857][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:26,858][org.apache.spark.executor.Executor]Running task 195.0 in stage 4.0 (TID 200)
[INFO][2018-05-31 16:56:26,858][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO][2018-05-31 16:56:26,858][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:26,858][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,862][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_194 stored as values in memory (estimated size 6.6 KB, free 903.8 MB)
[INFO][2018-05-31 16:56:26,863][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_194 in memory on 10.194.32.157:61802 (size: 6.6 KB, free: 904.3 MB)
[INFO][2018-05-31 16:56:26,868][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:26,868][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO][2018-05-31 16:56:26,868][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:26,869][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO][2018-05-31 16:56:26,871][org.apache.spark.executor.Executor]Finished task 194.0 in stage 4.0 (TID 199). 3748 bytes result sent to driver
[INFO][2018-05-31 16:56:26,871][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_195 stored as values in memory (estimated size 7.6 KB, free 903.8 MB)
[INFO][2018-05-31 16:56:26,871][org.apache.spark.scheduler.TaskSetManager]Starting task 196.0 in stage 4.0 (TID 201, localhost, executor driver, partition 196, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:26,872][org.apache.spark.scheduler.TaskSetManager]Finished task 194.0 in stage 4.0 (TID 199) in 17 ms on localhost (executor driver) (195/200)
[INFO][2018-05-31 16:56:26,872][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_195 in memory on 10.194.32.157:61802 (size: 7.6 KB, free: 904.3 MB)
[INFO][2018-05-31 16:56:26,872][org.apache.spark.executor.Executor]Running task 196.0 in stage 4.0 (TID 201)
[INFO][2018-05-31 16:56:26,875][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:26,875][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,878][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:26,878][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,882][org.apache.spark.executor.Executor]Finished task 195.0 in stage 4.0 (TID 200). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:26,882][org.apache.spark.scheduler.TaskSetManager]Starting task 197.0 in stage 4.0 (TID 202, localhost, executor driver, partition 197, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:26,882][org.apache.spark.scheduler.TaskSetManager]Finished task 195.0 in stage 4.0 (TID 200) in 25 ms on localhost (executor driver) (196/200)
[INFO][2018-05-31 16:56:26,883][org.apache.spark.executor.Executor]Running task 197.0 in stage 4.0 (TID 202)
[INFO][2018-05-31 16:56:26,883][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_196 stored as values in memory (estimated size 8.0 KB, free 903.7 MB)
[INFO][2018-05-31 16:56:26,884][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_196 in memory on 10.194.32.157:61802 (size: 8.0 KB, free: 904.3 MB)
[INFO][2018-05-31 16:56:26,886][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:26,886][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,887][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:26,887][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,888][org.apache.spark.executor.Executor]Finished task 196.0 in stage 4.0 (TID 201). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:26,889][org.apache.spark.scheduler.TaskSetManager]Starting task 198.0 in stage 4.0 (TID 203, localhost, executor driver, partition 198, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:26,889][org.apache.spark.scheduler.TaskSetManager]Finished task 196.0 in stage 4.0 (TID 201) in 18 ms on localhost (executor driver) (197/200)
[INFO][2018-05-31 16:56:26,889][org.apache.spark.executor.Executor]Running task 198.0 in stage 4.0 (TID 203)
[INFO][2018-05-31 16:56:26,889][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_197 stored as values in memory (estimated size 6.6 KB, free 903.7 MB)
[INFO][2018-05-31 16:56:26,890][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_197 in memory on 10.194.32.157:61802 (size: 6.6 KB, free: 904.3 MB)
[INFO][2018-05-31 16:56:26,893][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:26,893][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,894][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:26,894][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,897][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_198 stored as values in memory (estimated size 8.7 KB, free 903.7 MB)
[INFO][2018-05-31 16:56:26,897][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_198 in memory on 10.194.32.157:61802 (size: 8.7 KB, free: 904.3 MB)
[INFO][2018-05-31 16:56:26,898][org.apache.spark.executor.Executor]Finished task 197.0 in stage 4.0 (TID 202). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:26,899][org.apache.spark.scheduler.TaskSetManager]Starting task 199.0 in stage 4.0 (TID 204, localhost, executor driver, partition 199, ANY, 5043 bytes)
[INFO][2018-05-31 16:56:26,899][org.apache.spark.scheduler.TaskSetManager]Finished task 197.0 in stage 4.0 (TID 202) in 17 ms on localhost (executor driver) (198/200)
[INFO][2018-05-31 16:56:26,899][org.apache.spark.executor.Executor]Running task 199.0 in stage 4.0 (TID 204)
[INFO][2018-05-31 16:56:26,907][org.apache.spark.executor.Executor]Finished task 198.0 in stage 4.0 (TID 203). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:26,908][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:26,908][org.apache.spark.scheduler.TaskSetManager]Finished task 198.0 in stage 4.0 (TID 203) in 19 ms on localhost (executor driver) (199/200)
[INFO][2018-05-31 16:56:26,908][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,909][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-05-31 16:56:26,909][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,912][org.apache.spark.storage.memory.MemoryStore]Block rdd_44_199 stored as values in memory (estimated size 7.1 KB, free 903.7 MB)
[INFO][2018-05-31 16:56:26,912][org.apache.spark.storage.BlockManagerInfo]Added rdd_44_199 in memory on 10.194.32.157:61802 (size: 7.1 KB, free: 904.3 MB)
[INFO][2018-05-31 16:56:26,922][org.apache.spark.executor.Executor]Finished task 199.0 in stage 4.0 (TID 204). 3705 bytes result sent to driver
[INFO][2018-05-31 16:56:26,922][org.apache.spark.scheduler.TaskSetManager]Finished task 199.0 in stage 4.0 (TID 204) in 23 ms on localhost (executor driver) (200/200)
[INFO][2018-05-31 16:56:26,922][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 4.0, whose tasks have all completed, from pool 
[INFO][2018-05-31 16:56:26,923][org.apache.spark.scheduler.DAGScheduler]ShuffleMapStage 4 (csv at NetType.scala:169) finished in 3.076 s
[INFO][2018-05-31 16:56:26,923][org.apache.spark.scheduler.DAGScheduler]looking for newly runnable stages
[INFO][2018-05-31 16:56:26,923][org.apache.spark.scheduler.DAGScheduler]running: Set()
[INFO][2018-05-31 16:56:26,923][org.apache.spark.scheduler.DAGScheduler]waiting: Set(ResultStage 5)
[INFO][2018-05-31 16:56:26,923][org.apache.spark.scheduler.DAGScheduler]failed: Set()
[INFO][2018-05-31 16:56:26,924][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 5 (ShuffledRowRDD[47] at csv at NetType.scala:169), which has no missing parents
[INFO][2018-05-31 16:56:26,939][org.apache.spark.storage.memory.MemoryStore]Block broadcast_11 stored as values in memory (estimated size 75.5 KB, free 903.7 MB)
[INFO][2018-05-31 16:56:26,942][org.apache.spark.storage.memory.MemoryStore]Block broadcast_11_piece0 stored as bytes in memory (estimated size 28.5 KB, free 903.6 MB)
[INFO][2018-05-31 16:56:26,943][org.apache.spark.storage.BlockManagerInfo]Added broadcast_11_piece0 in memory on 10.194.32.157:61802 (size: 28.5 KB, free: 904.2 MB)
[INFO][2018-05-31 16:56:26,943][org.apache.spark.SparkContext]Created broadcast 11 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-31 16:56:26,943][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 5 (ShuffledRowRDD[47] at csv at NetType.scala:169) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-31 16:56:26,943][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 5.0 with 1 tasks
[INFO][2018-05-31 16:56:26,944][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 5.0 (TID 205, localhost, executor driver, partition 0, ANY, 4726 bytes)
[INFO][2018-05-31 16:56:26,944][org.apache.spark.executor.Executor]Running task 0.0 in stage 5.0 (TID 205)
[INFO][2018-05-31 16:56:26,969][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 200 non-empty blocks out of 200 blocks
[INFO][2018-05-31 16:56:26,969][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2018-05-31 16:56:26,978][org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter]File Output Committer Algorithm version is 1
[INFO][2018-05-31 16:56:26,979][org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol]Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[INFO][2018-05-31 16:56:27,441][org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter]Saved output of task 'attempt_20180531165626_0005_m_000000_0' to hdfs://vm-xaj-bigdata-da-d01:8020/yst/seven/data/pointData/_temporary/0/task_20180531165626_0005_m_000000
[INFO][2018-05-31 16:56:27,442][org.apache.spark.mapred.SparkHadoopMapRedUtil]attempt_20180531165626_0005_m_000000_0: Committed
[INFO][2018-05-31 16:56:27,445][org.apache.spark.executor.Executor]Finished task 0.0 in stage 5.0 (TID 205). 1435 bytes result sent to driver
[INFO][2018-05-31 16:56:27,456][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 5.0 (TID 205) in 512 ms on localhost (executor driver) (1/1)
[INFO][2018-05-31 16:56:27,456][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 5.0, whose tasks have all completed, from pool 
[INFO][2018-05-31 16:56:27,456][org.apache.spark.scheduler.DAGScheduler]ResultStage 5 (csv at NetType.scala:169) finished in 0.512 s
[INFO][2018-05-31 16:56:27,457][org.apache.spark.scheduler.DAGScheduler]Job 2 finished: csv at NetType.scala:169, took 9.700551 s
[INFO][2018-05-31 16:56:27,557][org.apache.spark.sql.execution.datasources.FileFormatWriter]Job null committed.
[INFO][2018-05-31 16:56:27,572][com.seven.spark.sparksql.NetType$]save point data is success timeout is 0:00:11.258
[INFO][2018-05-31 16:56:27,572][com.seven.spark.sparksql.NetType$]save net data is start
[INFO][2018-05-31 16:56:27,745][org.apache.spark.sql.execution.datasources.FileSourceStrategy]Pruning directories with: 
[INFO][2018-05-31 16:56:27,746][org.apache.spark.sql.execution.datasources.FileSourceStrategy]Post-Scan Filters: (length(trim(value#2387)) > 0)
[INFO][2018-05-31 16:56:27,746][org.apache.spark.sql.execution.datasources.FileSourceStrategy]Output Data Schema: struct<value: string>
[INFO][2018-05-31 16:56:27,746][org.apache.spark.sql.execution.FileSourceScanExec]Pushed Filters: 
[INFO][2018-05-31 16:56:27,758][org.apache.spark.storage.memory.MemoryStore]Block broadcast_12 stored as values in memory (estimated size 261.0 KB, free 903.4 MB)
[INFO][2018-05-31 16:56:27,797][org.apache.spark.storage.memory.MemoryStore]Block broadcast_12_piece0 stored as bytes in memory (estimated size 22.7 KB, free 903.3 MB)
[INFO][2018-05-31 16:56:27,798][org.apache.spark.storage.BlockManagerInfo]Added broadcast_12_piece0 in memory on 10.194.32.157:61802 (size: 22.7 KB, free: 904.2 MB)
[INFO][2018-05-31 16:56:27,798][org.apache.spark.SparkContext]Created broadcast 12 from csv at NetType.scala:181
[INFO][2018-05-31 16:56:27,799][org.apache.spark.sql.execution.FileSourceScanExec]Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[INFO][2018-05-31 16:56:27,809][org.apache.spark.SparkContext]Starting job: csv at NetType.scala:181
[INFO][2018-05-31 16:56:27,810][org.apache.spark.scheduler.DAGScheduler]Got job 3 (csv at NetType.scala:181) with 1 output partitions
[INFO][2018-05-31 16:56:27,810][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 6 (csv at NetType.scala:181)
[INFO][2018-05-31 16:56:27,810][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-31 16:56:27,810][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-31 16:56:27,812][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 6 (MapPartitionsRDD[52] at csv at NetType.scala:181), which has no missing parents
[INFO][2018-05-31 16:56:27,814][org.apache.spark.storage.memory.MemoryStore]Block broadcast_13 stored as values in memory (estimated size 8.2 KB, free 903.3 MB)
[INFO][2018-05-31 16:56:27,816][org.apache.spark.storage.memory.MemoryStore]Block broadcast_13_piece0 stored as bytes in memory (estimated size 4.3 KB, free 903.3 MB)
[INFO][2018-05-31 16:56:27,817][org.apache.spark.storage.BlockManagerInfo]Added broadcast_13_piece0 in memory on 10.194.32.157:61802 (size: 4.3 KB, free: 904.2 MB)
[INFO][2018-05-31 16:56:27,817][org.apache.spark.SparkContext]Created broadcast 13 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-31 16:56:27,817][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[52] at csv at NetType.scala:181) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-31 16:56:27,817][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 6.0 with 1 tasks
[INFO][2018-05-31 16:56:27,818][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 6.0 (TID 206, localhost, executor driver, partition 0, ANY, 5348 bytes)
[INFO][2018-05-31 16:56:27,818][org.apache.spark.executor.Executor]Running task 0.0 in stage 6.0 (TID 206)
[INFO][2018-05-31 16:56:27,820][org.apache.spark.sql.execution.datasources.FileScanRDD]Reading File path: hdfs://vm-xaj-bigdata-da-d01:8020/yst/sta_vem/net_community/net_community__0d157f5f_7f23_4d97_95e9_2cb7853bec85, range: 0-518103, partition values: [empty row]
[INFO][2018-05-31 16:56:27,898][org.apache.spark.executor.Executor]Finished task 0.0 in stage 6.0 (TID 206). 1427 bytes result sent to driver
[INFO][2018-05-31 16:56:27,898][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 6.0 (TID 206) in 80 ms on localhost (executor driver) (1/1)
[INFO][2018-05-31 16:56:27,899][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 6.0, whose tasks have all completed, from pool 
[INFO][2018-05-31 16:56:27,899][org.apache.spark.scheduler.DAGScheduler]ResultStage 6 (csv at NetType.scala:181) finished in 0.081 s
[INFO][2018-05-31 16:56:27,899][org.apache.spark.scheduler.DAGScheduler]Job 3 finished: csv at NetType.scala:181, took 0.089562 s
[INFO][2018-05-31 16:56:27,905][org.apache.spark.sql.execution.datasources.FileSourceStrategy]Pruning directories with: 
[INFO][2018-05-31 16:56:27,905][org.apache.spark.sql.execution.datasources.FileSourceStrategy]Post-Scan Filters: 
[INFO][2018-05-31 16:56:27,905][org.apache.spark.sql.execution.datasources.FileSourceStrategy]Output Data Schema: struct<value: string>
[INFO][2018-05-31 16:56:27,905][org.apache.spark.sql.execution.FileSourceScanExec]Pushed Filters: 
[INFO][2018-05-31 16:56:27,909][org.apache.spark.storage.memory.MemoryStore]Block broadcast_14 stored as values in memory (estimated size 261.0 KB, free 903.1 MB)
[INFO][2018-05-31 16:56:27,926][org.apache.spark.storage.memory.MemoryStore]Block broadcast_14_piece0 stored as bytes in memory (estimated size 22.7 KB, free 903.1 MB)
[INFO][2018-05-31 16:56:27,927][org.apache.spark.storage.BlockManagerInfo]Added broadcast_14_piece0 in memory on 10.194.32.157:61802 (size: 22.7 KB, free: 904.2 MB)
[INFO][2018-05-31 16:56:27,928][org.apache.spark.SparkContext]Created broadcast 14 from csv at NetType.scala:181
[INFO][2018-05-31 16:56:27,929][org.apache.spark.sql.execution.FileSourceScanExec]Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[INFO][2018-05-31 16:56:27,967][org.apache.spark.sql.execution.datasources.FileSourceStrategy]Pruning directories with: 
[INFO][2018-05-31 16:56:27,968][org.apache.spark.sql.execution.datasources.FileSourceStrategy]Post-Scan Filters: 
[INFO][2018-05-31 16:56:27,969][org.apache.spark.sql.execution.datasources.FileSourceStrategy]Output Data Schema: struct<_c0: string, _c1: string, _c2: string, _c3: string, _c4: string ... 28 more fields>
[INFO][2018-05-31 16:56:27,969][org.apache.spark.sql.execution.FileSourceScanExec]Pushed Filters: 
[INFO][2018-05-31 16:56:27,976][org.apache.spark.storage.memory.MemoryStore]Block broadcast_15 stored as values in memory (estimated size 261.1 KB, free 902.8 MB)
[INFO][2018-05-31 16:56:28,023][org.apache.spark.storage.memory.MemoryStore]Block broadcast_15_piece0 stored as bytes in memory (estimated size 22.7 KB, free 902.8 MB)
[INFO][2018-05-31 16:56:28,023][org.apache.spark.storage.BlockManagerInfo]Added broadcast_15_piece0 in memory on 10.194.32.157:61802 (size: 22.7 KB, free: 904.2 MB)
[INFO][2018-05-31 16:56:28,024][org.apache.spark.SparkContext]Created broadcast 15 from rdd at NetType.scala:182
[INFO][2018-05-31 16:56:28,025][org.apache.spark.sql.execution.FileSourceScanExec]Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[INFO][2018-05-31 16:56:28,353][org.apache.spark.sql.execution.SparkSqlParser]Parsing command: isDelete = '0'
[INFO][2018-05-31 16:56:28,421][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator]Code generated in 17.653004 ms
[INFO][2018-05-31 16:56:28,426][org.apache.spark.sql.execution.SparkSqlParser]Parsing command: net_community
[INFO][2018-05-31 16:56:28,435][org.apache.spark.sql.execution.SparkSqlParser]Parsing command: 
select
v.id,n.netName,v.parentId,v.createDate,v.isDelete,v.type,v.province,v.city,v.county,
v.address,v.operators,v.lat,v.lng,n.nettypeId,n.propertyManagementId,n.tradingAreaRemark,
n.householdCheckInNum,n.buildingNum,n.undergroundParkingNum,n.undergroundCarparkNum,
n.elevatorHallNum,n.houseCompletionTime,n.propertyCosts,n.estatePrice,selfServiceTerminal,n.householdTotalNum
from net_community n
left join vem_nettype v
on v.communityId = n.id
where v.type = 'net' and v.id <> '101' and v.id <> '7667' and v.id <> '620'
    
[INFO][2018-05-31 16:56:28,524][org.apache.spark.sql.execution.columnar.InMemoryTableScanExec]Predicate isnotnull(id#2582) generates partition filter: ((id.count#3564 - id.nullCount#3563) > 0)
[INFO][2018-05-31 16:56:28,525][org.apache.spark.sql.execution.columnar.InMemoryTableScanExec]Predicate isnotnull(type#451) generates partition filter: ((type.count#3754 - type.nullCount#3753) > 0)
[INFO][2018-05-31 16:56:28,525][org.apache.spark.sql.execution.columnar.InMemoryTableScanExec]Predicate isnotnull(id#195) generates partition filter: ((id.count#3714 - id.nullCount#3713) > 0)
[INFO][2018-05-31 16:56:28,525][org.apache.spark.sql.execution.columnar.InMemoryTableScanExec]Predicate (type#451 = net) generates partition filter: ((type.lowerBound#3752 <= net) && (net <= type.upperBound#3751))
[INFO][2018-05-31 16:56:28,525][org.apache.spark.sql.execution.columnar.InMemoryTableScanExec]Predicate isnotnull(communityId#835) generates partition filter: ((communityId.count#3814 - communityId.nullCount#3813) > 0)
[INFO][2018-05-31 16:56:28,550][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator]Code generated in 8.299509 ms
[INFO][2018-05-31 16:56:28,572][org.apache.spark.SparkContext]Starting job: run at ThreadPoolExecutor.java:1149
[INFO][2018-05-31 16:56:28,573][org.apache.spark.scheduler.DAGScheduler]Got job 4 (run at ThreadPoolExecutor.java:1149) with 2 output partitions
[INFO][2018-05-31 16:56:28,573][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 7 (run at ThreadPoolExecutor.java:1149)
[INFO][2018-05-31 16:56:28,573][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-31 16:56:28,574][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-31 16:56:28,574][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 7 (MapPartitionsRDD[68] at run at ThreadPoolExecutor.java:1149), which has no missing parents
[INFO][2018-05-31 16:56:28,577][org.apache.spark.storage.memory.MemoryStore]Block broadcast_16 stored as values in memory (estimated size 71.7 KB, free 902.7 MB)
[INFO][2018-05-31 16:56:28,581][org.apache.spark.storage.memory.MemoryStore]Block broadcast_16_piece0 stored as bytes in memory (estimated size 23.1 KB, free 902.7 MB)
[INFO][2018-05-31 16:56:28,582][org.apache.spark.storage.BlockManagerInfo]Added broadcast_16_piece0 in memory on 10.194.32.157:61802 (size: 23.1 KB, free: 904.2 MB)
[INFO][2018-05-31 16:56:28,583][org.apache.spark.SparkContext]Created broadcast 16 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-31 16:56:28,584][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ResultStage 7 (MapPartitionsRDD[68] at run at ThreadPoolExecutor.java:1149) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2018-05-31 16:56:28,584][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 7.0 with 2 tasks
[INFO][2018-05-31 16:56:28,587][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 7.0 (TID 207, localhost, executor driver, partition 0, PROCESS_LOCAL, 5344 bytes)
[INFO][2018-05-31 16:56:28,588][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 7.0 (TID 208, localhost, executor driver, partition 1, PROCESS_LOCAL, 5344 bytes)
[INFO][2018-05-31 16:56:28,588][org.apache.spark.executor.Executor]Running task 1.0 in stage 7.0 (TID 208)
[INFO][2018-05-31 16:56:28,588][org.apache.spark.executor.Executor]Running task 0.0 in stage 7.0 (TID 207)
[INFO][2018-05-31 16:56:28,592][org.apache.spark.storage.BlockManager]Found block rdd_15_1 locally
[INFO][2018-05-31 16:56:28,592][org.apache.spark.storage.BlockManager]Found block rdd_15_0 locally
[INFO][2018-05-31 16:56:28,598][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator]Code generated in 5.400309 ms
[INFO][2018-05-31 16:56:28,601][org.apache.spark.sql.execution.columnar.InMemoryTableScanExec]Skipping partition based on stats id.lowerBound: 67458, id.upperBound: 80077, id.nullCount: 0, id.count: 10000, id.sizeInBytes: 90000, nettypeName.lowerBound:   信丰雷登工业园11号机, nettypeName.upperBound: （香洲）银桦无人店, nettypeName.nullCount: 0, nettypeName.count: 10000, nettypeName.sizeInBytes: 271894, parentId.lowerBound: 40163, parentId.upperBound: 58494, parentId.nullCount: 0, parentId.count: 10000, parentId.sizeInBytes: 90000, createPerson.lowerBound: initial_zy, createPerson.upperBound: initial_zy, createPerson.nullCount: 0, createPerson.count: 10000, createPerson.sizeInBytes: 140000, createDate.lowerBound: 2018-05-11 19:56:26.0, createDate.upperBound: 2018-05-11 20:03:35.0, createDate.nullCount: 0, createDate.count: 10000, createDate.sizeInBytes: 250000, updatePerson.lowerBound: 1000168, updatePerson.upperBound: ywwu, updatePerson.nullCount: 0, updatePerson.count: 10000, updatePerson.sizeInBytes: 139877, updateDate.lowerBound: 2018-05-11 21:47:36.0, updateDate.upperBound: 2018-05-30 14:49:13.0, updateDate.nullCount: 0, updateDate.count: 10000, updateDate.sizeInBytes: 250000, isDelete.lowerBound: 0, isDelete.upperBound: 0, isDelete.nullCount: 0, isDelete.count: 10000, isDelete.sizeInBytes: 50000, type.lowerBound: point, type.upperBound: point, type.nullCount: 0, type.count: 10000, type.sizeInBytes: 90000, office.lowerBound: 1001, office.upperBound: 982, office.nullCount: 200, office.count: 10000, office.sizeInBytes: 76671, operatorid.lowerBound: 1032, operatorid.upperBound: 8438, operatorid.nullCount: 0, operatorid.count: 10000, operatorid.sizeInBytes: 80000, province.lowerBound: null, province.upperBound: null, province.nullCount: 10000, province.count: 10000, province.sizeInBytes: 40000, city.lowerBound: null, city.upperBound: null, city.nullCount: 10000, city.count: 10000, city.sizeInBytes: 40000, county.lowerBound: null, county.upperBound: null, county.nullCount: 10000, county.count: 10000, county.sizeInBytes: 40000, contactMan.lowerBound: 丁丸洪, contactMan.upperBound: 龙兴元, contactMan.nullCount: 7474, contactMan.count: 10000, contactMan.sizeInBytes: 60559, contactPhone.lowerBound:  15530955333, contactPhone.upperBound: 8860906, contactPhone.nullCount: 7487, contactPhone.count: 10000, contactPhone.sizeInBytes: 67719, address.lowerBound: null, address.upperBound: null, address.nullCount: 10000, address.count: 10000, address.sizeInBytes: 40000, operators.lowerBound:   王甫涛, operators.upperBound: （黄冈中学）武汉鑫安大校园后勤服务有限公司, operators.nullCount: 0, operators.count: 10000, operators.sizeInBytes: 252135, channelTypeCode.lowerBound:  , channelTypeCode.upperBound:  , channelTypeCode.nullCount: 0, channelTypeCode.count: 10000, channelTypeCode.sizeInBytes: 50000, checkStatusCode.lowerBound:  , checkStatusCode.upperBound:  , checkStatusCode.nullCount: 0, checkStatusCode.count: 10000, checkStatusCode.sizeInBytes: 50000, communityId.lowerBound: null, communityId.upperBound: null, communityId.nullCount: 10000, communityId.count: 10000, communityId.sizeInBytes: 40000, lat.lowerBound: 18.215776311561, lat.upperBound: 50.250437650987, lat.nullCount: 78, lat.count: 10000, lat.sizeInBytes: 187551, lng.lowerBound: 100.09117991355, lng.upperBound: 99.17205635701, lng.nullCount: 78, lng.count: 10000, lng.sizeInBytes: 187647, auditor.lowerBound: initial_zy, auditor.upperBound: initial_zy, auditor.nullCount: 0, auditor.count: 10000, auditor.sizeInBytes: 140000, auditDate.lowerBound: null, auditDate.upperBound: null, auditDate.nullCount: 10000, auditDate.count: 10000, auditDate.sizeInBytes: 40000, a25.lowerBound: DS_ZFJVEM_PRD, a25.upperBound: DS_ZFJVEM_PRD, a25.nullCount: 0, a25.count: 10000, a25.sizeInBytes: 170000, a26.lowerBound: job_hsta_vem_nettype, a26.upperBound: job_hsta_vem_nettype, a26.nullCount: 0, a26.count: 10000, a26.sizeInBytes: 240000, a27.lowerBound: vem_nettype, a27.upperBound: vem_nettype, a27.nullCount: 0, a27.count: 10000, a27.sizeInBytes: 150000, a28.lowerBound: 20180530, a28.upperBound: 20180530, a28.nullCount: 0, a28.count: 10000, a28.sizeInBytes: 120000, a29.lowerBound: 2018-05-31 07:30:35.0, a29.upperBound: 2018-05-31 07:30:36.0, a29.nullCount: 0, a29.count: 10000, a29.sizeInBytes: 250000
[INFO][2018-05-31 16:56:28,606][org.apache.spark.executor.Executor]Finished task 1.0 in stage 7.0 (TID 208). 43074 bytes result sent to driver
[INFO][2018-05-31 16:56:28,607][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 7.0 (TID 208) in 19 ms on localhost (executor driver) (1/2)
[INFO][2018-05-31 16:56:28,612][org.apache.spark.sql.execution.columnar.InMemoryTableScanExec]Skipping partition based on stats id.lowerBound: 44609, id.upperBound: 54608, id.nullCount: 0, id.count: 10000, id.sizeInBytes: 90000, nettypeName.lowerBound:  五中学校商店, nettypeName.upperBound: （自贩机）含山县西门车站, nettypeName.nullCount: 0, nettypeName.count: 10000, nettypeName.sizeInBytes: 249832, parentId.lowerBound: 1, parentId.upperBound: 9, parentId.nullCount: 0, parentId.count: 10000, parentId.sizeInBytes: 56954, createPerson.lowerBound: initial_zy, createPerson.upperBound: initial_zy, createPerson.nullCount: 0, createPerson.count: 10000, createPerson.sizeInBytes: 140000, createDate.lowerBound: 2018-05-11 19:47:40.0, createDate.upperBound: 2018-05-11 19:47:40.0, createDate.nullCount: 0, createDate.count: 10000, createDate.sizeInBytes: 250000, updatePerson.lowerBound: amei1, updatePerson.upperBound: yjmin1, updatePerson.nullCount: 0, updatePerson.count: 10000, updatePerson.sizeInBytes: 139984, updateDate.lowerBound: 2018-05-11 21:43:46.0, updateDate.upperBound: 2018-05-25 09:44:13.0, updateDate.nullCount: 0, updateDate.count: 10000, updateDate.sizeInBytes: 250000, isDelete.lowerBound: 0, isDelete.upperBound: 0, isDelete.nullCount: 0, isDelete.count: 10000, isDelete.sizeInBytes: 50000, type.lowerBound: net, type.upperBound: net, type.nullCount: 0, type.count: 10000, type.sizeInBytes: 70000, office.lowerBound: 1001, office.upperBound: 982, office.nullCount: 206, office.count: 10000, office.sizeInBytes: 77187, operatorid.lowerBound: 1003, operatorid.upperBound: 8438, operatorid.nullCount: 0, operatorid.count: 10000, operatorid.sizeInBytes: 80000, province.lowerBound: 江苏省, province.upperBound: 辽宁省, province.nullCount: 9996, province.count: 10000, province.sizeInBytes: 40036, city.lowerBound: 温州市, city.upperBound: 阜新市, city.nullCount: 9996, city.count: 10000, city.sizeInBytes: 40039, county.lowerBound: 乐清市, county.upperBound: 藁城区, county.nullCount: 9996, county.count: 10000, county.sizeInBytes: 40039, contactMan.lowerBound:  陆梅花/刘琪, contactMan.upperBound: 龚恒东, contactMan.nullCount: 4236, contactMan.count: 10000, contactMan.sizeInBytes: 87026, contactPhone.lowerBound:  15530955333, contactPhone.upperBound: 无, contactPhone.nullCount: 4485, contactPhone.count: 10000, contactPhone.sizeInBytes: 100814, address.lowerBound: 张家港市华达路康得新公司内, address.upperBound: 张家港市华达路康得新公司内, address.nullCount: 9999, address.count: 10000, address.sizeInBytes: 40039, operators.lowerBound:   王甫涛, operators.upperBound: （鄂州四中）-明华康校园超市, operators.nullCount: 1, operators.count: 10000, operators.sizeInBytes: 257110, channelTypeCode.lowerBound:  , channelTypeCode.upperBound:  , channelTypeCode.nullCount: 0, channelTypeCode.count: 10000, channelTypeCode.sizeInBytes: 50000, checkStatusCode.lowerBound:  , checkStatusCode.upperBound:  , checkStatusCode.nullCount: 0, checkStatusCode.count: 10000, checkStatusCode.sizeInBytes: 50000, communityId.lowerBound: null, communityId.upperBound: null, communityId.nullCount: 10000, communityId.count: 10000, communityId.sizeInBytes: 40000, lat.lowerBound: 28.261839, lat.upperBound: 41.990902, lat.nullCount: 9996, lat.count: 10000, lat.sizeInBytes: 40035, lng.lowerBound: 114.828096, lng.upperBound: 121.652705, lng.nullCount: 9996, lng.count: 10000, lng.sizeInBytes: 40040, auditor.lowerBound: initial_zy, auditor.upperBound: initial_zy, auditor.nullCount: 0, auditor.count: 10000, auditor.sizeInBytes: 140000, auditDate.lowerBound: null, auditDate.upperBound: null, auditDate.nullCount: 10000, auditDate.count: 10000, auditDate.sizeInBytes: 40000, a25.lowerBound: DS_ZFJVEM_PRD, a25.upperBound: DS_ZFJVEM_PRD, a25.nullCount: 0, a25.count: 10000, a25.sizeInBytes: 170000, a26.lowerBound: job_hsta_vem_nettype, a26.upperBound: job_hsta_vem_nettype, a26.nullCount: 0, a26.count: 10000, a26.sizeInBytes: 240000, a27.lowerBound: vem_nettype, a27.upperBound: vem_nettype, a27.nullCount: 0, a27.count: 10000, a27.sizeInBytes: 150000, a28.lowerBound: 20180530, a28.upperBound: 20180530, a28.nullCount: 0, a28.count: 10000, a28.sizeInBytes: 120000, a29.lowerBound: 2018-05-31 07:30:33.0, a29.upperBound: 2018-05-31 07:30:35.0, a29.nullCount: 0, a29.count: 10000, a29.sizeInBytes: 250000
[INFO][2018-05-31 16:56:28,612][org.apache.spark.sql.execution.columnar.InMemoryTableScanExec]Skipping partition based on stats id.lowerBound: 54609, id.upperBound: 66121, id.nullCount: 0, id.count: 10000, id.sizeInBytes: 90000, nettypeName.lowerBound:              童话城堡亲子乐园门口, nettypeName.upperBound: �恒大影院, nettypeName.nullCount: 0, nettypeName.count: 10000, nettypeName.sizeInBytes: 284752, parentId.lowerBound: 1, parentId.upperBound: 9, parentId.nullCount: 0, parentId.count: 10000, parentId.sizeInBytes: 77463, createPerson.lowerBound: initial_zy, createPerson.upperBound: initial_zy, createPerson.nullCount: 0, createPerson.count: 10000, createPerson.sizeInBytes: 140000, createDate.lowerBound: 2018-05-11 19:47:40.0, createDate.upperBound: 2018-05-11 19:56:26.0, createDate.nullCount: 0, createDate.count: 10000, createDate.sizeInBytes: 250000, updatePerson.lowerBound: 110008, updatePerson.upperBound: ywwu, updatePerson.nullCount: 0, updatePerson.count: 10000, updatePerson.sizeInBytes: 139922, updateDate.lowerBound: 2018-05-11 21:43:46.0, updateDate.upperBound: 2018-05-30 15:05:36.0, updateDate.nullCount: 0, updateDate.count: 10000, updateDate.sizeInBytes: 250000, isDelete.lowerBound: 0, isDelete.upperBound: 0, isDelete.nullCount: 0, isDelete.count: 10000, isDelete.sizeInBytes: 50000, type.lowerBound: net, type.upperBound: point, type.nullCount: 0, type.count: 10000, type.sizeInBytes: 82228, office.lowerBound: 1001, office.upperBound: 982, office.nullCount: 292, office.count: 10000, office.sizeInBytes: 76009, operatorid.lowerBound: 1000, operatorid.upperBound: 8437, operatorid.nullCount: 0, operatorid.count: 10000, operatorid.sizeInBytes: 80000, province.lowerBound: 广西壮族自治区, province.upperBound: 辽宁省, province.nullCount: 9995, province.count: 10000, province.sizeInBytes: 40057, city.lowerBound: 大连市, city.upperBound: 苏州市, city.nullCount: 9995, city.count: 10000, city.sizeInBytes: 40045, county.lowerBound: 吴中区, county.upperBound: 金州区, county.nullCount: 9995, county.count: 10000, county.sizeInBytes: 40045, contactMan.lowerBound:  吉日格勒, contactMan.upperBound: 龚江平, contactMan.nullCount: 5064, contactMan.count: 10000, contactMan.sizeInBytes: 80672, contactPhone.lowerBound: 0, contactPhone.upperBound: 无, contactPhone.nullCount: 5276, contactPhone.count: 10000, contactPhone.sizeInBytes: 91883, address.lowerBound: 太湖湿地公园, address.upperBound: 常州高铁北, address.nullCount: 9997, address.count: 10000, address.sizeInBytes: 40048, operators.lowerBound:   王甫涛, operators.upperBound: （黄冈中学）武汉鑫安大校园后勤服务有限公司, operators.nullCount: 0, operators.count: 10000, operators.sizeInBytes: 260650, channelTypeCode.lowerBound:  , channelTypeCode.upperBound:  , channelTypeCode.nullCount: 0, channelTypeCode.count: 10000, channelTypeCode.sizeInBytes: 50000, checkStatusCode.lowerBound:  , checkStatusCode.upperBound:  , checkStatusCode.nullCount: 0, checkStatusCode.count: 10000, checkStatusCode.sizeInBytes: 50000, communityId.lowerBound: null, communityId.upperBound: null, communityId.nullCount: 10000, communityId.count: 10000, communityId.sizeInBytes: 40000, lat.lowerBound: 18.229251627917, lat.upperBound: 50.250842029062, lat.nullCount: 3881, lat.count: 10000, lat.sizeInBytes: 131007, lng.lowerBound: 100.08157011278, lng.upperBound: 99.233024150017, lng.nullCount: 3881, lng.count: 10000, lng.sizeInBytes: 130977, auditor.lowerBound: initial_zy, auditor.upperBound: initial_zy, auditor.nullCount: 0, auditor.count: 10000, auditor.sizeInBytes: 140000, auditDate.lowerBound: null, auditDate.upperBound: null, auditDate.nullCount: 10000, auditDate.count: 10000, auditDate.sizeInBytes: 40000, a25.lowerBound: DS_ZFJVEM_PRD, a25.upperBound: DS_ZFJVEM_PRD, a25.nullCount: 0, a25.count: 10000, a25.sizeInBytes: 170000, a26.lowerBound: job_hsta_vem_nettype, a26.upperBound: job_hsta_vem_nettype, a26.nullCount: 0, a26.count: 10000, a26.sizeInBytes: 240000, a27.lowerBound: vem_nettype, a27.upperBound: vem_nettype, a27.nullCount: 0, a27.count: 10000, a27.sizeInBytes: 150000, a28.lowerBound: 20180530, a28.upperBound: 20180530, a28.nullCount: 0, a28.count: 10000, a28.sizeInBytes: 120000, a29.lowerBound: 2018-05-31 07:30:35.0, a29.upperBound: 2018-05-31 07:30:35.0, a29.nullCount: 0, a29.count: 10000, a29.sizeInBytes: 250000
[INFO][2018-05-31 16:56:28,612][org.apache.spark.sql.execution.columnar.InMemoryTableScanExec]Skipping partition based on stats id.lowerBound: 66122, id.upperBound: 67457, id.nullCount: 0, id.count: 1336, id.sizeInBytes: 12024, nettypeName.lowerBound:  火箭健身广场, nettypeName.upperBound: （未运营）竹溪一中食堂门口, nettypeName.nullCount: 0, nettypeName.count: 1336, nettypeName.sizeInBytes: 44629, parentId.lowerBound: 47737, parentId.upperBound: 49271, parentId.nullCount: 0, parentId.count: 1336, parentId.sizeInBytes: 12024, createPerson.lowerBound: initial_zy, createPerson.upperBound: initial_zy, createPerson.nullCount: 0, createPerson.count: 1336, createPerson.sizeInBytes: 18704, createDate.lowerBound: 2018-05-11 19:56:26.0, createDate.upperBound: 2018-05-11 19:56:26.0, createDate.nullCount: 0, createDate.count: 1336, createDate.sizeInBytes: 33400, updatePerson.lowerBound: 4B0238, updatePerson.upperBound: initial_zy, updatePerson.nullCount: 0, updatePerson.count: 1336, updatePerson.sizeInBytes: 18632, updateDate.lowerBound: 2018-05-11 21:47:36.0, updateDate.upperBound: 2018-05-27 00:05:02.0, updateDate.nullCount: 0, updateDate.count: 1336, updateDate.sizeInBytes: 33400, isDelete.lowerBound: 0, isDelete.upperBound: 0, isDelete.nullCount: 0, isDelete.count: 1336, isDelete.sizeInBytes: 6680, type.lowerBound: point, type.upperBound: point, type.nullCount: 0, type.count: 1336, type.sizeInBytes: 12024, office.lowerBound: 1044, office.upperBound: 980, office.nullCount: 0, office.count: 1336, office.sizeInBytes: 10520, operatorid.lowerBound: 1014, operatorid.upperBound: 8419, operatorid.nullCount: 0, operatorid.count: 1336, operatorid.sizeInBytes: 10688, province.lowerBound: null, province.upperBound: null, province.nullCount: 1336, province.count: 1336, province.sizeInBytes: 5344, city.lowerBound: null, city.upperBound: null, city.nullCount: 1336, city.count: 1336, city.sizeInBytes: 5344, county.lowerBound: null, county.upperBound: null, county.nullCount: 1336, county.count: 1336, county.sizeInBytes: 5344, contactMan.lowerBound: 丁宏峰, contactMan.upperBound: 龙艳, contactMan.nullCount: 169, contactMan.count: 1336, contactMan.sizeInBytes: 14770, contactPhone.lowerBound: 0459-5872494, contactPhone.upperBound: 81717020, contactPhone.nullCount: 169, contactPhone.count: 1336, contactPhone.sizeInBytes: 18221, address.lowerBound: null, address.upperBound: null, address.nullCount: 1336, address.count: 1336, address.sizeInBytes: 5344, operators.lowerBound: 丁宏峰, operators.upperBound: （鄂州四中）-明华康校园超市, operators.nullCount: 0, operators.count: 1336, operators.sizeInBytes: 37411, channelTypeCode.lowerBound:  , channelTypeCode.upperBound:  , channelTypeCode.nullCount: 0, channelTypeCode.count: 1336, channelTypeCode.sizeInBytes: 6680, checkStatusCode.lowerBound:  , checkStatusCode.upperBound:  , checkStatusCode.nullCount: 0, checkStatusCode.count: 1336, checkStatusCode.sizeInBytes: 6680, communityId.lowerBound: null, communityId.upperBound: null, communityId.nullCount: 1336, communityId.count: 1336, communityId.sizeInBytes: 5344, lat.lowerBound: 22.008755288623, lat.upperBound: 47.318250128944, lat.nullCount: 0, lat.count: 1336, lat.sizeInBytes: 25230, lng.lowerBound: 100.2095886143, lng.upperBound: 99.711413976028, lng.nullCount: 0, lng.count: 1336, lng.sizeInBytes: 25248, auditor.lowerBound: initial_zy, auditor.upperBound: initial_zy, auditor.nullCount: 0, auditor.count: 1336, auditor.sizeInBytes: 18704, auditDate.lowerBound: null, auditDate.upperBound: null, auditDate.nullCount: 1336, auditDate.count: 1336, auditDate.sizeInBytes: 5344, a25.lowerBound: DS_ZFJVEM_PRD, a25.upperBound: DS_ZFJVEM_PRD, a25.nullCount: 0, a25.count: 1336, a25.sizeInBytes: 22712, a26.lowerBound: job_hsta_vem_nettype, a26.upperBound: job_hsta_vem_nettype, a26.nullCount: 0, a26.count: 1336, a26.sizeInBytes: 32064, a27.lowerBound: vem_nettype, a27.upperBound: vem_nettype, a27.nullCount: 0, a27.count: 1336, a27.sizeInBytes: 20040, a28.lowerBound: 20180530, a28.upperBound: 20180530, a28.nullCount: 0, a28.count: 1336, a28.sizeInBytes: 16032, a29.lowerBound: 2018-05-31 07:30:35.0, a29.upperBound: 2018-05-31 07:30:35.0, a29.nullCount: 0, a29.count: 1336, a29.sizeInBytes: 33400
[INFO][2018-05-31 16:56:28,614][org.apache.spark.executor.Executor]Finished task 0.0 in stage 7.0 (TID 207). 93795 bytes result sent to driver
[INFO][2018-05-31 16:56:28,614][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 7.0 (TID 207) in 28 ms on localhost (executor driver) (2/2)
[INFO][2018-05-31 16:56:28,615][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 7.0, whose tasks have all completed, from pool 
[INFO][2018-05-31 16:56:28,615][org.apache.spark.scheduler.DAGScheduler]ResultStage 7 (run at ThreadPoolExecutor.java:1149) finished in 0.029 s
[INFO][2018-05-31 16:56:28,616][org.apache.spark.scheduler.DAGScheduler]Job 4 finished: run at ThreadPoolExecutor.java:1149, took 0.043169 s
[INFO][2018-05-31 16:56:28,653][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator]Code generated in 6.958191 ms
[INFO][2018-05-31 16:56:28,671][org.apache.spark.storage.memory.MemoryStore]Block broadcast_17 stored as values in memory (estimated size 16.1 MB, free 886.6 MB)
[INFO][2018-05-31 16:56:28,690][org.apache.spark.storage.memory.MemoryStore]Block broadcast_17_piece0 stored as bytes in memory (estimated size 151.9 KB, free 886.5 MB)
[INFO][2018-05-31 16:56:28,690][org.apache.spark.storage.BlockManagerInfo]Added broadcast_17_piece0 in memory on 10.194.32.157:61802 (size: 151.9 KB, free: 904.0 MB)
[INFO][2018-05-31 16:56:28,691][org.apache.spark.SparkContext]Created broadcast 17 from run at ThreadPoolExecutor.java:1149
[INFO][2018-05-31 16:56:28,712][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator]Code generated in 16.050054 ms
[INFO][2018-05-31 16:56:28,872][org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter]File Output Committer Algorithm version is 1
[INFO][2018-05-31 16:56:28,872][org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol]Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[INFO][2018-05-31 16:56:28,912][org.apache.spark.SparkContext]Starting job: csv at NetType.scala:239
[INFO][2018-05-31 16:56:28,913][org.apache.spark.scheduler.DAGScheduler]Registering RDD 73 (csv at NetType.scala:239)
[INFO][2018-05-31 16:56:28,913][org.apache.spark.scheduler.DAGScheduler]Got job 5 (csv at NetType.scala:239) with 1 output partitions
[INFO][2018-05-31 16:56:28,913][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 9 (csv at NetType.scala:239)
[INFO][2018-05-31 16:56:28,913][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 8)
[INFO][2018-05-31 16:56:28,913][org.apache.spark.scheduler.DAGScheduler]Missing parents: List(ShuffleMapStage 8)
[INFO][2018-05-31 16:56:28,914][org.apache.spark.scheduler.DAGScheduler]Submitting ShuffleMapStage 8 (MapPartitionsRDD[73] at csv at NetType.scala:239), which has no missing parents
[INFO][2018-05-31 16:56:28,927][org.apache.spark.storage.memory.MemoryStore]Block broadcast_18 stored as values in memory (estimated size 90.3 KB, free 886.4 MB)
[INFO][2018-05-31 16:56:28,930][org.apache.spark.storage.memory.MemoryStore]Block broadcast_18_piece0 stored as bytes in memory (estimated size 28.5 KB, free 886.4 MB)
[INFO][2018-05-31 16:56:28,930][org.apache.spark.storage.BlockManagerInfo]Added broadcast_18_piece0 in memory on 10.194.32.157:61802 (size: 28.5 KB, free: 904.0 MB)
[INFO][2018-05-31 16:56:28,930][org.apache.spark.SparkContext]Created broadcast 18 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-31 16:56:28,930][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ShuffleMapStage 8 (MapPartitionsRDD[73] at csv at NetType.scala:239) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-31 16:56:28,930][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 8.0 with 1 tasks
[INFO][2018-05-31 16:56:28,931][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 8.0 (TID 209, localhost, executor driver, partition 0, ANY, 5337 bytes)
[INFO][2018-05-31 16:56:28,931][org.apache.spark.executor.Executor]Running task 0.0 in stage 8.0 (TID 209)
[INFO][2018-05-31 16:56:28,939][org.apache.spark.sql.execution.datasources.FileScanRDD]Reading File path: hdfs://vm-xaj-bigdata-da-d01:8020/yst/sta_vem/net_community/net_community__0d157f5f_7f23_4d97_95e9_2cb7853bec85, range: 0-518103, partition values: [empty row]
[INFO][2018-05-31 16:56:29,280][org.apache.spark.ContextCleaner]Cleaned accumulator 264
[INFO][2018-05-31 16:56:29,281][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_12_piece0 on 10.194.32.157:61802 in memory (size: 22.7 KB, free: 904.0 MB)
[INFO][2018-05-31 16:56:29,281][org.apache.spark.ContextCleaner]Cleaned accumulator 312
[INFO][2018-05-31 16:56:29,281][org.apache.spark.ContextCleaner]Cleaned accumulator 266
[INFO][2018-05-31 16:56:29,282][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_14_piece0 on 10.194.32.157:61802 in memory (size: 22.7 KB, free: 904.0 MB)
[INFO][2018-05-31 16:56:29,282][org.apache.spark.ContextCleaner]Cleaned accumulator 234
[INFO][2018-05-31 16:56:29,282][org.apache.spark.ContextCleaner]Cleaned accumulator 265
[INFO][2018-05-31 16:56:29,282][org.apache.spark.ContextCleaner]Cleaned accumulator 233
[INFO][2018-05-31 16:56:29,282][org.apache.spark.ContextCleaner]Cleaned accumulator 235
[INFO][2018-05-31 16:56:29,283][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_16_piece0 on 10.194.32.157:61802 in memory (size: 23.1 KB, free: 904.0 MB)
[INFO][2018-05-31 16:56:29,283][org.apache.spark.ContextCleaner]Cleaned accumulator 262
[INFO][2018-05-31 16:56:29,283][org.apache.spark.ContextCleaner]Cleaned accumulator 236
[INFO][2018-05-31 16:56:29,283][org.apache.spark.ContextCleaner]Cleaned accumulator 237
[INFO][2018-05-31 16:56:29,283][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_13_piece0 on 10.194.32.157:61802 in memory (size: 4.3 KB, free: 904.0 MB)
[INFO][2018-05-31 16:56:29,284][org.apache.spark.ContextCleaner]Cleaned accumulator 232
[INFO][2018-05-31 16:56:29,284][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_11_piece0 on 10.194.32.157:61802 in memory (size: 28.5 KB, free: 904.1 MB)
[INFO][2018-05-31 16:56:29,284][org.apache.spark.ContextCleaner]Cleaned accumulator 263
[INFO][2018-05-31 16:56:29,329][org.apache.spark.storage.memory.MemoryStore]Block rdd_65_0 stored as values in memory (estimated size 305.8 KB, free 886.8 MB)
[INFO][2018-05-31 16:56:29,330][org.apache.spark.storage.BlockManagerInfo]Added rdd_65_0 in memory on 10.194.32.157:61802 (size: 305.8 KB, free: 903.8 MB)
[INFO][2018-05-31 16:56:29,344][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator]Code generated in 13.252785 ms
[INFO][2018-05-31 16:56:29,384][org.apache.spark.storage.memory.MemoryStore]Block rdd_71_0 stored as values in memory (estimated size 274.6 KB, free 886.6 MB)
[INFO][2018-05-31 16:56:29,384][org.apache.spark.storage.BlockManagerInfo]Added rdd_71_0 in memory on 10.194.32.157:61802 (size: 274.6 KB, free: 903.5 MB)
[INFO][2018-05-31 16:56:29,402][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator]Code generated in 17.635188 ms
[INFO][2018-05-31 16:56:29,420][org.apache.spark.executor.Executor]Finished task 0.0 in stage 8.0 (TID 209). 3058 bytes result sent to driver
[INFO][2018-05-31 16:56:29,420][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 8.0 (TID 209) in 489 ms on localhost (executor driver) (1/1)
[INFO][2018-05-31 16:56:29,420][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 8.0, whose tasks have all completed, from pool 
[INFO][2018-05-31 16:56:29,421][org.apache.spark.scheduler.DAGScheduler]ShuffleMapStage 8 (csv at NetType.scala:239) finished in 0.490 s
[INFO][2018-05-31 16:56:29,421][org.apache.spark.scheduler.DAGScheduler]looking for newly runnable stages
[INFO][2018-05-31 16:56:29,421][org.apache.spark.scheduler.DAGScheduler]running: Set()
[INFO][2018-05-31 16:56:29,421][org.apache.spark.scheduler.DAGScheduler]waiting: Set(ResultStage 9)
[INFO][2018-05-31 16:56:29,421][org.apache.spark.scheduler.DAGScheduler]failed: Set()
[INFO][2018-05-31 16:56:29,421][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 9 (ShuffledRowRDD[74] at csv at NetType.scala:239), which has no missing parents
[INFO][2018-05-31 16:56:29,437][org.apache.spark.storage.memory.MemoryStore]Block broadcast_19 stored as values in memory (estimated size 75.8 KB, free 886.5 MB)
[INFO][2018-05-31 16:56:29,439][org.apache.spark.storage.memory.MemoryStore]Block broadcast_19_piece0 stored as bytes in memory (estimated size 28.6 KB, free 886.5 MB)
[INFO][2018-05-31 16:56:29,440][org.apache.spark.storage.BlockManagerInfo]Added broadcast_19_piece0 in memory on 10.194.32.157:61802 (size: 28.6 KB, free: 903.5 MB)
[INFO][2018-05-31 16:56:29,440][org.apache.spark.SparkContext]Created broadcast 19 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-31 16:56:29,440][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 9 (ShuffledRowRDD[74] at csv at NetType.scala:239) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-31 16:56:29,441][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 9.0 with 1 tasks
[INFO][2018-05-31 16:56:29,441][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 9.0 (TID 210, localhost, executor driver, partition 0, ANY, 4726 bytes)
[INFO][2018-05-31 16:56:29,441][org.apache.spark.executor.Executor]Running task 0.0 in stage 9.0 (TID 210)
[INFO][2018-05-31 16:56:29,450][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
[INFO][2018-05-31 16:56:29,451][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO][2018-05-31 16:56:29,451][org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter]File Output Committer Algorithm version is 1
[INFO][2018-05-31 16:56:29,452][org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol]Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[INFO][2018-05-31 16:56:29,665][org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter]Saved output of task 'attempt_20180531165629_0009_m_000000_0' to hdfs://vm-xaj-bigdata-da-d01:8020/yst/seven/data/netData/_temporary/0/task_20180531165629_0009_m_000000
[INFO][2018-05-31 16:56:29,665][org.apache.spark.mapred.SparkHadoopMapRedUtil]attempt_20180531165629_0009_m_000000_0: Committed
[INFO][2018-05-31 16:56:29,666][org.apache.spark.executor.Executor]Finished task 0.0 in stage 9.0 (TID 210). 1392 bytes result sent to driver
[INFO][2018-05-31 16:56:29,669][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 9.0 (TID 210) in 227 ms on localhost (executor driver) (1/1)
[INFO][2018-05-31 16:56:29,669][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 9.0, whose tasks have all completed, from pool 
[INFO][2018-05-31 16:56:29,669][org.apache.spark.scheduler.DAGScheduler]ResultStage 9 (csv at NetType.scala:239) finished in 0.228 s
[INFO][2018-05-31 16:56:29,669][org.apache.spark.scheduler.DAGScheduler]Job 5 finished: csv at NetType.scala:239, took 0.756922 s
[INFO][2018-05-31 16:56:29,764][org.apache.spark.sql.execution.datasources.FileFormatWriter]Job null committed.
[INFO][2018-05-31 16:56:29,765][com.seven.spark.sparksql.NetType$]save net data is success timeout is 0:00:02.193
[INFO][2018-05-31 16:56:29,774][org.spark_project.jetty.server.AbstractConnector]Stopped Spark@6cab17fd{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-05-31 16:56:29,776][org.apache.spark.ui.SparkUI]Stopped Spark web UI at http://10.194.32.157:4040
[INFO][2018-05-31 16:56:29,787][org.apache.spark.MapOutputTrackerMasterEndpoint]MapOutputTrackerMasterEndpoint stopped!
[INFO][2018-05-31 16:56:29,885][org.apache.spark.storage.memory.MemoryStore]MemoryStore cleared
[INFO][2018-05-31 16:56:29,886][org.apache.spark.storage.BlockManager]BlockManager stopped
[INFO][2018-05-31 16:56:29,886][org.apache.spark.storage.BlockManagerMaster]BlockManagerMaster stopped
[INFO][2018-05-31 16:56:29,891][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint]OutputCommitCoordinator stopped!
[INFO][2018-05-31 16:56:29,893][org.apache.spark.SparkContext]Successfully stopped SparkContext
[INFO][2018-05-31 16:56:29,893][com.seven.spark.sparksql.NetType$]NetType$ is success ! ! !
[INFO][2018-05-31 16:56:29,896][org.apache.spark.util.ShutdownHookManager]Shutdown hook called
[INFO][2018-05-31 16:56:29,897][org.apache.spark.util.ShutdownHookManager]Deleting directory /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/spark-bfdb732d-4e44-42b6-adb0-938e1800f3df
[INFO][2018-05-31 17:02:50,307][org.apache.spark.SparkContext]Running Spark version 2.2.0
[INFO][2018-05-31 17:02:51,888][org.apache.spark.SparkContext]Submitted application: tmp
[INFO][2018-05-31 17:02:51,915][org.apache.spark.SecurityManager]Changing view acls to: seven
[INFO][2018-05-31 17:02:51,918][org.apache.spark.SecurityManager]Changing modify acls to: seven
[INFO][2018-05-31 17:02:51,918][org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO][2018-05-31 17:02:51,919][org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO][2018-05-31 17:02:51,920][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(seven); groups with view permissions: Set(); users  with modify permissions: Set(seven); groups with modify permissions: Set()
[INFO][2018-05-31 17:02:52,354][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 61936.
[INFO][2018-05-31 17:02:52,374][org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO][2018-05-31 17:02:52,396][org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO][2018-05-31 17:02:52,399][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2018-05-31 17:02:52,400][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
[INFO][2018-05-31 17:02:52,416][org.apache.spark.storage.DiskBlockManager]Created local directory at /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/blockmgr-7a435880-dd1f-4d88-bcbb-76b50486e4d1
[INFO][2018-05-31 17:02:52,438][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 912.3 MB
[INFO][2018-05-31 17:02:52,558][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO][2018-05-31 17:02:52,830][org.spark_project.jetty.util.log]Logging initialized @4037ms
[INFO][2018-05-31 17:02:52,972][org.spark_project.jetty.server.Server]jetty-9.3.z-SNAPSHOT
[INFO][2018-05-31 17:02:52,997][org.spark_project.jetty.server.Server]Started @4208ms
[INFO][2018-05-31 17:02:53,029][org.spark_project.jetty.server.AbstractConnector]Started ServerConnector@5d465e4b{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-05-31 17:02:53,029][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4040.
[INFO][2018-05-31 17:02:53,064][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3e2822{/jobs,null,AVAILABLE,@Spark}
[INFO][2018-05-31 17:02:53,065][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@35fe2125{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 17:02:53,065][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@34645867{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2018-05-31 17:02:53,066][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1255b1d1{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 17:02:53,067][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7c22d4f{/stages,null,AVAILABLE,@Spark}
[INFO][2018-05-31 17:02:53,068][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@60bdf15d{/stages/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 17:02:53,068][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@51e4ccb3{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2018-05-31 17:02:53,071][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@28a0fd6c{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 17:02:53,073][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@66629f63{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2018-05-31 17:02:53,074][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@27a5328c{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 17:02:53,075][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6c345c5f{/storage,null,AVAILABLE,@Spark}
[INFO][2018-05-31 17:02:53,076][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@65e61854{/storage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 17:02:53,076][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4fcee388{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2018-05-31 17:02:53,078][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3af17be2{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 17:02:53,085][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@37f21974{/environment,null,AVAILABLE,@Spark}
[INFO][2018-05-31 17:02:53,086][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6e521c1e{/environment/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 17:02:53,091][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5d5d9e5{/executors,null,AVAILABLE,@Spark}
[INFO][2018-05-31 17:02:53,127][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4ef27d66{/executors/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 17:02:53,164][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1d9bec4d{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2018-05-31 17:02:53,165][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@10c8f62{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 17:02:53,173][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@25f7391e{/static,null,AVAILABLE,@Spark}
[INFO][2018-05-31 17:02:53,174][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@593e824f{/,null,AVAILABLE,@Spark}
[INFO][2018-05-31 17:02:53,175][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6d8792db{/api,null,AVAILABLE,@Spark}
[INFO][2018-05-31 17:02:53,221][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2f162cc0{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-31 17:02:53,222][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7c041b41{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-31 17:02:53,227][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://10.194.32.157:4040
[INFO][2018-05-31 17:02:53,392][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
[INFO][2018-05-31 17:02:53,471][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 61938.
[INFO][2018-05-31 17:02:53,472][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 10.194.32.157:61938
[INFO][2018-05-31 17:02:53,474][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2018-05-31 17:02:53,476][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 10.194.32.157, 61938, None)
[INFO][2018-05-31 17:02:53,483][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 10.194.32.157:61938 with 912.3 MB RAM, BlockManagerId(driver, 10.194.32.157, 61938, None)
[INFO][2018-05-31 17:02:53,501][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 10.194.32.157, 61938, None)
[INFO][2018-05-31 17:02:53,503][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 10.194.32.157, 61938, None)
[INFO][2018-05-31 17:02:53,909][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@55b62629{/metrics/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 17:02:54,015][root]jobStarttime:2018-05-31 17:02:54
[INFO][2018-05-31 17:04:41,974][org.apache.spark.SparkContext]Running Spark version 2.2.0
[INFO][2018-05-31 17:04:43,374][org.apache.spark.SparkContext]Submitted application: tmp
[INFO][2018-05-31 17:04:43,412][org.apache.spark.SecurityManager]Changing view acls to: seven
[INFO][2018-05-31 17:04:43,413][org.apache.spark.SecurityManager]Changing modify acls to: seven
[INFO][2018-05-31 17:04:43,414][org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO][2018-05-31 17:04:43,415][org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO][2018-05-31 17:04:43,416][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(seven); groups with view permissions: Set(); users  with modify permissions: Set(seven); groups with modify permissions: Set()
[INFO][2018-05-31 17:04:44,151][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 61977.
[INFO][2018-05-31 17:04:44,190][org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO][2018-05-31 17:04:44,270][org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO][2018-05-31 17:04:44,292][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2018-05-31 17:04:44,293][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
[INFO][2018-05-31 17:04:44,313][org.apache.spark.storage.DiskBlockManager]Created local directory at /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/blockmgr-dc0579d1-2fa8-4326-866a-7ee21336ff12
[INFO][2018-05-31 17:04:44,351][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 912.3 MB
[INFO][2018-05-31 17:04:44,522][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO][2018-05-31 17:04:44,866][org.spark_project.jetty.util.log]Logging initialized @4439ms
[INFO][2018-05-31 17:04:44,979][org.spark_project.jetty.server.Server]jetty-9.3.z-SNAPSHOT
[INFO][2018-05-31 17:04:44,998][org.spark_project.jetty.server.Server]Started @4574ms
[INFO][2018-05-31 17:04:45,019][org.spark_project.jetty.server.AbstractConnector]Started ServerConnector@41a90fa8{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-05-31 17:04:45,019][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4040.
[INFO][2018-05-31 17:04:45,047][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@29a60c27{/jobs,null,AVAILABLE,@Spark}
[INFO][2018-05-31 17:04:45,047][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@34645867{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 17:04:45,048][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@60b71e8f{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2018-05-31 17:04:45,049][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7c22d4f{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 17:04:45,051][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@60bdf15d{/stages,null,AVAILABLE,@Spark}
[INFO][2018-05-31 17:04:45,052][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@51e4ccb3{/stages/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 17:04:45,053][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@495083a0{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2018-05-31 17:04:45,057][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@66629f63{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 17:04:45,058][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@27a5328c{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2018-05-31 17:04:45,059][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6c345c5f{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 17:04:45,059][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@65e61854{/storage,null,AVAILABLE,@Spark}
[INFO][2018-05-31 17:04:45,060][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4fcee388{/storage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 17:04:45,061][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3af17be2{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2018-05-31 17:04:45,061][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@37f21974{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 17:04:45,062][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6e521c1e{/environment,null,AVAILABLE,@Spark}
[INFO][2018-05-31 17:04:45,063][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5d5d9e5{/environment/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 17:04:45,064][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4ef27d66{/executors,null,AVAILABLE,@Spark}
[INFO][2018-05-31 17:04:45,064][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1d9bec4d{/executors/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 17:04:45,065][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@10c8f62{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2018-05-31 17:04:45,066][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@25f7391e{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 17:04:45,083][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5ab14cb9{/static,null,AVAILABLE,@Spark}
[INFO][2018-05-31 17:04:45,083][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6d8792db{/,null,AVAILABLE,@Spark}
[INFO][2018-05-31 17:04:45,084][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@493dfb8e{/api,null,AVAILABLE,@Spark}
[INFO][2018-05-31 17:04:45,085][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7c041b41{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-31 17:04:45,086][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@61078690{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-31 17:04:45,088][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://10.194.32.157:4040
[INFO][2018-05-31 17:04:45,220][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
[INFO][2018-05-31 17:04:45,248][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 61978.
[INFO][2018-05-31 17:04:45,249][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 10.194.32.157:61978
[INFO][2018-05-31 17:04:45,251][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2018-05-31 17:04:45,253][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 10.194.32.157, 61978, None)
[INFO][2018-05-31 17:04:45,256][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 10.194.32.157:61978 with 912.3 MB RAM, BlockManagerId(driver, 10.194.32.157, 61978, None)
[INFO][2018-05-31 17:04:45,259][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 10.194.32.157, 61978, None)
[INFO][2018-05-31 17:04:45,259][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 10.194.32.157, 61978, None)
[INFO][2018-05-31 17:04:45,464][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5e63cad{/metrics/json,null,AVAILABLE,@Spark}
[INFO][2018-05-31 17:04:45,483][root]jobStarttime:2018-05-31 17:04:45
[INFO][2018-06-01 11:47:34,194][org.apache.spark.SparkContext]Running Spark version 2.2.0
[INFO][2018-06-01 11:47:35,393][org.apache.spark.SparkContext]Submitted application: tmp
[INFO][2018-06-01 11:47:35,455][org.apache.spark.SecurityManager]Changing view acls to: seven
[INFO][2018-06-01 11:47:35,457][org.apache.spark.SecurityManager]Changing modify acls to: seven
[INFO][2018-06-01 11:47:35,458][org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO][2018-06-01 11:47:35,459][org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO][2018-06-01 11:47:35,462][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(seven); groups with view permissions: Set(); users  with modify permissions: Set(seven); groups with modify permissions: Set()
[INFO][2018-06-01 11:47:36,063][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 52644.
[INFO][2018-06-01 11:47:36,097][org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO][2018-06-01 11:47:36,126][org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO][2018-06-01 11:47:36,130][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2018-06-01 11:47:36,131][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
[INFO][2018-06-01 11:47:36,146][org.apache.spark.storage.DiskBlockManager]Created local directory at /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/blockmgr-116d0a69-cfda-4820-ae71-abb36d6b0eea
[INFO][2018-06-01 11:47:36,208][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 912.3 MB
[INFO][2018-06-01 11:47:36,282][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO][2018-06-01 11:47:36,414][org.spark_project.jetty.util.log]Logging initialized @3922ms
[INFO][2018-06-01 11:47:36,489][org.spark_project.jetty.server.Server]jetty-9.3.z-SNAPSHOT
[INFO][2018-06-01 11:47:36,505][org.spark_project.jetty.server.Server]Started @4014ms
[INFO][2018-06-01 11:47:36,539][org.spark_project.jetty.server.AbstractConnector]Started ServerConnector@463f3050{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-06-01 11:47:36,539][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4040.
[INFO][2018-06-01 11:47:36,567][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@168cd36b{/jobs,null,AVAILABLE,@Spark}
[INFO][2018-06-01 11:47:36,568][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7a7cc52c{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2018-06-01 11:47:36,569][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@524a2ffb{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2018-06-01 11:47:36,570][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@72456279{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2018-06-01 11:47:36,571][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@21f459fc{/stages,null,AVAILABLE,@Spark}
[INFO][2018-06-01 11:47:36,572][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1416cf9f{/stages/json,null,AVAILABLE,@Spark}
[INFO][2018-06-01 11:47:36,573][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@bfc14b9{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2018-06-01 11:47:36,576][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@a518813{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2018-06-01 11:47:36,577][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@75361cf6{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2018-06-01 11:47:36,577][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6ba7383d{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2018-06-01 11:47:36,578][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@710d89e2{/storage,null,AVAILABLE,@Spark}
[INFO][2018-06-01 11:47:36,579][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4fc142ec{/storage/json,null,AVAILABLE,@Spark}
[INFO][2018-06-01 11:47:36,580][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@29eda4f8{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2018-06-01 11:47:36,580][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5e048149{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2018-06-01 11:47:36,581][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3d5790ea{/environment,null,AVAILABLE,@Spark}
[INFO][2018-06-01 11:47:36,582][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@67a3bd51{/environment/json,null,AVAILABLE,@Spark}
[INFO][2018-06-01 11:47:36,583][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@56913163{/executors,null,AVAILABLE,@Spark}
[INFO][2018-06-01 11:47:36,586][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@a18649a{/executors/json,null,AVAILABLE,@Spark}
[INFO][2018-06-01 11:47:36,587][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@396639b{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2018-06-01 11:47:36,587][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@62573c86{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2018-06-01 11:47:36,596][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@14229fa7{/static,null,AVAILABLE,@Spark}
[INFO][2018-06-01 11:47:36,597][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@55a88417{/,null,AVAILABLE,@Spark}
[INFO][2018-06-01 11:47:36,599][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@8a2a6a{/api,null,AVAILABLE,@Spark}
[INFO][2018-06-01 11:47:36,599][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@140d1230{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2018-06-01 11:47:36,600][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@10bea4{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2018-06-01 11:47:36,604][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://10.194.32.157:4040
[INFO][2018-06-01 11:47:36,725][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
[INFO][2018-06-01 11:47:36,759][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 52646.
[INFO][2018-06-01 11:47:36,760][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 10.194.32.157:52646
[INFO][2018-06-01 11:47:36,764][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2018-06-01 11:47:36,772][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 10.194.32.157, 52646, None)
[INFO][2018-06-01 11:47:36,776][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 10.194.32.157:52646 with 912.3 MB RAM, BlockManagerId(driver, 10.194.32.157, 52646, None)
[INFO][2018-06-01 11:47:36,779][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 10.194.32.157, 52646, None)
[INFO][2018-06-01 11:47:36,781][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 10.194.32.157, 52646, None)
[INFO][2018-06-01 11:47:37,086][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6cd64b3f{/metrics/json,null,AVAILABLE,@Spark}
[INFO][2018-06-01 11:47:37,126][root]jobStarttime:2018-06-01 11:47:37
[INFO][2018-06-01 11:50:38,315][org.apache.spark.SparkContext]Running Spark version 2.2.0
[INFO][2018-06-01 11:50:39,238][org.apache.spark.SparkContext]Submitted application: tmp
[INFO][2018-06-01 11:50:39,277][org.apache.spark.SecurityManager]Changing view acls to: seven
[INFO][2018-06-01 11:50:39,281][org.apache.spark.SecurityManager]Changing modify acls to: seven
[INFO][2018-06-01 11:50:39,282][org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO][2018-06-01 11:50:39,282][org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO][2018-06-01 11:50:39,283][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(seven); groups with view permissions: Set(); users  with modify permissions: Set(seven); groups with modify permissions: Set()
[INFO][2018-06-01 11:50:39,609][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 52707.
[INFO][2018-06-01 11:50:39,627][org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO][2018-06-01 11:50:39,647][org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO][2018-06-01 11:50:39,650][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2018-06-01 11:50:39,650][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
[INFO][2018-06-01 11:50:39,661][org.apache.spark.storage.DiskBlockManager]Created local directory at /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/blockmgr-78d9dcc9-5cbe-4ebf-b752-53ffd3e63e9d
[INFO][2018-06-01 11:50:39,684][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 912.3 MB
[INFO][2018-06-01 11:50:39,819][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO][2018-06-01 11:50:39,936][org.spark_project.jetty.util.log]Logging initialized @2732ms
[INFO][2018-06-01 11:50:40,000][org.spark_project.jetty.server.Server]jetty-9.3.z-SNAPSHOT
[INFO][2018-06-01 11:50:40,013][org.spark_project.jetty.server.Server]Started @2810ms
[INFO][2018-06-01 11:50:40,045][org.spark_project.jetty.server.AbstractConnector]Started ServerConnector@37fc78a7{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-06-01 11:50:40,046][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4040.
[INFO][2018-06-01 11:50:40,075][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4602c2a9{/jobs,null,AVAILABLE,@Spark}
[INFO][2018-06-01 11:50:40,076][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2cf23c81{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2018-06-01 11:50:40,077][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@35fe2125{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2018-06-01 11:50:40,078][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2484f433{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2018-06-01 11:50:40,079][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1255b1d1{/stages,null,AVAILABLE,@Spark}
[INFO][2018-06-01 11:50:40,080][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7c22d4f{/stages/json,null,AVAILABLE,@Spark}
[INFO][2018-06-01 11:50:40,081][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@60bdf15d{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2018-06-01 11:50:40,082][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@495083a0{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2018-06-01 11:50:40,090][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@28a0fd6c{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2018-06-01 11:50:40,095][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@66629f63{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2018-06-01 11:50:40,096][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@27a5328c{/storage,null,AVAILABLE,@Spark}
[INFO][2018-06-01 11:50:40,101][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6c345c5f{/storage/json,null,AVAILABLE,@Spark}
[INFO][2018-06-01 11:50:40,102][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@65e61854{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2018-06-01 11:50:40,102][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4fcee388{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2018-06-01 11:50:40,103][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3af17be2{/environment,null,AVAILABLE,@Spark}
[INFO][2018-06-01 11:50:40,104][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@37f21974{/environment/json,null,AVAILABLE,@Spark}
[INFO][2018-06-01 11:50:40,105][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6e521c1e{/executors,null,AVAILABLE,@Spark}
[INFO][2018-06-01 11:50:40,107][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5d5d9e5{/executors/json,null,AVAILABLE,@Spark}
[INFO][2018-06-01 11:50:40,108][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4ef27d66{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2018-06-01 11:50:40,109][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1d9bec4d{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2018-06-01 11:50:40,118][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@10c8f62{/static,null,AVAILABLE,@Spark}
[INFO][2018-06-01 11:50:40,122][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3954d008{/,null,AVAILABLE,@Spark}
[INFO][2018-06-01 11:50:40,127][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@593e824f{/api,null,AVAILABLE,@Spark}
[INFO][2018-06-01 11:50:40,128][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@ce5a68e{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2018-06-01 11:50:40,130][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2f162cc0{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2018-06-01 11:50:40,133][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://10.194.32.157:4040
[INFO][2018-06-01 11:50:40,280][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
[INFO][2018-06-01 11:50:40,302][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 52708.
[INFO][2018-06-01 11:50:40,305][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 10.194.32.157:52708
[INFO][2018-06-01 11:50:40,310][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2018-06-01 11:50:40,314][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 10.194.32.157, 52708, None)
[INFO][2018-06-01 11:50:40,318][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 10.194.32.157:52708 with 912.3 MB RAM, BlockManagerId(driver, 10.194.32.157, 52708, None)
[INFO][2018-06-01 11:50:40,322][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 10.194.32.157, 52708, None)
[INFO][2018-06-01 11:50:40,323][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 10.194.32.157, 52708, None)
[INFO][2018-06-01 11:50:40,657][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@535b8c24{/metrics/json,null,AVAILABLE,@Spark}
[INFO][2018-06-01 11:50:40,697][root]jobStarttime:2018-06-01 11:50:40
[INFO][2018-06-01 11:53:59,471][root]jobEndtime:2018-06-01 11:53:59;
[INFO][2018-06-01 11:53:59,474][root]jobResult:Sucessfull!
[INFO][2018-06-04 09:09:29,189][org.apache.spark.SparkContext]Running Spark version 2.2.0
[INFO][2018-06-04 09:09:30,261][org.apache.spark.SparkContext]Submitted application: tmp
[INFO][2018-06-04 09:09:30,290][org.apache.spark.SecurityManager]Changing view acls to: seven
[INFO][2018-06-04 09:09:30,290][org.apache.spark.SecurityManager]Changing modify acls to: seven
[INFO][2018-06-04 09:09:30,291][org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO][2018-06-04 09:09:30,292][org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO][2018-06-04 09:09:30,292][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(seven); groups with view permissions: Set(); users  with modify permissions: Set(seven); groups with modify permissions: Set()
[INFO][2018-06-04 09:09:30,653][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 50216.
[INFO][2018-06-04 09:09:30,682][org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO][2018-06-04 09:09:30,712][org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO][2018-06-04 09:09:30,716][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2018-06-04 09:09:30,716][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
[INFO][2018-06-04 09:09:30,735][org.apache.spark.storage.DiskBlockManager]Created local directory at /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/blockmgr-c4242f6d-a5d7-4dbf-af93-f8c1d9a92440
[INFO][2018-06-04 09:09:30,769][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 912.3 MB
[INFO][2018-06-04 09:09:30,849][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO][2018-06-04 09:09:30,970][org.spark_project.jetty.util.log]Logging initialized @3064ms
[INFO][2018-06-04 09:09:31,045][org.spark_project.jetty.server.Server]jetty-9.3.z-SNAPSHOT
[INFO][2018-06-04 09:09:31,062][org.spark_project.jetty.server.Server]Started @3158ms
[INFO][2018-06-04 09:09:31,090][org.spark_project.jetty.server.AbstractConnector]Started ServerConnector@4e76dac{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-06-04 09:09:31,090][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4040.
[INFO][2018-06-04 09:09:31,118][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4204541c{/jobs,null,AVAILABLE,@Spark}
[INFO][2018-06-04 09:09:31,118][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5f3b9c57{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2018-06-04 09:09:31,119][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2cf23c81{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2018-06-04 09:09:31,120][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@94f6bfb{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2018-06-04 09:09:31,120][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2484f433{/stages,null,AVAILABLE,@Spark}
[INFO][2018-06-04 09:09:31,121][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1255b1d1{/stages/json,null,AVAILABLE,@Spark}
[INFO][2018-06-04 09:09:31,121][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7c22d4f{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2018-06-04 09:09:31,122][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@51e4ccb3{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2018-06-04 09:09:31,123][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@495083a0{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2018-06-04 09:09:31,123][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@28a0fd6c{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2018-06-04 09:09:31,123][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@66629f63{/storage,null,AVAILABLE,@Spark}
[INFO][2018-06-04 09:09:31,124][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@27a5328c{/storage/json,null,AVAILABLE,@Spark}
[INFO][2018-06-04 09:09:31,125][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6c345c5f{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2018-06-04 09:09:31,125][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@65e61854{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2018-06-04 09:09:31,126][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4fcee388{/environment,null,AVAILABLE,@Spark}
[INFO][2018-06-04 09:09:31,126][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3af17be2{/environment/json,null,AVAILABLE,@Spark}
[INFO][2018-06-04 09:09:31,127][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@37f21974{/executors,null,AVAILABLE,@Spark}
[INFO][2018-06-04 09:09:31,128][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6e521c1e{/executors/json,null,AVAILABLE,@Spark}
[INFO][2018-06-04 09:09:31,128][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5d5d9e5{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2018-06-04 09:09:31,129][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4ef27d66{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2018-06-04 09:09:31,138][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1d9bec4d{/static,null,AVAILABLE,@Spark}
[INFO][2018-06-04 09:09:31,139][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6c6357f9{/,null,AVAILABLE,@Spark}
[INFO][2018-06-04 09:09:31,140][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3954d008{/api,null,AVAILABLE,@Spark}
[INFO][2018-06-04 09:09:31,141][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@493dfb8e{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2018-06-04 09:09:31,141][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@ce5a68e{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2018-06-04 09:09:31,144][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://10.194.32.157:4040
[INFO][2018-06-04 09:09:31,310][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
[INFO][2018-06-04 09:09:31,344][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 50217.
[INFO][2018-06-04 09:09:31,345][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 10.194.32.157:50217
[INFO][2018-06-04 09:09:31,347][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2018-06-04 09:09:31,350][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 10.194.32.157, 50217, None)
[INFO][2018-06-04 09:09:31,353][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 10.194.32.157:50217 with 912.3 MB RAM, BlockManagerId(driver, 10.194.32.157, 50217, None)
[INFO][2018-06-04 09:09:31,356][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 10.194.32.157, 50217, None)
[INFO][2018-06-04 09:09:31,357][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 10.194.32.157, 50217, None)
[INFO][2018-06-04 09:09:31,592][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@676f0a60{/metrics/json,null,AVAILABLE,@Spark}
[INFO][2018-06-04 09:09:31,625][root]jobStarttime:2018-06-04 09:09:31
[INFO][2018-06-04 09:10:38,481][org.apache.spark.SparkContext]Running Spark version 2.2.0
[INFO][2018-06-04 09:10:39,340][org.apache.spark.SparkContext]Submitted application: tmp
[INFO][2018-06-04 09:10:39,372][org.apache.spark.SecurityManager]Changing view acls to: seven
[INFO][2018-06-04 09:10:39,372][org.apache.spark.SecurityManager]Changing modify acls to: seven
[INFO][2018-06-04 09:10:39,373][org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO][2018-06-04 09:10:39,374][org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO][2018-06-04 09:10:39,375][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(seven); groups with view permissions: Set(); users  with modify permissions: Set(seven); groups with modify permissions: Set()
[INFO][2018-06-04 09:10:39,707][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 50245.
[INFO][2018-06-04 09:10:39,728][org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO][2018-06-04 09:10:39,748][org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO][2018-06-04 09:10:39,751][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2018-06-04 09:10:39,751][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
[INFO][2018-06-04 09:10:39,760][org.apache.spark.storage.DiskBlockManager]Created local directory at /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/blockmgr-5d5aa939-0e9f-4309-b45e-f9b241287f57
[INFO][2018-06-04 09:10:39,808][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 912.3 MB
[INFO][2018-06-04 09:10:39,856][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO][2018-06-04 09:10:39,950][org.spark_project.jetty.util.log]Logging initialized @3955ms
[INFO][2018-06-04 09:10:40,010][org.spark_project.jetty.server.Server]jetty-9.3.z-SNAPSHOT
[INFO][2018-06-04 09:10:40,023][org.spark_project.jetty.server.Server]Started @4029ms
[INFO][2018-06-04 09:10:40,058][org.spark_project.jetty.server.AbstractConnector]Started ServerConnector@3a44227e{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-06-04 09:10:40,058][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4040.
[INFO][2018-06-04 09:10:40,084][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@408e96d9{/jobs,null,AVAILABLE,@Spark}
[INFO][2018-06-04 09:10:40,085][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4e1ce44{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2018-06-04 09:10:40,086][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7a7cc52c{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2018-06-04 09:10:40,087][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@332820f4{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2018-06-04 09:10:40,087][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@72456279{/stages,null,AVAILABLE,@Spark}
[INFO][2018-06-04 09:10:40,088][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@21f459fc{/stages/json,null,AVAILABLE,@Spark}
[INFO][2018-06-04 09:10:40,089][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1416cf9f{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2018-06-04 09:10:40,091][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6a9b9909{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2018-06-04 09:10:40,092][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@a518813{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2018-06-04 09:10:40,093][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@75361cf6{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2018-06-04 09:10:40,093][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6ba7383d{/storage,null,AVAILABLE,@Spark}
[INFO][2018-06-04 09:10:40,094][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@710d89e2{/storage/json,null,AVAILABLE,@Spark}
[INFO][2018-06-04 09:10:40,095][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4fc142ec{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2018-06-04 09:10:40,095][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@29eda4f8{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2018-06-04 09:10:40,096][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5e048149{/environment,null,AVAILABLE,@Spark}
[INFO][2018-06-04 09:10:40,097][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3d5790ea{/environment/json,null,AVAILABLE,@Spark}
[INFO][2018-06-04 09:10:40,098][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@67a3bd51{/executors,null,AVAILABLE,@Spark}
[INFO][2018-06-04 09:10:40,099][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@56913163{/executors/json,null,AVAILABLE,@Spark}
[INFO][2018-06-04 09:10:40,100][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@a18649a{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2018-06-04 09:10:40,101][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@396639b{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2018-06-04 09:10:40,108][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@62573c86{/static,null,AVAILABLE,@Spark}
[INFO][2018-06-04 09:10:40,109][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@8f2098e{/,null,AVAILABLE,@Spark}
[INFO][2018-06-04 09:10:40,111][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@55a88417{/api,null,AVAILABLE,@Spark}
[INFO][2018-06-04 09:10:40,112][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@481e91b6{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2018-06-04 09:10:40,112][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@140d1230{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2018-06-04 09:10:40,116][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://10.194.32.157:4040
[INFO][2018-06-04 09:10:40,241][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
[INFO][2018-06-04 09:10:40,276][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 50246.
[INFO][2018-06-04 09:10:40,277][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 10.194.32.157:50246
[INFO][2018-06-04 09:10:40,286][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2018-06-04 09:10:40,289][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 10.194.32.157, 50246, None)
[INFO][2018-06-04 09:10:40,292][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 10.194.32.157:50246 with 912.3 MB RAM, BlockManagerId(driver, 10.194.32.157, 50246, None)
[INFO][2018-06-04 09:10:40,296][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 10.194.32.157, 50246, None)
[INFO][2018-06-04 09:10:40,297][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 10.194.32.157, 50246, None)
[INFO][2018-06-04 09:10:40,487][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7d57dbb5{/metrics/json,null,AVAILABLE,@Spark}
[INFO][2018-06-04 09:10:40,525][root]jobStarttime:2018-06-04 09:10:40
[INFO][2018-06-04 09:11:33,127][org.apache.spark.SparkContext]Running Spark version 2.2.0
[INFO][2018-06-04 09:11:33,692][org.apache.spark.SparkContext]Submitted application: tmp
[INFO][2018-06-04 09:11:33,722][org.apache.spark.SecurityManager]Changing view acls to: seven
[INFO][2018-06-04 09:11:33,722][org.apache.spark.SecurityManager]Changing modify acls to: seven
[INFO][2018-06-04 09:11:33,723][org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO][2018-06-04 09:11:33,724][org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO][2018-06-04 09:11:33,724][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(seven); groups with view permissions: Set(); users  with modify permissions: Set(seven); groups with modify permissions: Set()
[INFO][2018-06-04 09:11:34,056][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 50265.
[INFO][2018-06-04 09:11:34,076][org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO][2018-06-04 09:11:34,094][org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO][2018-06-04 09:11:34,097][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2018-06-04 09:11:34,097][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
[INFO][2018-06-04 09:11:34,106][org.apache.spark.storage.DiskBlockManager]Created local directory at /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/blockmgr-1ce1d12f-de0e-4128-b90c-84f000ee7a4a
[INFO][2018-06-04 09:11:34,151][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 912.3 MB
[INFO][2018-06-04 09:11:34,205][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO][2018-06-04 09:11:34,287][org.spark_project.jetty.util.log]Logging initialized @2713ms
[INFO][2018-06-04 09:11:34,353][org.spark_project.jetty.server.Server]jetty-9.3.z-SNAPSHOT
[INFO][2018-06-04 09:11:34,366][org.spark_project.jetty.server.Server]Started @2794ms
[INFO][2018-06-04 09:11:34,392][org.spark_project.jetty.server.AbstractConnector]Started ServerConnector@2823513{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-06-04 09:11:34,392][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4040.
[INFO][2018-06-04 09:11:34,424][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@408e96d9{/jobs,null,AVAILABLE,@Spark}
[INFO][2018-06-04 09:11:34,425][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4e1ce44{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2018-06-04 09:11:34,427][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7a7cc52c{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2018-06-04 09:11:34,428][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@332820f4{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2018-06-04 09:11:34,429][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@72456279{/stages,null,AVAILABLE,@Spark}
[INFO][2018-06-04 09:11:34,430][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@21f459fc{/stages/json,null,AVAILABLE,@Spark}
[INFO][2018-06-04 09:11:34,430][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1416cf9f{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2018-06-04 09:11:34,432][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6a9b9909{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2018-06-04 09:11:34,434][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@a518813{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2018-06-04 09:11:34,435][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@75361cf6{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2018-06-04 09:11:34,435][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6ba7383d{/storage,null,AVAILABLE,@Spark}
[INFO][2018-06-04 09:11:34,436][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@710d89e2{/storage/json,null,AVAILABLE,@Spark}
[INFO][2018-06-04 09:11:34,437][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4fc142ec{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2018-06-04 09:11:34,438][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@29eda4f8{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2018-06-04 09:11:34,438][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5e048149{/environment,null,AVAILABLE,@Spark}
[INFO][2018-06-04 09:11:34,439][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3d5790ea{/environment/json,null,AVAILABLE,@Spark}
[INFO][2018-06-04 09:11:34,440][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@67a3bd51{/executors,null,AVAILABLE,@Spark}
[INFO][2018-06-04 09:11:34,441][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@56913163{/executors/json,null,AVAILABLE,@Spark}
[INFO][2018-06-04 09:11:34,441][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@a18649a{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2018-06-04 09:11:34,442][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@396639b{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2018-06-04 09:11:34,455][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@62573c86{/static,null,AVAILABLE,@Spark}
[INFO][2018-06-04 09:11:34,456][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@8f2098e{/,null,AVAILABLE,@Spark}
[INFO][2018-06-04 09:11:34,457][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@55a88417{/api,null,AVAILABLE,@Spark}
[INFO][2018-06-04 09:11:34,458][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@481e91b6{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2018-06-04 09:11:34,459][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@140d1230{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2018-06-04 09:11:34,461][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://10.194.32.157:4040
[INFO][2018-06-04 09:11:34,556][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
[INFO][2018-06-04 09:11:34,584][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 50266.
[INFO][2018-06-04 09:11:34,585][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 10.194.32.157:50266
[INFO][2018-06-04 09:11:34,587][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2018-06-04 09:11:34,588][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 10.194.32.157, 50266, None)
[INFO][2018-06-04 09:11:34,592][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 10.194.32.157:50266 with 912.3 MB RAM, BlockManagerId(driver, 10.194.32.157, 50266, None)
[INFO][2018-06-04 09:11:34,598][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 10.194.32.157, 50266, None)
[INFO][2018-06-04 09:11:34,599][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 10.194.32.157, 50266, None)
[INFO][2018-06-04 09:11:34,834][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7a3e5cd3{/metrics/json,null,AVAILABLE,@Spark}
[INFO][2018-06-04 09:11:34,878][root]jobStarttime:2018-06-04 09:11:34
[INFO][2018-06-04 17:20:01,233][org.apache.spark.SparkContext]Running Spark version 2.2.0
[INFO][2018-06-04 17:20:02,188][org.apache.spark.SparkContext]Submitted application: tmp
[INFO][2018-06-04 17:20:02,237][org.apache.spark.SecurityManager]Changing view acls to: seven
[INFO][2018-06-04 17:20:02,238][org.apache.spark.SecurityManager]Changing modify acls to: seven
[INFO][2018-06-04 17:20:02,240][org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO][2018-06-04 17:20:02,240][org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO][2018-06-04 17:20:02,241][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(seven); groups with view permissions: Set(); users  with modify permissions: Set(seven); groups with modify permissions: Set()
[INFO][2018-06-04 17:20:02,745][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 62024.
[INFO][2018-06-04 17:20:02,789][org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO][2018-06-04 17:20:02,830][org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO][2018-06-04 17:20:02,837][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2018-06-04 17:20:02,838][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
[INFO][2018-06-04 17:20:02,853][org.apache.spark.storage.DiskBlockManager]Created local directory at /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/blockmgr-327b2685-177d-4a6a-8d4d-46a0bd539644
[INFO][2018-06-04 17:20:02,931][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 912.3 MB
[INFO][2018-06-04 17:20:03,039][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO][2018-06-04 17:20:03,160][org.spark_project.jetty.util.log]Logging initialized @3747ms
[INFO][2018-06-04 17:20:03,256][org.spark_project.jetty.server.Server]jetty-9.3.z-SNAPSHOT
[INFO][2018-06-04 17:20:03,283][org.spark_project.jetty.server.Server]Started @3870ms
[INFO][2018-06-04 17:20:03,341][org.spark_project.jetty.server.AbstractConnector]Started ServerConnector@1ab8a7ad{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-06-04 17:20:03,342][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4040.
[INFO][2018-06-04 17:20:03,428][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@408e96d9{/jobs,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:20:03,431][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4e1ce44{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:20:03,433][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7a7cc52c{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:20:03,436][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@332820f4{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:20:03,439][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@72456279{/stages,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:20:03,442][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@21f459fc{/stages/json,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:20:03,443][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1416cf9f{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:20:03,453][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6a9b9909{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:20:03,455][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@a518813{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:20:03,456][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@75361cf6{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:20:03,468][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6ba7383d{/storage,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:20:03,470][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@710d89e2{/storage/json,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:20:03,470][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4fc142ec{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:20:03,472][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@29eda4f8{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:20:03,473][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5e048149{/environment,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:20:03,474][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3d5790ea{/environment/json,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:20:03,475][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@67a3bd51{/executors,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:20:03,475][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@56913163{/executors/json,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:20:03,476][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@a18649a{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:20:03,477][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@396639b{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:20:03,488][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@62573c86{/static,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:20:03,489][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@8f2098e{/,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:20:03,490][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@55a88417{/api,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:20:03,490][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@481e91b6{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:20:03,491][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@140d1230{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:20:03,494][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://10.194.32.157:4040
[INFO][2018-06-04 17:20:03,654][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
[INFO][2018-06-04 17:20:03,701][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 62025.
[INFO][2018-06-04 17:20:03,701][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 10.194.32.157:62025
[INFO][2018-06-04 17:20:03,704][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2018-06-04 17:20:03,711][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 10.194.32.157, 62025, None)
[INFO][2018-06-04 17:20:03,716][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 10.194.32.157:62025 with 912.3 MB RAM, BlockManagerId(driver, 10.194.32.157, 62025, None)
[INFO][2018-06-04 17:20:03,719][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 10.194.32.157, 62025, None)
[INFO][2018-06-04 17:20:03,720][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 10.194.32.157, 62025, None)
[INFO][2018-06-04 17:20:04,153][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7a3e5cd3{/metrics/json,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:20:04,235][root]jobStarttime:2018-06-04 17:20:04
[INFO][2018-06-04 17:20:29,711][root]jobEndtime:2018-06-04 17:20:29;
[INFO][2018-06-04 17:20:29,712][root]jobResult:Sucessfull!
[INFO][2018-06-04 17:20:38,890][org.apache.spark.SparkContext]Running Spark version 2.2.0
[INFO][2018-06-04 17:20:39,780][org.apache.spark.SparkContext]Submitted application: tmp
[INFO][2018-06-04 17:20:39,822][org.apache.spark.SecurityManager]Changing view acls to: seven
[INFO][2018-06-04 17:20:39,823][org.apache.spark.SecurityManager]Changing modify acls to: seven
[INFO][2018-06-04 17:20:39,825][org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO][2018-06-04 17:20:39,826][org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO][2018-06-04 17:20:39,827][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(seven); groups with view permissions: Set(); users  with modify permissions: Set(seven); groups with modify permissions: Set()
[INFO][2018-06-04 17:20:40,135][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 62035.
[INFO][2018-06-04 17:20:40,156][org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO][2018-06-04 17:20:40,174][org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO][2018-06-04 17:20:40,177][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2018-06-04 17:20:40,178][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
[INFO][2018-06-04 17:20:40,186][org.apache.spark.storage.DiskBlockManager]Created local directory at /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/blockmgr-83e5daac-405c-42d4-b1d5-cd2f6c62073e
[INFO][2018-06-04 17:20:40,202][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 912.3 MB
[INFO][2018-06-04 17:20:40,273][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO][2018-06-04 17:20:40,352][org.spark_project.jetty.util.log]Logging initialized @2348ms
[INFO][2018-06-04 17:20:40,411][org.spark_project.jetty.server.Server]jetty-9.3.z-SNAPSHOT
[INFO][2018-06-04 17:20:40,427][org.spark_project.jetty.server.Server]Started @2425ms
[INFO][2018-06-04 17:20:40,449][org.spark_project.jetty.server.AbstractConnector]Started ServerConnector@3c635cca{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-06-04 17:20:40,450][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4040.
[INFO][2018-06-04 17:20:40,474][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@60d8c0dc{/jobs,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:20:40,475][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@33352f32{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:20:40,475][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1e044120{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:20:40,476][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@35fe2125{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:20:40,477][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@34645867{/stages,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:20:40,477][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@60b71e8f{/stages/json,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:20:40,478][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@464649c{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:20:40,480][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@47da3952{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:20:40,480][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@46e8a539{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:20:40,481][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5fd62371{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:20:40,482][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2b62442c{/storage,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:20:40,482][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@841e575{/storage/json,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:20:40,483][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1e5f4170{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:20:40,483][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6b5966e1{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:20:40,484][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1568159{/environment,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:20:40,485][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6f80fafe{/environment/json,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:20:40,485][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@f9879ac{/executors,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:20:40,487][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5f4d427e{/executors/json,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:20:40,488][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@224b4d61{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:20:40,489][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@303e3593{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:20:40,512][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@362a019c{/static,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:20:40,522][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6fff253c{/,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:20:40,523][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@591e58fa{/api,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:20:40,524][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@64bc21ac{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:20:40,528][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5d25e6bb{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:20:40,532][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://10.194.32.157:4040
[INFO][2018-06-04 17:20:40,696][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
[INFO][2018-06-04 17:20:40,727][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 62036.
[INFO][2018-06-04 17:20:40,732][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 10.194.32.157:62036
[INFO][2018-06-04 17:20:40,738][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2018-06-04 17:20:40,747][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 10.194.32.157, 62036, None)
[INFO][2018-06-04 17:20:40,753][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 10.194.32.157:62036 with 912.3 MB RAM, BlockManagerId(driver, 10.194.32.157, 62036, None)
[INFO][2018-06-04 17:20:40,764][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 10.194.32.157, 62036, None)
[INFO][2018-06-04 17:20:40,764][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 10.194.32.157, 62036, None)
[INFO][2018-06-04 17:20:41,122][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2617f816{/metrics/json,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:20:41,140][root]jobStarttime:2018-06-04 17:20:41
[INFO][2018-06-04 17:20:55,524][root]jobEndtime:2018-06-04 17:20:55;
[INFO][2018-06-04 17:20:55,526][root]jobResult:Sucessfull!
[INFO][2018-06-04 17:23:49,633][org.apache.spark.SparkContext]Running Spark version 2.2.0
[INFO][2018-06-04 17:23:50,795][org.apache.spark.SparkContext]Submitted application: tmp
[INFO][2018-06-04 17:23:50,827][org.apache.spark.SecurityManager]Changing view acls to: seven
[INFO][2018-06-04 17:23:50,828][org.apache.spark.SecurityManager]Changing modify acls to: seven
[INFO][2018-06-04 17:23:50,829][org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO][2018-06-04 17:23:50,830][org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO][2018-06-04 17:23:50,830][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(seven); groups with view permissions: Set(); users  with modify permissions: Set(seven); groups with modify permissions: Set()
[INFO][2018-06-04 17:23:51,211][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 62071.
[INFO][2018-06-04 17:23:51,229][org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO][2018-06-04 17:23:51,246][org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO][2018-06-04 17:23:51,279][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2018-06-04 17:23:51,280][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
[INFO][2018-06-04 17:23:51,291][org.apache.spark.storage.DiskBlockManager]Created local directory at /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/blockmgr-4ec9e899-86e6-4f3c-b9f9-9276402b7cda
[INFO][2018-06-04 17:23:51,308][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 912.3 MB
[INFO][2018-06-04 17:23:51,381][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO][2018-06-04 17:23:51,471][org.spark_project.jetty.util.log]Logging initialized @2815ms
[INFO][2018-06-04 17:23:51,523][org.spark_project.jetty.server.Server]jetty-9.3.z-SNAPSHOT
[INFO][2018-06-04 17:23:51,535][org.spark_project.jetty.server.Server]Started @2880ms
[INFO][2018-06-04 17:23:51,552][org.spark_project.jetty.server.AbstractConnector]Started ServerConnector@66fac4cb{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-06-04 17:23:51,552][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4040.
[INFO][2018-06-04 17:23:51,571][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4204541c{/jobs,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:23:51,571][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5f3b9c57{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:23:51,572][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2cf23c81{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:23:51,573][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@94f6bfb{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:23:51,573][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2484f433{/stages,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:23:51,574][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1255b1d1{/stages/json,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:23:51,574][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7c22d4f{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:23:51,576][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@51e4ccb3{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:23:51,576][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@495083a0{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:23:51,577][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@28a0fd6c{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:23:51,577][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@66629f63{/storage,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:23:51,577][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@27a5328c{/storage/json,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:23:51,578][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6c345c5f{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:23:51,578][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@65e61854{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:23:51,579][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4fcee388{/environment,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:23:51,580][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3af17be2{/environment/json,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:23:51,581][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@37f21974{/executors,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:23:51,581][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6e521c1e{/executors/json,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:23:51,581][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5d5d9e5{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:23:51,582][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4ef27d66{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:23:51,592][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1d9bec4d{/static,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:23:51,593][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6c6357f9{/,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:23:51,594][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3954d008{/api,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:23:51,599][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@493dfb8e{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:23:51,601][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@ce5a68e{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:23:51,604][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://10.194.32.157:4040
[INFO][2018-06-04 17:23:51,677][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
[INFO][2018-06-04 17:23:51,695][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 62072.
[INFO][2018-06-04 17:23:51,697][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 10.194.32.157:62072
[INFO][2018-06-04 17:23:51,702][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2018-06-04 17:23:51,704][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 10.194.32.157, 62072, None)
[INFO][2018-06-04 17:23:51,707][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 10.194.32.157:62072 with 912.3 MB RAM, BlockManagerId(driver, 10.194.32.157, 62072, None)
[INFO][2018-06-04 17:23:51,709][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 10.194.32.157, 62072, None)
[INFO][2018-06-04 17:23:51,710][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 10.194.32.157, 62072, None)
[INFO][2018-06-04 17:23:51,966][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@676f0a60{/metrics/json,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:23:51,993][root]jobStarttime:2018-06-04 17:23:51
[INFO][2018-06-04 17:24:05,363][root]jobEndtime:2018-06-04 17:24:05;
[INFO][2018-06-04 17:24:05,363][root]jobResult:Sucessfull!
[INFO][2018-06-04 17:25:45,593][org.apache.spark.SparkContext]Running Spark version 2.2.0
[INFO][2018-06-04 17:25:46,667][org.apache.spark.SparkContext]Submitted application: tmp
[INFO][2018-06-04 17:25:46,686][org.apache.spark.SecurityManager]Changing view acls to: seven
[INFO][2018-06-04 17:25:46,687][org.apache.spark.SecurityManager]Changing modify acls to: seven
[INFO][2018-06-04 17:25:46,687][org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO][2018-06-04 17:25:46,688][org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO][2018-06-04 17:25:46,688][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(seven); groups with view permissions: Set(); users  with modify permissions: Set(seven); groups with modify permissions: Set()
[INFO][2018-06-04 17:25:47,015][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 62096.
[INFO][2018-06-04 17:25:47,033][org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO][2018-06-04 17:25:47,052][org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO][2018-06-04 17:25:47,055][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2018-06-04 17:25:47,055][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
[INFO][2018-06-04 17:25:47,065][org.apache.spark.storage.DiskBlockManager]Created local directory at /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/blockmgr-40970ec7-f99f-4a38-b003-436a31ef32d8
[INFO][2018-06-04 17:25:47,084][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 912.3 MB
[INFO][2018-06-04 17:25:47,168][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO][2018-06-04 17:25:47,278][org.spark_project.jetty.util.log]Logging initialized @2754ms
[INFO][2018-06-04 17:25:47,334][org.spark_project.jetty.server.Server]jetty-9.3.z-SNAPSHOT
[INFO][2018-06-04 17:25:47,347][org.spark_project.jetty.server.Server]Started @2824ms
[INFO][2018-06-04 17:25:47,365][org.spark_project.jetty.server.AbstractConnector]Started ServerConnector@3c635cca{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-06-04 17:25:47,366][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4040.
[INFO][2018-06-04 17:25:47,389][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4602c2a9{/jobs,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:25:47,390][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2cf23c81{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:25:47,391][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@35fe2125{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:25:47,392][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2484f433{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:25:47,392][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1255b1d1{/stages,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:25:47,393][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7c22d4f{/stages/json,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:25:47,393][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@60bdf15d{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:25:47,394][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@495083a0{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:25:47,395][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@28a0fd6c{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:25:47,396][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@66629f63{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:25:47,397][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@27a5328c{/storage,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:25:47,397][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6c345c5f{/storage/json,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:25:47,398][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@65e61854{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:25:47,398][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4fcee388{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:25:47,401][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3af17be2{/environment,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:25:47,402][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@37f21974{/environment/json,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:25:47,403][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6e521c1e{/executors,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:25:47,403][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5d5d9e5{/executors/json,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:25:47,404][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4ef27d66{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:25:47,405][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1d9bec4d{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:25:47,416][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@10c8f62{/static,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:25:47,417][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3954d008{/,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:25:47,419][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@593e824f{/api,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:25:47,420][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@ce5a68e{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:25:47,420][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2f162cc0{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:25:47,423][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://10.194.32.157:4040
[INFO][2018-06-04 17:25:47,543][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
[INFO][2018-06-04 17:25:47,564][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 62097.
[INFO][2018-06-04 17:25:47,568][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 10.194.32.157:62097
[INFO][2018-06-04 17:25:47,573][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2018-06-04 17:25:47,579][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 10.194.32.157, 62097, None)
[INFO][2018-06-04 17:25:47,582][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 10.194.32.157:62097 with 912.3 MB RAM, BlockManagerId(driver, 10.194.32.157, 62097, None)
[INFO][2018-06-04 17:25:47,587][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 10.194.32.157, 62097, None)
[INFO][2018-06-04 17:25:47,587][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 10.194.32.157, 62097, None)
[INFO][2018-06-04 17:25:47,850][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@535b8c24{/metrics/json,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:25:47,877][root]jobStarttime:2018-06-04 17:25:47
[INFO][2018-06-04 17:25:58,605][root]jobEndtime:2018-06-04 17:25:58;
[INFO][2018-06-04 17:25:58,607][root]jobResult:Sucessfull!
[INFO][2018-06-04 17:26:28,857][org.apache.spark.SparkContext]Running Spark version 2.2.0
[INFO][2018-06-04 17:26:29,711][org.apache.spark.SparkContext]Submitted application: tmp
[INFO][2018-06-04 17:26:29,741][org.apache.spark.SecurityManager]Changing view acls to: seven
[INFO][2018-06-04 17:26:29,741][org.apache.spark.SecurityManager]Changing modify acls to: seven
[INFO][2018-06-04 17:26:29,742][org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO][2018-06-04 17:26:29,742][org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO][2018-06-04 17:26:29,743][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(seven); groups with view permissions: Set(); users  with modify permissions: Set(seven); groups with modify permissions: Set()
[INFO][2018-06-04 17:26:30,262][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 62119.
[INFO][2018-06-04 17:26:30,282][org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO][2018-06-04 17:26:30,303][org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO][2018-06-04 17:26:30,306][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2018-06-04 17:26:30,306][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
[INFO][2018-06-04 17:26:30,318][org.apache.spark.storage.DiskBlockManager]Created local directory at /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/blockmgr-44189258-9b44-4937-b318-089a6df1963c
[INFO][2018-06-04 17:26:30,334][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 912.3 MB
[INFO][2018-06-04 17:26:30,411][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO][2018-06-04 17:26:30,546][org.spark_project.jetty.util.log]Logging initialized @2924ms
[INFO][2018-06-04 17:26:30,622][org.spark_project.jetty.server.Server]jetty-9.3.z-SNAPSHOT
[INFO][2018-06-04 17:26:30,638][org.spark_project.jetty.server.Server]Started @3017ms
[INFO][2018-06-04 17:26:30,659][org.spark_project.jetty.server.AbstractConnector]Started ServerConnector@4e76dac{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-06-04 17:26:30,659][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4040.
[INFO][2018-06-04 17:26:30,691][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4204541c{/jobs,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:26:30,692][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5f3b9c57{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:26:30,693][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2cf23c81{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:26:30,694][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@94f6bfb{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:26:30,695][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2484f433{/stages,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:26:30,696][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1255b1d1{/stages/json,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:26:30,697][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7c22d4f{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:26:30,705][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@51e4ccb3{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:26:30,706][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@495083a0{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:26:30,707][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@28a0fd6c{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:26:30,708][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@66629f63{/storage,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:26:30,709][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@27a5328c{/storage/json,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:26:30,709][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6c345c5f{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:26:30,710][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@65e61854{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:26:30,711][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4fcee388{/environment,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:26:30,712][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3af17be2{/environment/json,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:26:30,712][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@37f21974{/executors,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:26:30,713][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6e521c1e{/executors/json,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:26:30,714][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5d5d9e5{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:26:30,715][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4ef27d66{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:26:30,725][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1d9bec4d{/static,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:26:30,726][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6c6357f9{/,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:26:30,727][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3954d008{/api,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:26:30,836][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@493dfb8e{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:26:30,837][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@ce5a68e{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:26:30,840][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://10.194.32.157:4040
[INFO][2018-06-04 17:26:31,015][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
[INFO][2018-06-04 17:26:31,043][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 62120.
[INFO][2018-06-04 17:26:31,044][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 10.194.32.157:62120
[INFO][2018-06-04 17:26:31,047][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2018-06-04 17:26:31,049][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 10.194.32.157, 62120, None)
[INFO][2018-06-04 17:26:31,053][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 10.194.32.157:62120 with 912.3 MB RAM, BlockManagerId(driver, 10.194.32.157, 62120, None)
[INFO][2018-06-04 17:26:31,055][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 10.194.32.157, 62120, None)
[INFO][2018-06-04 17:26:31,057][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 10.194.32.157, 62120, None)
[INFO][2018-06-04 17:26:31,324][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@676f0a60{/metrics/json,null,AVAILABLE,@Spark}
[INFO][2018-06-04 17:26:31,347][root]jobStarttime:2018-06-04 17:26:31
[INFO][2018-06-04 17:26:42,380][root]jobEndtime:2018-06-04 17:26:42;
[INFO][2018-06-04 17:26:42,382][root]jobResult:Sucessfull!
[INFO][2018-06-05 09:05:03,122][org.apache.spark.SparkContext]Running Spark version 2.2.0
[INFO][2018-06-05 09:05:04,201][org.apache.spark.SparkContext]Submitted application: tmp
[INFO][2018-06-05 09:05:04,240][org.apache.spark.SecurityManager]Changing view acls to: seven
[INFO][2018-06-05 09:05:04,242][org.apache.spark.SecurityManager]Changing modify acls to: seven
[INFO][2018-06-05 09:05:04,243][org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO][2018-06-05 09:05:04,244][org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO][2018-06-05 09:05:04,244][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(seven); groups with view permissions: Set(); users  with modify permissions: Set(seven); groups with modify permissions: Set()
[INFO][2018-06-05 09:05:04,703][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 62843.
[INFO][2018-06-05 09:05:04,736][org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO][2018-06-05 09:05:04,778][org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO][2018-06-05 09:05:04,782][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2018-06-05 09:05:04,783][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
[INFO][2018-06-05 09:05:04,800][org.apache.spark.storage.DiskBlockManager]Created local directory at /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/blockmgr-4ea3603f-34ec-4cdb-b2d0-2db3d31231a7
[INFO][2018-06-05 09:05:04,841][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 912.3 MB
[INFO][2018-06-05 09:05:04,969][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO][2018-06-05 09:05:05,141][org.spark_project.jetty.util.log]Logging initialized @3192ms
[INFO][2018-06-05 09:05:05,240][org.spark_project.jetty.server.Server]jetty-9.3.z-SNAPSHOT
[INFO][2018-06-05 09:05:05,260][org.spark_project.jetty.server.Server]Started @3313ms
[INFO][2018-06-05 09:05:05,290][org.spark_project.jetty.server.AbstractConnector]Started ServerConnector@577e8015{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-06-05 09:05:05,291][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4040.
[INFO][2018-06-05 09:05:05,336][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@60fa3495{/jobs,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:05:05,337][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3624da92{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:05:05,338][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@94f6bfb{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:05:05,339][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@60b71e8f{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:05:05,341][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@464649c{/stages,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:05:05,342][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5f59185e{/stages/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:05:05,342][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@47da3952{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:05:05,345][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5fd62371{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:05:05,346][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2b62442c{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:05:05,347][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@841e575{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:05:05,348][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1e5f4170{/storage,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:05:05,349][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6b5966e1{/storage/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:05:05,350][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1568159{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:05:05,351][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6f80fafe{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:05:05,352][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@f9879ac{/environment,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:05:05,352][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5f4d427e{/environment/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:05:05,353][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@224b4d61{/executors,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:05:05,354][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@303e3593{/executors/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:05:05,355][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@362a019c{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:05:05,356][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5c48c0c0{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:05:05,367][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@674c583e{/static,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:05:05,368][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2f94c4db{/,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:05:05,374][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@72ccd81a{/api,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:05:05,375][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@9d157ff{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:05:05,376][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5df417a7{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:05:05,381][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://10.194.32.157:4040
[INFO][2018-06-05 09:05:05,549][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
[INFO][2018-06-05 09:05:05,582][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 62844.
[INFO][2018-06-05 09:05:05,583][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 10.194.32.157:62844
[INFO][2018-06-05 09:05:05,585][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2018-06-05 09:05:05,591][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 10.194.32.157, 62844, None)
[INFO][2018-06-05 09:05:05,595][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 10.194.32.157:62844 with 912.3 MB RAM, BlockManagerId(driver, 10.194.32.157, 62844, None)
[INFO][2018-06-05 09:05:05,599][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 10.194.32.157, 62844, None)
[INFO][2018-06-05 09:05:05,600][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 10.194.32.157, 62844, None)
[INFO][2018-06-05 09:05:05,865][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4a951911{/metrics/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:05:05,901][root]jobStarttime:2018-06-05 09:05:05
[INFO][2018-06-05 09:05:17,165][root]jobEndtime:2018-06-05 09:05:17;
[INFO][2018-06-05 09:05:17,166][root]jobResult:Sucessfull!
[INFO][2018-06-05 09:05:29,228][org.apache.spark.SparkContext]Running Spark version 2.2.0
[INFO][2018-06-05 09:05:29,929][org.apache.spark.SparkContext]Submitted application: tmp
[INFO][2018-06-05 09:05:29,951][org.apache.spark.SecurityManager]Changing view acls to: seven
[INFO][2018-06-05 09:05:29,952][org.apache.spark.SecurityManager]Changing modify acls to: seven
[INFO][2018-06-05 09:05:29,953][org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO][2018-06-05 09:05:29,953][org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO][2018-06-05 09:05:29,954][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(seven); groups with view permissions: Set(); users  with modify permissions: Set(seven); groups with modify permissions: Set()
[INFO][2018-06-05 09:05:30,255][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 62860.
[INFO][2018-06-05 09:05:30,272][org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO][2018-06-05 09:05:30,287][org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO][2018-06-05 09:05:30,290][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2018-06-05 09:05:30,291][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
[INFO][2018-06-05 09:05:30,301][org.apache.spark.storage.DiskBlockManager]Created local directory at /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/blockmgr-4d70f99e-9292-429a-b7ed-30037a46d393
[INFO][2018-06-05 09:05:30,318][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 912.3 MB
[INFO][2018-06-05 09:05:30,423][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO][2018-06-05 09:05:30,525][org.spark_project.jetty.util.log]Logging initialized @2107ms
[INFO][2018-06-05 09:05:30,578][org.spark_project.jetty.server.Server]jetty-9.3.z-SNAPSHOT
[INFO][2018-06-05 09:05:30,590][org.spark_project.jetty.server.Server]Started @2174ms
[INFO][2018-06-05 09:05:30,607][org.spark_project.jetty.server.AbstractConnector]Started ServerConnector@40a8ed42{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-06-05 09:05:30,607][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4040.
[INFO][2018-06-05 09:05:30,629][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3e2822{/jobs,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:05:30,630][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@35fe2125{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:05:30,630][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@34645867{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:05:30,631][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1255b1d1{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:05:30,632][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7c22d4f{/stages,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:05:30,632][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@60bdf15d{/stages/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:05:30,633][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@51e4ccb3{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:05:30,635][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@28a0fd6c{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:05:30,636][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@66629f63{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:05:30,636][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@27a5328c{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:05:30,637][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6c345c5f{/storage,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:05:30,637][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@65e61854{/storage/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:05:30,638][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4fcee388{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:05:30,638][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3af17be2{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:05:30,639][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@37f21974{/environment,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:05:30,639][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6e521c1e{/environment/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:05:30,640][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5d5d9e5{/executors,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:05:30,641][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4ef27d66{/executors/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:05:30,642][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1d9bec4d{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:05:30,643][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@10c8f62{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:05:30,651][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@25f7391e{/static,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:05:30,652][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@593e824f{/,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:05:30,653][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6d8792db{/api,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:05:30,659][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2f162cc0{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:05:30,662][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7c041b41{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:05:30,664][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://10.194.32.157:4040
[INFO][2018-06-05 09:05:30,780][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
[INFO][2018-06-05 09:05:30,837][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 62861.
[INFO][2018-06-05 09:05:30,838][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 10.194.32.157:62861
[INFO][2018-06-05 09:05:30,840][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2018-06-05 09:05:30,841][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 10.194.32.157, 62861, None)
[INFO][2018-06-05 09:05:30,845][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 10.194.32.157:62861 with 912.3 MB RAM, BlockManagerId(driver, 10.194.32.157, 62861, None)
[INFO][2018-06-05 09:05:30,858][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 10.194.32.157, 62861, None)
[INFO][2018-06-05 09:05:30,859][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 10.194.32.157, 62861, None)
[INFO][2018-06-05 09:05:31,061][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4010d494{/metrics/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:05:31,096][root]jobStarttime:2018-06-05 09:05:31
[INFO][2018-06-05 09:05:41,278][root]jobEndtime:2018-06-05 09:05:41;
[INFO][2018-06-05 09:05:41,278][root]jobResult:Sucessfull!
[INFO][2018-06-05 09:26:15,442][org.apache.spark.SparkContext]Running Spark version 2.2.0
[INFO][2018-06-05 09:26:16,285][org.apache.spark.SparkContext]Submitted application: tmp
[INFO][2018-06-05 09:26:16,304][org.apache.spark.SecurityManager]Changing view acls to: seven
[INFO][2018-06-05 09:26:16,304][org.apache.spark.SecurityManager]Changing modify acls to: seven
[INFO][2018-06-05 09:26:16,305][org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO][2018-06-05 09:26:16,305][org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO][2018-06-05 09:26:16,306][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(seven); groups with view permissions: Set(); users  with modify permissions: Set(seven); groups with modify permissions: Set()
[INFO][2018-06-05 09:26:16,622][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 63056.
[INFO][2018-06-05 09:26:16,640][org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO][2018-06-05 09:26:16,660][org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO][2018-06-05 09:26:16,662][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2018-06-05 09:26:16,663][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
[INFO][2018-06-05 09:26:16,671][org.apache.spark.storage.DiskBlockManager]Created local directory at /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/blockmgr-d283eae3-f062-469c-9d6d-4ee21a21d384
[INFO][2018-06-05 09:26:16,691][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 912.3 MB
[INFO][2018-06-05 09:26:16,776][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO][2018-06-05 09:26:16,867][org.spark_project.jetty.util.log]Logging initialized @2351ms
[INFO][2018-06-05 09:26:16,921][org.spark_project.jetty.server.Server]jetty-9.3.z-SNAPSHOT
[INFO][2018-06-05 09:26:16,933][org.spark_project.jetty.server.Server]Started @2418ms
[INFO][2018-06-05 09:26:16,950][org.spark_project.jetty.server.AbstractConnector]Started ServerConnector@16820d85{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-06-05 09:26:16,950][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4040.
[INFO][2018-06-05 09:26:16,971][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4602c2a9{/jobs,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:26:16,972][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2cf23c81{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:26:16,972][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@35fe2125{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:26:16,973][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2484f433{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:26:16,974][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1255b1d1{/stages,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:26:16,974][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7c22d4f{/stages/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:26:16,975][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@60bdf15d{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:26:16,976][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@495083a0{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:26:16,977][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@28a0fd6c{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:26:16,977][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@66629f63{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:26:16,978][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@27a5328c{/storage,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:26:16,979][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6c345c5f{/storage/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:26:16,980][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@65e61854{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:26:16,980][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4fcee388{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:26:16,981][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3af17be2{/environment,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:26:16,982][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@37f21974{/environment/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:26:16,983][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6e521c1e{/executors,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:26:16,983][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5d5d9e5{/executors/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:26:16,984][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4ef27d66{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:26:16,984][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1d9bec4d{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:26:16,995][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@10c8f62{/static,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:26:16,996][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3954d008{/,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:26:16,997][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@593e824f{/api,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:26:16,998][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@ce5a68e{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:26:16,999][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2f162cc0{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:26:17,001][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://10.194.32.157:4040
[INFO][2018-06-05 09:26:17,093][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
[INFO][2018-06-05 09:26:17,113][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 63057.
[INFO][2018-06-05 09:26:17,113][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 10.194.32.157:63057
[INFO][2018-06-05 09:26:17,115][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2018-06-05 09:26:17,116][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 10.194.32.157, 63057, None)
[INFO][2018-06-05 09:26:17,120][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 10.194.32.157:63057 with 912.3 MB RAM, BlockManagerId(driver, 10.194.32.157, 63057, None)
[INFO][2018-06-05 09:26:17,126][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 10.194.32.157, 63057, None)
[INFO][2018-06-05 09:26:17,127][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 10.194.32.157, 63057, None)
[INFO][2018-06-05 09:26:17,346][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@535b8c24{/metrics/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:26:17,373][root]jobStarttime:2018-06-05 09:26:17
[INFO][2018-06-05 09:26:30,726][root]jobEndtime:2018-06-05 09:26:30;
[INFO][2018-06-05 09:26:30,728][root]jobResult:Sucessfull!
[INFO][2018-06-05 09:31:49,349][org.apache.spark.SparkContext]Running Spark version 2.2.0
[INFO][2018-06-05 09:31:50,197][org.apache.spark.SparkContext]Submitted application: tmp
[INFO][2018-06-05 09:31:50,220][org.apache.spark.SecurityManager]Changing view acls to: seven
[INFO][2018-06-05 09:31:50,221][org.apache.spark.SecurityManager]Changing modify acls to: seven
[INFO][2018-06-05 09:31:50,221][org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO][2018-06-05 09:31:50,222][org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO][2018-06-05 09:31:50,223][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(seven); groups with view permissions: Set(); users  with modify permissions: Set(seven); groups with modify permissions: Set()
[INFO][2018-06-05 09:31:50,536][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 63161.
[INFO][2018-06-05 09:31:50,556][org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO][2018-06-05 09:31:50,574][org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO][2018-06-05 09:31:50,577][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2018-06-05 09:31:50,578][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
[INFO][2018-06-05 09:31:50,587][org.apache.spark.storage.DiskBlockManager]Created local directory at /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/blockmgr-752cf667-e0f3-4ea9-867e-2e412bebb1b7
[INFO][2018-06-05 09:31:50,607][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 912.3 MB
[INFO][2018-06-05 09:31:50,689][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO][2018-06-05 09:31:50,780][org.spark_project.jetty.util.log]Logging initialized @2360ms
[INFO][2018-06-05 09:31:50,834][org.spark_project.jetty.server.Server]jetty-9.3.z-SNAPSHOT
[INFO][2018-06-05 09:31:50,847][org.spark_project.jetty.server.Server]Started @2428ms
[INFO][2018-06-05 09:31:50,872][org.spark_project.jetty.server.AbstractConnector]Started ServerConnector@5d465e4b{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-06-05 09:31:50,873][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4040.
[INFO][2018-06-05 09:31:50,896][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3e2822{/jobs,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:31:50,897][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@35fe2125{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:31:50,897][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@34645867{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:31:50,898][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1255b1d1{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:31:50,899][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7c22d4f{/stages,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:31:50,899][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@60bdf15d{/stages/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:31:50,900][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@51e4ccb3{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:31:50,901][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@28a0fd6c{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:31:50,902][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@66629f63{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:31:50,902][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@27a5328c{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:31:50,903][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6c345c5f{/storage,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:31:50,904][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@65e61854{/storage/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:31:50,904][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4fcee388{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:31:50,905][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3af17be2{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:31:50,906][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@37f21974{/environment,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:31:50,907][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6e521c1e{/environment/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:31:50,908][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5d5d9e5{/executors,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:31:50,908][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4ef27d66{/executors/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:31:50,909][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1d9bec4d{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:31:50,910][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@10c8f62{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:31:50,916][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@25f7391e{/static,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:31:50,917][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@593e824f{/,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:31:50,922][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6d8792db{/api,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:31:50,923][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2f162cc0{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:31:50,924][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7c041b41{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:31:50,926][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://10.194.32.157:4040
[INFO][2018-06-05 09:31:51,021][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
[INFO][2018-06-05 09:31:51,046][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 63162.
[INFO][2018-06-05 09:31:51,047][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 10.194.32.157:63162
[INFO][2018-06-05 09:31:51,049][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2018-06-05 09:31:51,050][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 10.194.32.157, 63162, None)
[INFO][2018-06-05 09:31:51,054][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 10.194.32.157:63162 with 912.3 MB RAM, BlockManagerId(driver, 10.194.32.157, 63162, None)
[INFO][2018-06-05 09:31:51,058][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 10.194.32.157, 63162, None)
[INFO][2018-06-05 09:31:51,059][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 10.194.32.157, 63162, None)
[INFO][2018-06-05 09:31:51,345][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@55b62629{/metrics/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:31:51,376][root]jobStarttime:2018-06-05 09:31:51
[INFO][2018-06-05 09:32:15,012][root]jobEndtime:2018-06-05 09:32:15;
[INFO][2018-06-05 09:32:15,012][root]jobResult:Sucessfull!
[INFO][2018-06-05 09:53:24,200][org.apache.spark.SparkContext]Running Spark version 2.2.0
[INFO][2018-06-05 09:53:25,106][org.apache.spark.SparkContext]Submitted application: NetBaseInfo
[INFO][2018-06-05 09:53:25,126][org.apache.spark.SecurityManager]Changing view acls to: seven
[INFO][2018-06-05 09:53:25,127][org.apache.spark.SecurityManager]Changing modify acls to: seven
[INFO][2018-06-05 09:53:25,128][org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO][2018-06-05 09:53:25,128][org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO][2018-06-05 09:53:25,129][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(seven); groups with view permissions: Set(); users  with modify permissions: Set(seven); groups with modify permissions: Set()
[INFO][2018-06-05 09:53:25,405][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 63365.
[INFO][2018-06-05 09:53:25,421][org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO][2018-06-05 09:53:25,436][org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO][2018-06-05 09:53:25,439][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2018-06-05 09:53:25,440][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
[INFO][2018-06-05 09:53:25,449][org.apache.spark.storage.DiskBlockManager]Created local directory at /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/blockmgr-6b180f04-84b8-4bc2-adbf-a860dde7f1f0
[INFO][2018-06-05 09:53:25,471][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 912.3 MB
[INFO][2018-06-05 09:53:25,543][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO][2018-06-05 09:53:25,619][org.spark_project.jetty.util.log]Logging initialized @2290ms
[INFO][2018-06-05 09:53:25,680][org.spark_project.jetty.server.Server]jetty-9.3.z-SNAPSHOT
[INFO][2018-06-05 09:53:25,691][org.spark_project.jetty.server.Server]Started @2363ms
[INFO][2018-06-05 09:53:25,707][org.spark_project.jetty.server.AbstractConnector]Started ServerConnector@5f2f577{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-06-05 09:53:25,707][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4040.
[INFO][2018-06-05 09:53:25,728][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4602c2a9{/jobs,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:53:25,728][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2cf23c81{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:53:25,729][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@35fe2125{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:53:25,730][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2484f433{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:53:25,731][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1255b1d1{/stages,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:53:25,731][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7c22d4f{/stages/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:53:25,732][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@60bdf15d{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:53:25,733][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@495083a0{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:53:25,734][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@28a0fd6c{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:53:25,735][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@66629f63{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:53:25,735][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@27a5328c{/storage,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:53:25,736][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6c345c5f{/storage/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:53:25,736][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@65e61854{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:53:25,737][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4fcee388{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:53:25,737][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3af17be2{/environment,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:53:25,738][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@37f21974{/environment/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:53:25,739][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6e521c1e{/executors,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:53:25,739][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5d5d9e5{/executors/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:53:25,740][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4ef27d66{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:53:25,741][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1d9bec4d{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:53:25,749][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@10c8f62{/static,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:53:25,750][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3954d008{/,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:53:25,756][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@593e824f{/api,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:53:25,757][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@ce5a68e{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:53:25,757][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2f162cc0{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:53:25,759][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://10.194.32.157:4040
[INFO][2018-06-05 09:53:25,838][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
[INFO][2018-06-05 09:53:25,858][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 63366.
[INFO][2018-06-05 09:53:25,859][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 10.194.32.157:63366
[INFO][2018-06-05 09:53:25,861][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2018-06-05 09:53:25,863][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 10.194.32.157, 63366, None)
[INFO][2018-06-05 09:53:25,869][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 10.194.32.157:63366 with 912.3 MB RAM, BlockManagerId(driver, 10.194.32.157, 63366, None)
[INFO][2018-06-05 09:53:25,875][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 10.194.32.157, 63366, None)
[INFO][2018-06-05 09:53:25,876][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 10.194.32.157, 63366, None)
[INFO][2018-06-05 09:53:26,082][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@535b8c24{/metrics/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:53:26,120][root]jobStarttime:2018-06-05 09:53:26
[INFO][2018-06-05 09:53:39,130][root]jobEndtime:2018-06-05 09:53:39;
[INFO][2018-06-05 09:53:39,130][root]jobResult:Sucessfull!
[INFO][2018-06-05 09:58:33,889][org.apache.spark.SparkContext]Running Spark version 2.2.0
[INFO][2018-06-05 09:58:35,061][org.apache.spark.SparkContext]Submitted application: tmp
[INFO][2018-06-05 09:58:35,096][org.apache.spark.SecurityManager]Changing view acls to: seven
[INFO][2018-06-05 09:58:35,097][org.apache.spark.SecurityManager]Changing modify acls to: seven
[INFO][2018-06-05 09:58:35,098][org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO][2018-06-05 09:58:35,099][org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO][2018-06-05 09:58:35,099][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(seven); groups with view permissions: Set(); users  with modify permissions: Set(seven); groups with modify permissions: Set()
[INFO][2018-06-05 09:58:35,515][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 63396.
[INFO][2018-06-05 09:58:35,538][org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO][2018-06-05 09:58:35,568][org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO][2018-06-05 09:58:35,572][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2018-06-05 09:58:35,573][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
[INFO][2018-06-05 09:58:35,584][org.apache.spark.storage.DiskBlockManager]Created local directory at /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/blockmgr-5ba2adb8-38da-43ab-8d6b-ee353792e159
[INFO][2018-06-05 09:58:35,616][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 912.3 MB
[INFO][2018-06-05 09:58:35,762][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO][2018-06-05 09:58:35,885][org.spark_project.jetty.util.log]Logging initialized @3613ms
[INFO][2018-06-05 09:58:36,011][org.spark_project.jetty.server.Server]jetty-9.3.z-SNAPSHOT
[INFO][2018-06-05 09:58:36,042][org.spark_project.jetty.server.Server]Started @3771ms
[INFO][2018-06-05 09:58:36,082][org.spark_project.jetty.server.AbstractConnector]Started ServerConnector@220392b{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-06-05 09:58:36,082][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4040.
[INFO][2018-06-05 09:58:36,133][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@60fa3495{/jobs,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:58:36,134][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3624da92{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:58:36,135][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@94f6bfb{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:58:36,136][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@60b71e8f{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:58:36,140][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@464649c{/stages,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:58:36,143][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5f59185e{/stages/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:58:36,145][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@47da3952{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:58:36,147][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5fd62371{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:58:36,150][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2b62442c{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:58:36,154][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@841e575{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:58:36,156][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1e5f4170{/storage,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:58:36,157][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6b5966e1{/storage/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:58:36,158][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1568159{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:58:36,158][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6f80fafe{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:58:36,159][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@f9879ac{/environment,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:58:36,160][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5f4d427e{/environment/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:58:36,161][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@224b4d61{/executors,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:58:36,162][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@303e3593{/executors/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:58:36,163][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@362a019c{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:58:36,165][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5c48c0c0{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:58:36,223][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@674c583e{/static,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:58:36,224][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2f94c4db{/,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:58:36,265][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@72ccd81a{/api,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:58:36,266][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@9d157ff{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:58:36,268][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5df417a7{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:58:36,274][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://10.194.32.157:4040
[INFO][2018-06-05 09:58:36,486][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
[INFO][2018-06-05 09:58:36,537][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 63397.
[INFO][2018-06-05 09:58:36,538][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 10.194.32.157:63397
[INFO][2018-06-05 09:58:36,540][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2018-06-05 09:58:36,546][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 10.194.32.157, 63397, None)
[INFO][2018-06-05 09:58:36,552][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 10.194.32.157:63397 with 912.3 MB RAM, BlockManagerId(driver, 10.194.32.157, 63397, None)
[INFO][2018-06-05 09:58:36,554][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 10.194.32.157, 63397, None)
[INFO][2018-06-05 09:58:36,555][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 10.194.32.157, 63397, None)
[INFO][2018-06-05 09:58:36,954][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4a951911{/metrics/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 09:58:36,985][root]jobStarttime:2018-06-05 09:58:36
[INFO][2018-06-05 10:03:14,630][root]jobEndtime:2018-06-05 10:03:14;
[INFO][2018-06-05 10:03:14,633][root]jobResult:Sucessfull!
[INFO][2018-06-05 10:05:58,462][org.apache.spark.SparkContext]Running Spark version 2.2.0
[INFO][2018-06-05 10:05:59,572][org.apache.spark.SparkContext]Submitted application: OperateStat
[INFO][2018-06-05 10:05:59,602][org.apache.spark.SecurityManager]Changing view acls to: seven
[INFO][2018-06-05 10:05:59,603][org.apache.spark.SecurityManager]Changing modify acls to: seven
[INFO][2018-06-05 10:05:59,603][org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO][2018-06-05 10:05:59,604][org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO][2018-06-05 10:05:59,605][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(seven); groups with view permissions: Set(); users  with modify permissions: Set(seven); groups with modify permissions: Set()
[INFO][2018-06-05 10:06:00,155][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 63496.
[INFO][2018-06-05 10:06:00,182][org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO][2018-06-05 10:06:00,204][org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO][2018-06-05 10:06:00,207][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2018-06-05 10:06:00,207][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
[INFO][2018-06-05 10:06:00,218][org.apache.spark.storage.DiskBlockManager]Created local directory at /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/blockmgr-81d3f489-75c9-4a28-a2ab-4738236a1198
[INFO][2018-06-05 10:06:00,283][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 912.3 MB
[INFO][2018-06-05 10:06:00,345][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO][2018-06-05 10:06:00,466][org.spark_project.jetty.util.log]Logging initialized @3879ms
[INFO][2018-06-05 10:06:00,527][org.spark_project.jetty.server.Server]jetty-9.3.z-SNAPSHOT
[INFO][2018-06-05 10:06:00,543][org.spark_project.jetty.server.Server]Started @3957ms
[INFO][2018-06-05 10:06:00,564][org.spark_project.jetty.server.AbstractConnector]Started ServerConnector@67792e2b{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-06-05 10:06:00,565][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4040.
[INFO][2018-06-05 10:06:00,596][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@472698d{/jobs,null,AVAILABLE,@Spark}
[INFO][2018-06-05 10:06:00,597][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3872bc37{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 10:06:00,598][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@12968227{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2018-06-05 10:06:00,599][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@48eb9836{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 10:06:00,600][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@11b455e5{/stages,null,AVAILABLE,@Spark}
[INFO][2018-06-05 10:06:00,601][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3e3861d7{/stages/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 10:06:00,601][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6d025d1d{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2018-06-05 10:06:00,628][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5bb51241{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 10:06:00,629][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@e8ea697{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2018-06-05 10:06:00,636][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@70f822e{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 10:06:00,637][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@45bf6f39{/storage,null,AVAILABLE,@Spark}
[INFO][2018-06-05 10:06:00,637][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@17a703f5{/storage/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 10:06:00,638][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@618ad2aa{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2018-06-05 10:06:00,639][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3531f3ca{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 10:06:00,639][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4867ab9f{/environment,null,AVAILABLE,@Spark}
[INFO][2018-06-05 10:06:00,640][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5fe7f967{/environment/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 10:06:00,641][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2caa5d7c{/executors,null,AVAILABLE,@Spark}
[INFO][2018-06-05 10:06:00,641][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3eabe84a{/executors/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 10:06:00,642][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@38fc5554{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2018-06-05 10:06:00,643][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3f049056{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 10:06:00,679][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3e1a3801{/static,null,AVAILABLE,@Spark}
[INFO][2018-06-05 10:06:00,680][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6ba30587{/,null,AVAILABLE,@Spark}
[INFO][2018-06-05 10:06:00,684][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5633dafd{/api,null,AVAILABLE,@Spark}
[INFO][2018-06-05 10:06:00,686][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@778d82e9{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2018-06-05 10:06:00,689][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@59901c4d{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2018-06-05 10:06:00,693][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://10.194.32.157:4040
[INFO][2018-06-05 10:06:00,899][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
[INFO][2018-06-05 10:06:00,933][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 63497.
[INFO][2018-06-05 10:06:00,934][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 10.194.32.157:63497
[INFO][2018-06-05 10:06:00,937][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2018-06-05 10:06:00,939][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 10.194.32.157, 63497, None)
[INFO][2018-06-05 10:06:00,945][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 10.194.32.157:63497 with 912.3 MB RAM, BlockManagerId(driver, 10.194.32.157, 63497, None)
[INFO][2018-06-05 10:06:00,952][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 10.194.32.157, 63497, None)
[INFO][2018-06-05 10:06:00,953][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 10.194.32.157, 63497, None)
[INFO][2018-06-05 10:06:01,183][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5875de6a{/metrics/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 10:06:01,227][root]jobStarttime:2018-06-05 10:06:01
[INFO][2018-06-05 10:11:11,041][org.apache.spark.SparkContext]Running Spark version 2.2.0
[INFO][2018-06-05 10:11:12,621][org.apache.spark.SparkContext]Submitted application: OperateStat
[INFO][2018-06-05 10:11:12,652][org.apache.spark.SecurityManager]Changing view acls to: seven
[INFO][2018-06-05 10:11:12,653][org.apache.spark.SecurityManager]Changing modify acls to: seven
[INFO][2018-06-05 10:11:12,654][org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO][2018-06-05 10:11:12,655][org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO][2018-06-05 10:11:12,656][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(seven); groups with view permissions: Set(); users  with modify permissions: Set(seven); groups with modify permissions: Set()
[INFO][2018-06-05 10:11:13,184][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 63555.
[INFO][2018-06-05 10:11:13,208][org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO][2018-06-05 10:11:13,228][org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO][2018-06-05 10:11:13,234][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2018-06-05 10:11:13,235][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
[INFO][2018-06-05 10:11:13,247][org.apache.spark.storage.DiskBlockManager]Created local directory at /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/blockmgr-1fbfee2a-6c08-42a0-abae-d3a46abeb983
[INFO][2018-06-05 10:11:13,289][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 912.3 MB
[INFO][2018-06-05 10:11:13,427][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO][2018-06-05 10:11:13,571][org.spark_project.jetty.util.log]Logging initialized @4145ms
[INFO][2018-06-05 10:11:13,692][org.spark_project.jetty.server.Server]jetty-9.3.z-SNAPSHOT
[INFO][2018-06-05 10:11:13,711][org.spark_project.jetty.server.Server]Started @4293ms
[INFO][2018-06-05 10:11:13,737][org.spark_project.jetty.server.AbstractConnector]Started ServerConnector@3cf1a9ff{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-06-05 10:11:13,737][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4040.
[INFO][2018-06-05 10:11:13,773][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@60fa3495{/jobs,null,AVAILABLE,@Spark}
[INFO][2018-06-05 10:11:13,774][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3624da92{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 10:11:13,777][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@94f6bfb{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2018-06-05 10:11:13,791][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@60b71e8f{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 10:11:13,792][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@464649c{/stages,null,AVAILABLE,@Spark}
[INFO][2018-06-05 10:11:13,809][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5f59185e{/stages/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 10:11:13,817][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@47da3952{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2018-06-05 10:11:13,819][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5fd62371{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 10:11:13,820][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2b62442c{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2018-06-05 10:11:13,820][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@841e575{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 10:11:13,821][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1e5f4170{/storage,null,AVAILABLE,@Spark}
[INFO][2018-06-05 10:11:13,822][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6b5966e1{/storage/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 10:11:13,827][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1568159{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2018-06-05 10:11:13,828][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6f80fafe{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 10:11:13,828][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@f9879ac{/environment,null,AVAILABLE,@Spark}
[INFO][2018-06-05 10:11:13,830][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5f4d427e{/environment/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 10:11:13,831][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@224b4d61{/executors,null,AVAILABLE,@Spark}
[INFO][2018-06-05 10:11:13,831][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@303e3593{/executors/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 10:11:13,832][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@362a019c{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2018-06-05 10:11:13,834][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5c48c0c0{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 10:11:13,843][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@674c583e{/static,null,AVAILABLE,@Spark}
[INFO][2018-06-05 10:11:13,844][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2f94c4db{/,null,AVAILABLE,@Spark}
[INFO][2018-06-05 10:11:13,845][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@72ccd81a{/api,null,AVAILABLE,@Spark}
[INFO][2018-06-05 10:11:13,846][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@9d157ff{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2018-06-05 10:11:13,847][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5df417a7{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2018-06-05 10:11:13,849][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://10.194.32.157:4040
[INFO][2018-06-05 10:11:14,091][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
[INFO][2018-06-05 10:11:14,149][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 63556.
[INFO][2018-06-05 10:11:14,152][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 10.194.32.157:63556
[INFO][2018-06-05 10:11:14,154][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2018-06-05 10:11:14,159][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 10.194.32.157, 63556, None)
[INFO][2018-06-05 10:11:14,163][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 10.194.32.157:63556 with 912.3 MB RAM, BlockManagerId(driver, 10.194.32.157, 63556, None)
[INFO][2018-06-05 10:11:14,168][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 10.194.32.157, 63556, None)
[INFO][2018-06-05 10:11:14,169][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 10.194.32.157, 63556, None)
[INFO][2018-06-05 10:11:14,429][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4a951911{/metrics/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 10:11:14,455][root]jobStarttime:2018-06-05 10:11:14
[INFO][2018-06-05 10:18:22,698][root]jobEndtime:2018-06-05 10:18:22;
[INFO][2018-06-05 10:18:22,699][root]jobResult:Sucessfull!
[INFO][2018-06-05 10:19:18,297][org.apache.spark.SparkContext]Running Spark version 2.2.0
[INFO][2018-06-05 10:19:19,731][org.apache.spark.SparkContext]Submitted application: OperateStat
[INFO][2018-06-05 10:19:19,762][org.apache.spark.SecurityManager]Changing view acls to: seven
[INFO][2018-06-05 10:19:19,763][org.apache.spark.SecurityManager]Changing modify acls to: seven
[INFO][2018-06-05 10:19:19,764][org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO][2018-06-05 10:19:19,765][org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO][2018-06-05 10:19:19,766][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(seven); groups with view permissions: Set(); users  with modify permissions: Set(seven); groups with modify permissions: Set()
[INFO][2018-06-05 10:19:20,233][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 63704.
[INFO][2018-06-05 10:19:20,257][org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO][2018-06-05 10:19:20,283][org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO][2018-06-05 10:19:20,287][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2018-06-05 10:19:20,288][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
[INFO][2018-06-05 10:19:20,300][org.apache.spark.storage.DiskBlockManager]Created local directory at /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/blockmgr-c2eb5fb3-5f44-40b0-aaa6-58e97c87e445
[INFO][2018-06-05 10:19:20,325][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 912.3 MB
[INFO][2018-06-05 10:19:20,476][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO][2018-06-05 10:19:20,592][org.spark_project.jetty.util.log]Logging initialized @3651ms
[INFO][2018-06-05 10:19:20,685][org.spark_project.jetty.server.Server]jetty-9.3.z-SNAPSHOT
[INFO][2018-06-05 10:19:20,745][org.spark_project.jetty.server.Server]Started @3805ms
[INFO][2018-06-05 10:19:20,771][org.spark_project.jetty.server.AbstractConnector]Started ServerConnector@41a90fa8{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-06-05 10:19:20,772][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4040.
[INFO][2018-06-05 10:19:20,803][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@29a60c27{/jobs,null,AVAILABLE,@Spark}
[INFO][2018-06-05 10:19:20,807][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@34645867{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 10:19:20,808][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@60b71e8f{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2018-06-05 10:19:20,809][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7c22d4f{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 10:19:20,811][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@60bdf15d{/stages,null,AVAILABLE,@Spark}
[INFO][2018-06-05 10:19:20,812][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@51e4ccb3{/stages/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 10:19:20,812][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@495083a0{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2018-06-05 10:19:20,814][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@66629f63{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 10:19:20,814][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@27a5328c{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2018-06-05 10:19:20,815][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6c345c5f{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 10:19:20,816][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@65e61854{/storage,null,AVAILABLE,@Spark}
[INFO][2018-06-05 10:19:20,819][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4fcee388{/storage/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 10:19:20,820][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3af17be2{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2018-06-05 10:19:20,821][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@37f21974{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 10:19:20,822][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6e521c1e{/environment,null,AVAILABLE,@Spark}
[INFO][2018-06-05 10:19:20,823][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5d5d9e5{/environment/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 10:19:20,824][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4ef27d66{/executors,null,AVAILABLE,@Spark}
[INFO][2018-06-05 10:19:20,827][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1d9bec4d{/executors/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 10:19:20,828][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@10c8f62{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2018-06-05 10:19:20,829][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@25f7391e{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 10:19:20,845][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5ab14cb9{/static,null,AVAILABLE,@Spark}
[INFO][2018-06-05 10:19:20,847][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6d8792db{/,null,AVAILABLE,@Spark}
[INFO][2018-06-05 10:19:20,848][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@493dfb8e{/api,null,AVAILABLE,@Spark}
[INFO][2018-06-05 10:19:20,849][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7c041b41{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2018-06-05 10:19:20,862][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@61078690{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2018-06-05 10:19:20,864][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://10.194.32.157:4040
[INFO][2018-06-05 10:19:21,044][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
[INFO][2018-06-05 10:19:21,089][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 63705.
[INFO][2018-06-05 10:19:21,090][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 10.194.32.157:63705
[INFO][2018-06-05 10:19:21,092][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2018-06-05 10:19:21,097][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 10.194.32.157, 63705, None)
[INFO][2018-06-05 10:19:21,101][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 10.194.32.157:63705 with 912.3 MB RAM, BlockManagerId(driver, 10.194.32.157, 63705, None)
[INFO][2018-06-05 10:19:21,107][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 10.194.32.157, 63705, None)
[INFO][2018-06-05 10:19:21,107][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 10.194.32.157, 63705, None)
[INFO][2018-06-05 10:19:21,389][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5e63cad{/metrics/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 10:19:21,413][root]jobStarttime:2018-06-05 10:19:21
[INFO][2018-06-05 10:24:32,381][root]jobEndtime:2018-06-05 10:24:32;
[INFO][2018-06-05 10:24:32,382][root]jobResult:Sucessfull!
[INFO][2018-06-05 11:17:07,061][org.apache.spark.SparkContext]Running Spark version 2.2.0
[INFO][2018-06-05 11:17:08,109][org.apache.spark.SparkContext]Submitted application: NetBaseInfo
[INFO][2018-06-05 11:17:08,149][org.apache.spark.SecurityManager]Changing view acls to: seven
[INFO][2018-06-05 11:17:08,150][org.apache.spark.SecurityManager]Changing modify acls to: seven
[INFO][2018-06-05 11:17:08,150][org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO][2018-06-05 11:17:08,151][org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO][2018-06-05 11:17:08,152][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(seven); groups with view permissions: Set(); users  with modify permissions: Set(seven); groups with modify permissions: Set()
[INFO][2018-06-05 11:17:08,664][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 64481.
[INFO][2018-06-05 11:17:08,699][org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO][2018-06-05 11:17:08,734][org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO][2018-06-05 11:17:08,736][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2018-06-05 11:17:08,737][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
[INFO][2018-06-05 11:17:08,751][org.apache.spark.storage.DiskBlockManager]Created local directory at /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/blockmgr-5ad9ae68-44c2-4bea-912f-5b9d0e26e6b9
[INFO][2018-06-05 11:17:08,781][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 912.3 MB
[INFO][2018-06-05 11:17:08,894][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO][2018-06-05 11:17:08,978][org.spark_project.jetty.util.log]Logging initialized @3803ms
[INFO][2018-06-05 11:17:09,036][org.spark_project.jetty.server.Server]jetty-9.3.z-SNAPSHOT
[INFO][2018-06-05 11:17:09,053][org.spark_project.jetty.server.Server]Started @3879ms
[INFO][2018-06-05 11:17:09,075][org.spark_project.jetty.server.AbstractConnector]Started ServerConnector@498426af{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-06-05 11:17:09,075][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4040.
[INFO][2018-06-05 11:17:09,107][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@60fa3495{/jobs,null,AVAILABLE,@Spark}
[INFO][2018-06-05 11:17:09,107][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3624da92{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 11:17:09,108][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@94f6bfb{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2018-06-05 11:17:09,109][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@60b71e8f{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 11:17:09,109][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@464649c{/stages,null,AVAILABLE,@Spark}
[INFO][2018-06-05 11:17:09,110][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5f59185e{/stages/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 11:17:09,111][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@47da3952{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2018-06-05 11:17:09,112][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5fd62371{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 11:17:09,113][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2b62442c{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2018-06-05 11:17:09,116][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@841e575{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 11:17:09,118][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1e5f4170{/storage,null,AVAILABLE,@Spark}
[INFO][2018-06-05 11:17:09,119][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6b5966e1{/storage/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 11:17:09,121][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1568159{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2018-06-05 11:17:09,122][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6f80fafe{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 11:17:09,124][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@f9879ac{/environment,null,AVAILABLE,@Spark}
[INFO][2018-06-05 11:17:09,124][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5f4d427e{/environment/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 11:17:09,127][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@224b4d61{/executors,null,AVAILABLE,@Spark}
[INFO][2018-06-05 11:17:09,129][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@303e3593{/executors/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 11:17:09,129][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@362a019c{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2018-06-05 11:17:09,130][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5c48c0c0{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 11:17:09,147][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@674c583e{/static,null,AVAILABLE,@Spark}
[INFO][2018-06-05 11:17:09,148][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2f94c4db{/,null,AVAILABLE,@Spark}
[INFO][2018-06-05 11:17:09,151][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@72ccd81a{/api,null,AVAILABLE,@Spark}
[INFO][2018-06-05 11:17:09,152][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@9d157ff{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2018-06-05 11:17:09,155][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5df417a7{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2018-06-05 11:17:09,157][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://10.194.32.157:4040
[INFO][2018-06-05 11:17:09,313][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
[INFO][2018-06-05 11:17:09,363][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 64482.
[INFO][2018-06-05 11:17:09,365][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 10.194.32.157:64482
[INFO][2018-06-05 11:17:09,367][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2018-06-05 11:17:09,369][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 10.194.32.157, 64482, None)
[INFO][2018-06-05 11:17:09,373][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 10.194.32.157:64482 with 912.3 MB RAM, BlockManagerId(driver, 10.194.32.157, 64482, None)
[INFO][2018-06-05 11:17:09,385][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 10.194.32.157, 64482, None)
[INFO][2018-06-05 11:17:09,386][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 10.194.32.157, 64482, None)
[INFO][2018-06-05 11:17:09,764][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4a951911{/metrics/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 11:17:09,787][root]jobStarttime:2018-06-05 11:17:09
[INFO][2018-06-05 11:17:19,744][root]jobEndtime:2018-06-05 11:17:19;
[INFO][2018-06-05 11:17:19,744][root]jobResult:Sucessfull!
[INFO][2018-06-05 13:33:44,741][org.apache.spark.SparkContext]Running Spark version 2.2.0
[INFO][2018-06-05 13:33:46,041][org.apache.spark.SparkContext]Submitted application: OperateStat
[INFO][2018-06-05 13:33:46,100][org.apache.spark.SecurityManager]Changing view acls to: seven
[INFO][2018-06-05 13:33:46,101][org.apache.spark.SecurityManager]Changing modify acls to: seven
[INFO][2018-06-05 13:33:46,102][org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO][2018-06-05 13:33:46,104][org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO][2018-06-05 13:33:46,105][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(seven); groups with view permissions: Set(); users  with modify permissions: Set(seven); groups with modify permissions: Set()
[INFO][2018-06-05 13:33:46,635][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 49818.
[INFO][2018-06-05 13:33:46,672][org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO][2018-06-05 13:33:46,732][org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO][2018-06-05 13:33:46,740][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2018-06-05 13:33:46,741][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
[INFO][2018-06-05 13:33:46,775][org.apache.spark.storage.DiskBlockManager]Created local directory at /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/blockmgr-d5d32c8c-8474-4b30-9788-aa34da26a2cf
[INFO][2018-06-05 13:33:46,892][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 912.3 MB
[INFO][2018-06-05 13:33:47,085][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO][2018-06-05 13:33:47,270][org.spark_project.jetty.util.log]Logging initialized @5976ms
[INFO][2018-06-05 13:33:47,368][org.spark_project.jetty.server.Server]jetty-9.3.z-SNAPSHOT
[INFO][2018-06-05 13:33:47,388][org.spark_project.jetty.server.Server]Started @6095ms
[INFO][2018-06-05 13:33:47,428][org.spark_project.jetty.server.AbstractConnector]Started ServerConnector@66d8bfd0{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-06-05 13:33:47,430][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4040.
[INFO][2018-06-05 13:33:47,474][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@10cd6753{/jobs,null,AVAILABLE,@Spark}
[INFO][2018-06-05 13:33:47,475][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@332820f4{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 13:33:47,481][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@72456279{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2018-06-05 13:33:47,497][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1416cf9f{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 13:33:47,504][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@bfc14b9{/stages,null,AVAILABLE,@Spark}
[INFO][2018-06-05 13:33:47,505][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2dfe5525{/stages/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 13:33:47,514][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6a9b9909{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2018-06-05 13:33:47,530][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6ba7383d{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 13:33:47,537][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@710d89e2{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2018-06-05 13:33:47,538][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4fc142ec{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 13:33:47,539][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@29eda4f8{/storage,null,AVAILABLE,@Spark}
[INFO][2018-06-05 13:33:47,542][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5e048149{/storage/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 13:33:47,543][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3d5790ea{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2018-06-05 13:33:47,545][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@67a3bd51{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 13:33:47,550][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@56913163{/environment,null,AVAILABLE,@Spark}
[INFO][2018-06-05 13:33:47,551][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@a18649a{/environment/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 13:33:47,552][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@396639b{/executors,null,AVAILABLE,@Spark}
[INFO][2018-06-05 13:33:47,553][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@62573c86{/executors/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 13:33:47,554][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@14229fa7{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2018-06-05 13:33:47,555][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7158daf2{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 13:33:47,591][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@102efc59{/static,null,AVAILABLE,@Spark}
[INFO][2018-06-05 13:33:47,592][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6631cb64{/,null,AVAILABLE,@Spark}
[INFO][2018-06-05 13:33:47,593][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@481e91b6{/api,null,AVAILABLE,@Spark}
[INFO][2018-06-05 13:33:47,594][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7c1e32c9{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2018-06-05 13:33:47,595][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3dd818e8{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2018-06-05 13:33:47,603][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://10.194.32.157:4040
[INFO][2018-06-05 13:33:47,851][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
[INFO][2018-06-05 13:33:47,916][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 49819.
[INFO][2018-06-05 13:33:47,918][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 10.194.32.157:49819
[INFO][2018-06-05 13:33:47,922][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2018-06-05 13:33:47,930][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 10.194.32.157, 49819, None)
[INFO][2018-06-05 13:33:47,934][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 10.194.32.157:49819 with 912.3 MB RAM, BlockManagerId(driver, 10.194.32.157, 49819, None)
[INFO][2018-06-05 13:33:47,945][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 10.194.32.157, 49819, None)
[INFO][2018-06-05 13:33:47,946][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 10.194.32.157, 49819, None)
[INFO][2018-06-05 13:33:48,319][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@b112b13{/metrics/json,null,AVAILABLE,@Spark}
[INFO][2018-06-05 13:33:48,360][root]jobStarttime:2018-06-05 13:33:48
[INFO][2018-06-06 13:43:10,090][org.apache.spark.SparkContext]Running Spark version 2.2.0
[INFO][2018-06-06 13:43:11,535][org.apache.spark.SparkContext]Submitted application: OperateStat
[INFO][2018-06-06 13:43:11,612][org.apache.spark.SecurityManager]Changing view acls to: seven
[INFO][2018-06-06 13:43:11,613][org.apache.spark.SecurityManager]Changing modify acls to: seven
[INFO][2018-06-06 13:43:11,616][org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO][2018-06-06 13:43:11,617][org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO][2018-06-06 13:43:11,621][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(seven); groups with view permissions: Set(); users  with modify permissions: Set(seven); groups with modify permissions: Set()
[INFO][2018-06-06 13:43:12,150][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 53958.
[INFO][2018-06-06 13:43:12,283][org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO][2018-06-06 13:43:12,318][org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO][2018-06-06 13:43:12,325][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2018-06-06 13:43:12,326][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
[INFO][2018-06-06 13:43:12,348][org.apache.spark.storage.DiskBlockManager]Created local directory at /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/blockmgr-a7ee98d6-94ca-416c-a6c0-221a5fc1be8a
[INFO][2018-06-06 13:43:12,437][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 912.3 MB
[INFO][2018-06-06 13:43:12,601][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO][2018-06-06 13:43:12,737][org.spark_project.jetty.util.log]Logging initialized @5936ms
[INFO][2018-06-06 13:43:12,826][org.spark_project.jetty.server.Server]jetty-9.3.z-SNAPSHOT
[INFO][2018-06-06 13:43:12,845][org.spark_project.jetty.server.Server]Started @6045ms
[INFO][2018-06-06 13:43:12,885][org.spark_project.jetty.server.AbstractConnector]Started ServerConnector@58496c97{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-06-06 13:43:12,886][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4040.
[INFO][2018-06-06 13:43:12,925][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@700f518a{/jobs,null,AVAILABLE,@Spark}
[INFO][2018-06-06 13:43:12,927][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@53a9fcfd{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 13:43:12,929][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4d192aef{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2018-06-06 13:43:12,930][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@fb6097b{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 13:43:12,931][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1290c49{/stages,null,AVAILABLE,@Spark}
[INFO][2018-06-06 13:43:12,932][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@55d9b8f0{/stages/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 13:43:12,933][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@43d38654{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2018-06-06 13:43:12,935][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1d75e7af{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 13:43:12,936][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@34b27915{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2018-06-06 13:43:12,936][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1b9776f5{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 13:43:12,937][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@79d9214d{/storage,null,AVAILABLE,@Spark}
[INFO][2018-06-06 13:43:12,941][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1dd7796b{/storage/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 13:43:12,942][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@57402ba1{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2018-06-06 13:43:12,943][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@702b06fb{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 13:43:12,943][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5c534b5b{/environment,null,AVAILABLE,@Spark}
[INFO][2018-06-06 13:43:12,954][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2b22a1cc{/environment/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 13:43:12,955][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2418ba04{/executors,null,AVAILABLE,@Spark}
[INFO][2018-06-06 13:43:12,956][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2ab0702e{/executors/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 13:43:12,958][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@10f19647{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2018-06-06 13:43:12,959][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3936df72{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 13:43:12,976][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@12d1f1d4{/static,null,AVAILABLE,@Spark}
[INFO][2018-06-06 13:43:12,977][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7c8f9c2e{/,null,AVAILABLE,@Spark}
[INFO][2018-06-06 13:43:12,979][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3cbf1ba4{/api,null,AVAILABLE,@Spark}
[INFO][2018-06-06 13:43:12,980][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4cb40e3b{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2018-06-06 13:43:12,981][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3a543f31{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2018-06-06 13:43:12,984][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://10.194.32.157:4040
[INFO][2018-06-06 13:43:13,177][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
[INFO][2018-06-06 13:43:13,219][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 53959.
[INFO][2018-06-06 13:43:13,221][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 10.194.32.157:53959
[INFO][2018-06-06 13:43:13,227][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2018-06-06 13:43:13,232][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 10.194.32.157, 53959, None)
[INFO][2018-06-06 13:43:13,239][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 10.194.32.157:53959 with 912.3 MB RAM, BlockManagerId(driver, 10.194.32.157, 53959, None)
[INFO][2018-06-06 13:43:13,247][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 10.194.32.157, 53959, None)
[INFO][2018-06-06 13:43:13,249][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 10.194.32.157, 53959, None)
[INFO][2018-06-06 13:43:13,515][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@43c87306{/metrics/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 13:43:13,570][root]jobStarttime:2018-06-06 13:43:13
[INFO][2018-06-06 15:36:52,307][org.apache.spark.SparkContext]Running Spark version 2.2.0
[INFO][2018-06-06 15:36:53,401][org.apache.spark.SparkContext]Submitted application: SalesDefectNetByNetCommunity$
[INFO][2018-06-06 15:36:53,441][org.apache.spark.SecurityManager]Changing view acls to: seven
[INFO][2018-06-06 15:36:53,443][org.apache.spark.SecurityManager]Changing modify acls to: seven
[INFO][2018-06-06 15:36:53,444][org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO][2018-06-06 15:36:53,445][org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO][2018-06-06 15:36:53,446][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(seven); groups with view permissions: Set(); users  with modify permissions: Set(seven); groups with modify permissions: Set()
[INFO][2018-06-06 15:36:53,865][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 55709.
[INFO][2018-06-06 15:36:53,908][org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO][2018-06-06 15:36:53,940][org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO][2018-06-06 15:36:53,945][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2018-06-06 15:36:53,945][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
[INFO][2018-06-06 15:36:53,963][org.apache.spark.storage.DiskBlockManager]Created local directory at /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/blockmgr-ef6f0690-1cc3-4e0e-bb09-a069efbe9393
[INFO][2018-06-06 15:36:53,995][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 912.3 MB
[INFO][2018-06-06 15:36:54,064][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO][2018-06-06 15:36:54,214][org.spark_project.jetty.util.log]Logging initialized @3326ms
[INFO][2018-06-06 15:36:54,303][org.spark_project.jetty.server.Server]jetty-9.3.z-SNAPSHOT
[INFO][2018-06-06 15:36:54,323][org.spark_project.jetty.server.Server]Started @3436ms
[INFO][2018-06-06 15:36:54,360][org.spark_project.jetty.server.AbstractConnector]Started ServerConnector@89c10b7{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-06-06 15:36:54,360][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4040.
[INFO][2018-06-06 15:36:54,388][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4a9f80d3{/jobs,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:36:54,388][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3e2822{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:36:54,389][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@29a60c27{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:36:54,390][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1ca25c47{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:36:54,391][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@533b266e{/stages,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:36:54,391][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@89ff02e{/stages/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:36:54,392][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@62679465{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:36:54,393][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5f2606b{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:36:54,394][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3ebff828{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:36:54,395][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@33352f32{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:36:54,396][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1e044120{/storage,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:36:54,396][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3624da92{/storage/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:36:54,397][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@94f6bfb{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:36:54,398][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2484f433{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:36:54,399][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1255b1d1{/environment,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:36:54,399][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7c22d4f{/environment/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:36:54,400][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@60bdf15d{/executors,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:36:54,401][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@51e4ccb3{/executors/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:36:54,402][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@495083a0{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:36:54,403][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@28a0fd6c{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:36:54,414][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@66629f63{/static,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:36:54,415][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1d9bec4d{/,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:36:54,417][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@10c8f62{/api,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:36:54,419][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@439a8f59{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:36:54,420][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@31024624{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:36:54,423][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://10.194.32.157:4040
[INFO][2018-06-06 15:36:54,620][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
[INFO][2018-06-06 15:36:54,652][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 55710.
[INFO][2018-06-06 15:36:54,655][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 10.194.32.157:55710
[INFO][2018-06-06 15:36:54,657][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2018-06-06 15:36:54,660][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 10.194.32.157, 55710, None)
[INFO][2018-06-06 15:36:54,664][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 10.194.32.157:55710 with 912.3 MB RAM, BlockManagerId(driver, 10.194.32.157, 55710, None)
[INFO][2018-06-06 15:36:54,669][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 10.194.32.157, 55710, None)
[INFO][2018-06-06 15:36:54,671][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 10.194.32.157, 55710, None)
[INFO][2018-06-06 15:36:54,923][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6df20ade{/metrics/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:36:54,952][com.seven.spark.rdd.SalesDefectNetByNetCommunity$]SalesDefectNetByNetCommunity$ is start . . .
[INFO][2018-06-06 15:36:55,601][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0 stored as values in memory (estimated size 228.1 KB, free 912.1 MB)
[INFO][2018-06-06 15:36:55,873][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0_piece0 stored as bytes in memory (estimated size 22.1 KB, free 912.1 MB)
[INFO][2018-06-06 15:36:55,877][org.apache.spark.storage.BlockManagerInfo]Added broadcast_0_piece0 in memory on 10.194.32.157:55710 (size: 22.1 KB, free: 912.3 MB)
[INFO][2018-06-06 15:36:55,884][org.apache.spark.SparkContext]Created broadcast 0 from textFile at SalesDefectNetByNetCommunity.scala:53
[WARN][2018-06-06 15:36:57,657][org.apache.hadoop.util.NativeCodeLoader]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO][2018-06-06 15:36:57,866][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 1
[INFO][2018-06-06 15:36:57,976][org.apache.spark.SparkContext]Starting job: collect at SalesDefectNetByNetCommunity.scala:64
[INFO][2018-06-06 15:36:57,998][org.apache.spark.scheduler.DAGScheduler]Got job 0 (collect at SalesDefectNetByNetCommunity.scala:64) with 2 output partitions
[INFO][2018-06-06 15:36:57,999][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 0 (collect at SalesDefectNetByNetCommunity.scala:64)
[INFO][2018-06-06 15:36:58,000][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-06-06 15:36:58,003][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-06-06 15:36:58,016][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 0 (MapPartitionsRDD[3] at mapPartitions at SalesDefectNetByNetCommunity.scala:56), which has no missing parents
[INFO][2018-06-06 15:36:58,041][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1 stored as values in memory (estimated size 3.7 KB, free 912.1 MB)
[INFO][2018-06-06 15:36:58,057][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.1 KB, free 912.1 MB)
[INFO][2018-06-06 15:36:58,058][org.apache.spark.storage.BlockManagerInfo]Added broadcast_1_piece0 in memory on 10.194.32.157:55710 (size: 2.1 KB, free: 912.3 MB)
[INFO][2018-06-06 15:36:58,059][org.apache.spark.SparkContext]Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO][2018-06-06 15:36:58,090][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at mapPartitions at SalesDefectNetByNetCommunity.scala:56) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2018-06-06 15:36:58,091][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 0.0 with 2 tasks
[INFO][2018-06-06 15:36:58,151][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 4930 bytes)
[INFO][2018-06-06 15:36:58,154][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 4930 bytes)
[INFO][2018-06-06 15:36:58,162][org.apache.spark.executor.Executor]Running task 1.0 in stage 0.0 (TID 1)
[INFO][2018-06-06 15:36:58,162][org.apache.spark.executor.Executor]Running task 0.0 in stage 0.0 (TID 0)
[INFO][2018-06-06 15:36:58,224][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/sta_vem/net_community/net_community__b4d64c39_5c6b_44c1_81be_f9aa19c55a30:0+278620
[INFO][2018-06-06 15:36:58,224][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/sta_vem/net_community/net_community__b4d64c39_5c6b_44c1_81be_f9aa19c55a30:278620+278620
[INFO][2018-06-06 15:36:58,543][org.apache.spark.executor.Executor]Finished task 1.0 in stage 0.0 (TID 1). 290507 bytes result sent to driver
[INFO][2018-06-06 15:36:58,567][org.apache.spark.executor.Executor]Finished task 0.0 in stage 0.0 (TID 0). 289675 bytes result sent to driver
[INFO][2018-06-06 15:36:58,569][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 0.0 (TID 1) in 413 ms on localhost (executor driver) (1/2)
[INFO][2018-06-06 15:36:58,582][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 0.0 (TID 0) in 450 ms on localhost (executor driver) (2/2)
[INFO][2018-06-06 15:36:58,584][org.apache.spark.scheduler.DAGScheduler]ResultStage 0 (collect at SalesDefectNetByNetCommunity.scala:64) finished in 0.465 s
[INFO][2018-06-06 15:36:58,585][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO][2018-06-06 15:36:58,595][org.apache.spark.scheduler.DAGScheduler]Job 0 finished: collect at SalesDefectNetByNetCommunity.scala:64, took 0.619414 s
[INFO][2018-06-06 15:36:58,624][org.apache.spark.storage.memory.MemoryStore]Block broadcast_2 stored as values in memory (estimated size 1060.4 KB, free 911.0 MB)
[INFO][2018-06-06 15:36:58,644][org.apache.spark.storage.memory.MemoryStore]Block broadcast_2_piece0 stored as bytes in memory (estimated size 203.6 KB, free 910.8 MB)
[INFO][2018-06-06 15:36:58,645][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_1_piece0 on 10.194.32.157:55710 in memory (size: 2.1 KB, free: 912.3 MB)
[INFO][2018-06-06 15:36:58,646][org.apache.spark.storage.BlockManagerInfo]Added broadcast_2_piece0 in memory on 10.194.32.157:55710 (size: 203.6 KB, free: 912.1 MB)
[INFO][2018-06-06 15:36:58,646][org.apache.spark.SparkContext]Created broadcast 2 from broadcast at SalesDefectNetByNetCommunity.scala:34
[INFO][2018-06-06 15:36:58,653][org.apache.spark.storage.memory.MemoryStore]Block broadcast_3 stored as values in memory (estimated size 228.1 KB, free 910.6 MB)
[INFO][2018-06-06 15:36:58,676][org.apache.spark.storage.memory.MemoryStore]Block broadcast_3_piece0 stored as bytes in memory (estimated size 22.1 KB, free 910.6 MB)
[INFO][2018-06-06 15:36:58,679][org.apache.spark.storage.BlockManagerInfo]Added broadcast_3_piece0 in memory on 10.194.32.157:55710 (size: 22.1 KB, free: 912.1 MB)
[INFO][2018-06-06 15:36:58,680][org.apache.spark.SparkContext]Created broadcast 3 from textFile at SalesDefectNetByNetCommunity.scala:79
[INFO][2018-06-06 15:36:58,719][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 1
[INFO][2018-06-06 15:36:58,741][org.apache.spark.SparkContext]Starting job: collect at SalesDefectNetByNetCommunity.scala:89
[INFO][2018-06-06 15:36:58,742][org.apache.spark.scheduler.DAGScheduler]Got job 1 (collect at SalesDefectNetByNetCommunity.scala:89) with 2 output partitions
[INFO][2018-06-06 15:36:58,743][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 1 (collect at SalesDefectNetByNetCommunity.scala:89)
[INFO][2018-06-06 15:36:58,743][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-06-06 15:36:58,743][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-06-06 15:36:58,743][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 1 (MapPartitionsRDD[7] at mapPartitions at SalesDefectNetByNetCommunity.scala:82), which has no missing parents
[INFO][2018-06-06 15:36:58,746][org.apache.spark.storage.memory.MemoryStore]Block broadcast_4 stored as values in memory (estimated size 3.7 KB, free 910.6 MB)
[INFO][2018-06-06 15:36:58,751][org.apache.spark.storage.memory.MemoryStore]Block broadcast_4_piece0 stored as bytes in memory (estimated size 2.0 KB, free 910.6 MB)
[INFO][2018-06-06 15:36:58,751][org.apache.spark.storage.BlockManagerInfo]Added broadcast_4_piece0 in memory on 10.194.32.157:55710 (size: 2.0 KB, free: 912.1 MB)
[INFO][2018-06-06 15:36:58,752][org.apache.spark.SparkContext]Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[INFO][2018-06-06 15:36:58,753][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[7] at mapPartitions at SalesDefectNetByNetCommunity.scala:82) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2018-06-06 15:36:58,753][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 1.0 with 2 tasks
[INFO][2018-06-06 15:36:58,754][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 1.0 (TID 2, localhost, executor driver, partition 0, ANY, 4926 bytes)
[INFO][2018-06-06 15:36:58,755][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 1.0 (TID 3, localhost, executor driver, partition 1, ANY, 4926 bytes)
[INFO][2018-06-06 15:36:58,755][org.apache.spark.executor.Executor]Running task 0.0 in stage 1.0 (TID 2)
[INFO][2018-06-06 15:36:58,758][org.apache.spark.executor.Executor]Running task 1.0 in stage 1.0 (TID 3)
[INFO][2018-06-06 15:36:58,760][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/sta_vem/vem_nettype/vem_nettype__5b80085f_0bae_4bd8_97d4_919813b3ce8b:0+6079084
[INFO][2018-06-06 15:36:58,764][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/sta_vem/vem_nettype/vem_nettype__5b80085f_0bae_4bd8_97d4_919813b3ce8b:6079084+6079084
[INFO][2018-06-06 15:37:03,806][org.apache.spark.executor.Executor]Finished task 1.0 in stage 1.0 (TID 3). 14530 bytes result sent to driver
[INFO][2018-06-06 15:37:03,831][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 1.0 (TID 3) in 5077 ms on localhost (executor driver) (1/2)
[INFO][2018-06-06 15:37:04,107][org.apache.spark.executor.Executor]Finished task 0.0 in stage 1.0 (TID 2). 240458 bytes result sent to driver
[INFO][2018-06-06 15:37:04,131][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 1.0 (TID 2) in 5377 ms on localhost (executor driver) (2/2)
[INFO][2018-06-06 15:37:04,132][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO][2018-06-06 15:37:04,132][org.apache.spark.scheduler.DAGScheduler]ResultStage 1 (collect at SalesDefectNetByNetCommunity.scala:89) finished in 5.378 s
[INFO][2018-06-06 15:37:04,133][org.apache.spark.scheduler.DAGScheduler]Job 1 finished: collect at SalesDefectNetByNetCommunity.scala:89, took 5.390866 s
[INFO][2018-06-06 15:37:04,145][org.apache.spark.storage.memory.MemoryStore]Block broadcast_5 stored as values in memory (estimated size 1924.3 KB, free 908.7 MB)
[INFO][2018-06-06 15:37:04,169][org.apache.spark.storage.memory.MemoryStore]Block broadcast_5_piece0 stored as bytes in memory (estimated size 111.1 KB, free 908.6 MB)
[INFO][2018-06-06 15:37:04,170][org.apache.spark.storage.BlockManagerInfo]Added broadcast_5_piece0 in memory on 10.194.32.157:55710 (size: 111.1 KB, free: 911.9 MB)
[INFO][2018-06-06 15:37:04,171][org.apache.spark.SparkContext]Created broadcast 5 from broadcast at SalesDefectNetByNetCommunity.scala:38
[INFO][2018-06-06 15:37:04,181][org.apache.spark.storage.memory.MemoryStore]Block broadcast_6 stored as values in memory (estimated size 228.1 KB, free 908.4 MB)
[INFO][2018-06-06 15:37:04,202][org.apache.spark.storage.memory.MemoryStore]Block broadcast_6_piece0 stored as bytes in memory (estimated size 22.1 KB, free 908.3 MB)
[INFO][2018-06-06 15:37:04,203][org.apache.spark.storage.BlockManagerInfo]Added broadcast_6_piece0 in memory on 10.194.32.157:55710 (size: 22.1 KB, free: 911.9 MB)
[INFO][2018-06-06 15:37:04,204][org.apache.spark.SparkContext]Created broadcast 6 from textFile at SalesDefectNetByNetCommunity.scala:104
[INFO][2018-06-06 15:37:04,253][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 1
[INFO][2018-06-06 15:37:04,293][org.apache.spark.SparkContext]Starting job: foreach at SalesDefectNetByNetCommunity.scala:126
[INFO][2018-06-06 15:37:04,304][org.apache.spark.scheduler.DAGScheduler]Registering RDD 11 (mapPartitions at SalesDefectNetByNetCommunity.scala:107)
[INFO][2018-06-06 15:37:04,304][org.apache.spark.scheduler.DAGScheduler]Got job 2 (foreach at SalesDefectNetByNetCommunity.scala:126) with 2 output partitions
[INFO][2018-06-06 15:37:04,304][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 3 (foreach at SalesDefectNetByNetCommunity.scala:126)
[INFO][2018-06-06 15:37:04,304][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 2)
[INFO][2018-06-06 15:37:04,307][org.apache.spark.scheduler.DAGScheduler]Missing parents: List(ShuffleMapStage 2)
[INFO][2018-06-06 15:37:04,309][org.apache.spark.scheduler.DAGScheduler]Submitting ShuffleMapStage 2 (MapPartitionsRDD[11] at mapPartitions at SalesDefectNetByNetCommunity.scala:107), which has no missing parents
[INFO][2018-06-06 15:37:04,322][org.apache.spark.storage.memory.MemoryStore]Block broadcast_7 stored as values in memory (estimated size 4.9 KB, free 908.3 MB)
[INFO][2018-06-06 15:37:04,327][org.apache.spark.storage.memory.MemoryStore]Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.8 KB, free 908.3 MB)
[INFO][2018-06-06 15:37:04,328][org.apache.spark.storage.BlockManagerInfo]Added broadcast_7_piece0 in memory on 10.194.32.157:55710 (size: 2.8 KB, free: 911.9 MB)
[INFO][2018-06-06 15:37:04,328][org.apache.spark.SparkContext]Created broadcast 7 from broadcast at DAGScheduler.scala:1006
[INFO][2018-06-06 15:37:04,334][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[11] at mapPartitions at SalesDefectNetByNetCommunity.scala:107) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2018-06-06 15:37:04,335][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 2.0 with 2 tasks
[INFO][2018-06-06 15:37:04,337][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 2.0 (TID 4, localhost, executor driver, partition 0, ANY, 4915 bytes)
[INFO][2018-06-06 15:37:04,338][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 2.0 (TID 5, localhost, executor driver, partition 1, ANY, 4915 bytes)
[INFO][2018-06-06 15:37:04,339][org.apache.spark.executor.Executor]Running task 0.0 in stage 2.0 (TID 4)
[INFO][2018-06-06 15:37:04,340][org.apache.spark.executor.Executor]Running task 1.0 in stage 2.0 (TID 5)
[INFO][2018-06-06 15:37:04,350][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/sta_vem/vem_machine/vem_machine__12fe7e5b_689f_498e_b2a6_8411fd976c5b:3998718+3998719
[INFO][2018-06-06 15:37:04,350][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/sta_vem/vem_machine/vem_machine__12fe7e5b_689f_498e_b2a6_8411fd976c5b:0+3998718
[INFO][2018-06-06 15:37:04,764][org.apache.spark.executor.Executor]Finished task 0.0 in stage 2.0 (TID 4). 1000 bytes result sent to driver
[INFO][2018-06-06 15:37:04,796][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 2.0 (TID 4) in 461 ms on localhost (executor driver) (1/2)
[INFO][2018-06-06 15:37:07,668][org.apache.spark.executor.Executor]Finished task 1.0 in stage 2.0 (TID 5). 1129 bytes result sent to driver
[INFO][2018-06-06 15:37:07,669][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 2.0 (TID 5) in 3332 ms on localhost (executor driver) (2/2)
[INFO][2018-06-06 15:37:07,669][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO][2018-06-06 15:37:07,670][org.apache.spark.scheduler.DAGScheduler]ShuffleMapStage 2 (mapPartitions at SalesDefectNetByNetCommunity.scala:107) finished in 3.335 s
[INFO][2018-06-06 15:37:07,670][org.apache.spark.scheduler.DAGScheduler]looking for newly runnable stages
[INFO][2018-06-06 15:37:07,671][org.apache.spark.scheduler.DAGScheduler]running: Set()
[INFO][2018-06-06 15:37:07,671][org.apache.spark.scheduler.DAGScheduler]waiting: Set(ResultStage 3)
[INFO][2018-06-06 15:37:07,672][org.apache.spark.scheduler.DAGScheduler]failed: Set()
[INFO][2018-06-06 15:37:07,677][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 3 (MapPartitionsRDD[14] at filter at SalesDefectNetByNetCommunity.scala:121), which has no missing parents
[INFO][2018-06-06 15:37:07,683][org.apache.spark.storage.memory.MemoryStore]Block broadcast_8 stored as values in memory (estimated size 4.6 KB, free 908.3 MB)
[INFO][2018-06-06 15:37:07,686][org.apache.spark.storage.memory.MemoryStore]Block broadcast_8_piece0 stored as bytes in memory (estimated size 2.6 KB, free 908.3 MB)
[INFO][2018-06-06 15:37:07,686][org.apache.spark.storage.BlockManagerInfo]Added broadcast_8_piece0 in memory on 10.194.32.157:55710 (size: 2.6 KB, free: 911.9 MB)
[INFO][2018-06-06 15:37:07,687][org.apache.spark.SparkContext]Created broadcast 8 from broadcast at DAGScheduler.scala:1006
[INFO][2018-06-06 15:37:07,688][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ResultStage 3 (MapPartitionsRDD[14] at filter at SalesDefectNetByNetCommunity.scala:121) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2018-06-06 15:37:07,688][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 3.0 with 2 tasks
[INFO][2018-06-06 15:37:07,689][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 3.0 (TID 6, localhost, executor driver, partition 0, ANY, 4621 bytes)
[INFO][2018-06-06 15:37:07,689][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 3.0 (TID 7, localhost, executor driver, partition 1, ANY, 4621 bytes)
[INFO][2018-06-06 15:37:07,690][org.apache.spark.executor.Executor]Running task 0.0 in stage 3.0 (TID 6)
[INFO][2018-06-06 15:37:07,690][org.apache.spark.executor.Executor]Running task 1.0 in stage 3.0 (TID 7)
[INFO][2018-06-06 15:37:07,708][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 2 blocks
[INFO][2018-06-06 15:37:07,708][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 2 blocks
[INFO][2018-06-06 15:37:07,710][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 5 ms
[INFO][2018-06-06 15:37:07,710][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 5 ms
[INFO][2018-06-06 15:37:07,773][org.apache.spark.storage.memory.MemoryStore]Block rdd_14_1 stored as values in memory (estimated size 152.0 B, free 908.3 MB)
[INFO][2018-06-06 15:37:07,774][org.apache.spark.storage.BlockManagerInfo]Added rdd_14_1 in memory on 10.194.32.157:55710 (size: 152.0 B, free: 911.9 MB)
[INFO][2018-06-06 15:37:07,779][org.apache.spark.storage.memory.MemoryStore]Block rdd_14_0 stored as values in memory (estimated size 216.0 B, free 908.3 MB)
[INFO][2018-06-06 15:37:07,779][org.apache.spark.storage.BlockManagerInfo]Added rdd_14_0 in memory on 10.194.32.157:55710 (size: 216.0 B, free: 911.9 MB)
[INFO][2018-06-06 15:37:07,792][org.apache.spark.executor.Executor]Finished task 1.0 in stage 3.0 (TID 7). 1750 bytes result sent to driver
[INFO][2018-06-06 15:37:07,792][org.apache.spark.executor.Executor]Finished task 0.0 in stage 3.0 (TID 6). 1750 bytes result sent to driver
[INFO][2018-06-06 15:37:07,793][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 3.0 (TID 7) in 104 ms on localhost (executor driver) (1/2)
[INFO][2018-06-06 15:37:07,793][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 3.0 (TID 6) in 105 ms on localhost (executor driver) (2/2)
[INFO][2018-06-06 15:37:07,793][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 3.0, whose tasks have all completed, from pool 
[INFO][2018-06-06 15:37:07,794][org.apache.spark.scheduler.DAGScheduler]ResultStage 3 (foreach at SalesDefectNetByNetCommunity.scala:126) finished in 0.106 s
[INFO][2018-06-06 15:37:07,795][org.apache.spark.scheduler.DAGScheduler]Job 2 finished: foreach at SalesDefectNetByNetCommunity.scala:126, took 3.501563 s
[INFO][2018-06-06 15:37:07,795][com.seven.spark.rdd.SalesDefectNetByNetCommunity$]SalesDefectNetByNetCommunity$ is success . . .
[INFO][2018-06-06 15:37:07,800][org.apache.spark.SparkContext]Invoking stop() from shutdown hook
[INFO][2018-06-06 15:37:07,809][org.spark_project.jetty.server.AbstractConnector]Stopped Spark@89c10b7{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-06-06 15:37:07,811][org.apache.spark.ui.SparkUI]Stopped Spark web UI at http://10.194.32.157:4040
[INFO][2018-06-06 15:37:07,822][org.apache.spark.MapOutputTrackerMasterEndpoint]MapOutputTrackerMasterEndpoint stopped!
[INFO][2018-06-06 15:37:07,842][org.apache.spark.storage.memory.MemoryStore]MemoryStore cleared
[INFO][2018-06-06 15:37:07,842][org.apache.spark.storage.BlockManager]BlockManager stopped
[INFO][2018-06-06 15:37:07,843][org.apache.spark.storage.BlockManagerMaster]BlockManagerMaster stopped
[INFO][2018-06-06 15:37:07,849][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint]OutputCommitCoordinator stopped!
[INFO][2018-06-06 15:37:07,851][org.apache.spark.SparkContext]Successfully stopped SparkContext
[INFO][2018-06-06 15:37:07,852][org.apache.spark.util.ShutdownHookManager]Shutdown hook called
[INFO][2018-06-06 15:37:07,853][org.apache.spark.util.ShutdownHookManager]Deleting directory /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/spark-c58132c1-39e0-4ac8-8314-cf23e4757032
[INFO][2018-06-06 15:37:52,649][org.apache.spark.SparkContext]Running Spark version 2.2.0
[INFO][2018-06-06 15:37:53,339][org.apache.spark.SparkContext]Submitted application: SalesDefectNetByNetCommunity$
[INFO][2018-06-06 15:37:53,366][org.apache.spark.SecurityManager]Changing view acls to: seven
[INFO][2018-06-06 15:37:53,366][org.apache.spark.SecurityManager]Changing modify acls to: seven
[INFO][2018-06-06 15:37:53,367][org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO][2018-06-06 15:37:53,368][org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO][2018-06-06 15:37:53,369][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(seven); groups with view permissions: Set(); users  with modify permissions: Set(seven); groups with modify permissions: Set()
[INFO][2018-06-06 15:37:53,808][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 55728.
[INFO][2018-06-06 15:37:53,835][org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO][2018-06-06 15:37:53,867][org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO][2018-06-06 15:37:53,872][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2018-06-06 15:37:53,873][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
[INFO][2018-06-06 15:37:53,897][org.apache.spark.storage.DiskBlockManager]Created local directory at /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/blockmgr-9d511f43-9ef7-4dd5-93df-04f4d7db952b
[INFO][2018-06-06 15:37:53,997][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 912.3 MB
[INFO][2018-06-06 15:37:54,097][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO][2018-06-06 15:37:54,205][org.spark_project.jetty.util.log]Logging initialized @3556ms
[INFO][2018-06-06 15:37:54,262][org.spark_project.jetty.server.Server]jetty-9.3.z-SNAPSHOT
[INFO][2018-06-06 15:37:54,275][org.spark_project.jetty.server.Server]Started @3627ms
[INFO][2018-06-06 15:37:54,306][org.spark_project.jetty.server.AbstractConnector]Started ServerConnector@40712ee9{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-06-06 15:37:54,306][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4040.
[INFO][2018-06-06 15:37:54,333][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5477a1ca{/jobs,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:37:54,334][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@700f518a{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:37:54,334][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@13da7ab0{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:37:54,335][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@77eb5790{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:37:54,336][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@319c3a25{/stages,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:37:54,337][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@ef1695a{/stages/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:37:54,337][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@81b5db0{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:37:54,341][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3e17a0a1{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:37:54,341][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4d8286c4{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:37:54,342][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@161f6623{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:37:54,343][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6778aea6{/storage,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:37:54,344][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@69228e85{/storage/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:37:54,344][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5853495b{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:37:54,345][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2f61d591{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:37:54,346][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7173ae5b{/environment,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:37:54,346][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@53a9fcfd{/environment/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:37:54,347][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4d192aef{/executors,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:37:54,348][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@84487f4{/executors/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:37:54,349][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@fb6097b{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:37:54,349][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1290c49{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:37:54,364][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@55d9b8f0{/static,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:37:54,365][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@62573c86{/,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:37:54,366][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@14229fa7{/api,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:37:54,367][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@e8e0dec{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:37:54,368][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@75fa1be3{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:37:54,371][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://10.194.32.157:4040
[INFO][2018-06-06 15:37:54,474][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
[INFO][2018-06-06 15:37:54,496][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 55729.
[INFO][2018-06-06 15:37:54,497][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 10.194.32.157:55729
[INFO][2018-06-06 15:37:54,499][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2018-06-06 15:37:54,500][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 10.194.32.157, 55729, None)
[INFO][2018-06-06 15:37:54,504][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 10.194.32.157:55729 with 912.3 MB RAM, BlockManagerId(driver, 10.194.32.157, 55729, None)
[INFO][2018-06-06 15:37:54,506][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 10.194.32.157, 55729, None)
[INFO][2018-06-06 15:37:54,507][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 10.194.32.157, 55729, None)
[INFO][2018-06-06 15:37:54,706][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@48f4713c{/metrics/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:37:54,742][com.seven.spark.rdd.SalesDefectNetByNetCommunity$]SalesDefectNetByNetCommunity$ is start . . .
[INFO][2018-06-06 15:37:55,289][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0 stored as values in memory (estimated size 228.1 KB, free 912.1 MB)
[INFO][2018-06-06 15:37:55,553][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0_piece0 stored as bytes in memory (estimated size 22.1 KB, free 912.1 MB)
[INFO][2018-06-06 15:37:55,558][org.apache.spark.storage.BlockManagerInfo]Added broadcast_0_piece0 in memory on 10.194.32.157:55729 (size: 22.1 KB, free: 912.3 MB)
[INFO][2018-06-06 15:37:55,566][org.apache.spark.SparkContext]Created broadcast 0 from textFile at SalesDefectNetByNetCommunity.scala:53
[WARN][2018-06-06 15:37:56,165][org.apache.hadoop.util.NativeCodeLoader]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO][2018-06-06 15:37:56,309][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 1
[INFO][2018-06-06 15:37:56,408][org.apache.spark.SparkContext]Starting job: collect at SalesDefectNetByNetCommunity.scala:64
[INFO][2018-06-06 15:37:56,421][org.apache.spark.scheduler.DAGScheduler]Got job 0 (collect at SalesDefectNetByNetCommunity.scala:64) with 2 output partitions
[INFO][2018-06-06 15:37:56,422][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 0 (collect at SalesDefectNetByNetCommunity.scala:64)
[INFO][2018-06-06 15:37:56,422][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-06-06 15:37:56,424][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-06-06 15:37:56,434][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 0 (MapPartitionsRDD[3] at mapPartitions at SalesDefectNetByNetCommunity.scala:56), which has no missing parents
[INFO][2018-06-06 15:37:56,460][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1 stored as values in memory (estimated size 3.7 KB, free 912.1 MB)
[INFO][2018-06-06 15:37:56,472][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.1 KB, free 912.1 MB)
[INFO][2018-06-06 15:37:56,473][org.apache.spark.storage.BlockManagerInfo]Added broadcast_1_piece0 in memory on 10.194.32.157:55729 (size: 2.1 KB, free: 912.3 MB)
[INFO][2018-06-06 15:37:56,474][org.apache.spark.SparkContext]Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO][2018-06-06 15:37:56,495][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at mapPartitions at SalesDefectNetByNetCommunity.scala:56) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2018-06-06 15:37:56,496][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 0.0 with 2 tasks
[INFO][2018-06-06 15:37:56,553][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 4930 bytes)
[INFO][2018-06-06 15:37:56,557][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 4930 bytes)
[INFO][2018-06-06 15:37:56,584][org.apache.spark.executor.Executor]Running task 1.0 in stage 0.0 (TID 1)
[INFO][2018-06-06 15:37:56,584][org.apache.spark.executor.Executor]Running task 0.0 in stage 0.0 (TID 0)
[INFO][2018-06-06 15:37:56,651][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/sta_vem/net_community/net_community__b4d64c39_5c6b_44c1_81be_f9aa19c55a30:0+278620
[INFO][2018-06-06 15:37:56,651][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/sta_vem/net_community/net_community__b4d64c39_5c6b_44c1_81be_f9aa19c55a30:278620+278620
[INFO][2018-06-06 15:37:56,928][org.apache.spark.executor.Executor]Finished task 1.0 in stage 0.0 (TID 1). 290507 bytes result sent to driver
[INFO][2018-06-06 15:37:56,933][org.apache.spark.executor.Executor]Finished task 0.0 in stage 0.0 (TID 0). 289675 bytes result sent to driver
[INFO][2018-06-06 15:37:56,958][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 0.0 (TID 0) in 421 ms on localhost (executor driver) (1/2)
[INFO][2018-06-06 15:37:56,959][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 0.0 (TID 1) in 403 ms on localhost (executor driver) (2/2)
[INFO][2018-06-06 15:37:56,961][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO][2018-06-06 15:37:56,966][org.apache.spark.scheduler.DAGScheduler]ResultStage 0 (collect at SalesDefectNetByNetCommunity.scala:64) finished in 0.446 s
[INFO][2018-06-06 15:37:56,973][org.apache.spark.scheduler.DAGScheduler]Job 0 finished: collect at SalesDefectNetByNetCommunity.scala:64, took 0.564976 s
[INFO][2018-06-06 15:37:57,026][org.apache.spark.storage.memory.MemoryStore]Block broadcast_2 stored as values in memory (estimated size 1060.4 KB, free 911.0 MB)
[INFO][2018-06-06 15:37:57,042][org.apache.spark.storage.memory.MemoryStore]Block broadcast_2_piece0 stored as bytes in memory (estimated size 203.6 KB, free 910.8 MB)
[INFO][2018-06-06 15:37:57,043][org.apache.spark.storage.BlockManagerInfo]Added broadcast_2_piece0 in memory on 10.194.32.157:55729 (size: 203.6 KB, free: 912.1 MB)
[INFO][2018-06-06 15:37:57,044][org.apache.spark.SparkContext]Created broadcast 2 from broadcast at SalesDefectNetByNetCommunity.scala:34
[INFO][2018-06-06 15:37:57,050][org.apache.spark.storage.memory.MemoryStore]Block broadcast_3 stored as values in memory (estimated size 228.1 KB, free 910.6 MB)
[INFO][2018-06-06 15:37:57,069][org.apache.spark.storage.memory.MemoryStore]Block broadcast_3_piece0 stored as bytes in memory (estimated size 22.1 KB, free 910.6 MB)
[INFO][2018-06-06 15:37:57,070][org.apache.spark.storage.BlockManagerInfo]Added broadcast_3_piece0 in memory on 10.194.32.157:55729 (size: 22.1 KB, free: 912.1 MB)
[INFO][2018-06-06 15:37:57,071][org.apache.spark.SparkContext]Created broadcast 3 from textFile at SalesDefectNetByNetCommunity.scala:79
[INFO][2018-06-06 15:37:57,176][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 1
[INFO][2018-06-06 15:37:57,195][org.apache.spark.SparkContext]Starting job: collect at SalesDefectNetByNetCommunity.scala:89
[INFO][2018-06-06 15:37:57,196][org.apache.spark.scheduler.DAGScheduler]Got job 1 (collect at SalesDefectNetByNetCommunity.scala:89) with 2 output partitions
[INFO][2018-06-06 15:37:57,196][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 1 (collect at SalesDefectNetByNetCommunity.scala:89)
[INFO][2018-06-06 15:37:57,196][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-06-06 15:37:57,196][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-06-06 15:37:57,197][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 1 (MapPartitionsRDD[7] at mapPartitions at SalesDefectNetByNetCommunity.scala:82), which has no missing parents
[INFO][2018-06-06 15:37:57,200][org.apache.spark.storage.memory.MemoryStore]Block broadcast_4 stored as values in memory (estimated size 3.7 KB, free 910.6 MB)
[INFO][2018-06-06 15:37:57,211][org.apache.spark.storage.memory.MemoryStore]Block broadcast_4_piece0 stored as bytes in memory (estimated size 2.0 KB, free 910.6 MB)
[INFO][2018-06-06 15:37:57,211][org.apache.spark.storage.BlockManagerInfo]Added broadcast_4_piece0 in memory on 10.194.32.157:55729 (size: 2.0 KB, free: 912.1 MB)
[INFO][2018-06-06 15:37:57,212][org.apache.spark.SparkContext]Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[INFO][2018-06-06 15:37:57,213][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[7] at mapPartitions at SalesDefectNetByNetCommunity.scala:82) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2018-06-06 15:37:57,213][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 1.0 with 2 tasks
[INFO][2018-06-06 15:37:57,214][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 1.0 (TID 2, localhost, executor driver, partition 0, ANY, 4926 bytes)
[INFO][2018-06-06 15:37:57,215][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 1.0 (TID 3, localhost, executor driver, partition 1, ANY, 4926 bytes)
[INFO][2018-06-06 15:37:57,216][org.apache.spark.executor.Executor]Running task 0.0 in stage 1.0 (TID 2)
[INFO][2018-06-06 15:37:57,216][org.apache.spark.executor.Executor]Running task 1.0 in stage 1.0 (TID 3)
[INFO][2018-06-06 15:37:57,222][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/sta_vem/vem_nettype/vem_nettype__5b80085f_0bae_4bd8_97d4_919813b3ce8b:6079084+6079084
[INFO][2018-06-06 15:37:57,222][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/sta_vem/vem_nettype/vem_nettype__5b80085f_0bae_4bd8_97d4_919813b3ce8b:0+6079084
[INFO][2018-06-06 15:38:01,171][org.apache.spark.executor.Executor]Finished task 1.0 in stage 1.0 (TID 3). 14530 bytes result sent to driver
[INFO][2018-06-06 15:38:01,188][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 1.0 (TID 3) in 3973 ms on localhost (executor driver) (1/2)
[INFO][2018-06-06 15:38:01,773][org.apache.spark.executor.Executor]Finished task 0.0 in stage 1.0 (TID 2). 240458 bytes result sent to driver
[INFO][2018-06-06 15:38:01,803][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 1.0 (TID 2) in 4588 ms on localhost (executor driver) (2/2)
[INFO][2018-06-06 15:38:01,803][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO][2018-06-06 15:38:01,804][org.apache.spark.scheduler.DAGScheduler]ResultStage 1 (collect at SalesDefectNetByNetCommunity.scala:89) finished in 4.590 s
[INFO][2018-06-06 15:38:01,805][org.apache.spark.scheduler.DAGScheduler]Job 1 finished: collect at SalesDefectNetByNetCommunity.scala:89, took 4.609688 s
[INFO][2018-06-06 15:38:01,872][org.apache.spark.storage.memory.MemoryStore]Block broadcast_5 stored as values in memory (estimated size 1924.3 KB, free 908.7 MB)
[INFO][2018-06-06 15:38:01,894][org.apache.spark.storage.memory.MemoryStore]Block broadcast_5_piece0 stored as bytes in memory (estimated size 111.1 KB, free 908.6 MB)
[INFO][2018-06-06 15:38:01,895][org.apache.spark.storage.BlockManagerInfo]Added broadcast_5_piece0 in memory on 10.194.32.157:55729 (size: 111.1 KB, free: 911.9 MB)
[INFO][2018-06-06 15:38:01,895][org.apache.spark.SparkContext]Created broadcast 5 from broadcast at SalesDefectNetByNetCommunity.scala:38
[INFO][2018-06-06 15:38:23,720][org.apache.spark.storage.memory.MemoryStore]Block broadcast_6 stored as values in memory (estimated size 228.1 KB, free 908.4 MB)
[INFO][2018-06-06 15:38:23,745][org.apache.spark.storage.memory.MemoryStore]Block broadcast_6_piece0 stored as bytes in memory (estimated size 22.1 KB, free 908.3 MB)
[INFO][2018-06-06 15:38:23,746][org.apache.spark.storage.BlockManagerInfo]Added broadcast_6_piece0 in memory on 10.194.32.157:55729 (size: 22.1 KB, free: 911.9 MB)
[INFO][2018-06-06 15:38:23,747][org.apache.spark.SparkContext]Created broadcast 6 from textFile at SalesDefectNetByNetCommunity.scala:104
[INFO][2018-06-06 15:38:23,794][org.apache.spark.SparkContext]Invoking stop() from shutdown hook
[INFO][2018-06-06 15:38:23,802][org.spark_project.jetty.server.AbstractConnector]Stopped Spark@40712ee9{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-06-06 15:38:23,805][org.apache.spark.ui.SparkUI]Stopped Spark web UI at http://10.194.32.157:4040
[INFO][2018-06-06 15:38:23,811][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 1
[INFO][2018-06-06 15:38:23,819][org.apache.spark.MapOutputTrackerMasterEndpoint]MapOutputTrackerMasterEndpoint stopped!
[INFO][2018-06-06 15:38:23,840][org.apache.spark.storage.memory.MemoryStore]MemoryStore cleared
[INFO][2018-06-06 15:38:23,841][org.apache.spark.storage.BlockManager]BlockManager stopped
[INFO][2018-06-06 15:38:23,842][org.apache.spark.storage.BlockManagerMaster]BlockManagerMaster stopped
[INFO][2018-06-06 15:38:23,845][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint]OutputCommitCoordinator stopped!
[INFO][2018-06-06 15:38:23,846][org.apache.spark.SparkContext]Successfully stopped SparkContext
[INFO][2018-06-06 15:38:23,847][org.apache.spark.util.ShutdownHookManager]Shutdown hook called
[INFO][2018-06-06 15:38:23,848][org.apache.spark.util.ShutdownHookManager]Deleting directory /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/spark-d09687b0-a71d-4da0-a89a-355227bf6bae
[INFO][2018-06-06 15:38:37,084][org.apache.spark.SparkContext]Running Spark version 2.2.0
[INFO][2018-06-06 15:38:38,144][org.apache.spark.SparkContext]Submitted application: SalesDefectNetByNetCommunity$
[INFO][2018-06-06 15:38:38,169][org.apache.spark.SecurityManager]Changing view acls to: seven
[INFO][2018-06-06 15:38:38,169][org.apache.spark.SecurityManager]Changing modify acls to: seven
[INFO][2018-06-06 15:38:38,170][org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO][2018-06-06 15:38:38,171][org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO][2018-06-06 15:38:38,172][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(seven); groups with view permissions: Set(); users  with modify permissions: Set(seven); groups with modify permissions: Set()
[INFO][2018-06-06 15:38:38,544][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 55742.
[INFO][2018-06-06 15:38:38,570][org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO][2018-06-06 15:38:38,595][org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO][2018-06-06 15:38:38,599][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2018-06-06 15:38:38,599][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
[INFO][2018-06-06 15:38:38,609][org.apache.spark.storage.DiskBlockManager]Created local directory at /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/blockmgr-951827a2-0dbb-4465-823f-4dca5a43cb0a
[INFO][2018-06-06 15:38:38,670][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 912.3 MB
[INFO][2018-06-06 15:38:38,735][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO][2018-06-06 15:38:38,915][org.spark_project.jetty.util.log]Logging initialized @3311ms
[INFO][2018-06-06 15:38:38,979][org.spark_project.jetty.server.Server]jetty-9.3.z-SNAPSHOT
[INFO][2018-06-06 15:38:38,994][org.spark_project.jetty.server.Server]Started @3392ms
[INFO][2018-06-06 15:38:39,034][org.spark_project.jetty.server.AbstractConnector]Started ServerConnector@2dd8239{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-06-06 15:38:39,034][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4040.
[INFO][2018-06-06 15:38:39,070][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@61a1ea2c{/jobs,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:38:39,075][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@10cd6753{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:38:39,075][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@47af099e{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:38:39,078][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2c8662ac{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:38:39,080][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3724b43e{/stages,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:38:39,082][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@68e7c8c3{/stages/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:38:39,083][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@238bfd6c{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:38:39,088][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@199bc830{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:38:39,098][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@27b45ea{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:38:39,099][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@790a251b{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:38:39,100][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@150ede8b{/storage,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:38:39,101][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3e15bb06{/storage/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:38:39,105][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4e1ce44{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:38:39,112][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7a7cc52c{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:38:39,113][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@524a2ffb{/environment,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:38:39,114][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@332820f4{/environment/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:38:39,116][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@72456279{/executors,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:38:39,116][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@21f459fc{/executors/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:38:39,124][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1416cf9f{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:38:39,126][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@bfc14b9{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:38:39,135][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2dfe5525{/static,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:38:39,136][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5c534b5b{/,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:38:39,138][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2b22a1cc{/api,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:38:39,139][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@10f19647{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:38:39,140][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3936df72{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:38:39,143][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://10.194.32.157:4040
[INFO][2018-06-06 15:38:39,308][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
[INFO][2018-06-06 15:38:39,343][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 55743.
[INFO][2018-06-06 15:38:39,348][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 10.194.32.157:55743
[INFO][2018-06-06 15:38:39,352][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2018-06-06 15:38:39,354][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 10.194.32.157, 55743, None)
[INFO][2018-06-06 15:38:39,359][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 10.194.32.157:55743 with 912.3 MB RAM, BlockManagerId(driver, 10.194.32.157, 55743, None)
[INFO][2018-06-06 15:38:39,370][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 10.194.32.157, 55743, None)
[INFO][2018-06-06 15:38:39,370][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 10.194.32.157, 55743, None)
[INFO][2018-06-06 15:38:39,650][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1dbd580{/metrics/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:38:39,686][com.seven.spark.rdd.SalesDefectNetByNetCommunity$]SalesDefectNetByNetCommunity$ is start . . .
[INFO][2018-06-06 15:38:40,649][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0 stored as values in memory (estimated size 228.1 KB, free 912.1 MB)
[INFO][2018-06-06 15:38:40,912][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0_piece0 stored as bytes in memory (estimated size 22.1 KB, free 912.1 MB)
[INFO][2018-06-06 15:38:40,915][org.apache.spark.storage.BlockManagerInfo]Added broadcast_0_piece0 in memory on 10.194.32.157:55743 (size: 22.1 KB, free: 912.3 MB)
[INFO][2018-06-06 15:38:40,947][org.apache.spark.SparkContext]Created broadcast 0 from textFile at SalesDefectNetByNetCommunity.scala:53
[WARN][2018-06-06 15:38:42,489][org.apache.hadoop.util.NativeCodeLoader]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO][2018-06-06 15:38:42,632][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 1
[INFO][2018-06-06 15:38:42,729][org.apache.spark.SparkContext]Starting job: collect at SalesDefectNetByNetCommunity.scala:64
[INFO][2018-06-06 15:38:42,743][org.apache.spark.scheduler.DAGScheduler]Got job 0 (collect at SalesDefectNetByNetCommunity.scala:64) with 2 output partitions
[INFO][2018-06-06 15:38:42,743][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 0 (collect at SalesDefectNetByNetCommunity.scala:64)
[INFO][2018-06-06 15:38:42,744][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-06-06 15:38:42,745][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-06-06 15:38:42,754][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 0 (MapPartitionsRDD[3] at mapPartitions at SalesDefectNetByNetCommunity.scala:56), which has no missing parents
[INFO][2018-06-06 15:38:42,778][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1 stored as values in memory (estimated size 3.7 KB, free 912.1 MB)
[INFO][2018-06-06 15:38:42,789][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.1 KB, free 912.1 MB)
[INFO][2018-06-06 15:38:42,790][org.apache.spark.storage.BlockManagerInfo]Added broadcast_1_piece0 in memory on 10.194.32.157:55743 (size: 2.1 KB, free: 912.3 MB)
[INFO][2018-06-06 15:38:42,791][org.apache.spark.SparkContext]Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO][2018-06-06 15:38:42,809][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at mapPartitions at SalesDefectNetByNetCommunity.scala:56) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2018-06-06 15:38:42,810][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 0.0 with 2 tasks
[INFO][2018-06-06 15:38:42,857][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 4930 bytes)
[INFO][2018-06-06 15:38:42,860][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 4930 bytes)
[INFO][2018-06-06 15:38:42,868][org.apache.spark.executor.Executor]Running task 0.0 in stage 0.0 (TID 0)
[INFO][2018-06-06 15:38:42,868][org.apache.spark.executor.Executor]Running task 1.0 in stage 0.0 (TID 1)
[INFO][2018-06-06 15:38:42,939][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/sta_vem/net_community/net_community__b4d64c39_5c6b_44c1_81be_f9aa19c55a30:278620+278620
[INFO][2018-06-06 15:38:42,940][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/sta_vem/net_community/net_community__b4d64c39_5c6b_44c1_81be_f9aa19c55a30:0+278620
[INFO][2018-06-06 15:38:43,238][org.apache.spark.executor.Executor]Finished task 1.0 in stage 0.0 (TID 1). 290550 bytes result sent to driver
[INFO][2018-06-06 15:38:43,266][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 0.0 (TID 1) in 403 ms on localhost (executor driver) (1/2)
[INFO][2018-06-06 15:38:43,285][org.apache.spark.executor.Executor]Finished task 0.0 in stage 0.0 (TID 0). 289718 bytes result sent to driver
[INFO][2018-06-06 15:38:43,302][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 0.0 (TID 0) in 459 ms on localhost (executor driver) (2/2)
[INFO][2018-06-06 15:38:43,303][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO][2018-06-06 15:38:43,304][org.apache.spark.scheduler.DAGScheduler]ResultStage 0 (collect at SalesDefectNetByNetCommunity.scala:64) finished in 0.473 s
[INFO][2018-06-06 15:38:43,312][org.apache.spark.scheduler.DAGScheduler]Job 0 finished: collect at SalesDefectNetByNetCommunity.scala:64, took 0.581790 s
[INFO][2018-06-06 15:38:43,365][org.apache.spark.storage.memory.MemoryStore]Block broadcast_2 stored as values in memory (estimated size 1060.4 KB, free 911.0 MB)
[INFO][2018-06-06 15:38:43,384][org.apache.spark.storage.memory.MemoryStore]Block broadcast_2_piece0 stored as bytes in memory (estimated size 203.6 KB, free 910.8 MB)
[INFO][2018-06-06 15:38:43,386][org.apache.spark.storage.BlockManagerInfo]Added broadcast_2_piece0 in memory on 10.194.32.157:55743 (size: 203.6 KB, free: 912.1 MB)
[INFO][2018-06-06 15:38:43,387][org.apache.spark.SparkContext]Created broadcast 2 from broadcast at SalesDefectNetByNetCommunity.scala:34
[INFO][2018-06-06 15:38:43,395][org.apache.spark.storage.memory.MemoryStore]Block broadcast_3 stored as values in memory (estimated size 228.1 KB, free 910.6 MB)
[INFO][2018-06-06 15:38:43,431][org.apache.spark.storage.memory.MemoryStore]Block broadcast_3_piece0 stored as bytes in memory (estimated size 22.1 KB, free 910.6 MB)
[INFO][2018-06-06 15:38:43,432][org.apache.spark.storage.BlockManagerInfo]Added broadcast_3_piece0 in memory on 10.194.32.157:55743 (size: 22.1 KB, free: 912.1 MB)
[INFO][2018-06-06 15:38:43,433][org.apache.spark.SparkContext]Created broadcast 3 from textFile at SalesDefectNetByNetCommunity.scala:79
[INFO][2018-06-06 15:38:43,596][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 1
[INFO][2018-06-06 15:38:43,619][org.apache.spark.SparkContext]Starting job: collect at SalesDefectNetByNetCommunity.scala:89
[INFO][2018-06-06 15:38:43,621][org.apache.spark.scheduler.DAGScheduler]Got job 1 (collect at SalesDefectNetByNetCommunity.scala:89) with 2 output partitions
[INFO][2018-06-06 15:38:43,621][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 1 (collect at SalesDefectNetByNetCommunity.scala:89)
[INFO][2018-06-06 15:38:43,621][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-06-06 15:38:43,621][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-06-06 15:38:43,622][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 1 (MapPartitionsRDD[7] at mapPartitions at SalesDefectNetByNetCommunity.scala:82), which has no missing parents
[INFO][2018-06-06 15:38:43,627][org.apache.spark.storage.memory.MemoryStore]Block broadcast_4 stored as values in memory (estimated size 3.7 KB, free 910.6 MB)
[INFO][2018-06-06 15:38:43,633][org.apache.spark.storage.memory.MemoryStore]Block broadcast_4_piece0 stored as bytes in memory (estimated size 2.0 KB, free 910.6 MB)
[INFO][2018-06-06 15:38:43,634][org.apache.spark.storage.BlockManagerInfo]Added broadcast_4_piece0 in memory on 10.194.32.157:55743 (size: 2.0 KB, free: 912.1 MB)
[INFO][2018-06-06 15:38:43,635][org.apache.spark.SparkContext]Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[INFO][2018-06-06 15:38:43,636][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[7] at mapPartitions at SalesDefectNetByNetCommunity.scala:82) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2018-06-06 15:38:43,636][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 1.0 with 2 tasks
[INFO][2018-06-06 15:38:43,637][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 1.0 (TID 2, localhost, executor driver, partition 0, ANY, 4926 bytes)
[INFO][2018-06-06 15:38:43,638][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 1.0 (TID 3, localhost, executor driver, partition 1, ANY, 4926 bytes)
[INFO][2018-06-06 15:38:43,638][org.apache.spark.executor.Executor]Running task 0.0 in stage 1.0 (TID 2)
[INFO][2018-06-06 15:38:43,638][org.apache.spark.executor.Executor]Running task 1.0 in stage 1.0 (TID 3)
[INFO][2018-06-06 15:38:43,643][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/sta_vem/vem_nettype/vem_nettype__5b80085f_0bae_4bd8_97d4_919813b3ce8b:0+6079084
[INFO][2018-06-06 15:38:43,643][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/sta_vem/vem_nettype/vem_nettype__5b80085f_0bae_4bd8_97d4_919813b3ce8b:6079084+6079084
[INFO][2018-06-06 15:38:48,437][org.apache.spark.executor.Executor]Finished task 1.0 in stage 1.0 (TID 3). 14530 bytes result sent to driver
[INFO][2018-06-06 15:38:48,452][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 1.0 (TID 3) in 4814 ms on localhost (executor driver) (1/2)
[INFO][2018-06-06 15:38:49,119][org.apache.spark.executor.Executor]Finished task 0.0 in stage 1.0 (TID 2). 240458 bytes result sent to driver
[INFO][2018-06-06 15:38:49,136][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 1.0 (TID 2) in 5499 ms on localhost (executor driver) (2/2)
[INFO][2018-06-06 15:38:49,137][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO][2018-06-06 15:38:49,137][org.apache.spark.scheduler.DAGScheduler]ResultStage 1 (collect at SalesDefectNetByNetCommunity.scala:89) finished in 5.500 s
[INFO][2018-06-06 15:38:49,138][org.apache.spark.scheduler.DAGScheduler]Job 1 finished: collect at SalesDefectNetByNetCommunity.scala:89, took 5.518829 s
[INFO][2018-06-06 15:38:49,207][org.apache.spark.storage.memory.MemoryStore]Block broadcast_5 stored as values in memory (estimated size 1924.3 KB, free 908.7 MB)
[INFO][2018-06-06 15:38:49,232][org.apache.spark.storage.memory.MemoryStore]Block broadcast_5_piece0 stored as bytes in memory (estimated size 111.1 KB, free 908.6 MB)
[INFO][2018-06-06 15:38:49,233][org.apache.spark.storage.BlockManagerInfo]Added broadcast_5_piece0 in memory on 10.194.32.157:55743 (size: 111.1 KB, free: 911.9 MB)
[INFO][2018-06-06 15:38:49,234][org.apache.spark.SparkContext]Created broadcast 5 from broadcast at SalesDefectNetByNetCommunity.scala:38
[WARN][2018-06-06 15:39:24,788][org.apache.spark.executor.Executor]Issue communicating with driver in heartbeater
org.apache.spark.rpc.RpcTimeoutException: Futures timed out after [10 seconds]. This timeout is controlled by spark.executor.heartbeatInterval
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:76)
	at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:92)
	at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$reportHeartBeat(Executor.scala:726)
	at org.apache.spark.executor.Executor$$anon$2$$anonfun$run$1.apply$mcV$sp(Executor.scala:755)
	at org.apache.spark.executor.Executor$$anon$2$$anonfun$run$1.apply(Executor.scala:755)
	at org.apache.spark.executor.Executor$$anon$2$$anonfun$run$1.apply(Executor.scala:755)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1954)
	at org.apache.spark.executor.Executor$$anon$2.run(Executor.scala:755)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.runAndReset$$$capture(FutureTask.java:308)
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.TimeoutException: Futures timed out after [10 seconds]
	at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:223)
	at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:227)
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:201)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
	... 15 more
[WARN][2018-06-06 15:39:24,832][org.apache.spark.rpc.netty.NettyRpcEnv]Ignored message: HeartbeatResponse(false)
[INFO][2018-06-06 15:39:33,729][org.apache.spark.storage.memory.MemoryStore]Block broadcast_6 stored as values in memory (estimated size 228.1 KB, free 908.4 MB)
[INFO][2018-06-06 15:39:33,756][org.apache.spark.storage.memory.MemoryStore]Block broadcast_6_piece0 stored as bytes in memory (estimated size 22.1 KB, free 908.3 MB)
[INFO][2018-06-06 15:39:33,757][org.apache.spark.storage.BlockManagerInfo]Added broadcast_6_piece0 in memory on 10.194.32.157:55743 (size: 22.1 KB, free: 911.9 MB)
[INFO][2018-06-06 15:39:33,757][org.apache.spark.SparkContext]Created broadcast 6 from textFile at SalesDefectNetByNetCommunity.scala:104
[INFO][2018-06-06 15:39:33,781][org.apache.spark.SparkContext]Invoking stop() from shutdown hook
[INFO][2018-06-06 15:39:33,805][org.spark_project.jetty.server.AbstractConnector]Stopped Spark@2dd8239{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-06-06 15:39:33,808][org.apache.spark.ui.SparkUI]Stopped Spark web UI at http://10.194.32.157:4040
[INFO][2018-06-06 15:39:33,831][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 1
[INFO][2018-06-06 15:39:33,840][org.apache.spark.MapOutputTrackerMasterEndpoint]MapOutputTrackerMasterEndpoint stopped!
[INFO][2018-06-06 15:39:33,858][org.apache.spark.storage.memory.MemoryStore]MemoryStore cleared
[INFO][2018-06-06 15:39:33,859][org.apache.spark.storage.BlockManager]BlockManager stopped
[INFO][2018-06-06 15:39:33,860][org.apache.spark.storage.BlockManagerMaster]BlockManagerMaster stopped
[INFO][2018-06-06 15:39:33,863][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint]OutputCommitCoordinator stopped!
[INFO][2018-06-06 15:39:33,874][org.apache.spark.SparkContext]Successfully stopped SparkContext
[INFO][2018-06-06 15:39:33,874][org.apache.spark.util.ShutdownHookManager]Shutdown hook called
[INFO][2018-06-06 15:39:33,875][org.apache.spark.util.ShutdownHookManager]Deleting directory /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/spark-3770e770-cac7-4e17-bc1d-6e8c292a4ad9
[INFO][2018-06-06 15:39:56,161][org.apache.spark.SparkContext]Running Spark version 2.2.0
[INFO][2018-06-06 15:39:57,260][org.apache.spark.SparkContext]Submitted application: SalesDefectNetByNetCommunity$
[INFO][2018-06-06 15:39:57,289][org.apache.spark.SecurityManager]Changing view acls to: seven
[INFO][2018-06-06 15:39:57,290][org.apache.spark.SecurityManager]Changing modify acls to: seven
[INFO][2018-06-06 15:39:57,291][org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO][2018-06-06 15:39:57,291][org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO][2018-06-06 15:39:57,292][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(seven); groups with view permissions: Set(); users  with modify permissions: Set(seven); groups with modify permissions: Set()
[INFO][2018-06-06 15:39:57,656][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 55764.
[INFO][2018-06-06 15:39:57,677][org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO][2018-06-06 15:39:57,705][org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO][2018-06-06 15:39:57,707][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2018-06-06 15:39:57,708][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
[INFO][2018-06-06 15:39:57,717][org.apache.spark.storage.DiskBlockManager]Created local directory at /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/blockmgr-74a2e583-39ac-497a-9071-f9e1a03b354b
[INFO][2018-06-06 15:39:57,771][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 912.3 MB
[INFO][2018-06-06 15:39:57,830][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO][2018-06-06 15:39:57,922][org.spark_project.jetty.util.log]Logging initialized @3301ms
[INFO][2018-06-06 15:39:57,978][org.spark_project.jetty.server.Server]jetty-9.3.z-SNAPSHOT
[INFO][2018-06-06 15:39:57,992][org.spark_project.jetty.server.Server]Started @3372ms
[INFO][2018-06-06 15:39:58,020][org.spark_project.jetty.server.AbstractConnector]Started ServerConnector@2dd8239{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-06-06 15:39:58,021][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4040.
[INFO][2018-06-06 15:39:58,046][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@61a1ea2c{/jobs,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:39:58,047][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@10cd6753{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:39:58,047][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@47af099e{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:39:58,049][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2c8662ac{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:39:58,049][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3724b43e{/stages,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:39:58,050][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@68e7c8c3{/stages/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:39:58,051][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@238bfd6c{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:39:58,053][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@199bc830{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:39:58,054][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@27b45ea{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:39:58,055][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@790a251b{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:39:58,055][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@150ede8b{/storage,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:39:58,056][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3e15bb06{/storage/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:39:58,057][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4e1ce44{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:39:58,057][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7a7cc52c{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:39:58,058][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@524a2ffb{/environment,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:39:58,059][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@332820f4{/environment/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:39:58,060][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@72456279{/executors,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:39:58,060][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@21f459fc{/executors/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:39:58,061][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1416cf9f{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:39:58,062][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@bfc14b9{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:39:58,072][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2dfe5525{/static,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:39:58,073][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5c534b5b{/,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:39:58,074][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2b22a1cc{/api,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:39:58,075][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@10f19647{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:39:58,078][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3936df72{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:39:58,081][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://10.194.32.157:4040
[INFO][2018-06-06 15:39:58,217][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
[INFO][2018-06-06 15:39:58,246][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 55765.
[INFO][2018-06-06 15:39:58,247][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 10.194.32.157:55765
[INFO][2018-06-06 15:39:58,250][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2018-06-06 15:39:58,253][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 10.194.32.157, 55765, None)
[INFO][2018-06-06 15:39:58,257][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 10.194.32.157:55765 with 912.3 MB RAM, BlockManagerId(driver, 10.194.32.157, 55765, None)
[INFO][2018-06-06 15:39:58,262][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 10.194.32.157, 55765, None)
[INFO][2018-06-06 15:39:58,263][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 10.194.32.157, 55765, None)
[INFO][2018-06-06 15:39:58,479][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@767f6ee7{/metrics/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:39:58,503][com.seven.spark.rdd.SalesDefectNetByNetCommunity$]SalesDefectNetByNetCommunity$ is start . . .
[INFO][2018-06-06 15:39:59,109][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0 stored as values in memory (estimated size 228.1 KB, free 912.1 MB)
[INFO][2018-06-06 15:39:59,405][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0_piece0 stored as bytes in memory (estimated size 22.1 KB, free 912.1 MB)
[INFO][2018-06-06 15:39:59,407][org.apache.spark.storage.BlockManagerInfo]Added broadcast_0_piece0 in memory on 10.194.32.157:55765 (size: 22.1 KB, free: 912.3 MB)
[INFO][2018-06-06 15:39:59,412][org.apache.spark.SparkContext]Created broadcast 0 from textFile at SalesDefectNetByNetCommunity.scala:53
[WARN][2018-06-06 15:40:01,023][org.apache.hadoop.util.NativeCodeLoader]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO][2018-06-06 15:40:01,179][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 1
[INFO][2018-06-06 15:40:01,275][org.apache.spark.SparkContext]Starting job: collect at SalesDefectNetByNetCommunity.scala:64
[INFO][2018-06-06 15:40:01,290][org.apache.spark.scheduler.DAGScheduler]Got job 0 (collect at SalesDefectNetByNetCommunity.scala:64) with 2 output partitions
[INFO][2018-06-06 15:40:01,290][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 0 (collect at SalesDefectNetByNetCommunity.scala:64)
[INFO][2018-06-06 15:40:01,291][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-06-06 15:40:01,293][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-06-06 15:40:01,301][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 0 (MapPartitionsRDD[3] at mapPartitions at SalesDefectNetByNetCommunity.scala:56), which has no missing parents
[INFO][2018-06-06 15:40:01,333][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1 stored as values in memory (estimated size 3.7 KB, free 912.1 MB)
[INFO][2018-06-06 15:40:01,343][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.1 KB, free 912.1 MB)
[INFO][2018-06-06 15:40:01,345][org.apache.spark.storage.BlockManagerInfo]Added broadcast_1_piece0 in memory on 10.194.32.157:55765 (size: 2.1 KB, free: 912.3 MB)
[INFO][2018-06-06 15:40:01,346][org.apache.spark.SparkContext]Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO][2018-06-06 15:40:01,369][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at mapPartitions at SalesDefectNetByNetCommunity.scala:56) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2018-06-06 15:40:01,371][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 0.0 with 2 tasks
[INFO][2018-06-06 15:40:01,421][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 4930 bytes)
[INFO][2018-06-06 15:40:01,423][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 4930 bytes)
[INFO][2018-06-06 15:40:01,432][org.apache.spark.executor.Executor]Running task 0.0 in stage 0.0 (TID 0)
[INFO][2018-06-06 15:40:01,432][org.apache.spark.executor.Executor]Running task 1.0 in stage 0.0 (TID 1)
[INFO][2018-06-06 15:40:01,515][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/sta_vem/net_community/net_community__b4d64c39_5c6b_44c1_81be_f9aa19c55a30:278620+278620
[INFO][2018-06-06 15:40:01,515][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/sta_vem/net_community/net_community__b4d64c39_5c6b_44c1_81be_f9aa19c55a30:0+278620
[INFO][2018-06-06 15:40:01,861][org.apache.spark.executor.Executor]Finished task 1.0 in stage 0.0 (TID 1). 290550 bytes result sent to driver
[INFO][2018-06-06 15:40:01,894][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 0.0 (TID 1) in 469 ms on localhost (executor driver) (1/2)
[INFO][2018-06-06 15:40:02,016][org.apache.spark.executor.Executor]Finished task 0.0 in stage 0.0 (TID 0). 289718 bytes result sent to driver
[INFO][2018-06-06 15:40:02,027][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 0.0 (TID 0) in 622 ms on localhost (executor driver) (2/2)
[INFO][2018-06-06 15:40:02,029][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO][2018-06-06 15:40:02,031][org.apache.spark.scheduler.DAGScheduler]ResultStage 0 (collect at SalesDefectNetByNetCommunity.scala:64) finished in 0.637 s
[INFO][2018-06-06 15:40:02,038][org.apache.spark.scheduler.DAGScheduler]Job 0 finished: collect at SalesDefectNetByNetCommunity.scala:64, took 0.762583 s
[INFO][2018-06-06 15:40:02,136][org.apache.spark.storage.memory.MemoryStore]Block broadcast_2 stored as values in memory (estimated size 1060.4 KB, free 911.0 MB)
[INFO][2018-06-06 15:40:02,153][org.apache.spark.storage.memory.MemoryStore]Block broadcast_2_piece0 stored as bytes in memory (estimated size 203.6 KB, free 910.8 MB)
[INFO][2018-06-06 15:40:02,155][org.apache.spark.storage.BlockManagerInfo]Added broadcast_2_piece0 in memory on 10.194.32.157:55765 (size: 203.6 KB, free: 912.1 MB)
[INFO][2018-06-06 15:40:02,155][org.apache.spark.SparkContext]Created broadcast 2 from broadcast at SalesDefectNetByNetCommunity.scala:34
[INFO][2018-06-06 15:40:02,175][org.apache.spark.storage.memory.MemoryStore]Block broadcast_3 stored as values in memory (estimated size 228.1 KB, free 910.6 MB)
[INFO][2018-06-06 15:40:02,201][org.apache.spark.storage.memory.MemoryStore]Block broadcast_3_piece0 stored as bytes in memory (estimated size 22.1 KB, free 910.6 MB)
[INFO][2018-06-06 15:40:02,202][org.apache.spark.storage.BlockManagerInfo]Added broadcast_3_piece0 in memory on 10.194.32.157:55765 (size: 22.1 KB, free: 912.1 MB)
[INFO][2018-06-06 15:40:02,203][org.apache.spark.SparkContext]Created broadcast 3 from textFile at SalesDefectNetByNetCommunity.scala:79
[INFO][2018-06-06 15:40:02,370][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 1
[INFO][2018-06-06 15:40:02,398][org.apache.spark.SparkContext]Starting job: collect at SalesDefectNetByNetCommunity.scala:89
[INFO][2018-06-06 15:40:02,403][org.apache.spark.scheduler.DAGScheduler]Got job 1 (collect at SalesDefectNetByNetCommunity.scala:89) with 2 output partitions
[INFO][2018-06-06 15:40:02,404][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 1 (collect at SalesDefectNetByNetCommunity.scala:89)
[INFO][2018-06-06 15:40:02,404][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-06-06 15:40:02,404][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-06-06 15:40:02,406][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 1 (MapPartitionsRDD[7] at mapPartitions at SalesDefectNetByNetCommunity.scala:82), which has no missing parents
[INFO][2018-06-06 15:40:02,412][org.apache.spark.storage.memory.MemoryStore]Block broadcast_4 stored as values in memory (estimated size 3.7 KB, free 910.6 MB)
[INFO][2018-06-06 15:40:02,427][org.apache.spark.storage.memory.MemoryStore]Block broadcast_4_piece0 stored as bytes in memory (estimated size 2.0 KB, free 910.6 MB)
[INFO][2018-06-06 15:40:02,428][org.apache.spark.storage.BlockManagerInfo]Added broadcast_4_piece0 in memory on 10.194.32.157:55765 (size: 2.0 KB, free: 912.1 MB)
[INFO][2018-06-06 15:40:02,428][org.apache.spark.SparkContext]Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[INFO][2018-06-06 15:40:02,430][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[7] at mapPartitions at SalesDefectNetByNetCommunity.scala:82) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2018-06-06 15:40:02,431][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 1.0 with 2 tasks
[INFO][2018-06-06 15:40:02,432][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 1.0 (TID 2, localhost, executor driver, partition 0, ANY, 4926 bytes)
[INFO][2018-06-06 15:40:02,433][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 1.0 (TID 3, localhost, executor driver, partition 1, ANY, 4926 bytes)
[INFO][2018-06-06 15:40:02,433][org.apache.spark.executor.Executor]Running task 0.0 in stage 1.0 (TID 2)
[INFO][2018-06-06 15:40:02,433][org.apache.spark.executor.Executor]Running task 1.0 in stage 1.0 (TID 3)
[INFO][2018-06-06 15:40:02,440][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/sta_vem/vem_nettype/vem_nettype__5b80085f_0bae_4bd8_97d4_919813b3ce8b:0+6079084
[INFO][2018-06-06 15:40:02,444][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/sta_vem/vem_nettype/vem_nettype__5b80085f_0bae_4bd8_97d4_919813b3ce8b:6079084+6079084
[INFO][2018-06-06 15:40:07,029][org.apache.spark.executor.Executor]Finished task 1.0 in stage 1.0 (TID 3). 14530 bytes result sent to driver
[INFO][2018-06-06 15:40:07,095][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 1.0 (TID 3) in 4662 ms on localhost (executor driver) (1/2)
[INFO][2018-06-06 15:40:07,356][org.apache.spark.executor.Executor]Finished task 0.0 in stage 1.0 (TID 2). 240458 bytes result sent to driver
[INFO][2018-06-06 15:40:07,382][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 1.0 (TID 2) in 4950 ms on localhost (executor driver) (2/2)
[INFO][2018-06-06 15:40:07,382][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO][2018-06-06 15:40:07,383][org.apache.spark.scheduler.DAGScheduler]ResultStage 1 (collect at SalesDefectNetByNetCommunity.scala:89) finished in 4.951 s
[INFO][2018-06-06 15:40:07,384][org.apache.spark.scheduler.DAGScheduler]Job 1 finished: collect at SalesDefectNetByNetCommunity.scala:89, took 4.982252 s
[INFO][2018-06-06 15:42:18,951][org.apache.spark.storage.memory.MemoryStore]Block broadcast_5 stored as values in memory (estimated size 1924.3 KB, free 908.7 MB)
[WARN][2018-06-06 15:42:18,952][org.apache.spark.rpc.netty.NettyRpcEnv]Ignored message: HeartbeatResponse(false)
[WARN][2018-06-06 15:42:18,957][org.apache.spark.executor.Executor]Issue communicating with driver in heartbeater
org.apache.spark.rpc.RpcTimeoutException: Futures timed out after [10 seconds]. This timeout is controlled by spark.executor.heartbeatInterval
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:76)
	at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:92)
	at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$reportHeartBeat(Executor.scala:726)
	at org.apache.spark.executor.Executor$$anon$2$$anonfun$run$1.apply$mcV$sp(Executor.scala:755)
	at org.apache.spark.executor.Executor$$anon$2$$anonfun$run$1.apply(Executor.scala:755)
	at org.apache.spark.executor.Executor$$anon$2$$anonfun$run$1.apply(Executor.scala:755)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1954)
	at org.apache.spark.executor.Executor$$anon$2.run(Executor.scala:755)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.runAndReset$$$capture(FutureTask.java:308)
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.TimeoutException: Futures timed out after [10 seconds]
	at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:223)
	at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:227)
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:201)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
	... 15 more
[INFO][2018-06-06 15:42:18,997][org.apache.spark.storage.memory.MemoryStore]Block broadcast_5_piece0 stored as bytes in memory (estimated size 111.1 KB, free 908.6 MB)
[INFO][2018-06-06 15:42:18,999][org.apache.spark.storage.BlockManagerInfo]Added broadcast_5_piece0 in memory on 10.194.32.157:55765 (size: 111.1 KB, free: 911.9 MB)
[INFO][2018-06-06 15:42:19,003][org.apache.spark.SparkContext]Created broadcast 5 from broadcast at SalesDefectNetByNetCommunity.scala:38
[INFO][2018-06-06 15:42:19,010][org.apache.spark.storage.memory.MemoryStore]Block broadcast_6 stored as values in memory (estimated size 228.1 KB, free 908.4 MB)
[INFO][2018-06-06 15:42:19,027][org.apache.spark.SparkContext]Invoking stop() from shutdown hook
[INFO][2018-06-06 15:42:19,045][org.spark_project.jetty.server.AbstractConnector]Stopped Spark@2dd8239{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-06-06 15:42:19,049][org.apache.spark.ui.SparkUI]Stopped Spark web UI at http://10.194.32.157:4040
[INFO][2018-06-06 15:42:19,052][org.apache.spark.storage.memory.MemoryStore]Block broadcast_6_piece0 stored as bytes in memory (estimated size 22.1 KB, free 908.3 MB)
[INFO][2018-06-06 15:42:19,053][org.apache.spark.storage.BlockManagerInfo]Added broadcast_6_piece0 in memory on 10.194.32.157:55765 (size: 22.1 KB, free: 911.9 MB)
[INFO][2018-06-06 15:42:19,055][org.apache.spark.SparkContext]Created broadcast 6 from textFile at SalesDefectNetByNetCommunity.scala:104
[INFO][2018-06-06 15:42:19,089][org.apache.spark.MapOutputTrackerMasterEndpoint]MapOutputTrackerMasterEndpoint stopped!
[INFO][2018-06-06 15:42:19,111][org.apache.spark.storage.memory.MemoryStore]MemoryStore cleared
[INFO][2018-06-06 15:42:19,112][org.apache.spark.storage.BlockManager]BlockManager stopped
[ERROR][2018-06-06 15:42:19,113][org.apache.spark.util.Utils]Exception encountered
java.lang.IllegalStateException: Block broadcast_6 not found
	at org.apache.spark.storage.BlockInfoManager$$anonfun$2.apply(BlockInfoManager.scala:293)
	at org.apache.spark.storage.BlockInfoManager$$anonfun$2.apply(BlockInfoManager.scala:293)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.storage.BlockInfoManager.unlock(BlockInfoManager.scala:292)
	at org.apache.spark.storage.BlockManager.releaseLock(BlockManager.scala:720)
	at org.apache.spark.broadcast.TorrentBroadcast.org$apache$spark$broadcast$TorrentBroadcast$$releaseLock(TorrentBroadcast.scala:257)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:214)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303)
	at org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:206)
	at org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:66)
	at org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:66)
	at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:96)
	at org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70)
	at org.apache.spark.rdd.HadoopRDD.getJobConf(HadoopRDD.scala:139)
	at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:190)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:250)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:250)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:250)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:250)
	at org.apache.spark.Partitioner$$anonfun$defaultPartitioner$2.apply(Partitioner.scala:66)
	at org.apache.spark.Partitioner$$anonfun$defaultPartitioner$2.apply(Partitioner.scala:66)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.immutable.List.map(List.scala:296)
	at org.apache.spark.Partitioner$.defaultPartitioner(Partitioner.scala:66)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$reduceByKey$3.apply(PairRDDFunctions.scala:329)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$reduceByKey$3.apply(PairRDDFunctions.scala:329)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)
	at org.apache.spark.rdd.PairRDDFunctions.reduceByKey(PairRDDFunctions.scala:328)
	at com.seven.spark.rdd.SalesDefectNetByNetCommunity$.salesDefectNetData(SalesDefectNetByNetCommunity.scala:114)
	at com.seven.spark.rdd.SalesDefectNetByNetCommunity$.main(SalesDefectNetByNetCommunity.scala:42)
	at com.seven.spark.rdd.SalesDefectNetByNetCommunity.main(SalesDefectNetByNetCommunity.scala)
[INFO][2018-06-06 15:42:19,113][org.apache.spark.storage.BlockManagerMaster]BlockManagerMaster stopped
[INFO][2018-06-06 15:42:19,116][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint]OutputCommitCoordinator stopped!
[INFO][2018-06-06 15:42:19,118][org.apache.spark.SparkContext]Successfully stopped SparkContext
[INFO][2018-06-06 15:42:19,119][org.apache.spark.util.ShutdownHookManager]Shutdown hook called
[INFO][2018-06-06 15:42:19,120][org.apache.spark.util.ShutdownHookManager]Deleting directory /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/spark-52be84c6-18fb-457d-a0fd-395ebc109da3
[INFO][2018-06-06 15:43:56,229][org.apache.spark.SparkContext]Running Spark version 2.2.0
[INFO][2018-06-06 15:43:57,292][org.apache.spark.SparkContext]Submitted application: SalesDefectNetByNetCommunity$
[INFO][2018-06-06 15:43:57,314][org.apache.spark.SecurityManager]Changing view acls to: seven
[INFO][2018-06-06 15:43:57,315][org.apache.spark.SecurityManager]Changing modify acls to: seven
[INFO][2018-06-06 15:43:57,315][org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO][2018-06-06 15:43:57,316][org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO][2018-06-06 15:43:57,316][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(seven); groups with view permissions: Set(); users  with modify permissions: Set(seven); groups with modify permissions: Set()
[INFO][2018-06-06 15:43:57,603][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 55795.
[INFO][2018-06-06 15:43:57,625][org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO][2018-06-06 15:43:57,642][org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO][2018-06-06 15:43:57,645][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2018-06-06 15:43:57,645][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
[INFO][2018-06-06 15:43:57,657][org.apache.spark.storage.DiskBlockManager]Created local directory at /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/blockmgr-eeb7deb7-2ab4-49c7-a69d-bbc4ff538e39
[INFO][2018-06-06 15:43:57,676][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 912.3 MB
[INFO][2018-06-06 15:43:57,745][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO][2018-06-06 15:43:57,916][org.spark_project.jetty.util.log]Logging initialized @2644ms
[INFO][2018-06-06 15:43:57,996][org.spark_project.jetty.server.Server]jetty-9.3.z-SNAPSHOT
[INFO][2018-06-06 15:43:58,016][org.spark_project.jetty.server.Server]Started @2746ms
[INFO][2018-06-06 15:43:58,046][org.spark_project.jetty.server.AbstractConnector]Started ServerConnector@3b6ed1a4{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-06-06 15:43:58,047][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4040.
[INFO][2018-06-06 15:43:58,117][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4a9f80d3{/jobs,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:43:58,118][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3e2822{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:43:58,118][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@29a60c27{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:43:58,120][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1ca25c47{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:43:58,125][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@533b266e{/stages,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:43:58,126][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@89ff02e{/stages/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:43:58,126][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@62679465{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:43:58,128][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5f2606b{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:43:58,129][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3ebff828{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:43:58,130][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@33352f32{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:43:58,131][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1e044120{/storage,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:43:58,132][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3624da92{/storage/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:43:58,133][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@94f6bfb{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:43:58,133][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2484f433{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:43:58,134][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1255b1d1{/environment,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:43:58,135][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7c22d4f{/environment/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:43:58,136][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@60bdf15d{/executors,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:43:58,139][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@51e4ccb3{/executors/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:43:58,140][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@495083a0{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:43:58,141][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@28a0fd6c{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:43:58,149][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@66629f63{/static,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:43:58,150][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1d9bec4d{/,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:43:58,151][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@10c8f62{/api,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:43:58,152][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@439a8f59{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:43:58,153][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@31024624{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:43:58,155][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://10.194.32.157:4040
[INFO][2018-06-06 15:43:58,275][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
[INFO][2018-06-06 15:43:58,302][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 55796.
[INFO][2018-06-06 15:43:58,303][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 10.194.32.157:55796
[INFO][2018-06-06 15:43:58,304][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2018-06-06 15:43:58,307][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 10.194.32.157, 55796, None)
[INFO][2018-06-06 15:43:58,310][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 10.194.32.157:55796 with 912.3 MB RAM, BlockManagerId(driver, 10.194.32.157, 55796, None)
[INFO][2018-06-06 15:43:58,312][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 10.194.32.157, 55796, None)
[INFO][2018-06-06 15:43:58,313][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 10.194.32.157, 55796, None)
[INFO][2018-06-06 15:43:58,514][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6df20ade{/metrics/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:43:58,532][com.seven.spark.rdd.SalesDefectNetByNetCommunity$]SalesDefectNetByNetCommunity$ is start . . .
[INFO][2018-06-06 15:43:59,141][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0 stored as values in memory (estimated size 228.1 KB, free 912.1 MB)
[INFO][2018-06-06 15:43:59,463][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0_piece0 stored as bytes in memory (estimated size 22.1 KB, free 912.1 MB)
[INFO][2018-06-06 15:43:59,465][org.apache.spark.storage.BlockManagerInfo]Added broadcast_0_piece0 in memory on 10.194.32.157:55796 (size: 22.1 KB, free: 912.3 MB)
[INFO][2018-06-06 15:43:59,470][org.apache.spark.SparkContext]Created broadcast 0 from textFile at SalesDefectNetByNetCommunity.scala:53
[WARN][2018-06-06 15:44:00,996][org.apache.hadoop.util.NativeCodeLoader]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO][2018-06-06 15:44:01,139][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 1
[INFO][2018-06-06 15:44:01,214][org.apache.spark.SparkContext]Starting job: collect at SalesDefectNetByNetCommunity.scala:64
[INFO][2018-06-06 15:44:01,226][org.apache.spark.scheduler.DAGScheduler]Got job 0 (collect at SalesDefectNetByNetCommunity.scala:64) with 2 output partitions
[INFO][2018-06-06 15:44:01,226][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 0 (collect at SalesDefectNetByNetCommunity.scala:64)
[INFO][2018-06-06 15:44:01,227][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-06-06 15:44:01,228][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-06-06 15:44:01,235][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 0 (MapPartitionsRDD[3] at mapPartitions at SalesDefectNetByNetCommunity.scala:56), which has no missing parents
[INFO][2018-06-06 15:44:01,255][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1 stored as values in memory (estimated size 3.7 KB, free 912.1 MB)
[INFO][2018-06-06 15:44:01,265][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.1 KB, free 912.1 MB)
[INFO][2018-06-06 15:44:01,267][org.apache.spark.storage.BlockManagerInfo]Added broadcast_1_piece0 in memory on 10.194.32.157:55796 (size: 2.1 KB, free: 912.3 MB)
[INFO][2018-06-06 15:44:01,268][org.apache.spark.SparkContext]Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO][2018-06-06 15:44:01,285][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at mapPartitions at SalesDefectNetByNetCommunity.scala:56) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2018-06-06 15:44:01,286][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 0.0 with 2 tasks
[INFO][2018-06-06 15:44:01,328][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 4930 bytes)
[INFO][2018-06-06 15:44:01,333][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 4930 bytes)
[INFO][2018-06-06 15:44:01,342][org.apache.spark.executor.Executor]Running task 0.0 in stage 0.0 (TID 0)
[INFO][2018-06-06 15:44:01,342][org.apache.spark.executor.Executor]Running task 1.0 in stage 0.0 (TID 1)
[INFO][2018-06-06 15:44:01,399][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/sta_vem/net_community/net_community__b4d64c39_5c6b_44c1_81be_f9aa19c55a30:0+278620
[INFO][2018-06-06 15:44:01,399][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/sta_vem/net_community/net_community__b4d64c39_5c6b_44c1_81be_f9aa19c55a30:278620+278620
[INFO][2018-06-06 15:44:01,636][org.apache.spark.executor.Executor]Finished task 0.0 in stage 0.0 (TID 0). 289675 bytes result sent to driver
[INFO][2018-06-06 15:44:01,682][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 0.0 (TID 0) in 366 ms on localhost (executor driver) (1/2)
[INFO][2018-06-06 15:44:01,696][org.apache.spark.executor.Executor]Finished task 1.0 in stage 0.0 (TID 1). 290550 bytes result sent to driver
[INFO][2018-06-06 15:44:01,709][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 0.0 (TID 1) in 378 ms on localhost (executor driver) (2/2)
[INFO][2018-06-06 15:44:01,711][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO][2018-06-06 15:44:01,712][org.apache.spark.scheduler.DAGScheduler]ResultStage 0 (collect at SalesDefectNetByNetCommunity.scala:64) finished in 0.407 s
[INFO][2018-06-06 15:44:01,717][org.apache.spark.scheduler.DAGScheduler]Job 0 finished: collect at SalesDefectNetByNetCommunity.scala:64, took 0.502337 s
[INFO][2018-06-06 15:44:01,724][org.apache.spark.storage.memory.MemoryStore]Block broadcast_2 stored as values in memory (estimated size 1060.4 KB, free 911.0 MB)
[INFO][2018-06-06 15:44:01,737][org.apache.spark.storage.memory.MemoryStore]Block broadcast_2_piece0 stored as bytes in memory (estimated size 203.6 KB, free 910.8 MB)
[INFO][2018-06-06 15:44:01,738][org.apache.spark.storage.BlockManagerInfo]Added broadcast_2_piece0 in memory on 10.194.32.157:55796 (size: 203.6 KB, free: 912.1 MB)
[INFO][2018-06-06 15:44:01,739][org.apache.spark.SparkContext]Created broadcast 2 from broadcast at SalesDefectNetByNetCommunity.scala:34
[INFO][2018-06-06 15:44:01,746][org.apache.spark.storage.memory.MemoryStore]Block broadcast_3 stored as values in memory (estimated size 228.1 KB, free 910.6 MB)
[INFO][2018-06-06 15:44:01,763][org.apache.spark.storage.memory.MemoryStore]Block broadcast_3_piece0 stored as bytes in memory (estimated size 22.1 KB, free 910.6 MB)
[INFO][2018-06-06 15:44:01,763][org.apache.spark.storage.BlockManagerInfo]Added broadcast_3_piece0 in memory on 10.194.32.157:55796 (size: 22.1 KB, free: 912.1 MB)
[INFO][2018-06-06 15:44:01,764][org.apache.spark.SparkContext]Created broadcast 3 from textFile at SalesDefectNetByNetCommunity.scala:79
[INFO][2018-06-06 15:44:01,798][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 1
[INFO][2018-06-06 15:44:01,818][org.apache.spark.SparkContext]Starting job: collect at SalesDefectNetByNetCommunity.scala:89
[INFO][2018-06-06 15:44:01,819][org.apache.spark.scheduler.DAGScheduler]Got job 1 (collect at SalesDefectNetByNetCommunity.scala:89) with 2 output partitions
[INFO][2018-06-06 15:44:01,819][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 1 (collect at SalesDefectNetByNetCommunity.scala:89)
[INFO][2018-06-06 15:44:01,819][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-06-06 15:44:01,819][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-06-06 15:44:01,820][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 1 (MapPartitionsRDD[7] at mapPartitions at SalesDefectNetByNetCommunity.scala:82), which has no missing parents
[INFO][2018-06-06 15:44:01,823][org.apache.spark.storage.memory.MemoryStore]Block broadcast_4 stored as values in memory (estimated size 3.7 KB, free 910.6 MB)
[INFO][2018-06-06 15:44:01,828][org.apache.spark.storage.memory.MemoryStore]Block broadcast_4_piece0 stored as bytes in memory (estimated size 2.0 KB, free 910.6 MB)
[INFO][2018-06-06 15:44:01,829][org.apache.spark.storage.BlockManagerInfo]Added broadcast_4_piece0 in memory on 10.194.32.157:55796 (size: 2.0 KB, free: 912.1 MB)
[INFO][2018-06-06 15:44:01,829][org.apache.spark.SparkContext]Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[INFO][2018-06-06 15:44:01,830][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[7] at mapPartitions at SalesDefectNetByNetCommunity.scala:82) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2018-06-06 15:44:01,830][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 1.0 with 2 tasks
[INFO][2018-06-06 15:44:01,831][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 1.0 (TID 2, localhost, executor driver, partition 0, ANY, 4926 bytes)
[INFO][2018-06-06 15:44:01,832][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 1.0 (TID 3, localhost, executor driver, partition 1, ANY, 4926 bytes)
[INFO][2018-06-06 15:44:01,832][org.apache.spark.executor.Executor]Running task 1.0 in stage 1.0 (TID 3)
[INFO][2018-06-06 15:44:01,832][org.apache.spark.executor.Executor]Running task 0.0 in stage 1.0 (TID 2)
[INFO][2018-06-06 15:44:01,836][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/sta_vem/vem_nettype/vem_nettype__5b80085f_0bae_4bd8_97d4_919813b3ce8b:6079084+6079084
[INFO][2018-06-06 15:44:01,837][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/sta_vem/vem_nettype/vem_nettype__5b80085f_0bae_4bd8_97d4_919813b3ce8b:0+6079084
[INFO][2018-06-06 15:44:06,896][org.apache.spark.executor.Executor]Finished task 1.0 in stage 1.0 (TID 3). 14530 bytes result sent to driver
[INFO][2018-06-06 15:44:06,904][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 1.0 (TID 3) in 5073 ms on localhost (executor driver) (1/2)
[INFO][2018-06-06 15:44:07,251][org.apache.spark.executor.Executor]Finished task 0.0 in stage 1.0 (TID 2). 240458 bytes result sent to driver
[INFO][2018-06-06 15:44:07,272][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 1.0 (TID 2) in 5441 ms on localhost (executor driver) (2/2)
[INFO][2018-06-06 15:44:07,273][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO][2018-06-06 15:44:07,273][org.apache.spark.scheduler.DAGScheduler]ResultStage 1 (collect at SalesDefectNetByNetCommunity.scala:89) finished in 5.442 s
[INFO][2018-06-06 15:44:07,274][org.apache.spark.scheduler.DAGScheduler]Job 1 finished: collect at SalesDefectNetByNetCommunity.scala:89, took 5.456095 s
[INFO][2018-06-06 15:44:07,281][org.apache.spark.storage.memory.MemoryStore]Block broadcast_5 stored as values in memory (estimated size 1924.3 KB, free 908.7 MB)
[INFO][2018-06-06 15:44:07,303][org.apache.spark.storage.memory.MemoryStore]Block broadcast_5_piece0 stored as bytes in memory (estimated size 111.1 KB, free 908.6 MB)
[INFO][2018-06-06 15:44:07,305][org.apache.spark.storage.BlockManagerInfo]Added broadcast_5_piece0 in memory on 10.194.32.157:55796 (size: 111.1 KB, free: 911.9 MB)
[INFO][2018-06-06 15:44:07,306][org.apache.spark.SparkContext]Created broadcast 5 from broadcast at SalesDefectNetByNetCommunity.scala:38
[INFO][2018-06-06 15:44:07,311][org.apache.spark.storage.memory.MemoryStore]Block broadcast_6 stored as values in memory (estimated size 228.1 KB, free 908.4 MB)
[INFO][2018-06-06 15:44:07,331][org.apache.spark.storage.memory.MemoryStore]Block broadcast_6_piece0 stored as bytes in memory (estimated size 22.1 KB, free 908.3 MB)
[INFO][2018-06-06 15:44:07,332][org.apache.spark.storage.BlockManagerInfo]Added broadcast_6_piece0 in memory on 10.194.32.157:55796 (size: 22.1 KB, free: 911.9 MB)
[INFO][2018-06-06 15:44:07,333][org.apache.spark.SparkContext]Created broadcast 6 from textFile at SalesDefectNetByNetCommunity.scala:104
[INFO][2018-06-06 15:44:07,374][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 1
[INFO][2018-06-06 15:44:07,408][org.apache.spark.SparkContext]Starting job: foreach at SalesDefectNetByNetCommunity.scala:126
[INFO][2018-06-06 15:44:07,423][org.apache.spark.scheduler.DAGScheduler]Registering RDD 11 (mapPartitions at SalesDefectNetByNetCommunity.scala:107)
[INFO][2018-06-06 15:44:07,423][org.apache.spark.scheduler.DAGScheduler]Got job 2 (foreach at SalesDefectNetByNetCommunity.scala:126) with 2 output partitions
[INFO][2018-06-06 15:44:07,424][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 3 (foreach at SalesDefectNetByNetCommunity.scala:126)
[INFO][2018-06-06 15:44:07,424][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 2)
[INFO][2018-06-06 15:44:07,427][org.apache.spark.scheduler.DAGScheduler]Missing parents: List(ShuffleMapStage 2)
[INFO][2018-06-06 15:44:07,429][org.apache.spark.scheduler.DAGScheduler]Submitting ShuffleMapStage 2 (MapPartitionsRDD[11] at mapPartitions at SalesDefectNetByNetCommunity.scala:107), which has no missing parents
[INFO][2018-06-06 15:44:07,441][org.apache.spark.storage.memory.MemoryStore]Block broadcast_7 stored as values in memory (estimated size 4.9 KB, free 908.3 MB)
[INFO][2018-06-06 15:44:07,449][org.apache.spark.storage.memory.MemoryStore]Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.8 KB, free 908.3 MB)
[INFO][2018-06-06 15:44:07,450][org.apache.spark.storage.BlockManagerInfo]Added broadcast_7_piece0 in memory on 10.194.32.157:55796 (size: 2.8 KB, free: 911.9 MB)
[INFO][2018-06-06 15:44:07,450][org.apache.spark.SparkContext]Created broadcast 7 from broadcast at DAGScheduler.scala:1006
[INFO][2018-06-06 15:44:07,453][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[11] at mapPartitions at SalesDefectNetByNetCommunity.scala:107) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2018-06-06 15:44:07,454][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 2.0 with 2 tasks
[INFO][2018-06-06 15:44:07,456][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 2.0 (TID 4, localhost, executor driver, partition 0, ANY, 4915 bytes)
[INFO][2018-06-06 15:44:07,456][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 2.0 (TID 5, localhost, executor driver, partition 1, ANY, 4915 bytes)
[INFO][2018-06-06 15:44:07,458][org.apache.spark.executor.Executor]Running task 1.0 in stage 2.0 (TID 5)
[INFO][2018-06-06 15:44:07,467][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/sta_vem/vem_machine/vem_machine__12fe7e5b_689f_498e_b2a6_8411fd976c5b:3998718+3998719
[INFO][2018-06-06 15:44:07,472][org.apache.spark.executor.Executor]Running task 0.0 in stage 2.0 (TID 4)
[INFO][2018-06-06 15:44:07,479][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/sta_vem/vem_machine/vem_machine__12fe7e5b_689f_498e_b2a6_8411fd976c5b:0+3998718
[INFO][2018-06-06 15:44:07,991][org.apache.spark.executor.Executor]Finished task 0.0 in stage 2.0 (TID 4). 1000 bytes result sent to driver
[INFO][2018-06-06 15:44:08,010][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 2.0 (TID 4) in 556 ms on localhost (executor driver) (1/2)
[INFO][2018-06-06 15:44:11,487][org.apache.spark.executor.Executor]Finished task 1.0 in stage 2.0 (TID 5). 1086 bytes result sent to driver
[INFO][2018-06-06 15:44:11,488][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 2.0 (TID 5) in 4032 ms on localhost (executor driver) (2/2)
[INFO][2018-06-06 15:44:11,488][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO][2018-06-06 15:44:11,489][org.apache.spark.scheduler.DAGScheduler]ShuffleMapStage 2 (mapPartitions at SalesDefectNetByNetCommunity.scala:107) finished in 4.034 s
[INFO][2018-06-06 15:44:11,490][org.apache.spark.scheduler.DAGScheduler]looking for newly runnable stages
[INFO][2018-06-06 15:44:11,490][org.apache.spark.scheduler.DAGScheduler]running: Set()
[INFO][2018-06-06 15:44:11,491][org.apache.spark.scheduler.DAGScheduler]waiting: Set(ResultStage 3)
[INFO][2018-06-06 15:44:11,491][org.apache.spark.scheduler.DAGScheduler]failed: Set()
[INFO][2018-06-06 15:44:11,498][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 3 (MapPartitionsRDD[14] at filter at SalesDefectNetByNetCommunity.scala:121), which has no missing parents
[INFO][2018-06-06 15:44:11,505][org.apache.spark.storage.memory.MemoryStore]Block broadcast_8 stored as values in memory (estimated size 4.6 KB, free 908.3 MB)
[INFO][2018-06-06 15:44:11,509][org.apache.spark.storage.memory.MemoryStore]Block broadcast_8_piece0 stored as bytes in memory (estimated size 2.6 KB, free 908.3 MB)
[INFO][2018-06-06 15:44:11,510][org.apache.spark.storage.BlockManagerInfo]Added broadcast_8_piece0 in memory on 10.194.32.157:55796 (size: 2.6 KB, free: 911.9 MB)
[INFO][2018-06-06 15:44:11,510][org.apache.spark.SparkContext]Created broadcast 8 from broadcast at DAGScheduler.scala:1006
[INFO][2018-06-06 15:44:11,511][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ResultStage 3 (MapPartitionsRDD[14] at filter at SalesDefectNetByNetCommunity.scala:121) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2018-06-06 15:44:11,511][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 3.0 with 2 tasks
[INFO][2018-06-06 15:44:11,513][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 3.0 (TID 6, localhost, executor driver, partition 0, ANY, 4621 bytes)
[INFO][2018-06-06 15:44:11,513][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 3.0 (TID 7, localhost, executor driver, partition 1, ANY, 4621 bytes)
[INFO][2018-06-06 15:44:11,514][org.apache.spark.executor.Executor]Running task 1.0 in stage 3.0 (TID 7)
[INFO][2018-06-06 15:44:11,518][org.apache.spark.executor.Executor]Running task 0.0 in stage 3.0 (TID 6)
[INFO][2018-06-06 15:44:11,532][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 2 blocks
[INFO][2018-06-06 15:44:11,542][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 14 ms
[INFO][2018-06-06 15:44:11,557][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_7_piece0 on 10.194.32.157:55796 in memory (size: 2.8 KB, free: 911.9 MB)
[INFO][2018-06-06 15:44:11,545][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 2 blocks
[INFO][2018-06-06 15:44:11,561][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 16 ms
[INFO][2018-06-06 15:44:11,611][org.apache.spark.storage.memory.MemoryStore]Block rdd_14_1 stored as values in memory (estimated size 160.0 B, free 908.3 MB)
[INFO][2018-06-06 15:44:11,612][org.apache.spark.storage.BlockManagerInfo]Added rdd_14_1 in memory on 10.194.32.157:55796 (size: 160.0 B, free: 911.9 MB)
[INFO][2018-06-06 15:44:11,620][org.apache.spark.storage.memory.MemoryStore]Block rdd_14_0 stored as values in memory (estimated size 424.0 B, free 908.3 MB)
[INFO][2018-06-06 15:44:11,620][org.apache.spark.storage.BlockManagerInfo]Added rdd_14_0 in memory on 10.194.32.157:55796 (size: 424.0 B, free: 911.9 MB)
[INFO][2018-06-06 15:44:11,622][org.apache.spark.executor.Executor]Finished task 1.0 in stage 3.0 (TID 7). 1793 bytes result sent to driver
[INFO][2018-06-06 15:44:11,623][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 3.0 (TID 7) in 110 ms on localhost (executor driver) (1/2)
[INFO][2018-06-06 15:44:11,624][org.apache.spark.executor.Executor]Finished task 0.0 in stage 3.0 (TID 6). 1793 bytes result sent to driver
[INFO][2018-06-06 15:44:11,625][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 3.0 (TID 6) in 113 ms on localhost (executor driver) (2/2)
[INFO][2018-06-06 15:44:11,625][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 3.0, whose tasks have all completed, from pool 
[INFO][2018-06-06 15:44:11,626][org.apache.spark.scheduler.DAGScheduler]ResultStage 3 (foreach at SalesDefectNetByNetCommunity.scala:126) finished in 0.114 s
[INFO][2018-06-06 15:44:11,626][org.apache.spark.scheduler.DAGScheduler]Job 2 finished: foreach at SalesDefectNetByNetCommunity.scala:126, took 4.218297 s
[INFO][2018-06-06 15:44:11,627][com.seven.spark.rdd.SalesDefectNetByNetCommunity$]SalesDefectNetByNetCommunity$ is success . . .
[INFO][2018-06-06 15:44:11,630][org.apache.spark.SparkContext]Invoking stop() from shutdown hook
[INFO][2018-06-06 15:44:11,638][org.spark_project.jetty.server.AbstractConnector]Stopped Spark@3b6ed1a4{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-06-06 15:44:11,640][org.apache.spark.ui.SparkUI]Stopped Spark web UI at http://10.194.32.157:4040
[INFO][2018-06-06 15:44:11,649][org.apache.spark.MapOutputTrackerMasterEndpoint]MapOutputTrackerMasterEndpoint stopped!
[INFO][2018-06-06 15:44:11,671][org.apache.spark.storage.memory.MemoryStore]MemoryStore cleared
[INFO][2018-06-06 15:44:11,671][org.apache.spark.storage.BlockManager]BlockManager stopped
[INFO][2018-06-06 15:44:11,673][org.apache.spark.storage.BlockManagerMaster]BlockManagerMaster stopped
[INFO][2018-06-06 15:44:11,676][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint]OutputCommitCoordinator stopped!
[INFO][2018-06-06 15:44:11,678][org.apache.spark.SparkContext]Successfully stopped SparkContext
[INFO][2018-06-06 15:44:11,679][org.apache.spark.util.ShutdownHookManager]Shutdown hook called
[INFO][2018-06-06 15:44:11,680][org.apache.spark.util.ShutdownHookManager]Deleting directory /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/spark-735e2997-3fda-4b34-9d81-83b6b2879fc7
[INFO][2018-06-06 15:52:28,913][org.apache.spark.SparkContext]Running Spark version 2.2.0
[INFO][2018-06-06 15:52:29,821][org.apache.spark.SparkContext]Submitted application: SalesDefectNetByNetCommunity$
[INFO][2018-06-06 15:52:29,842][org.apache.spark.SecurityManager]Changing view acls to: seven
[INFO][2018-06-06 15:52:29,843][org.apache.spark.SecurityManager]Changing modify acls to: seven
[INFO][2018-06-06 15:52:29,845][org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO][2018-06-06 15:52:29,846][org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO][2018-06-06 15:52:29,846][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(seven); groups with view permissions: Set(); users  with modify permissions: Set(seven); groups with modify permissions: Set()
[INFO][2018-06-06 15:52:30,125][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 55874.
[INFO][2018-06-06 15:52:30,147][org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO][2018-06-06 15:52:30,163][org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO][2018-06-06 15:52:30,166][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2018-06-06 15:52:30,167][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
[INFO][2018-06-06 15:52:30,177][org.apache.spark.storage.DiskBlockManager]Created local directory at /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/blockmgr-31c364d2-95de-405d-9739-5c2d1c9c4cfd
[INFO][2018-06-06 15:52:30,195][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 912.3 MB
[INFO][2018-06-06 15:52:30,272][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO][2018-06-06 15:52:30,362][org.spark_project.jetty.util.log]Logging initialized @2542ms
[INFO][2018-06-06 15:52:30,443][org.spark_project.jetty.server.Server]jetty-9.3.z-SNAPSHOT
[INFO][2018-06-06 15:52:30,456][org.spark_project.jetty.server.Server]Started @2639ms
[INFO][2018-06-06 15:52:30,474][org.spark_project.jetty.server.AbstractConnector]Started ServerConnector@695b90c5{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-06-06 15:52:30,474][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4040.
[INFO][2018-06-06 15:52:30,513][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@70dd7e15{/jobs,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:52:30,514][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@60fa3495{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:52:30,514][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@79e18e38{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:52:30,517][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@69c79f09{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:52:30,517][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5fcacc0{/stages,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:52:30,518][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6d1d4d7{/stages/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:52:30,519][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6865c751{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:52:30,521][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5b6813df{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:52:30,524][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2b58f754{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:52:30,525][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2552f2cb{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:52:30,526][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5f3b9c57{/storage,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:52:30,527][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2cf23c81{/storage/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:52:30,530][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@35fe2125{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:52:30,531][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@34645867{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:52:30,533][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@60b71e8f{/environment,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:52:30,535][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@464649c{/environment/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:52:30,536][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5f59185e{/executors,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:52:30,537][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@47da3952{/executors/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:52:30,537][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@46e8a539{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:52:30,539][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5fd62371{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:52:30,549][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2b62442c{/static,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:52:30,550][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@362a019c{/,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:52:30,551][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5c48c0c0{/api,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:52:30,553][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5fb97279{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:52:30,554][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@61861a29{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:52:30,556][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://10.194.32.157:4040
[INFO][2018-06-06 15:52:30,709][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
[INFO][2018-06-06 15:52:30,747][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 55875.
[INFO][2018-06-06 15:52:30,748][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 10.194.32.157:55875
[INFO][2018-06-06 15:52:30,749][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2018-06-06 15:52:30,751][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 10.194.32.157, 55875, None)
[INFO][2018-06-06 15:52:30,755][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 10.194.32.157:55875 with 912.3 MB RAM, BlockManagerId(driver, 10.194.32.157, 55875, None)
[INFO][2018-06-06 15:52:30,757][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 10.194.32.157, 55875, None)
[INFO][2018-06-06 15:52:30,758][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 10.194.32.157, 55875, None)
[INFO][2018-06-06 15:52:30,904][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1f9d6c7b{/metrics/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:52:30,921][com.seven.spark.rdd.SalesDefectNetByNetCommunity$]SalesDefectNetByNetCommunity$ is start . . .
[INFO][2018-06-06 15:52:31,438][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0 stored as values in memory (estimated size 228.1 KB, free 912.1 MB)
[INFO][2018-06-06 15:52:31,648][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0_piece0 stored as bytes in memory (estimated size 22.1 KB, free 912.1 MB)
[INFO][2018-06-06 15:52:31,651][org.apache.spark.storage.BlockManagerInfo]Added broadcast_0_piece0 in memory on 10.194.32.157:55875 (size: 22.1 KB, free: 912.3 MB)
[INFO][2018-06-06 15:52:31,658][org.apache.spark.SparkContext]Created broadcast 0 from textFile at SalesDefectNetByNetCommunity.scala:53
[WARN][2018-06-06 15:52:33,190][org.apache.hadoop.util.NativeCodeLoader]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO][2018-06-06 15:52:33,327][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 1
[INFO][2018-06-06 15:52:33,403][org.apache.spark.SparkContext]Starting job: collect at SalesDefectNetByNetCommunity.scala:64
[INFO][2018-06-06 15:52:33,415][org.apache.spark.scheduler.DAGScheduler]Got job 0 (collect at SalesDefectNetByNetCommunity.scala:64) with 2 output partitions
[INFO][2018-06-06 15:52:33,416][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 0 (collect at SalesDefectNetByNetCommunity.scala:64)
[INFO][2018-06-06 15:52:33,416][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-06-06 15:52:33,421][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-06-06 15:52:33,433][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 0 (MapPartitionsRDD[3] at mapPartitions at SalesDefectNetByNetCommunity.scala:56), which has no missing parents
[INFO][2018-06-06 15:52:33,458][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1 stored as values in memory (estimated size 3.7 KB, free 912.1 MB)
[INFO][2018-06-06 15:52:33,465][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.1 KB, free 912.1 MB)
[INFO][2018-06-06 15:52:33,466][org.apache.spark.storage.BlockManagerInfo]Added broadcast_1_piece0 in memory on 10.194.32.157:55875 (size: 2.1 KB, free: 912.3 MB)
[INFO][2018-06-06 15:52:33,468][org.apache.spark.SparkContext]Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO][2018-06-06 15:52:33,485][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at mapPartitions at SalesDefectNetByNetCommunity.scala:56) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2018-06-06 15:52:33,486][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 0.0 with 2 tasks
[INFO][2018-06-06 15:52:33,523][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 4930 bytes)
[INFO][2018-06-06 15:52:33,525][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 4930 bytes)
[INFO][2018-06-06 15:52:33,534][org.apache.spark.executor.Executor]Running task 1.0 in stage 0.0 (TID 1)
[INFO][2018-06-06 15:52:33,534][org.apache.spark.executor.Executor]Running task 0.0 in stage 0.0 (TID 0)
[INFO][2018-06-06 15:52:33,595][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/sta_vem/net_community/net_community__b4d64c39_5c6b_44c1_81be_f9aa19c55a30:278620+278620
[INFO][2018-06-06 15:52:33,595][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/sta_vem/net_community/net_community__b4d64c39_5c6b_44c1_81be_f9aa19c55a30:0+278620
[INFO][2018-06-06 15:52:33,871][org.apache.spark.executor.Executor]Finished task 0.0 in stage 0.0 (TID 0). 289675 bytes result sent to driver
[INFO][2018-06-06 15:52:33,902][org.apache.spark.executor.Executor]Finished task 1.0 in stage 0.0 (TID 1). 290507 bytes result sent to driver
[INFO][2018-06-06 15:52:33,920][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 0.0 (TID 0) in 408 ms on localhost (executor driver) (1/2)
[INFO][2018-06-06 15:52:33,922][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 0.0 (TID 1) in 397 ms on localhost (executor driver) (2/2)
[INFO][2018-06-06 15:52:33,923][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO][2018-06-06 15:52:33,927][org.apache.spark.scheduler.DAGScheduler]ResultStage 0 (collect at SalesDefectNetByNetCommunity.scala:64) finished in 0.427 s
[INFO][2018-06-06 15:52:33,931][org.apache.spark.scheduler.DAGScheduler]Job 0 finished: collect at SalesDefectNetByNetCommunity.scala:64, took 0.527266 s
[INFO][2018-06-06 15:52:33,939][org.apache.spark.storage.memory.MemoryStore]Block broadcast_2 stored as values in memory (estimated size 1060.4 KB, free 911.0 MB)
[INFO][2018-06-06 15:52:33,958][org.apache.spark.storage.memory.MemoryStore]Block broadcast_2_piece0 stored as bytes in memory (estimated size 203.6 KB, free 910.8 MB)
[INFO][2018-06-06 15:52:33,959][org.apache.spark.storage.BlockManagerInfo]Added broadcast_2_piece0 in memory on 10.194.32.157:55875 (size: 203.6 KB, free: 912.1 MB)
[INFO][2018-06-06 15:52:33,959][org.apache.spark.SparkContext]Created broadcast 2 from broadcast at SalesDefectNetByNetCommunity.scala:34
[INFO][2018-06-06 15:52:33,965][org.apache.spark.storage.memory.MemoryStore]Block broadcast_3 stored as values in memory (estimated size 228.1 KB, free 910.6 MB)
[INFO][2018-06-06 15:52:33,984][org.apache.spark.storage.memory.MemoryStore]Block broadcast_3_piece0 stored as bytes in memory (estimated size 22.1 KB, free 910.6 MB)
[INFO][2018-06-06 15:52:33,984][org.apache.spark.storage.BlockManagerInfo]Added broadcast_3_piece0 in memory on 10.194.32.157:55875 (size: 22.1 KB, free: 912.1 MB)
[INFO][2018-06-06 15:52:33,985][org.apache.spark.SparkContext]Created broadcast 3 from textFile at SalesDefectNetByNetCommunity.scala:79
[INFO][2018-06-06 15:52:34,018][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 1
[INFO][2018-06-06 15:52:34,041][org.apache.spark.SparkContext]Starting job: collect at SalesDefectNetByNetCommunity.scala:89
[INFO][2018-06-06 15:52:34,042][org.apache.spark.scheduler.DAGScheduler]Got job 1 (collect at SalesDefectNetByNetCommunity.scala:89) with 2 output partitions
[INFO][2018-06-06 15:52:34,042][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 1 (collect at SalesDefectNetByNetCommunity.scala:89)
[INFO][2018-06-06 15:52:34,042][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-06-06 15:52:34,042][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-06-06 15:52:34,043][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 1 (MapPartitionsRDD[7] at mapPartitions at SalesDefectNetByNetCommunity.scala:82), which has no missing parents
[INFO][2018-06-06 15:52:34,045][org.apache.spark.storage.memory.MemoryStore]Block broadcast_4 stored as values in memory (estimated size 3.7 KB, free 910.6 MB)
[INFO][2018-06-06 15:52:34,059][org.apache.spark.storage.memory.MemoryStore]Block broadcast_4_piece0 stored as bytes in memory (estimated size 2.0 KB, free 910.6 MB)
[INFO][2018-06-06 15:52:34,063][org.apache.spark.storage.BlockManagerInfo]Added broadcast_4_piece0 in memory on 10.194.32.157:55875 (size: 2.0 KB, free: 912.1 MB)
[INFO][2018-06-06 15:52:34,065][org.apache.spark.SparkContext]Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[INFO][2018-06-06 15:52:34,067][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[7] at mapPartitions at SalesDefectNetByNetCommunity.scala:82) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2018-06-06 15:52:34,067][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 1.0 with 2 tasks
[INFO][2018-06-06 15:52:34,068][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 1.0 (TID 2, localhost, executor driver, partition 0, ANY, 4926 bytes)
[INFO][2018-06-06 15:52:34,069][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 1.0 (TID 3, localhost, executor driver, partition 1, ANY, 4926 bytes)
[INFO][2018-06-06 15:52:34,069][org.apache.spark.executor.Executor]Running task 0.0 in stage 1.0 (TID 2)
[INFO][2018-06-06 15:52:34,069][org.apache.spark.executor.Executor]Running task 1.0 in stage 1.0 (TID 3)
[INFO][2018-06-06 15:52:34,073][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/sta_vem/vem_nettype/vem_nettype__5b80085f_0bae_4bd8_97d4_919813b3ce8b:0+6079084
[INFO][2018-06-06 15:52:34,073][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/sta_vem/vem_nettype/vem_nettype__5b80085f_0bae_4bd8_97d4_919813b3ce8b:6079084+6079084
[INFO][2018-06-06 15:52:37,706][org.apache.spark.executor.Executor]Finished task 1.0 in stage 1.0 (TID 3). 14530 bytes result sent to driver
[INFO][2018-06-06 15:52:37,715][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 1.0 (TID 3) in 3647 ms on localhost (executor driver) (1/2)
[INFO][2018-06-06 15:52:39,439][org.apache.spark.executor.Executor]Finished task 0.0 in stage 1.0 (TID 2). 240458 bytes result sent to driver
[INFO][2018-06-06 15:52:39,463][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 1.0 (TID 2) in 5395 ms on localhost (executor driver) (2/2)
[INFO][2018-06-06 15:52:39,463][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO][2018-06-06 15:52:39,464][org.apache.spark.scheduler.DAGScheduler]ResultStage 1 (collect at SalesDefectNetByNetCommunity.scala:89) finished in 5.395 s
[INFO][2018-06-06 15:52:39,464][org.apache.spark.scheduler.DAGScheduler]Job 1 finished: collect at SalesDefectNetByNetCommunity.scala:89, took 5.423525 s
[INFO][2018-06-06 15:52:39,473][org.apache.spark.storage.memory.MemoryStore]Block broadcast_5 stored as values in memory (estimated size 1924.3 KB, free 908.7 MB)
[INFO][2018-06-06 15:52:39,502][org.apache.spark.storage.memory.MemoryStore]Block broadcast_5_piece0 stored as bytes in memory (estimated size 111.1 KB, free 908.6 MB)
[INFO][2018-06-06 15:52:39,503][org.apache.spark.storage.BlockManagerInfo]Added broadcast_5_piece0 in memory on 10.194.32.157:55875 (size: 111.1 KB, free: 911.9 MB)
[INFO][2018-06-06 15:52:39,504][org.apache.spark.SparkContext]Created broadcast 5 from broadcast at SalesDefectNetByNetCommunity.scala:38
[INFO][2018-06-06 15:52:39,509][org.apache.spark.storage.memory.MemoryStore]Block broadcast_6 stored as values in memory (estimated size 228.1 KB, free 908.4 MB)
[INFO][2018-06-06 15:52:39,525][org.apache.spark.storage.memory.MemoryStore]Block broadcast_6_piece0 stored as bytes in memory (estimated size 22.1 KB, free 908.3 MB)
[INFO][2018-06-06 15:52:39,526][org.apache.spark.storage.BlockManagerInfo]Added broadcast_6_piece0 in memory on 10.194.32.157:55875 (size: 22.1 KB, free: 911.9 MB)
[INFO][2018-06-06 15:52:39,528][org.apache.spark.SparkContext]Created broadcast 6 from textFile at SalesDefectNetByNetCommunity.scala:104
[INFO][2018-06-06 15:52:39,571][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 1
[INFO][2018-06-06 15:52:39,615][org.apache.spark.SparkContext]Starting job: foreach at SalesDefectNetByNetCommunity.scala:126
[INFO][2018-06-06 15:52:39,628][org.apache.spark.scheduler.DAGScheduler]Registering RDD 11 (mapPartitions at SalesDefectNetByNetCommunity.scala:107)
[INFO][2018-06-06 15:52:39,629][org.apache.spark.scheduler.DAGScheduler]Got job 2 (foreach at SalesDefectNetByNetCommunity.scala:126) with 2 output partitions
[INFO][2018-06-06 15:52:39,629][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 3 (foreach at SalesDefectNetByNetCommunity.scala:126)
[INFO][2018-06-06 15:52:39,629][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 2)
[INFO][2018-06-06 15:52:39,632][org.apache.spark.scheduler.DAGScheduler]Missing parents: List(ShuffleMapStage 2)
[INFO][2018-06-06 15:52:39,633][org.apache.spark.scheduler.DAGScheduler]Submitting ShuffleMapStage 2 (MapPartitionsRDD[11] at mapPartitions at SalesDefectNetByNetCommunity.scala:107), which has no missing parents
[INFO][2018-06-06 15:52:39,645][org.apache.spark.storage.memory.MemoryStore]Block broadcast_7 stored as values in memory (estimated size 4.9 KB, free 908.3 MB)
[INFO][2018-06-06 15:52:39,650][org.apache.spark.storage.memory.MemoryStore]Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.8 KB, free 908.3 MB)
[INFO][2018-06-06 15:52:39,652][org.apache.spark.storage.BlockManagerInfo]Added broadcast_7_piece0 in memory on 10.194.32.157:55875 (size: 2.8 KB, free: 911.9 MB)
[INFO][2018-06-06 15:52:39,654][org.apache.spark.SparkContext]Created broadcast 7 from broadcast at DAGScheduler.scala:1006
[INFO][2018-06-06 15:52:39,657][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[11] at mapPartitions at SalesDefectNetByNetCommunity.scala:107) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2018-06-06 15:52:39,657][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 2.0 with 2 tasks
[INFO][2018-06-06 15:52:39,659][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 2.0 (TID 4, localhost, executor driver, partition 0, ANY, 4915 bytes)
[INFO][2018-06-06 15:52:39,660][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 2.0 (TID 5, localhost, executor driver, partition 1, ANY, 4915 bytes)
[INFO][2018-06-06 15:52:39,660][org.apache.spark.executor.Executor]Running task 0.0 in stage 2.0 (TID 4)
[INFO][2018-06-06 15:52:39,660][org.apache.spark.executor.Executor]Running task 1.0 in stage 2.0 (TID 5)
[INFO][2018-06-06 15:52:39,667][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/sta_vem/vem_machine/vem_machine__12fe7e5b_689f_498e_b2a6_8411fd976c5b:3998718+3998719
[INFO][2018-06-06 15:52:39,667][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/sta_vem/vem_machine/vem_machine__12fe7e5b_689f_498e_b2a6_8411fd976c5b:0+3998718
[INFO][2018-06-06 15:52:39,956][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_3_piece0 on 10.194.32.157:55875 in memory (size: 22.1 KB, free: 911.9 MB)
[INFO][2018-06-06 15:52:39,959][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_4_piece0 on 10.194.32.157:55875 in memory (size: 2.0 KB, free: 911.9 MB)
[INFO][2018-06-06 15:52:40,209][org.apache.spark.executor.Executor]Finished task 0.0 in stage 2.0 (TID 4). 1000 bytes result sent to driver
[INFO][2018-06-06 15:52:40,227][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 2.0 (TID 4) in 569 ms on localhost (executor driver) (1/2)
[INFO][2018-06-06 15:52:42,672][org.apache.spark.executor.Executor]Finished task 1.0 in stage 2.0 (TID 5). 1086 bytes result sent to driver
[INFO][2018-06-06 15:52:42,673][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 2.0 (TID 5) in 3013 ms on localhost (executor driver) (2/2)
[INFO][2018-06-06 15:52:42,673][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO][2018-06-06 15:52:42,673][org.apache.spark.scheduler.DAGScheduler]ShuffleMapStage 2 (mapPartitions at SalesDefectNetByNetCommunity.scala:107) finished in 3.015 s
[INFO][2018-06-06 15:52:42,674][org.apache.spark.scheduler.DAGScheduler]looking for newly runnable stages
[INFO][2018-06-06 15:52:42,675][org.apache.spark.scheduler.DAGScheduler]running: Set()
[INFO][2018-06-06 15:52:42,675][org.apache.spark.scheduler.DAGScheduler]waiting: Set(ResultStage 3)
[INFO][2018-06-06 15:52:42,676][org.apache.spark.scheduler.DAGScheduler]failed: Set()
[INFO][2018-06-06 15:52:42,679][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 3 (MapPartitionsRDD[14] at filter at SalesDefectNetByNetCommunity.scala:121), which has no missing parents
[INFO][2018-06-06 15:52:42,685][org.apache.spark.storage.memory.MemoryStore]Block broadcast_8 stored as values in memory (estimated size 4.6 KB, free 908.6 MB)
[INFO][2018-06-06 15:52:42,689][org.apache.spark.storage.memory.MemoryStore]Block broadcast_8_piece0 stored as bytes in memory (estimated size 2.6 KB, free 908.6 MB)
[INFO][2018-06-06 15:52:42,690][org.apache.spark.storage.BlockManagerInfo]Added broadcast_8_piece0 in memory on 10.194.32.157:55875 (size: 2.6 KB, free: 911.9 MB)
[INFO][2018-06-06 15:52:42,691][org.apache.spark.SparkContext]Created broadcast 8 from broadcast at DAGScheduler.scala:1006
[INFO][2018-06-06 15:52:42,691][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ResultStage 3 (MapPartitionsRDD[14] at filter at SalesDefectNetByNetCommunity.scala:121) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2018-06-06 15:52:42,691][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 3.0 with 2 tasks
[INFO][2018-06-06 15:52:42,692][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 3.0 (TID 6, localhost, executor driver, partition 0, ANY, 4621 bytes)
[INFO][2018-06-06 15:52:42,693][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 3.0 (TID 7, localhost, executor driver, partition 1, ANY, 4621 bytes)
[INFO][2018-06-06 15:52:42,693][org.apache.spark.executor.Executor]Running task 1.0 in stage 3.0 (TID 7)
[INFO][2018-06-06 15:52:42,693][org.apache.spark.executor.Executor]Running task 0.0 in stage 3.0 (TID 6)
[INFO][2018-06-06 15:52:42,713][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 2 blocks
[INFO][2018-06-06 15:52:42,713][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 2 blocks
[INFO][2018-06-06 15:52:42,716][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 9 ms
[INFO][2018-06-06 15:52:42,716][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 9 ms
[INFO][2018-06-06 15:52:42,775][org.apache.spark.storage.memory.MemoryStore]Block rdd_14_1 stored as values in memory (estimated size 152.0 B, free 908.6 MB)
[INFO][2018-06-06 15:52:42,776][org.apache.spark.storage.BlockManagerInfo]Added rdd_14_1 in memory on 10.194.32.157:55875 (size: 152.0 B, free: 911.9 MB)
[WARN][2018-06-06 15:52:42,782][org.apache.spark.storage.BlockManager]Putting block rdd_14_0 failed due to an exception
[WARN][2018-06-06 15:52:42,782][org.apache.spark.storage.BlockManager]Block rdd_14_0 could not be removed as it was not found on disk or in memory
[INFO][2018-06-06 15:52:42,787][org.apache.spark.executor.Executor]Finished task 1.0 in stage 3.0 (TID 7). 1750 bytes result sent to driver
[ERROR][2018-06-06 15:52:42,788][org.apache.spark.executor.Executor]Exception in task 0.0 in stage 3.0 (TID 6)
java.lang.NullPointerException
	at com.seven.spark.rdd.SalesDefectNetByNetCommunity$$anonfun$9.apply(SalesDefectNetByNetCommunity.scala:123)
	at com.seven.spark.rdd.SalesDefectNetByNetCommunity$$anonfun$9.apply(SalesDefectNetByNetCommunity.scala:121)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:464)
	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:215)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1038)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1029)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:969)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1029)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:760)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO][2018-06-06 15:52:42,788][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 3.0 (TID 7) in 95 ms on localhost (executor driver) (1/2)
[WARN][2018-06-06 15:52:42,814][org.apache.spark.scheduler.TaskSetManager]Lost task 0.0 in stage 3.0 (TID 6, localhost, executor driver): java.lang.NullPointerException
	at com.seven.spark.rdd.SalesDefectNetByNetCommunity$$anonfun$9.apply(SalesDefectNetByNetCommunity.scala:123)
	at com.seven.spark.rdd.SalesDefectNetByNetCommunity$$anonfun$9.apply(SalesDefectNetByNetCommunity.scala:121)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:464)
	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:215)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1038)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1029)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:969)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1029)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:760)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[ERROR][2018-06-06 15:52:42,816][org.apache.spark.scheduler.TaskSetManager]Task 0 in stage 3.0 failed 1 times; aborting job
[INFO][2018-06-06 15:52:42,816][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 3.0, whose tasks have all completed, from pool 
[INFO][2018-06-06 15:52:42,822][org.apache.spark.scheduler.TaskSchedulerImpl]Cancelling stage 3
[INFO][2018-06-06 15:52:42,823][org.apache.spark.scheduler.DAGScheduler]ResultStage 3 (foreach at SalesDefectNetByNetCommunity.scala:126) failed in 0.130 s due to Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 6, localhost, executor driver): java.lang.NullPointerException
	at com.seven.spark.rdd.SalesDefectNetByNetCommunity$$anonfun$9.apply(SalesDefectNetByNetCommunity.scala:123)
	at com.seven.spark.rdd.SalesDefectNetByNetCommunity$$anonfun$9.apply(SalesDefectNetByNetCommunity.scala:121)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:464)
	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:215)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1038)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1029)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:969)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1029)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:760)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
[INFO][2018-06-06 15:52:42,825][org.apache.spark.scheduler.DAGScheduler]Job 2 failed: foreach at SalesDefectNetByNetCommunity.scala:126, took 3.208875 s
[INFO][2018-06-06 15:52:42,828][org.apache.spark.SparkContext]Invoking stop() from shutdown hook
[INFO][2018-06-06 15:52:42,836][org.spark_project.jetty.server.AbstractConnector]Stopped Spark@695b90c5{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-06-06 15:52:42,838][org.apache.spark.ui.SparkUI]Stopped Spark web UI at http://10.194.32.157:4040
[INFO][2018-06-06 15:52:42,847][org.apache.spark.MapOutputTrackerMasterEndpoint]MapOutputTrackerMasterEndpoint stopped!
[INFO][2018-06-06 15:52:42,866][org.apache.spark.storage.memory.MemoryStore]MemoryStore cleared
[INFO][2018-06-06 15:52:42,866][org.apache.spark.storage.BlockManager]BlockManager stopped
[INFO][2018-06-06 15:52:42,867][org.apache.spark.storage.BlockManagerMaster]BlockManagerMaster stopped
[INFO][2018-06-06 15:52:42,869][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint]OutputCommitCoordinator stopped!
[INFO][2018-06-06 15:52:42,871][org.apache.spark.SparkContext]Successfully stopped SparkContext
[INFO][2018-06-06 15:52:42,872][org.apache.spark.util.ShutdownHookManager]Shutdown hook called
[INFO][2018-06-06 15:52:42,873][org.apache.spark.util.ShutdownHookManager]Deleting directory /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/spark-5c113ea1-12be-454f-bc91-5752d38fed97
[INFO][2018-06-06 15:53:10,265][org.apache.spark.SparkContext]Running Spark version 2.2.0
[INFO][2018-06-06 15:53:10,994][org.apache.spark.SparkContext]Submitted application: SalesDefectNetByNetCommunity$
[INFO][2018-06-06 15:53:11,013][org.apache.spark.SecurityManager]Changing view acls to: seven
[INFO][2018-06-06 15:53:11,014][org.apache.spark.SecurityManager]Changing modify acls to: seven
[INFO][2018-06-06 15:53:11,014][org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO][2018-06-06 15:53:11,015][org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO][2018-06-06 15:53:11,015][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(seven); groups with view permissions: Set(); users  with modify permissions: Set(seven); groups with modify permissions: Set()
[INFO][2018-06-06 15:53:11,321][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 55889.
[INFO][2018-06-06 15:53:11,345][org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO][2018-06-06 15:53:11,360][org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO][2018-06-06 15:53:11,363][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2018-06-06 15:53:11,364][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
[INFO][2018-06-06 15:53:11,374][org.apache.spark.storage.DiskBlockManager]Created local directory at /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/blockmgr-dc2eb445-637d-49a8-a2dd-bfcbe22aa7a1
[INFO][2018-06-06 15:53:11,391][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 912.3 MB
[INFO][2018-06-06 15:53:11,468][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO][2018-06-06 15:53:11,543][org.spark_project.jetty.util.log]Logging initialized @2129ms
[INFO][2018-06-06 15:53:11,610][org.spark_project.jetty.server.Server]jetty-9.3.z-SNAPSHOT
[INFO][2018-06-06 15:53:11,623][org.spark_project.jetty.server.Server]Started @2210ms
[INFO][2018-06-06 15:53:11,642][org.spark_project.jetty.server.AbstractConnector]Started ServerConnector@57687f1c{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-06-06 15:53:11,643][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4040.
[INFO][2018-06-06 15:53:11,667][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@70dd7e15{/jobs,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:53:11,668][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@60fa3495{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:53:11,669][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@79e18e38{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:53:11,671][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@69c79f09{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:53:11,671][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5fcacc0{/stages,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:53:11,673][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6d1d4d7{/stages/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:53:11,674][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6865c751{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:53:11,677][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5b6813df{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:53:11,677][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2b58f754{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:53:11,678][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2552f2cb{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:53:11,679][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5f3b9c57{/storage,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:53:11,680][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2cf23c81{/storage/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:53:11,680][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@35fe2125{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:53:11,681][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@34645867{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:53:11,682][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@60b71e8f{/environment,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:53:11,682][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@464649c{/environment/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:53:11,683][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5f59185e{/executors,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:53:11,684][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@47da3952{/executors/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:53:11,684][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@46e8a539{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:53:11,685][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5fd62371{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:53:11,696][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2b62442c{/static,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:53:11,697][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@362a019c{/,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:53:11,700][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5c48c0c0{/api,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:53:11,701][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5fb97279{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:53:11,702][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@61861a29{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:53:11,704][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://10.194.32.157:4040
[INFO][2018-06-06 15:53:11,821][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
[INFO][2018-06-06 15:53:11,858][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 55890.
[INFO][2018-06-06 15:53:11,865][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 10.194.32.157:55890
[INFO][2018-06-06 15:53:11,867][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2018-06-06 15:53:11,869][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 10.194.32.157, 55890, None)
[INFO][2018-06-06 15:53:11,873][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 10.194.32.157:55890 with 912.3 MB RAM, BlockManagerId(driver, 10.194.32.157, 55890, None)
[INFO][2018-06-06 15:53:11,877][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 10.194.32.157, 55890, None)
[INFO][2018-06-06 15:53:11,878][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 10.194.32.157, 55890, None)
[INFO][2018-06-06 15:53:12,183][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1f9d6c7b{/metrics/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:53:12,207][com.seven.spark.rdd.SalesDefectNetByNetCommunity$]SalesDefectNetByNetCommunity$ is start . . .
[INFO][2018-06-06 15:53:12,746][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0 stored as values in memory (estimated size 228.1 KB, free 912.1 MB)
[INFO][2018-06-06 15:53:12,923][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0_piece0 stored as bytes in memory (estimated size 22.1 KB, free 912.1 MB)
[INFO][2018-06-06 15:53:12,925][org.apache.spark.storage.BlockManagerInfo]Added broadcast_0_piece0 in memory on 10.194.32.157:55890 (size: 22.1 KB, free: 912.3 MB)
[INFO][2018-06-06 15:53:12,930][org.apache.spark.SparkContext]Created broadcast 0 from textFile at SalesDefectNetByNetCommunity.scala:53
[WARN][2018-06-06 15:53:13,297][org.apache.hadoop.util.NativeCodeLoader]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO][2018-06-06 15:53:13,420][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 1
[INFO][2018-06-06 15:53:13,495][org.apache.spark.SparkContext]Starting job: collect at SalesDefectNetByNetCommunity.scala:64
[INFO][2018-06-06 15:53:13,505][org.apache.spark.scheduler.DAGScheduler]Got job 0 (collect at SalesDefectNetByNetCommunity.scala:64) with 2 output partitions
[INFO][2018-06-06 15:53:13,506][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 0 (collect at SalesDefectNetByNetCommunity.scala:64)
[INFO][2018-06-06 15:53:13,506][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-06-06 15:53:13,507][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-06-06 15:53:13,516][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 0 (MapPartitionsRDD[3] at mapPartitions at SalesDefectNetByNetCommunity.scala:56), which has no missing parents
[INFO][2018-06-06 15:53:13,538][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1 stored as values in memory (estimated size 3.7 KB, free 912.1 MB)
[INFO][2018-06-06 15:53:13,545][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.1 KB, free 912.1 MB)
[INFO][2018-06-06 15:53:13,546][org.apache.spark.storage.BlockManagerInfo]Added broadcast_1_piece0 in memory on 10.194.32.157:55890 (size: 2.1 KB, free: 912.3 MB)
[INFO][2018-06-06 15:53:13,547][org.apache.spark.SparkContext]Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO][2018-06-06 15:53:13,568][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at mapPartitions at SalesDefectNetByNetCommunity.scala:56) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2018-06-06 15:53:13,569][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 0.0 with 2 tasks
[INFO][2018-06-06 15:53:13,607][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 4930 bytes)
[INFO][2018-06-06 15:53:13,609][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 4930 bytes)
[INFO][2018-06-06 15:53:13,617][org.apache.spark.executor.Executor]Running task 0.0 in stage 0.0 (TID 0)
[INFO][2018-06-06 15:53:13,617][org.apache.spark.executor.Executor]Running task 1.0 in stage 0.0 (TID 1)
[INFO][2018-06-06 15:53:13,668][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/sta_vem/net_community/net_community__b4d64c39_5c6b_44c1_81be_f9aa19c55a30:0+278620
[INFO][2018-06-06 15:53:13,669][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/sta_vem/net_community/net_community__b4d64c39_5c6b_44c1_81be_f9aa19c55a30:278620+278620
[INFO][2018-06-06 15:53:13,999][org.apache.spark.executor.Executor]Finished task 1.0 in stage 0.0 (TID 1). 290507 bytes result sent to driver
[INFO][2018-06-06 15:53:13,999][org.apache.spark.executor.Executor]Finished task 0.0 in stage 0.0 (TID 0). 289675 bytes result sent to driver
[INFO][2018-06-06 15:53:14,028][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 0.0 (TID 1) in 417 ms on localhost (executor driver) (1/2)
[INFO][2018-06-06 15:53:14,029][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 0.0 (TID 0) in 435 ms on localhost (executor driver) (2/2)
[INFO][2018-06-06 15:53:14,030][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO][2018-06-06 15:53:14,033][org.apache.spark.scheduler.DAGScheduler]ResultStage 0 (collect at SalesDefectNetByNetCommunity.scala:64) finished in 0.448 s
[INFO][2018-06-06 15:53:14,038][org.apache.spark.scheduler.DAGScheduler]Job 0 finished: collect at SalesDefectNetByNetCommunity.scala:64, took 0.542768 s
[INFO][2018-06-06 15:53:14,047][org.apache.spark.storage.memory.MemoryStore]Block broadcast_2 stored as values in memory (estimated size 1060.4 KB, free 911.0 MB)
[INFO][2018-06-06 15:53:14,066][org.apache.spark.storage.memory.MemoryStore]Block broadcast_2_piece0 stored as bytes in memory (estimated size 203.6 KB, free 910.8 MB)
[INFO][2018-06-06 15:53:14,067][org.apache.spark.storage.BlockManagerInfo]Added broadcast_2_piece0 in memory on 10.194.32.157:55890 (size: 203.6 KB, free: 912.1 MB)
[INFO][2018-06-06 15:53:14,068][org.apache.spark.SparkContext]Created broadcast 2 from broadcast at SalesDefectNetByNetCommunity.scala:34
[INFO][2018-06-06 15:53:14,072][org.apache.spark.storage.memory.MemoryStore]Block broadcast_3 stored as values in memory (estimated size 228.1 KB, free 910.6 MB)
[INFO][2018-06-06 15:53:14,093][org.apache.spark.storage.memory.MemoryStore]Block broadcast_3_piece0 stored as bytes in memory (estimated size 22.1 KB, free 910.6 MB)
[INFO][2018-06-06 15:53:14,094][org.apache.spark.storage.BlockManagerInfo]Added broadcast_3_piece0 in memory on 10.194.32.157:55890 (size: 22.1 KB, free: 912.1 MB)
[INFO][2018-06-06 15:53:14,095][org.apache.spark.SparkContext]Created broadcast 3 from textFile at SalesDefectNetByNetCommunity.scala:79
[INFO][2018-06-06 15:53:14,132][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 1
[INFO][2018-06-06 15:53:14,151][org.apache.spark.SparkContext]Starting job: collect at SalesDefectNetByNetCommunity.scala:89
[INFO][2018-06-06 15:53:14,152][org.apache.spark.scheduler.DAGScheduler]Got job 1 (collect at SalesDefectNetByNetCommunity.scala:89) with 2 output partitions
[INFO][2018-06-06 15:53:14,153][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 1 (collect at SalesDefectNetByNetCommunity.scala:89)
[INFO][2018-06-06 15:53:14,153][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-06-06 15:53:14,153][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-06-06 15:53:14,153][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 1 (MapPartitionsRDD[7] at mapPartitions at SalesDefectNetByNetCommunity.scala:82), which has no missing parents
[INFO][2018-06-06 15:53:14,156][org.apache.spark.storage.memory.MemoryStore]Block broadcast_4 stored as values in memory (estimated size 3.7 KB, free 910.6 MB)
[INFO][2018-06-06 15:53:14,164][org.apache.spark.storage.memory.MemoryStore]Block broadcast_4_piece0 stored as bytes in memory (estimated size 2.0 KB, free 910.6 MB)
[INFO][2018-06-06 15:53:14,165][org.apache.spark.storage.BlockManagerInfo]Added broadcast_4_piece0 in memory on 10.194.32.157:55890 (size: 2.0 KB, free: 912.1 MB)
[INFO][2018-06-06 15:53:14,166][org.apache.spark.SparkContext]Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[INFO][2018-06-06 15:53:14,168][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[7] at mapPartitions at SalesDefectNetByNetCommunity.scala:82) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2018-06-06 15:53:14,168][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 1.0 with 2 tasks
[INFO][2018-06-06 15:53:14,169][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 1.0 (TID 2, localhost, executor driver, partition 0, ANY, 4926 bytes)
[INFO][2018-06-06 15:53:14,169][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 1.0 (TID 3, localhost, executor driver, partition 1, ANY, 4926 bytes)
[INFO][2018-06-06 15:53:14,170][org.apache.spark.executor.Executor]Running task 1.0 in stage 1.0 (TID 3)
[INFO][2018-06-06 15:53:14,170][org.apache.spark.executor.Executor]Running task 0.0 in stage 1.0 (TID 2)
[INFO][2018-06-06 15:53:14,175][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/sta_vem/vem_nettype/vem_nettype__5b80085f_0bae_4bd8_97d4_919813b3ce8b:6079084+6079084
[INFO][2018-06-06 15:53:14,177][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/sta_vem/vem_nettype/vem_nettype__5b80085f_0bae_4bd8_97d4_919813b3ce8b:0+6079084
[INFO][2018-06-06 15:53:14,517][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_1_piece0 on 10.194.32.157:55890 in memory (size: 2.1 KB, free: 912.1 MB)
[INFO][2018-06-06 15:53:18,290][org.apache.spark.executor.Executor]Finished task 1.0 in stage 1.0 (TID 3). 14530 bytes result sent to driver
[INFO][2018-06-06 15:53:18,298][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 1.0 (TID 3) in 4129 ms on localhost (executor driver) (1/2)
[INFO][2018-06-06 15:53:18,467][org.apache.spark.executor.Executor]Finished task 0.0 in stage 1.0 (TID 2). 240458 bytes result sent to driver
[INFO][2018-06-06 15:53:18,491][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 1.0 (TID 2) in 4322 ms on localhost (executor driver) (2/2)
[INFO][2018-06-06 15:53:18,491][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO][2018-06-06 15:53:18,492][org.apache.spark.scheduler.DAGScheduler]ResultStage 1 (collect at SalesDefectNetByNetCommunity.scala:89) finished in 4.322 s
[INFO][2018-06-06 15:53:18,492][org.apache.spark.scheduler.DAGScheduler]Job 1 finished: collect at SalesDefectNetByNetCommunity.scala:89, took 4.340257 s
[INFO][2018-06-06 15:53:18,502][org.apache.spark.storage.memory.MemoryStore]Block broadcast_5 stored as values in memory (estimated size 1924.3 KB, free 908.7 MB)
[INFO][2018-06-06 15:53:18,531][org.apache.spark.storage.memory.MemoryStore]Block broadcast_5_piece0 stored as bytes in memory (estimated size 111.1 KB, free 908.6 MB)
[INFO][2018-06-06 15:53:18,532][org.apache.spark.storage.BlockManagerInfo]Added broadcast_5_piece0 in memory on 10.194.32.157:55890 (size: 111.1 KB, free: 911.9 MB)
[INFO][2018-06-06 15:53:18,533][org.apache.spark.SparkContext]Created broadcast 5 from broadcast at SalesDefectNetByNetCommunity.scala:38
[INFO][2018-06-06 15:53:18,538][org.apache.spark.storage.memory.MemoryStore]Block broadcast_6 stored as values in memory (estimated size 228.1 KB, free 908.4 MB)
[INFO][2018-06-06 15:53:18,559][org.apache.spark.storage.memory.MemoryStore]Block broadcast_6_piece0 stored as bytes in memory (estimated size 22.1 KB, free 908.3 MB)
[INFO][2018-06-06 15:53:18,560][org.apache.spark.storage.BlockManagerInfo]Added broadcast_6_piece0 in memory on 10.194.32.157:55890 (size: 22.1 KB, free: 911.9 MB)
[INFO][2018-06-06 15:53:18,561][org.apache.spark.SparkContext]Created broadcast 6 from textFile at SalesDefectNetByNetCommunity.scala:104
[INFO][2018-06-06 15:53:18,597][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 1
[INFO][2018-06-06 15:53:18,627][org.apache.spark.SparkContext]Starting job: foreach at SalesDefectNetByNetCommunity.scala:126
[INFO][2018-06-06 15:53:18,638][org.apache.spark.scheduler.DAGScheduler]Registering RDD 11 (mapPartitions at SalesDefectNetByNetCommunity.scala:107)
[INFO][2018-06-06 15:53:18,638][org.apache.spark.scheduler.DAGScheduler]Got job 2 (foreach at SalesDefectNetByNetCommunity.scala:126) with 2 output partitions
[INFO][2018-06-06 15:53:18,639][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 3 (foreach at SalesDefectNetByNetCommunity.scala:126)
[INFO][2018-06-06 15:53:18,639][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 2)
[INFO][2018-06-06 15:53:18,641][org.apache.spark.scheduler.DAGScheduler]Missing parents: List(ShuffleMapStage 2)
[INFO][2018-06-06 15:53:18,642][org.apache.spark.scheduler.DAGScheduler]Submitting ShuffleMapStage 2 (MapPartitionsRDD[11] at mapPartitions at SalesDefectNetByNetCommunity.scala:107), which has no missing parents
[INFO][2018-06-06 15:53:18,653][org.apache.spark.storage.memory.MemoryStore]Block broadcast_7 stored as values in memory (estimated size 4.9 KB, free 908.3 MB)
[INFO][2018-06-06 15:53:18,657][org.apache.spark.storage.memory.MemoryStore]Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.8 KB, free 908.3 MB)
[INFO][2018-06-06 15:53:18,657][org.apache.spark.storage.BlockManagerInfo]Added broadcast_7_piece0 in memory on 10.194.32.157:55890 (size: 2.8 KB, free: 911.9 MB)
[INFO][2018-06-06 15:53:18,658][org.apache.spark.SparkContext]Created broadcast 7 from broadcast at DAGScheduler.scala:1006
[INFO][2018-06-06 15:53:18,660][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[11] at mapPartitions at SalesDefectNetByNetCommunity.scala:107) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2018-06-06 15:53:18,660][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 2.0 with 2 tasks
[INFO][2018-06-06 15:53:18,662][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 2.0 (TID 4, localhost, executor driver, partition 0, ANY, 4915 bytes)
[INFO][2018-06-06 15:53:18,663][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 2.0 (TID 5, localhost, executor driver, partition 1, ANY, 4915 bytes)
[INFO][2018-06-06 15:53:18,663][org.apache.spark.executor.Executor]Running task 1.0 in stage 2.0 (TID 5)
[INFO][2018-06-06 15:53:18,663][org.apache.spark.executor.Executor]Running task 0.0 in stage 2.0 (TID 4)
[INFO][2018-06-06 15:53:18,668][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/sta_vem/vem_machine/vem_machine__12fe7e5b_689f_498e_b2a6_8411fd976c5b:3998718+3998719
[INFO][2018-06-06 15:53:18,668][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/sta_vem/vem_machine/vem_machine__12fe7e5b_689f_498e_b2a6_8411fd976c5b:0+3998718
[INFO][2018-06-06 15:53:21,867][org.apache.spark.executor.Executor]Finished task 0.0 in stage 2.0 (TID 4). 957 bytes result sent to driver
[INFO][2018-06-06 15:53:21,901][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 2.0 (TID 4) in 3239 ms on localhost (executor driver) (1/2)
[INFO][2018-06-06 15:53:22,296][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_4_piece0 on 10.194.32.157:55890 in memory (size: 2.0 KB, free: 911.9 MB)
[INFO][2018-06-06 15:53:22,298][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_3_piece0 on 10.194.32.157:55890 in memory (size: 22.1 KB, free: 911.9 MB)
[INFO][2018-06-06 15:53:22,336][org.apache.spark.executor.Executor]Finished task 1.0 in stage 2.0 (TID 5). 1086 bytes result sent to driver
[INFO][2018-06-06 15:53:22,338][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 2.0 (TID 5) in 3676 ms on localhost (executor driver) (2/2)
[INFO][2018-06-06 15:53:22,338][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO][2018-06-06 15:53:22,339][org.apache.spark.scheduler.DAGScheduler]ShuffleMapStage 2 (mapPartitions at SalesDefectNetByNetCommunity.scala:107) finished in 3.678 s
[INFO][2018-06-06 15:53:22,339][org.apache.spark.scheduler.DAGScheduler]looking for newly runnable stages
[INFO][2018-06-06 15:53:22,340][org.apache.spark.scheduler.DAGScheduler]running: Set()
[INFO][2018-06-06 15:53:22,340][org.apache.spark.scheduler.DAGScheduler]waiting: Set(ResultStage 3)
[INFO][2018-06-06 15:53:22,341][org.apache.spark.scheduler.DAGScheduler]failed: Set()
[INFO][2018-06-06 15:53:22,345][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 3 (MapPartitionsRDD[14] at filter at SalesDefectNetByNetCommunity.scala:121), which has no missing parents
[INFO][2018-06-06 15:53:22,350][org.apache.spark.storage.memory.MemoryStore]Block broadcast_8 stored as values in memory (estimated size 4.6 KB, free 908.6 MB)
[INFO][2018-06-06 15:53:22,353][org.apache.spark.storage.memory.MemoryStore]Block broadcast_8_piece0 stored as bytes in memory (estimated size 2.6 KB, free 908.6 MB)
[INFO][2018-06-06 15:53:22,354][org.apache.spark.storage.BlockManagerInfo]Added broadcast_8_piece0 in memory on 10.194.32.157:55890 (size: 2.6 KB, free: 911.9 MB)
[INFO][2018-06-06 15:53:22,354][org.apache.spark.SparkContext]Created broadcast 8 from broadcast at DAGScheduler.scala:1006
[INFO][2018-06-06 15:53:22,355][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ResultStage 3 (MapPartitionsRDD[14] at filter at SalesDefectNetByNetCommunity.scala:121) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2018-06-06 15:53:22,355][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 3.0 with 2 tasks
[INFO][2018-06-06 15:53:22,356][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 3.0 (TID 6, localhost, executor driver, partition 0, ANY, 4621 bytes)
[INFO][2018-06-06 15:53:22,357][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 3.0 (TID 7, localhost, executor driver, partition 1, ANY, 4621 bytes)
[INFO][2018-06-06 15:53:22,357][org.apache.spark.executor.Executor]Running task 1.0 in stage 3.0 (TID 7)
[INFO][2018-06-06 15:53:22,357][org.apache.spark.executor.Executor]Running task 0.0 in stage 3.0 (TID 6)
[INFO][2018-06-06 15:53:22,385][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 2 blocks
[INFO][2018-06-06 15:53:22,387][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 8 ms
[INFO][2018-06-06 15:53:22,404][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 2 blocks
[INFO][2018-06-06 15:53:22,406][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 2 ms
[INFO][2018-06-06 15:53:22,545][org.apache.spark.storage.memory.MemoryStore]Block rdd_14_1 stored as values in memory (estimated size 152.0 B, free 908.6 MB)
[INFO][2018-06-06 15:53:22,545][org.apache.spark.storage.BlockManagerInfo]Added rdd_14_1 in memory on 10.194.32.157:55890 (size: 152.0 B, free: 911.9 MB)
[INFO][2018-06-06 15:53:22,566][org.apache.spark.storage.memory.MemoryStore]Block rdd_14_0 stored as values in memory (estimated size 352.0 B, free 908.6 MB)
[INFO][2018-06-06 15:53:22,567][org.apache.spark.storage.BlockManagerInfo]Added rdd_14_0 in memory on 10.194.32.157:55890 (size: 352.0 B, free: 911.9 MB)
[INFO][2018-06-06 15:53:22,577][org.apache.spark.executor.Executor]Finished task 0.0 in stage 3.0 (TID 6). 1793 bytes result sent to driver
[INFO][2018-06-06 15:53:22,579][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 3.0 (TID 6) in 224 ms on localhost (executor driver) (1/2)
[INFO][2018-06-06 15:53:22,579][org.apache.spark.executor.Executor]Finished task 1.0 in stage 3.0 (TID 7). 1750 bytes result sent to driver
[INFO][2018-06-06 15:53:22,584][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 3.0 (TID 7) in 227 ms on localhost (executor driver) (2/2)
[INFO][2018-06-06 15:53:22,584][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 3.0, whose tasks have all completed, from pool 
[INFO][2018-06-06 15:53:22,585][org.apache.spark.scheduler.DAGScheduler]ResultStage 3 (foreach at SalesDefectNetByNetCommunity.scala:126) finished in 0.230 s
[INFO][2018-06-06 15:53:22,586][org.apache.spark.scheduler.DAGScheduler]Job 2 finished: foreach at SalesDefectNetByNetCommunity.scala:126, took 3.958157 s
[INFO][2018-06-06 15:53:22,589][com.seven.spark.rdd.SalesDefectNetByNetCommunity$]SalesDefectNetByNetCommunity$ is success . . .
[INFO][2018-06-06 15:53:22,597][org.apache.spark.SparkContext]Invoking stop() from shutdown hook
[INFO][2018-06-06 15:53:22,608][org.spark_project.jetty.server.AbstractConnector]Stopped Spark@57687f1c{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-06-06 15:53:22,616][org.apache.spark.ui.SparkUI]Stopped Spark web UI at http://10.194.32.157:4040
[INFO][2018-06-06 15:53:22,626][org.apache.spark.MapOutputTrackerMasterEndpoint]MapOutputTrackerMasterEndpoint stopped!
[INFO][2018-06-06 15:53:22,655][org.apache.spark.storage.memory.MemoryStore]MemoryStore cleared
[INFO][2018-06-06 15:53:22,656][org.apache.spark.storage.BlockManager]BlockManager stopped
[INFO][2018-06-06 15:53:22,660][org.apache.spark.storage.BlockManagerMaster]BlockManagerMaster stopped
[INFO][2018-06-06 15:53:22,664][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint]OutputCommitCoordinator stopped!
[INFO][2018-06-06 15:53:22,667][org.apache.spark.SparkContext]Successfully stopped SparkContext
[INFO][2018-06-06 15:53:22,667][org.apache.spark.util.ShutdownHookManager]Shutdown hook called
[INFO][2018-06-06 15:53:22,670][org.apache.spark.util.ShutdownHookManager]Deleting directory /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/spark-92125914-5a83-4576-a9f4-dd0a4958573e
[INFO][2018-06-06 15:56:46,787][org.apache.spark.SparkContext]Running Spark version 2.2.0
[INFO][2018-06-06 15:56:47,769][org.apache.spark.SparkContext]Submitted application: SalesDefectNetByNetCommunity$
[INFO][2018-06-06 15:56:47,794][org.apache.spark.SecurityManager]Changing view acls to: seven
[INFO][2018-06-06 15:56:47,794][org.apache.spark.SecurityManager]Changing modify acls to: seven
[INFO][2018-06-06 15:56:47,795][org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO][2018-06-06 15:56:47,796][org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO][2018-06-06 15:56:47,796][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(seven); groups with view permissions: Set(); users  with modify permissions: Set(seven); groups with modify permissions: Set()
[INFO][2018-06-06 15:56:48,088][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 55908.
[INFO][2018-06-06 15:56:48,119][org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO][2018-06-06 15:56:48,139][org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO][2018-06-06 15:56:48,148][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2018-06-06 15:56:48,148][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
[INFO][2018-06-06 15:56:48,159][org.apache.spark.storage.DiskBlockManager]Created local directory at /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/blockmgr-b956a607-7a9c-4d18-be2d-e6cb1a42d2ad
[INFO][2018-06-06 15:56:48,181][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 912.3 MB
[INFO][2018-06-06 15:56:48,280][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO][2018-06-06 15:56:48,404][org.spark_project.jetty.util.log]Logging initialized @2519ms
[INFO][2018-06-06 15:56:48,481][org.spark_project.jetty.server.Server]jetty-9.3.z-SNAPSHOT
[INFO][2018-06-06 15:56:48,494][org.spark_project.jetty.server.Server]Started @2610ms
[INFO][2018-06-06 15:56:48,512][org.spark_project.jetty.server.AbstractConnector]Started ServerConnector@89c10b7{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-06-06 15:56:48,512][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4040.
[INFO][2018-06-06 15:56:48,543][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4a9f80d3{/jobs,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:56:48,544][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3e2822{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:56:48,544][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@29a60c27{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:56:48,546][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1ca25c47{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:56:48,546][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@533b266e{/stages,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:56:48,547][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@89ff02e{/stages/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:56:48,548][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@62679465{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:56:48,549][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5f2606b{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:56:48,550][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3ebff828{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:56:48,551][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@33352f32{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:56:48,551][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1e044120{/storage,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:56:48,552][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3624da92{/storage/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:56:48,553][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@94f6bfb{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:56:48,553][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2484f433{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:56:48,554][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1255b1d1{/environment,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:56:48,555][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7c22d4f{/environment/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:56:48,557][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@60bdf15d{/executors,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:56:48,558][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@51e4ccb3{/executors/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:56:48,570][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@495083a0{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:56:48,570][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@28a0fd6c{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:56:48,587][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@66629f63{/static,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:56:48,588][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1d9bec4d{/,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:56:48,589][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@10c8f62{/api,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:56:48,590][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@439a8f59{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:56:48,591][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@31024624{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:56:48,593][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://10.194.32.157:4040
[INFO][2018-06-06 15:56:48,741][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
[INFO][2018-06-06 15:56:48,774][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 55909.
[INFO][2018-06-06 15:56:48,775][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 10.194.32.157:55909
[INFO][2018-06-06 15:56:48,777][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2018-06-06 15:56:48,786][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 10.194.32.157, 55909, None)
[INFO][2018-06-06 15:56:48,790][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 10.194.32.157:55909 with 912.3 MB RAM, BlockManagerId(driver, 10.194.32.157, 55909, None)
[INFO][2018-06-06 15:56:48,795][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 10.194.32.157, 55909, None)
[INFO][2018-06-06 15:56:48,800][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 10.194.32.157, 55909, None)
[INFO][2018-06-06 15:56:49,004][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6df20ade{/metrics/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:56:49,025][com.seven.spark.rdd.SalesDefectNetByNetCommunity$]SalesDefectNetByNetCommunity$ is start . . .
[INFO][2018-06-06 15:56:49,677][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0 stored as values in memory (estimated size 228.1 KB, free 912.1 MB)
[INFO][2018-06-06 15:56:49,909][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0_piece0 stored as bytes in memory (estimated size 22.1 KB, free 912.1 MB)
[INFO][2018-06-06 15:56:49,911][org.apache.spark.storage.BlockManagerInfo]Added broadcast_0_piece0 in memory on 10.194.32.157:55909 (size: 22.1 KB, free: 912.3 MB)
[INFO][2018-06-06 15:56:49,917][org.apache.spark.SparkContext]Created broadcast 0 from textFile at SalesDefectNetByNetCommunity.scala:53
[WARN][2018-06-06 15:56:51,390][org.apache.hadoop.util.NativeCodeLoader]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO][2018-06-06 15:56:51,508][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 1
[INFO][2018-06-06 15:56:51,585][org.apache.spark.SparkContext]Starting job: collect at SalesDefectNetByNetCommunity.scala:64
[INFO][2018-06-06 15:56:51,596][org.apache.spark.scheduler.DAGScheduler]Got job 0 (collect at SalesDefectNetByNetCommunity.scala:64) with 2 output partitions
[INFO][2018-06-06 15:56:51,596][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 0 (collect at SalesDefectNetByNetCommunity.scala:64)
[INFO][2018-06-06 15:56:51,597][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-06-06 15:56:51,598][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-06-06 15:56:51,607][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 0 (MapPartitionsRDD[3] at mapPartitions at SalesDefectNetByNetCommunity.scala:56), which has no missing parents
[INFO][2018-06-06 15:56:51,627][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1 stored as values in memory (estimated size 3.7 KB, free 912.1 MB)
[INFO][2018-06-06 15:56:51,635][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.1 KB, free 912.1 MB)
[INFO][2018-06-06 15:56:51,636][org.apache.spark.storage.BlockManagerInfo]Added broadcast_1_piece0 in memory on 10.194.32.157:55909 (size: 2.1 KB, free: 912.3 MB)
[INFO][2018-06-06 15:56:51,637][org.apache.spark.SparkContext]Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO][2018-06-06 15:56:51,653][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at mapPartitions at SalesDefectNetByNetCommunity.scala:56) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2018-06-06 15:56:51,654][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 0.0 with 2 tasks
[INFO][2018-06-06 15:56:51,690][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 4930 bytes)
[INFO][2018-06-06 15:56:51,692][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 4930 bytes)
[INFO][2018-06-06 15:56:51,700][org.apache.spark.executor.Executor]Running task 1.0 in stage 0.0 (TID 1)
[INFO][2018-06-06 15:56:51,700][org.apache.spark.executor.Executor]Running task 0.0 in stage 0.0 (TID 0)
[INFO][2018-06-06 15:56:51,756][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/sta_vem/net_community/net_community__b4d64c39_5c6b_44c1_81be_f9aa19c55a30:278620+278620
[INFO][2018-06-06 15:56:51,756][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/sta_vem/net_community/net_community__b4d64c39_5c6b_44c1_81be_f9aa19c55a30:0+278620
[INFO][2018-06-06 15:56:51,989][org.apache.spark.executor.Executor]Finished task 0.0 in stage 0.0 (TID 0). 289675 bytes result sent to driver
[INFO][2018-06-06 15:56:51,989][org.apache.spark.executor.Executor]Finished task 1.0 in stage 0.0 (TID 1). 290507 bytes result sent to driver
[INFO][2018-06-06 15:56:52,020][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 0.0 (TID 1) in 328 ms on localhost (executor driver) (1/2)
[INFO][2018-06-06 15:56:52,021][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 0.0 (TID 0) in 342 ms on localhost (executor driver) (2/2)
[INFO][2018-06-06 15:56:52,023][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO][2018-06-06 15:56:52,028][org.apache.spark.scheduler.DAGScheduler]ResultStage 0 (collect at SalesDefectNetByNetCommunity.scala:64) finished in 0.358 s
[INFO][2018-06-06 15:56:52,032][org.apache.spark.scheduler.DAGScheduler]Job 0 finished: collect at SalesDefectNetByNetCommunity.scala:64, took 0.446308 s
[INFO][2018-06-06 15:56:52,042][org.apache.spark.storage.memory.MemoryStore]Block broadcast_2 stored as values in memory (estimated size 1060.4 KB, free 911.0 MB)
[INFO][2018-06-06 15:56:52,066][org.apache.spark.storage.memory.MemoryStore]Block broadcast_2_piece0 stored as bytes in memory (estimated size 203.6 KB, free 910.8 MB)
[INFO][2018-06-06 15:56:52,066][org.apache.spark.storage.BlockManagerInfo]Added broadcast_2_piece0 in memory on 10.194.32.157:55909 (size: 203.6 KB, free: 912.1 MB)
[INFO][2018-06-06 15:56:52,067][org.apache.spark.SparkContext]Created broadcast 2 from broadcast at SalesDefectNetByNetCommunity.scala:34
[INFO][2018-06-06 15:56:52,074][org.apache.spark.storage.memory.MemoryStore]Block broadcast_3 stored as values in memory (estimated size 228.1 KB, free 910.6 MB)
[INFO][2018-06-06 15:56:52,097][org.apache.spark.storage.memory.MemoryStore]Block broadcast_3_piece0 stored as bytes in memory (estimated size 22.1 KB, free 910.6 MB)
[INFO][2018-06-06 15:56:52,098][org.apache.spark.storage.BlockManagerInfo]Added broadcast_3_piece0 in memory on 10.194.32.157:55909 (size: 22.1 KB, free: 912.1 MB)
[INFO][2018-06-06 15:56:52,099][org.apache.spark.SparkContext]Created broadcast 3 from textFile at SalesDefectNetByNetCommunity.scala:79
[INFO][2018-06-06 15:56:52,133][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 1
[INFO][2018-06-06 15:56:52,149][org.apache.spark.SparkContext]Starting job: collect at SalesDefectNetByNetCommunity.scala:89
[INFO][2018-06-06 15:56:52,150][org.apache.spark.scheduler.DAGScheduler]Got job 1 (collect at SalesDefectNetByNetCommunity.scala:89) with 2 output partitions
[INFO][2018-06-06 15:56:52,150][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 1 (collect at SalesDefectNetByNetCommunity.scala:89)
[INFO][2018-06-06 15:56:52,150][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-06-06 15:56:52,150][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-06-06 15:56:52,151][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 1 (MapPartitionsRDD[7] at mapPartitions at SalesDefectNetByNetCommunity.scala:82), which has no missing parents
[INFO][2018-06-06 15:56:52,153][org.apache.spark.storage.memory.MemoryStore]Block broadcast_4 stored as values in memory (estimated size 3.7 KB, free 910.6 MB)
[INFO][2018-06-06 15:56:52,163][org.apache.spark.storage.memory.MemoryStore]Block broadcast_4_piece0 stored as bytes in memory (estimated size 2.0 KB, free 910.6 MB)
[INFO][2018-06-06 15:56:52,164][org.apache.spark.storage.BlockManagerInfo]Added broadcast_4_piece0 in memory on 10.194.32.157:55909 (size: 2.0 KB, free: 912.1 MB)
[INFO][2018-06-06 15:56:52,164][org.apache.spark.SparkContext]Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[INFO][2018-06-06 15:56:52,165][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[7] at mapPartitions at SalesDefectNetByNetCommunity.scala:82) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2018-06-06 15:56:52,166][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 1.0 with 2 tasks
[INFO][2018-06-06 15:56:52,167][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 1.0 (TID 2, localhost, executor driver, partition 0, ANY, 4926 bytes)
[INFO][2018-06-06 15:56:52,167][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 1.0 (TID 3, localhost, executor driver, partition 1, ANY, 4926 bytes)
[INFO][2018-06-06 15:56:52,168][org.apache.spark.executor.Executor]Running task 0.0 in stage 1.0 (TID 2)
[INFO][2018-06-06 15:56:52,168][org.apache.spark.executor.Executor]Running task 1.0 in stage 1.0 (TID 3)
[INFO][2018-06-06 15:56:52,172][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/sta_vem/vem_nettype/vem_nettype__5b80085f_0bae_4bd8_97d4_919813b3ce8b:0+6079084
[INFO][2018-06-06 15:56:52,175][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/sta_vem/vem_nettype/vem_nettype__5b80085f_0bae_4bd8_97d4_919813b3ce8b:6079084+6079084
[INFO][2018-06-06 15:56:52,329][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_1_piece0 on 10.194.32.157:55909 in memory (size: 2.1 KB, free: 912.1 MB)
[INFO][2018-06-06 15:56:55,806][org.apache.spark.executor.Executor]Finished task 1.0 in stage 1.0 (TID 3). 14530 bytes result sent to driver
[INFO][2018-06-06 15:56:55,817][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 1.0 (TID 3) in 3650 ms on localhost (executor driver) (1/2)
[INFO][2018-06-06 15:56:56,467][org.apache.spark.executor.Executor]Finished task 0.0 in stage 1.0 (TID 2). 240458 bytes result sent to driver
[INFO][2018-06-06 15:56:56,494][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 1.0 (TID 2) in 4328 ms on localhost (executor driver) (2/2)
[INFO][2018-06-06 15:56:56,494][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO][2018-06-06 15:56:56,495][org.apache.spark.scheduler.DAGScheduler]ResultStage 1 (collect at SalesDefectNetByNetCommunity.scala:89) finished in 4.329 s
[INFO][2018-06-06 15:56:56,495][org.apache.spark.scheduler.DAGScheduler]Job 1 finished: collect at SalesDefectNetByNetCommunity.scala:89, took 4.346122 s
[INFO][2018-06-06 15:56:56,503][org.apache.spark.storage.memory.MemoryStore]Block broadcast_5 stored as values in memory (estimated size 1924.3 KB, free 908.7 MB)
[INFO][2018-06-06 15:56:56,525][org.apache.spark.storage.memory.MemoryStore]Block broadcast_5_piece0 stored as bytes in memory (estimated size 111.1 KB, free 908.6 MB)
[INFO][2018-06-06 15:56:56,526][org.apache.spark.storage.BlockManagerInfo]Added broadcast_5_piece0 in memory on 10.194.32.157:55909 (size: 111.1 KB, free: 911.9 MB)
[INFO][2018-06-06 15:56:56,528][org.apache.spark.SparkContext]Created broadcast 5 from broadcast at SalesDefectNetByNetCommunity.scala:38
[INFO][2018-06-06 15:56:56,532][org.apache.spark.storage.memory.MemoryStore]Block broadcast_6 stored as values in memory (estimated size 228.1 KB, free 908.4 MB)
[INFO][2018-06-06 15:56:56,552][org.apache.spark.storage.memory.MemoryStore]Block broadcast_6_piece0 stored as bytes in memory (estimated size 22.1 KB, free 908.3 MB)
[INFO][2018-06-06 15:56:56,553][org.apache.spark.storage.BlockManagerInfo]Added broadcast_6_piece0 in memory on 10.194.32.157:55909 (size: 22.1 KB, free: 911.9 MB)
[INFO][2018-06-06 15:56:56,554][org.apache.spark.SparkContext]Created broadcast 6 from textFile at SalesDefectNetByNetCommunity.scala:104
[INFO][2018-06-06 15:56:56,603][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 1
[INFO][2018-06-06 15:56:56,630][org.apache.spark.SparkContext]Starting job: foreach at SalesDefectNetByNetCommunity.scala:128
[INFO][2018-06-06 15:56:56,642][org.apache.spark.scheduler.DAGScheduler]Registering RDD 11 (mapPartitions at SalesDefectNetByNetCommunity.scala:107)
[INFO][2018-06-06 15:56:56,643][org.apache.spark.scheduler.DAGScheduler]Got job 2 (foreach at SalesDefectNetByNetCommunity.scala:128) with 2 output partitions
[INFO][2018-06-06 15:56:56,643][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 3 (foreach at SalesDefectNetByNetCommunity.scala:128)
[INFO][2018-06-06 15:56:56,643][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 2)
[INFO][2018-06-06 15:56:56,646][org.apache.spark.scheduler.DAGScheduler]Missing parents: List(ShuffleMapStage 2)
[INFO][2018-06-06 15:56:56,647][org.apache.spark.scheduler.DAGScheduler]Submitting ShuffleMapStage 2 (MapPartitionsRDD[11] at mapPartitions at SalesDefectNetByNetCommunity.scala:107), which has no missing parents
[INFO][2018-06-06 15:56:56,659][org.apache.spark.storage.memory.MemoryStore]Block broadcast_7 stored as values in memory (estimated size 4.9 KB, free 908.3 MB)
[INFO][2018-06-06 15:56:56,663][org.apache.spark.storage.memory.MemoryStore]Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.8 KB, free 908.3 MB)
[INFO][2018-06-06 15:56:56,664][org.apache.spark.storage.BlockManagerInfo]Added broadcast_7_piece0 in memory on 10.194.32.157:55909 (size: 2.8 KB, free: 911.9 MB)
[INFO][2018-06-06 15:56:56,664][org.apache.spark.SparkContext]Created broadcast 7 from broadcast at DAGScheduler.scala:1006
[INFO][2018-06-06 15:56:56,667][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[11] at mapPartitions at SalesDefectNetByNetCommunity.scala:107) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2018-06-06 15:56:56,667][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 2.0 with 2 tasks
[INFO][2018-06-06 15:56:56,669][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 2.0 (TID 4, localhost, executor driver, partition 0, ANY, 4915 bytes)
[INFO][2018-06-06 15:56:56,669][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 2.0 (TID 5, localhost, executor driver, partition 1, ANY, 4915 bytes)
[INFO][2018-06-06 15:56:56,670][org.apache.spark.executor.Executor]Running task 0.0 in stage 2.0 (TID 4)
[INFO][2018-06-06 15:56:56,670][org.apache.spark.executor.Executor]Running task 1.0 in stage 2.0 (TID 5)
[INFO][2018-06-06 15:56:56,683][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/sta_vem/vem_machine/vem_machine__12fe7e5b_689f_498e_b2a6_8411fd976c5b:3998718+3998719
[INFO][2018-06-06 15:56:56,683][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/sta_vem/vem_machine/vem_machine__12fe7e5b_689f_498e_b2a6_8411fd976c5b:0+3998718
[INFO][2018-06-06 15:56:59,209][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_4_piece0 on 10.194.32.157:55909 in memory (size: 2.0 KB, free: 911.9 MB)
[INFO][2018-06-06 15:56:59,218][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_3_piece0 on 10.194.32.157:55909 in memory (size: 22.1 KB, free: 911.9 MB)
[INFO][2018-06-06 15:56:59,286][org.apache.spark.executor.Executor]Finished task 1.0 in stage 2.0 (TID 5). 1129 bytes result sent to driver
[INFO][2018-06-06 15:56:59,305][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 2.0 (TID 5) in 2636 ms on localhost (executor driver) (1/2)
[INFO][2018-06-06 15:56:59,377][org.apache.spark.executor.Executor]Finished task 0.0 in stage 2.0 (TID 4). 1000 bytes result sent to driver
[INFO][2018-06-06 15:56:59,378][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 2.0 (TID 4) in 2710 ms on localhost (executor driver) (2/2)
[INFO][2018-06-06 15:56:59,379][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO][2018-06-06 15:56:59,379][org.apache.spark.scheduler.DAGScheduler]ShuffleMapStage 2 (mapPartitions at SalesDefectNetByNetCommunity.scala:107) finished in 2.711 s
[INFO][2018-06-06 15:56:59,380][org.apache.spark.scheduler.DAGScheduler]looking for newly runnable stages
[INFO][2018-06-06 15:56:59,380][org.apache.spark.scheduler.DAGScheduler]running: Set()
[INFO][2018-06-06 15:56:59,381][org.apache.spark.scheduler.DAGScheduler]waiting: Set(ResultStage 3)
[INFO][2018-06-06 15:56:59,381][org.apache.spark.scheduler.DAGScheduler]failed: Set()
[INFO][2018-06-06 15:56:59,384][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 3 (ShuffledRDD[12] at reduceByKey at SalesDefectNetByNetCommunity.scala:114), which has no missing parents
[INFO][2018-06-06 15:56:59,388][org.apache.spark.storage.memory.MemoryStore]Block broadcast_8 stored as values in memory (estimated size 3.0 KB, free 908.6 MB)
[INFO][2018-06-06 15:56:59,391][org.apache.spark.storage.memory.MemoryStore]Block broadcast_8_piece0 stored as bytes in memory (estimated size 1892.0 B, free 908.6 MB)
[INFO][2018-06-06 15:56:59,392][org.apache.spark.storage.BlockManagerInfo]Added broadcast_8_piece0 in memory on 10.194.32.157:55909 (size: 1892.0 B, free: 911.9 MB)
[INFO][2018-06-06 15:56:59,392][org.apache.spark.SparkContext]Created broadcast 8 from broadcast at DAGScheduler.scala:1006
[INFO][2018-06-06 15:56:59,393][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ResultStage 3 (ShuffledRDD[12] at reduceByKey at SalesDefectNetByNetCommunity.scala:114) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2018-06-06 15:56:59,393][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 3.0 with 2 tasks
[INFO][2018-06-06 15:56:59,394][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 3.0 (TID 6, localhost, executor driver, partition 0, ANY, 4621 bytes)
[INFO][2018-06-06 15:56:59,394][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 3.0 (TID 7, localhost, executor driver, partition 1, ANY, 4621 bytes)
[INFO][2018-06-06 15:56:59,395][org.apache.spark.executor.Executor]Running task 0.0 in stage 3.0 (TID 6)
[INFO][2018-06-06 15:56:59,395][org.apache.spark.executor.Executor]Running task 1.0 in stage 3.0 (TID 7)
[INFO][2018-06-06 15:56:59,410][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 2 blocks
[INFO][2018-06-06 15:56:59,410][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 2 blocks
[INFO][2018-06-06 15:56:59,412][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 5 ms
[INFO][2018-06-06 15:56:59,412][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 5 ms
[INFO][2018-06-06 15:56:59,467][org.apache.spark.storage.memory.MemoryStore]Block rdd_12_0 stored as values in memory (estimated size 216.9 KB, free 908.4 MB)
[INFO][2018-06-06 15:56:59,467][org.apache.spark.storage.BlockManagerInfo]Added rdd_12_0 in memory on 10.194.32.157:55909 (size: 216.9 KB, free: 911.7 MB)
[INFO][2018-06-06 15:56:59,468][org.apache.spark.storage.memory.MemoryStore]Block rdd_12_1 stored as values in memory (estimated size 240.3 KB, free 908.1 MB)
[INFO][2018-06-06 15:56:59,468][org.apache.spark.storage.BlockManagerInfo]Added rdd_12_1 in memory on 10.194.32.157:55909 (size: 240.3 KB, free: 911.5 MB)
[INFO][2018-06-06 15:56:59,487][org.apache.spark.executor.Executor]Finished task 0.0 in stage 3.0 (TID 6). 1750 bytes result sent to driver
[INFO][2018-06-06 15:56:59,489][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 3.0 (TID 6) in 96 ms on localhost (executor driver) (1/2)
[INFO][2018-06-06 15:56:59,507][org.apache.spark.executor.Executor]Finished task 1.0 in stage 3.0 (TID 7). 1750 bytes result sent to driver
[INFO][2018-06-06 15:56:59,508][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 3.0 (TID 7) in 114 ms on localhost (executor driver) (2/2)
[INFO][2018-06-06 15:56:59,508][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 3.0, whose tasks have all completed, from pool 
[INFO][2018-06-06 15:56:59,509][org.apache.spark.scheduler.DAGScheduler]ResultStage 3 (foreach at SalesDefectNetByNetCommunity.scala:128) finished in 0.116 s
[INFO][2018-06-06 15:56:59,510][org.apache.spark.scheduler.DAGScheduler]Job 2 finished: foreach at SalesDefectNetByNetCommunity.scala:128, took 2.879629 s
[INFO][2018-06-06 15:56:59,510][com.seven.spark.rdd.SalesDefectNetByNetCommunity$]SalesDefectNetByNetCommunity$ is success . . .
[INFO][2018-06-06 15:56:59,513][org.apache.spark.SparkContext]Invoking stop() from shutdown hook
[INFO][2018-06-06 15:56:59,522][org.spark_project.jetty.server.AbstractConnector]Stopped Spark@89c10b7{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-06-06 15:56:59,523][org.apache.spark.ui.SparkUI]Stopped Spark web UI at http://10.194.32.157:4040
[INFO][2018-06-06 15:56:59,533][org.apache.spark.MapOutputTrackerMasterEndpoint]MapOutputTrackerMasterEndpoint stopped!
[INFO][2018-06-06 15:56:59,552][org.apache.spark.storage.memory.MemoryStore]MemoryStore cleared
[INFO][2018-06-06 15:56:59,553][org.apache.spark.storage.BlockManager]BlockManager stopped
[INFO][2018-06-06 15:56:59,554][org.apache.spark.storage.BlockManagerMaster]BlockManagerMaster stopped
[INFO][2018-06-06 15:56:59,556][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint]OutputCommitCoordinator stopped!
[INFO][2018-06-06 15:56:59,557][org.apache.spark.SparkContext]Successfully stopped SparkContext
[INFO][2018-06-06 15:56:59,557][org.apache.spark.util.ShutdownHookManager]Shutdown hook called
[INFO][2018-06-06 15:56:59,558][org.apache.spark.util.ShutdownHookManager]Deleting directory /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/spark-6137890f-a894-4ef4-9083-2eee9f147900
[INFO][2018-06-06 15:57:54,229][org.apache.spark.SparkContext]Running Spark version 2.2.0
[INFO][2018-06-06 15:57:55,153][org.apache.spark.SparkContext]Submitted application: SalesDefectNetByNetCommunity$
[INFO][2018-06-06 15:57:55,201][org.apache.spark.SecurityManager]Changing view acls to: seven
[INFO][2018-06-06 15:57:55,201][org.apache.spark.SecurityManager]Changing modify acls to: seven
[INFO][2018-06-06 15:57:55,204][org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO][2018-06-06 15:57:55,205][org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO][2018-06-06 15:57:55,207][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(seven); groups with view permissions: Set(); users  with modify permissions: Set(seven); groups with modify permissions: Set()
[INFO][2018-06-06 15:57:55,587][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 55931.
[INFO][2018-06-06 15:57:55,610][org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO][2018-06-06 15:57:55,627][org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO][2018-06-06 15:57:55,632][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2018-06-06 15:57:55,633][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
[INFO][2018-06-06 15:57:55,649][org.apache.spark.storage.DiskBlockManager]Created local directory at /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/blockmgr-9504b6cf-8f96-4ccf-b965-65b252b57d9b
[INFO][2018-06-06 15:57:55,670][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 912.3 MB
[INFO][2018-06-06 15:57:55,782][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO][2018-06-06 15:57:55,890][org.spark_project.jetty.util.log]Logging initialized @2646ms
[INFO][2018-06-06 15:57:55,951][org.spark_project.jetty.server.Server]jetty-9.3.z-SNAPSHOT
[INFO][2018-06-06 15:57:55,965][org.spark_project.jetty.server.Server]Started @2722ms
[INFO][2018-06-06 15:57:55,984][org.spark_project.jetty.server.AbstractConnector]Started ServerConnector@734e4fb7{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-06-06 15:57:55,984][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4040.
[INFO][2018-06-06 15:57:56,011][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4a9f80d3{/jobs,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:57:56,012][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3e2822{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:57:56,013][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@29a60c27{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:57:56,016][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1ca25c47{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:57:56,016][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@533b266e{/stages,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:57:56,017][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@89ff02e{/stages/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:57:56,018][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@62679465{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:57:56,019][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5f2606b{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:57:56,020][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3ebff828{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:57:56,023][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@33352f32{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:57:56,025][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1e044120{/storage,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:57:56,026][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3624da92{/storage/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:57:56,027][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@94f6bfb{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:57:56,029][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2484f433{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:57:56,030][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1255b1d1{/environment,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:57:56,031][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7c22d4f{/environment/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:57:56,032][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@60bdf15d{/executors,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:57:56,033][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@51e4ccb3{/executors/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:57:56,034][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@495083a0{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:57:56,034][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@28a0fd6c{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:57:56,047][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@66629f63{/static,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:57:56,047][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1d9bec4d{/,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:57:56,049][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@10c8f62{/api,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:57:56,050][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@439a8f59{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:57:56,051][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@31024624{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:57:56,056][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://10.194.32.157:4040
[INFO][2018-06-06 15:57:56,164][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
[INFO][2018-06-06 15:57:56,195][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 55932.
[INFO][2018-06-06 15:57:56,196][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 10.194.32.157:55932
[INFO][2018-06-06 15:57:56,198][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2018-06-06 15:57:56,200][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 10.194.32.157, 55932, None)
[INFO][2018-06-06 15:57:56,203][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 10.194.32.157:55932 with 912.3 MB RAM, BlockManagerId(driver, 10.194.32.157, 55932, None)
[INFO][2018-06-06 15:57:56,207][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 10.194.32.157, 55932, None)
[INFO][2018-06-06 15:57:56,207][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 10.194.32.157, 55932, None)
[INFO][2018-06-06 15:57:56,449][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6df20ade{/metrics/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:57:56,484][com.seven.spark.rdd.SalesDefectNetByNetCommunity$]SalesDefectNetByNetCommunity$ is start . . .
[INFO][2018-06-06 15:57:56,963][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0 stored as values in memory (estimated size 228.1 KB, free 912.1 MB)
[INFO][2018-06-06 15:57:57,278][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0_piece0 stored as bytes in memory (estimated size 22.1 KB, free 912.1 MB)
[INFO][2018-06-06 15:57:57,280][org.apache.spark.storage.BlockManagerInfo]Added broadcast_0_piece0 in memory on 10.194.32.157:55932 (size: 22.1 KB, free: 912.3 MB)
[INFO][2018-06-06 15:57:57,285][org.apache.spark.SparkContext]Created broadcast 0 from textFile at SalesDefectNetByNetCommunity.scala:53
[WARN][2018-06-06 15:57:58,754][org.apache.hadoop.util.NativeCodeLoader]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO][2018-06-06 15:57:58,901][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 1
[INFO][2018-06-06 15:57:59,007][org.apache.spark.SparkContext]Starting job: collect at SalesDefectNetByNetCommunity.scala:64
[INFO][2018-06-06 15:57:59,019][org.apache.spark.scheduler.DAGScheduler]Got job 0 (collect at SalesDefectNetByNetCommunity.scala:64) with 2 output partitions
[INFO][2018-06-06 15:57:59,020][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 0 (collect at SalesDefectNetByNetCommunity.scala:64)
[INFO][2018-06-06 15:57:59,020][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-06-06 15:57:59,022][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-06-06 15:57:59,030][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 0 (MapPartitionsRDD[3] at mapPartitions at SalesDefectNetByNetCommunity.scala:56), which has no missing parents
[INFO][2018-06-06 15:57:59,064][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1 stored as values in memory (estimated size 3.7 KB, free 912.1 MB)
[INFO][2018-06-06 15:57:59,074][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.1 KB, free 912.1 MB)
[INFO][2018-06-06 15:57:59,076][org.apache.spark.storage.BlockManagerInfo]Added broadcast_1_piece0 in memory on 10.194.32.157:55932 (size: 2.1 KB, free: 912.3 MB)
[INFO][2018-06-06 15:57:59,076][org.apache.spark.SparkContext]Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO][2018-06-06 15:57:59,102][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at mapPartitions at SalesDefectNetByNetCommunity.scala:56) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2018-06-06 15:57:59,103][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 0.0 with 2 tasks
[INFO][2018-06-06 15:57:59,153][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 4930 bytes)
[INFO][2018-06-06 15:57:59,155][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 4930 bytes)
[INFO][2018-06-06 15:57:59,165][org.apache.spark.executor.Executor]Running task 0.0 in stage 0.0 (TID 0)
[INFO][2018-06-06 15:57:59,169][org.apache.spark.executor.Executor]Running task 1.0 in stage 0.0 (TID 1)
[INFO][2018-06-06 15:57:59,269][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/sta_vem/net_community/net_community__b4d64c39_5c6b_44c1_81be_f9aa19c55a30:278620+278620
[INFO][2018-06-06 15:57:59,270][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/sta_vem/net_community/net_community__b4d64c39_5c6b_44c1_81be_f9aa19c55a30:0+278620
[INFO][2018-06-06 15:57:59,558][org.apache.spark.executor.Executor]Finished task 0.0 in stage 0.0 (TID 0). 289718 bytes result sent to driver
[INFO][2018-06-06 15:57:59,603][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 0.0 (TID 0) in 468 ms on localhost (executor driver) (1/2)
[INFO][2018-06-06 15:57:59,705][org.apache.spark.executor.Executor]Finished task 1.0 in stage 0.0 (TID 1). 290550 bytes result sent to driver
[INFO][2018-06-06 15:57:59,724][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 0.0 (TID 1) in 568 ms on localhost (executor driver) (2/2)
[INFO][2018-06-06 15:57:59,725][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO][2018-06-06 15:57:59,728][org.apache.spark.scheduler.DAGScheduler]ResultStage 0 (collect at SalesDefectNetByNetCommunity.scala:64) finished in 0.607 s
[INFO][2018-06-06 15:57:59,738][org.apache.spark.scheduler.DAGScheduler]Job 0 finished: collect at SalesDefectNetByNetCommunity.scala:64, took 0.729807 s
[INFO][2018-06-06 15:57:59,752][org.apache.spark.storage.memory.MemoryStore]Block broadcast_2 stored as values in memory (estimated size 1060.4 KB, free 911.0 MB)
[INFO][2018-06-06 15:57:59,777][org.apache.spark.storage.memory.MemoryStore]Block broadcast_2_piece0 stored as bytes in memory (estimated size 203.6 KB, free 910.8 MB)
[INFO][2018-06-06 15:57:59,785][org.apache.spark.storage.BlockManagerInfo]Added broadcast_2_piece0 in memory on 10.194.32.157:55932 (size: 203.6 KB, free: 912.1 MB)
[INFO][2018-06-06 15:57:59,785][org.apache.spark.SparkContext]Created broadcast 2 from broadcast at SalesDefectNetByNetCommunity.scala:34
[INFO][2018-06-06 15:57:59,792][org.apache.spark.storage.memory.MemoryStore]Block broadcast_3 stored as values in memory (estimated size 228.1 KB, free 910.6 MB)
[INFO][2018-06-06 15:57:59,828][org.apache.spark.storage.memory.MemoryStore]Block broadcast_3_piece0 stored as bytes in memory (estimated size 22.1 KB, free 910.6 MB)
[INFO][2018-06-06 15:57:59,829][org.apache.spark.storage.BlockManagerInfo]Added broadcast_3_piece0 in memory on 10.194.32.157:55932 (size: 22.1 KB, free: 912.1 MB)
[INFO][2018-06-06 15:57:59,830][org.apache.spark.SparkContext]Created broadcast 3 from textFile at SalesDefectNetByNetCommunity.scala:79
[INFO][2018-06-06 15:57:59,881][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 1
[INFO][2018-06-06 15:57:59,899][org.apache.spark.SparkContext]Starting job: collect at SalesDefectNetByNetCommunity.scala:89
[INFO][2018-06-06 15:57:59,900][org.apache.spark.scheduler.DAGScheduler]Got job 1 (collect at SalesDefectNetByNetCommunity.scala:89) with 2 output partitions
[INFO][2018-06-06 15:57:59,900][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 1 (collect at SalesDefectNetByNetCommunity.scala:89)
[INFO][2018-06-06 15:57:59,901][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-06-06 15:57:59,901][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-06-06 15:57:59,902][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 1 (MapPartitionsRDD[7] at mapPartitions at SalesDefectNetByNetCommunity.scala:82), which has no missing parents
[INFO][2018-06-06 15:57:59,908][org.apache.spark.storage.memory.MemoryStore]Block broadcast_4 stored as values in memory (estimated size 3.7 KB, free 910.6 MB)
[INFO][2018-06-06 15:57:59,922][org.apache.spark.storage.memory.MemoryStore]Block broadcast_4_piece0 stored as bytes in memory (estimated size 2.0 KB, free 910.6 MB)
[INFO][2018-06-06 15:57:59,923][org.apache.spark.storage.BlockManagerInfo]Added broadcast_4_piece0 in memory on 10.194.32.157:55932 (size: 2.0 KB, free: 912.1 MB)
[INFO][2018-06-06 15:57:59,924][org.apache.spark.SparkContext]Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[INFO][2018-06-06 15:57:59,926][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[7] at mapPartitions at SalesDefectNetByNetCommunity.scala:82) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2018-06-06 15:57:59,926][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 1.0 with 2 tasks
[INFO][2018-06-06 15:57:59,928][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 1.0 (TID 2, localhost, executor driver, partition 0, ANY, 4926 bytes)
[INFO][2018-06-06 15:57:59,928][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 1.0 (TID 3, localhost, executor driver, partition 1, ANY, 4926 bytes)
[INFO][2018-06-06 15:57:59,929][org.apache.spark.executor.Executor]Running task 0.0 in stage 1.0 (TID 2)
[INFO][2018-06-06 15:57:59,929][org.apache.spark.executor.Executor]Running task 1.0 in stage 1.0 (TID 3)
[INFO][2018-06-06 15:57:59,933][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/sta_vem/vem_nettype/vem_nettype__5b80085f_0bae_4bd8_97d4_919813b3ce8b:0+6079084
[INFO][2018-06-06 15:57:59,935][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/sta_vem/vem_nettype/vem_nettype__5b80085f_0bae_4bd8_97d4_919813b3ce8b:6079084+6079084
[INFO][2018-06-06 15:58:03,806][org.apache.spark.executor.Executor]Finished task 1.0 in stage 1.0 (TID 3). 14530 bytes result sent to driver
[INFO][2018-06-06 15:58:03,814][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 1.0 (TID 3) in 3886 ms on localhost (executor driver) (1/2)
[INFO][2018-06-06 15:58:03,899][org.apache.spark.executor.Executor]Finished task 0.0 in stage 1.0 (TID 2). 240458 bytes result sent to driver
[INFO][2018-06-06 15:58:03,921][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 1.0 (TID 2) in 3994 ms on localhost (executor driver) (2/2)
[INFO][2018-06-06 15:58:03,921][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO][2018-06-06 15:58:03,922][org.apache.spark.scheduler.DAGScheduler]ResultStage 1 (collect at SalesDefectNetByNetCommunity.scala:89) finished in 3.995 s
[INFO][2018-06-06 15:58:03,923][org.apache.spark.scheduler.DAGScheduler]Job 1 finished: collect at SalesDefectNetByNetCommunity.scala:89, took 4.023750 s
[INFO][2018-06-06 15:58:03,933][org.apache.spark.storage.memory.MemoryStore]Block broadcast_5 stored as values in memory (estimated size 1924.3 KB, free 908.7 MB)
[INFO][2018-06-06 15:58:03,959][org.apache.spark.storage.memory.MemoryStore]Block broadcast_5_piece0 stored as bytes in memory (estimated size 111.1 KB, free 908.6 MB)
[INFO][2018-06-06 15:58:03,960][org.apache.spark.storage.BlockManagerInfo]Added broadcast_5_piece0 in memory on 10.194.32.157:55932 (size: 111.1 KB, free: 911.9 MB)
[INFO][2018-06-06 15:58:03,960][org.apache.spark.SparkContext]Created broadcast 5 from broadcast at SalesDefectNetByNetCommunity.scala:38
[INFO][2018-06-06 15:58:03,964][org.apache.spark.storage.memory.MemoryStore]Block broadcast_6 stored as values in memory (estimated size 228.1 KB, free 908.4 MB)
[INFO][2018-06-06 15:58:03,983][org.apache.spark.storage.memory.MemoryStore]Block broadcast_6_piece0 stored as bytes in memory (estimated size 22.1 KB, free 908.3 MB)
[INFO][2018-06-06 15:58:03,984][org.apache.spark.storage.BlockManagerInfo]Added broadcast_6_piece0 in memory on 10.194.32.157:55932 (size: 22.1 KB, free: 911.9 MB)
[INFO][2018-06-06 15:58:03,985][org.apache.spark.SparkContext]Created broadcast 6 from textFile at SalesDefectNetByNetCommunity.scala:104
[INFO][2018-06-06 15:58:04,025][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 1
[INFO][2018-06-06 15:58:04,057][org.apache.spark.SparkContext]Starting job: foreach at SalesDefectNetByNetCommunity.scala:129
[INFO][2018-06-06 15:58:04,070][org.apache.spark.scheduler.DAGScheduler]Registering RDD 11 (mapPartitions at SalesDefectNetByNetCommunity.scala:107)
[INFO][2018-06-06 15:58:04,070][org.apache.spark.scheduler.DAGScheduler]Got job 2 (foreach at SalesDefectNetByNetCommunity.scala:129) with 2 output partitions
[INFO][2018-06-06 15:58:04,070][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 3 (foreach at SalesDefectNetByNetCommunity.scala:129)
[INFO][2018-06-06 15:58:04,071][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 2)
[INFO][2018-06-06 15:58:04,075][org.apache.spark.scheduler.DAGScheduler]Missing parents: List(ShuffleMapStage 2)
[INFO][2018-06-06 15:58:04,076][org.apache.spark.scheduler.DAGScheduler]Submitting ShuffleMapStage 2 (MapPartitionsRDD[11] at mapPartitions at SalesDefectNetByNetCommunity.scala:107), which has no missing parents
[INFO][2018-06-06 15:58:04,088][org.apache.spark.storage.memory.MemoryStore]Block broadcast_7 stored as values in memory (estimated size 4.9 KB, free 908.3 MB)
[INFO][2018-06-06 15:58:04,092][org.apache.spark.storage.memory.MemoryStore]Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.8 KB, free 908.3 MB)
[INFO][2018-06-06 15:58:04,092][org.apache.spark.storage.BlockManagerInfo]Added broadcast_7_piece0 in memory on 10.194.32.157:55932 (size: 2.8 KB, free: 911.9 MB)
[INFO][2018-06-06 15:58:04,093][org.apache.spark.SparkContext]Created broadcast 7 from broadcast at DAGScheduler.scala:1006
[INFO][2018-06-06 15:58:04,096][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[11] at mapPartitions at SalesDefectNetByNetCommunity.scala:107) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2018-06-06 15:58:04,096][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 2.0 with 2 tasks
[INFO][2018-06-06 15:58:04,098][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 2.0 (TID 4, localhost, executor driver, partition 0, ANY, 4915 bytes)
[INFO][2018-06-06 15:58:04,098][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 2.0 (TID 5, localhost, executor driver, partition 1, ANY, 4915 bytes)
[INFO][2018-06-06 15:58:04,098][org.apache.spark.executor.Executor]Running task 1.0 in stage 2.0 (TID 5)
[INFO][2018-06-06 15:58:04,098][org.apache.spark.executor.Executor]Running task 0.0 in stage 2.0 (TID 4)
[INFO][2018-06-06 15:58:04,105][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/sta_vem/vem_machine/vem_machine__12fe7e5b_689f_498e_b2a6_8411fd976c5b:0+3998718
[INFO][2018-06-06 15:58:04,105][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/sta_vem/vem_machine/vem_machine__12fe7e5b_689f_498e_b2a6_8411fd976c5b:3998718+3998719
[INFO][2018-06-06 15:58:04,952][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_3_piece0 on 10.194.32.157:55932 in memory (size: 22.1 KB, free: 911.9 MB)
[INFO][2018-06-06 15:58:04,962][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_4_piece0 on 10.194.32.157:55932 in memory (size: 2.0 KB, free: 911.9 MB)
[INFO][2018-06-06 15:58:05,426][org.apache.spark.executor.Executor]Finished task 0.0 in stage 2.0 (TID 4). 1043 bytes result sent to driver
[INFO][2018-06-06 15:58:05,442][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 2.0 (TID 4) in 1345 ms on localhost (executor driver) (1/2)
[INFO][2018-06-06 15:58:06,080][org.apache.spark.executor.Executor]Finished task 1.0 in stage 2.0 (TID 5). 1086 bytes result sent to driver
[INFO][2018-06-06 15:58:06,081][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 2.0 (TID 5) in 1983 ms on localhost (executor driver) (2/2)
[INFO][2018-06-06 15:58:06,082][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO][2018-06-06 15:58:06,082][org.apache.spark.scheduler.DAGScheduler]ShuffleMapStage 2 (mapPartitions at SalesDefectNetByNetCommunity.scala:107) finished in 1.986 s
[INFO][2018-06-06 15:58:06,083][org.apache.spark.scheduler.DAGScheduler]looking for newly runnable stages
[INFO][2018-06-06 15:58:06,084][org.apache.spark.scheduler.DAGScheduler]running: Set()
[INFO][2018-06-06 15:58:06,084][org.apache.spark.scheduler.DAGScheduler]waiting: Set(ResultStage 3)
[INFO][2018-06-06 15:58:06,085][org.apache.spark.scheduler.DAGScheduler]failed: Set()
[INFO][2018-06-06 15:58:06,089][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 3 (MapPartitionsRDD[13] at mapPartitions at SalesDefectNetByNetCommunity.scala:115), which has no missing parents
[INFO][2018-06-06 15:58:06,094][org.apache.spark.storage.memory.MemoryStore]Block broadcast_8 stored as values in memory (estimated size 4.3 KB, free 908.6 MB)
[INFO][2018-06-06 15:58:06,098][org.apache.spark.storage.memory.MemoryStore]Block broadcast_8_piece0 stored as bytes in memory (estimated size 2.4 KB, free 908.6 MB)
[INFO][2018-06-06 15:58:06,098][org.apache.spark.storage.BlockManagerInfo]Added broadcast_8_piece0 in memory on 10.194.32.157:55932 (size: 2.4 KB, free: 911.9 MB)
[INFO][2018-06-06 15:58:06,099][org.apache.spark.SparkContext]Created broadcast 8 from broadcast at DAGScheduler.scala:1006
[INFO][2018-06-06 15:58:06,099][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ResultStage 3 (MapPartitionsRDD[13] at mapPartitions at SalesDefectNetByNetCommunity.scala:115) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2018-06-06 15:58:06,100][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 3.0 with 2 tasks
[INFO][2018-06-06 15:58:06,101][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 3.0 (TID 6, localhost, executor driver, partition 0, ANY, 4621 bytes)
[INFO][2018-06-06 15:58:06,102][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 3.0 (TID 7, localhost, executor driver, partition 1, ANY, 4621 bytes)
[INFO][2018-06-06 15:58:06,102][org.apache.spark.executor.Executor]Running task 0.0 in stage 3.0 (TID 6)
[INFO][2018-06-06 15:58:06,102][org.apache.spark.executor.Executor]Running task 1.0 in stage 3.0 (TID 7)
[INFO][2018-06-06 15:58:06,122][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 2 blocks
[INFO][2018-06-06 15:58:06,124][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 7 ms
[INFO][2018-06-06 15:58:06,122][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 2 blocks
[INFO][2018-06-06 15:58:06,125][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 9 ms
[INFO][2018-06-06 15:58:06,253][org.apache.spark.storage.memory.MemoryStore]Block rdd_13_1 stored as values in memory (estimated size 77.1 KB, free 908.5 MB)
[INFO][2018-06-06 15:58:06,256][org.apache.spark.storage.BlockManagerInfo]Added rdd_13_1 in memory on 10.194.32.157:55932 (size: 77.1 KB, free: 911.9 MB)
[INFO][2018-06-06 15:58:06,255][org.apache.spark.storage.memory.MemoryStore]Block rdd_13_0 stored as values in memory (estimated size 77.0 KB, free 908.4 MB)
[INFO][2018-06-06 15:58:06,258][org.apache.spark.storage.BlockManagerInfo]Added rdd_13_0 in memory on 10.194.32.157:55932 (size: 77.0 KB, free: 911.8 MB)
[INFO][2018-06-06 15:58:06,306][org.apache.spark.executor.Executor]Finished task 1.0 in stage 3.0 (TID 7). 1750 bytes result sent to driver
[INFO][2018-06-06 15:58:06,307][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 3.0 (TID 7) in 206 ms on localhost (executor driver) (1/2)
[INFO][2018-06-06 15:58:06,308][org.apache.spark.executor.Executor]Finished task 0.0 in stage 3.0 (TID 6). 1750 bytes result sent to driver
[INFO][2018-06-06 15:58:06,308][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 3.0 (TID 6) in 208 ms on localhost (executor driver) (2/2)
[INFO][2018-06-06 15:58:06,308][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 3.0, whose tasks have all completed, from pool 
[INFO][2018-06-06 15:58:06,309][org.apache.spark.scheduler.DAGScheduler]ResultStage 3 (foreach at SalesDefectNetByNetCommunity.scala:129) finished in 0.209 s
[INFO][2018-06-06 15:58:06,309][org.apache.spark.scheduler.DAGScheduler]Job 2 finished: foreach at SalesDefectNetByNetCommunity.scala:129, took 2.251737 s
[INFO][2018-06-06 15:58:06,310][com.seven.spark.rdd.SalesDefectNetByNetCommunity$]SalesDefectNetByNetCommunity$ is success . . .
[INFO][2018-06-06 15:58:06,313][org.apache.spark.SparkContext]Invoking stop() from shutdown hook
[INFO][2018-06-06 15:58:06,324][org.spark_project.jetty.server.AbstractConnector]Stopped Spark@734e4fb7{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-06-06 15:58:06,326][org.apache.spark.ui.SparkUI]Stopped Spark web UI at http://10.194.32.157:4040
[INFO][2018-06-06 15:58:06,335][org.apache.spark.MapOutputTrackerMasterEndpoint]MapOutputTrackerMasterEndpoint stopped!
[INFO][2018-06-06 15:58:06,356][org.apache.spark.storage.memory.MemoryStore]MemoryStore cleared
[INFO][2018-06-06 15:58:06,357][org.apache.spark.storage.BlockManager]BlockManager stopped
[INFO][2018-06-06 15:58:06,359][org.apache.spark.storage.BlockManagerMaster]BlockManagerMaster stopped
[INFO][2018-06-06 15:58:06,362][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint]OutputCommitCoordinator stopped!
[INFO][2018-06-06 15:58:06,363][org.apache.spark.SparkContext]Successfully stopped SparkContext
[INFO][2018-06-06 15:58:06,364][org.apache.spark.util.ShutdownHookManager]Shutdown hook called
[INFO][2018-06-06 15:58:06,365][org.apache.spark.util.ShutdownHookManager]Deleting directory /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/spark-2ff42f07-5783-40a8-9c8b-f9d6c21b797e
[INFO][2018-06-06 15:59:44,364][org.apache.spark.SparkContext]Running Spark version 2.2.0
[INFO][2018-06-06 15:59:45,223][org.apache.spark.SparkContext]Submitted application: SalesDefectNetByNetCommunity$
[INFO][2018-06-06 15:59:45,241][org.apache.spark.SecurityManager]Changing view acls to: seven
[INFO][2018-06-06 15:59:45,242][org.apache.spark.SecurityManager]Changing modify acls to: seven
[INFO][2018-06-06 15:59:45,243][org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO][2018-06-06 15:59:45,244][org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO][2018-06-06 15:59:45,245][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(seven); groups with view permissions: Set(); users  with modify permissions: Set(seven); groups with modify permissions: Set()
[INFO][2018-06-06 15:59:45,515][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 55954.
[INFO][2018-06-06 15:59:45,537][org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO][2018-06-06 15:59:45,554][org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO][2018-06-06 15:59:45,556][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2018-06-06 15:59:45,557][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
[INFO][2018-06-06 15:59:45,566][org.apache.spark.storage.DiskBlockManager]Created local directory at /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/blockmgr-747f43f5-f722-417c-9d50-deb187555b8a
[INFO][2018-06-06 15:59:45,583][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 912.3 MB
[INFO][2018-06-06 15:59:45,631][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO][2018-06-06 15:59:45,729][org.spark_project.jetty.util.log]Logging initialized @2329ms
[INFO][2018-06-06 15:59:45,806][org.spark_project.jetty.server.Server]jetty-9.3.z-SNAPSHOT
[INFO][2018-06-06 15:59:45,818][org.spark_project.jetty.server.Server]Started @2419ms
[INFO][2018-06-06 15:59:45,837][org.spark_project.jetty.server.AbstractConnector]Started ServerConnector@89c10b7{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-06-06 15:59:45,837][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4040.
[INFO][2018-06-06 15:59:45,862][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4a9f80d3{/jobs,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:59:45,863][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3e2822{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:59:45,864][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@29a60c27{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:59:45,865][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1ca25c47{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:59:45,867][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@533b266e{/stages,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:59:45,867][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@89ff02e{/stages/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:59:45,868][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@62679465{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:59:45,869][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5f2606b{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:59:45,870][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3ebff828{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:59:45,871][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@33352f32{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:59:45,871][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1e044120{/storage,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:59:45,872][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3624da92{/storage/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:59:45,876][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@94f6bfb{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:59:45,882][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2484f433{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:59:45,882][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1255b1d1{/environment,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:59:45,883][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7c22d4f{/environment/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:59:45,884][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@60bdf15d{/executors,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:59:45,884][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@51e4ccb3{/executors/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:59:45,885][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@495083a0{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:59:45,886][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@28a0fd6c{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:59:45,894][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@66629f63{/static,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:59:45,895][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1d9bec4d{/,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:59:45,896][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@10c8f62{/api,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:59:45,897][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@439a8f59{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:59:45,898][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@31024624{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:59:45,900][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://10.194.32.157:4040
[INFO][2018-06-06 15:59:45,989][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
[INFO][2018-06-06 15:59:46,014][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 55955.
[INFO][2018-06-06 15:59:46,015][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 10.194.32.157:55955
[INFO][2018-06-06 15:59:46,017][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2018-06-06 15:59:46,018][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 10.194.32.157, 55955, None)
[INFO][2018-06-06 15:59:46,027][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 10.194.32.157:55955 with 912.3 MB RAM, BlockManagerId(driver, 10.194.32.157, 55955, None)
[INFO][2018-06-06 15:59:46,032][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 10.194.32.157, 55955, None)
[INFO][2018-06-06 15:59:46,033][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 10.194.32.157, 55955, None)
[INFO][2018-06-06 15:59:46,212][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6df20ade{/metrics/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 15:59:46,233][com.seven.spark.rdd.SalesDefectNetByNetCommunity$]SalesDefectNetByNetCommunity$ is start . . .
[INFO][2018-06-06 15:59:46,719][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0 stored as values in memory (estimated size 228.1 KB, free 912.1 MB)
[INFO][2018-06-06 15:59:46,919][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0_piece0 stored as bytes in memory (estimated size 22.1 KB, free 912.1 MB)
[INFO][2018-06-06 15:59:46,921][org.apache.spark.storage.BlockManagerInfo]Added broadcast_0_piece0 in memory on 10.194.32.157:55955 (size: 22.1 KB, free: 912.3 MB)
[INFO][2018-06-06 15:59:46,927][org.apache.spark.SparkContext]Created broadcast 0 from textFile at SalesDefectNetByNetCommunity.scala:53
[WARN][2018-06-06 15:59:48,384][org.apache.hadoop.util.NativeCodeLoader]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO][2018-06-06 15:59:48,500][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 1
[INFO][2018-06-06 15:59:48,577][org.apache.spark.SparkContext]Starting job: collect at SalesDefectNetByNetCommunity.scala:64
[INFO][2018-06-06 15:59:48,587][org.apache.spark.scheduler.DAGScheduler]Got job 0 (collect at SalesDefectNetByNetCommunity.scala:64) with 2 output partitions
[INFO][2018-06-06 15:59:48,587][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 0 (collect at SalesDefectNetByNetCommunity.scala:64)
[INFO][2018-06-06 15:59:48,588][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-06-06 15:59:48,589][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-06-06 15:59:48,598][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 0 (MapPartitionsRDD[3] at mapPartitions at SalesDefectNetByNetCommunity.scala:56), which has no missing parents
[INFO][2018-06-06 15:59:48,618][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1 stored as values in memory (estimated size 3.7 KB, free 912.1 MB)
[INFO][2018-06-06 15:59:48,632][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.1 KB, free 912.1 MB)
[INFO][2018-06-06 15:59:48,633][org.apache.spark.storage.BlockManagerInfo]Added broadcast_1_piece0 in memory on 10.194.32.157:55955 (size: 2.1 KB, free: 912.3 MB)
[INFO][2018-06-06 15:59:48,635][org.apache.spark.SparkContext]Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO][2018-06-06 15:59:48,656][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at mapPartitions at SalesDefectNetByNetCommunity.scala:56) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2018-06-06 15:59:48,658][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 0.0 with 2 tasks
[INFO][2018-06-06 15:59:48,702][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 4930 bytes)
[INFO][2018-06-06 15:59:48,705][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 4930 bytes)
[INFO][2018-06-06 15:59:48,717][org.apache.spark.executor.Executor]Running task 1.0 in stage 0.0 (TID 1)
[INFO][2018-06-06 15:59:48,717][org.apache.spark.executor.Executor]Running task 0.0 in stage 0.0 (TID 0)
[INFO][2018-06-06 15:59:48,778][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/sta_vem/net_community/net_community__b4d64c39_5c6b_44c1_81be_f9aa19c55a30:0+278620
[INFO][2018-06-06 15:59:48,779][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/sta_vem/net_community/net_community__b4d64c39_5c6b_44c1_81be_f9aa19c55a30:278620+278620
[INFO][2018-06-06 15:59:49,048][org.apache.spark.executor.Executor]Finished task 0.0 in stage 0.0 (TID 0). 289675 bytes result sent to driver
[INFO][2018-06-06 15:59:49,073][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 0.0 (TID 0) in 384 ms on localhost (executor driver) (1/2)
[INFO][2018-06-06 15:59:49,120][org.apache.spark.executor.Executor]Finished task 1.0 in stage 0.0 (TID 1). 290550 bytes result sent to driver
[INFO][2018-06-06 15:59:49,129][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 0.0 (TID 1) in 425 ms on localhost (executor driver) (2/2)
[INFO][2018-06-06 15:59:49,130][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO][2018-06-06 15:59:49,131][org.apache.spark.scheduler.DAGScheduler]ResultStage 0 (collect at SalesDefectNetByNetCommunity.scala:64) finished in 0.457 s
[INFO][2018-06-06 15:59:49,135][org.apache.spark.scheduler.DAGScheduler]Job 0 finished: collect at SalesDefectNetByNetCommunity.scala:64, took 0.558513 s
[INFO][2018-06-06 15:59:49,142][org.apache.spark.storage.memory.MemoryStore]Block broadcast_2 stored as values in memory (estimated size 1060.4 KB, free 911.0 MB)
[INFO][2018-06-06 15:59:49,157][org.apache.spark.storage.memory.MemoryStore]Block broadcast_2_piece0 stored as bytes in memory (estimated size 203.6 KB, free 910.8 MB)
[INFO][2018-06-06 15:59:49,158][org.apache.spark.storage.BlockManagerInfo]Added broadcast_2_piece0 in memory on 10.194.32.157:55955 (size: 203.6 KB, free: 912.1 MB)
[INFO][2018-06-06 15:59:49,158][org.apache.spark.SparkContext]Created broadcast 2 from broadcast at SalesDefectNetByNetCommunity.scala:34
[INFO][2018-06-06 15:59:49,164][org.apache.spark.storage.memory.MemoryStore]Block broadcast_3 stored as values in memory (estimated size 228.1 KB, free 910.6 MB)
[INFO][2018-06-06 15:59:49,183][org.apache.spark.storage.memory.MemoryStore]Block broadcast_3_piece0 stored as bytes in memory (estimated size 22.1 KB, free 910.6 MB)
[INFO][2018-06-06 15:59:49,183][org.apache.spark.storage.BlockManagerInfo]Added broadcast_3_piece0 in memory on 10.194.32.157:55955 (size: 22.1 KB, free: 912.1 MB)
[INFO][2018-06-06 15:59:49,184][org.apache.spark.SparkContext]Created broadcast 3 from textFile at SalesDefectNetByNetCommunity.scala:79
[INFO][2018-06-06 15:59:49,219][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 1
[INFO][2018-06-06 15:59:49,242][org.apache.spark.SparkContext]Starting job: collect at SalesDefectNetByNetCommunity.scala:89
[INFO][2018-06-06 15:59:49,244][org.apache.spark.scheduler.DAGScheduler]Got job 1 (collect at SalesDefectNetByNetCommunity.scala:89) with 2 output partitions
[INFO][2018-06-06 15:59:49,244][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 1 (collect at SalesDefectNetByNetCommunity.scala:89)
[INFO][2018-06-06 15:59:49,244][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-06-06 15:59:49,244][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-06-06 15:59:49,245][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 1 (MapPartitionsRDD[7] at mapPartitions at SalesDefectNetByNetCommunity.scala:82), which has no missing parents
[INFO][2018-06-06 15:59:49,247][org.apache.spark.storage.memory.MemoryStore]Block broadcast_4 stored as values in memory (estimated size 3.7 KB, free 910.6 MB)
[INFO][2018-06-06 15:59:49,263][org.apache.spark.storage.memory.MemoryStore]Block broadcast_4_piece0 stored as bytes in memory (estimated size 2.0 KB, free 910.6 MB)
[INFO][2018-06-06 15:59:49,263][org.apache.spark.storage.BlockManagerInfo]Added broadcast_4_piece0 in memory on 10.194.32.157:55955 (size: 2.0 KB, free: 912.1 MB)
[INFO][2018-06-06 15:59:49,264][org.apache.spark.SparkContext]Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[INFO][2018-06-06 15:59:49,265][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[7] at mapPartitions at SalesDefectNetByNetCommunity.scala:82) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2018-06-06 15:59:49,265][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 1.0 with 2 tasks
[INFO][2018-06-06 15:59:49,266][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 1.0 (TID 2, localhost, executor driver, partition 0, ANY, 4926 bytes)
[INFO][2018-06-06 15:59:49,266][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 1.0 (TID 3, localhost, executor driver, partition 1, ANY, 4926 bytes)
[INFO][2018-06-06 15:59:49,266][org.apache.spark.executor.Executor]Running task 0.0 in stage 1.0 (TID 2)
[INFO][2018-06-06 15:59:49,266][org.apache.spark.executor.Executor]Running task 1.0 in stage 1.0 (TID 3)
[INFO][2018-06-06 15:59:49,270][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/sta_vem/vem_nettype/vem_nettype__5b80085f_0bae_4bd8_97d4_919813b3ce8b:6079084+6079084
[INFO][2018-06-06 15:59:49,270][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/sta_vem/vem_nettype/vem_nettype__5b80085f_0bae_4bd8_97d4_919813b3ce8b:0+6079084
[INFO][2018-06-06 15:59:52,849][org.apache.spark.executor.Executor]Finished task 1.0 in stage 1.0 (TID 3). 14530 bytes result sent to driver
[INFO][2018-06-06 15:59:52,856][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 1.0 (TID 3) in 3590 ms on localhost (executor driver) (1/2)
[INFO][2018-06-06 15:59:53,817][org.apache.spark.executor.Executor]Finished task 0.0 in stage 1.0 (TID 2). 240458 bytes result sent to driver
[INFO][2018-06-06 15:59:53,840][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 1.0 (TID 2) in 4575 ms on localhost (executor driver) (2/2)
[INFO][2018-06-06 15:59:53,841][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO][2018-06-06 15:59:53,841][org.apache.spark.scheduler.DAGScheduler]ResultStage 1 (collect at SalesDefectNetByNetCommunity.scala:89) finished in 4.576 s
[INFO][2018-06-06 15:59:53,842][org.apache.spark.scheduler.DAGScheduler]Job 1 finished: collect at SalesDefectNetByNetCommunity.scala:89, took 4.599152 s
[INFO][2018-06-06 15:59:53,851][org.apache.spark.storage.memory.MemoryStore]Block broadcast_5 stored as values in memory (estimated size 1924.3 KB, free 908.7 MB)
[INFO][2018-06-06 15:59:53,874][org.apache.spark.storage.memory.MemoryStore]Block broadcast_5_piece0 stored as bytes in memory (estimated size 111.1 KB, free 908.6 MB)
[INFO][2018-06-06 15:59:53,875][org.apache.spark.storage.BlockManagerInfo]Added broadcast_5_piece0 in memory on 10.194.32.157:55955 (size: 111.1 KB, free: 911.9 MB)
[INFO][2018-06-06 15:59:53,875][org.apache.spark.SparkContext]Created broadcast 5 from broadcast at SalesDefectNetByNetCommunity.scala:38
[INFO][2018-06-06 15:59:53,881][org.apache.spark.storage.memory.MemoryStore]Block broadcast_6 stored as values in memory (estimated size 228.1 KB, free 908.4 MB)
[INFO][2018-06-06 15:59:53,903][org.apache.spark.storage.memory.MemoryStore]Block broadcast_6_piece0 stored as bytes in memory (estimated size 22.1 KB, free 908.3 MB)
[INFO][2018-06-06 15:59:53,904][org.apache.spark.storage.BlockManagerInfo]Added broadcast_6_piece0 in memory on 10.194.32.157:55955 (size: 22.1 KB, free: 911.9 MB)
[INFO][2018-06-06 15:59:53,905][org.apache.spark.SparkContext]Created broadcast 6 from textFile at SalesDefectNetByNetCommunity.scala:104
[INFO][2018-06-06 15:59:53,947][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 1
[INFO][2018-06-06 15:59:53,987][org.apache.spark.SparkContext]Starting job: foreach at SalesDefectNetByNetCommunity.scala:129
[INFO][2018-06-06 15:59:53,999][org.apache.spark.scheduler.DAGScheduler]Registering RDD 11 (mapPartitions at SalesDefectNetByNetCommunity.scala:107)
[INFO][2018-06-06 15:59:53,999][org.apache.spark.scheduler.DAGScheduler]Got job 2 (foreach at SalesDefectNetByNetCommunity.scala:129) with 2 output partitions
[INFO][2018-06-06 15:59:53,999][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 3 (foreach at SalesDefectNetByNetCommunity.scala:129)
[INFO][2018-06-06 15:59:54,000][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 2)
[INFO][2018-06-06 15:59:54,002][org.apache.spark.scheduler.DAGScheduler]Missing parents: List(ShuffleMapStage 2)
[INFO][2018-06-06 15:59:54,003][org.apache.spark.scheduler.DAGScheduler]Submitting ShuffleMapStage 2 (MapPartitionsRDD[11] at mapPartitions at SalesDefectNetByNetCommunity.scala:107), which has no missing parents
[INFO][2018-06-06 15:59:54,015][org.apache.spark.storage.memory.MemoryStore]Block broadcast_7 stored as values in memory (estimated size 4.9 KB, free 908.3 MB)
[INFO][2018-06-06 15:59:54,022][org.apache.spark.storage.memory.MemoryStore]Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.8 KB, free 908.3 MB)
[INFO][2018-06-06 15:59:54,023][org.apache.spark.storage.BlockManagerInfo]Added broadcast_7_piece0 in memory on 10.194.32.157:55955 (size: 2.8 KB, free: 911.9 MB)
[INFO][2018-06-06 15:59:54,025][org.apache.spark.SparkContext]Created broadcast 7 from broadcast at DAGScheduler.scala:1006
[INFO][2018-06-06 15:59:54,028][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[11] at mapPartitions at SalesDefectNetByNetCommunity.scala:107) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2018-06-06 15:59:54,028][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 2.0 with 2 tasks
[INFO][2018-06-06 15:59:54,033][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 2.0 (TID 4, localhost, executor driver, partition 0, ANY, 4915 bytes)
[INFO][2018-06-06 15:59:54,033][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 2.0 (TID 5, localhost, executor driver, partition 1, ANY, 4915 bytes)
[INFO][2018-06-06 15:59:54,034][org.apache.spark.executor.Executor]Running task 0.0 in stage 2.0 (TID 4)
[INFO][2018-06-06 15:59:54,034][org.apache.spark.executor.Executor]Running task 1.0 in stage 2.0 (TID 5)
[INFO][2018-06-06 15:59:54,042][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/sta_vem/vem_machine/vem_machine__12fe7e5b_689f_498e_b2a6_8411fd976c5b:3998718+3998719
[INFO][2018-06-06 15:59:54,043][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/sta_vem/vem_machine/vem_machine__12fe7e5b_689f_498e_b2a6_8411fd976c5b:0+3998718
[INFO][2018-06-06 15:59:54,146][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_4_piece0 on 10.194.32.157:55955 in memory (size: 2.0 KB, free: 911.9 MB)
[INFO][2018-06-06 15:59:54,149][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_3_piece0 on 10.194.32.157:55955 in memory (size: 22.1 KB, free: 911.9 MB)
[INFO][2018-06-06 15:59:54,828][org.apache.spark.executor.Executor]Finished task 1.0 in stage 2.0 (TID 5). 1086 bytes result sent to driver
[INFO][2018-06-06 15:59:54,846][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 2.0 (TID 5) in 813 ms on localhost (executor driver) (1/2)
[INFO][2018-06-06 15:59:56,824][org.apache.spark.executor.Executor]Finished task 0.0 in stage 2.0 (TID 4). 1000 bytes result sent to driver
[INFO][2018-06-06 15:59:56,825][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 2.0 (TID 4) in 2794 ms on localhost (executor driver) (2/2)
[INFO][2018-06-06 15:59:56,825][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO][2018-06-06 15:59:56,826][org.apache.spark.scheduler.DAGScheduler]ShuffleMapStage 2 (mapPartitions at SalesDefectNetByNetCommunity.scala:107) finished in 2.795 s
[INFO][2018-06-06 15:59:56,826][org.apache.spark.scheduler.DAGScheduler]looking for newly runnable stages
[INFO][2018-06-06 15:59:56,827][org.apache.spark.scheduler.DAGScheduler]running: Set()
[INFO][2018-06-06 15:59:56,827][org.apache.spark.scheduler.DAGScheduler]waiting: Set(ResultStage 3)
[INFO][2018-06-06 15:59:56,828][org.apache.spark.scheduler.DAGScheduler]failed: Set()
[INFO][2018-06-06 15:59:56,832][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 3 (MapPartitionsRDD[14] at filter at SalesDefectNetByNetCommunity.scala:123), which has no missing parents
[INFO][2018-06-06 15:59:56,836][org.apache.spark.storage.memory.MemoryStore]Block broadcast_8 stored as values in memory (estimated size 4.6 KB, free 908.6 MB)
[INFO][2018-06-06 15:59:56,839][org.apache.spark.storage.memory.MemoryStore]Block broadcast_8_piece0 stored as bytes in memory (estimated size 2.6 KB, free 908.6 MB)
[INFO][2018-06-06 15:59:56,840][org.apache.spark.storage.BlockManagerInfo]Added broadcast_8_piece0 in memory on 10.194.32.157:55955 (size: 2.6 KB, free: 911.9 MB)
[INFO][2018-06-06 15:59:56,840][org.apache.spark.SparkContext]Created broadcast 8 from broadcast at DAGScheduler.scala:1006
[INFO][2018-06-06 15:59:56,842][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ResultStage 3 (MapPartitionsRDD[14] at filter at SalesDefectNetByNetCommunity.scala:123) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2018-06-06 15:59:56,842][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 3.0 with 2 tasks
[INFO][2018-06-06 15:59:56,843][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 3.0 (TID 6, localhost, executor driver, partition 0, ANY, 4621 bytes)
[INFO][2018-06-06 15:59:56,844][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 3.0 (TID 7, localhost, executor driver, partition 1, ANY, 4621 bytes)
[INFO][2018-06-06 15:59:56,844][org.apache.spark.executor.Executor]Running task 0.0 in stage 3.0 (TID 6)
[INFO][2018-06-06 15:59:56,844][org.apache.spark.executor.Executor]Running task 1.0 in stage 3.0 (TID 7)
[INFO][2018-06-06 15:59:56,863][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 2 blocks
[INFO][2018-06-06 15:59:56,863][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 2 blocks
[INFO][2018-06-06 15:59:56,865][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 6 ms
[INFO][2018-06-06 15:59:56,865][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 6 ms
[INFO][2018-06-06 15:59:56,913][org.apache.spark.storage.memory.MemoryStore]Block rdd_14_0 stored as values in memory (estimated size 352.0 B, free 908.6 MB)
[INFO][2018-06-06 15:59:56,914][org.apache.spark.storage.memory.MemoryStore]Block rdd_14_1 stored as values in memory (estimated size 152.0 B, free 908.6 MB)
[INFO][2018-06-06 15:59:56,915][org.apache.spark.storage.BlockManagerInfo]Added rdd_14_0 in memory on 10.194.32.157:55955 (size: 352.0 B, free: 911.9 MB)
[INFO][2018-06-06 15:59:56,915][org.apache.spark.storage.BlockManagerInfo]Added rdd_14_1 in memory on 10.194.32.157:55955 (size: 152.0 B, free: 911.9 MB)
[INFO][2018-06-06 15:59:56,927][org.apache.spark.executor.Executor]Finished task 1.0 in stage 3.0 (TID 7). 1750 bytes result sent to driver
[INFO][2018-06-06 15:59:56,927][org.apache.spark.executor.Executor]Finished task 0.0 in stage 3.0 (TID 6). 1750 bytes result sent to driver
[INFO][2018-06-06 15:59:56,928][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 3.0 (TID 7) in 85 ms on localhost (executor driver) (1/2)
[INFO][2018-06-06 15:59:56,929][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 3.0 (TID 6) in 87 ms on localhost (executor driver) (2/2)
[INFO][2018-06-06 15:59:56,929][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 3.0, whose tasks have all completed, from pool 
[INFO][2018-06-06 15:59:56,930][org.apache.spark.scheduler.DAGScheduler]ResultStage 3 (foreach at SalesDefectNetByNetCommunity.scala:129) finished in 0.088 s
[INFO][2018-06-06 15:59:56,931][org.apache.spark.scheduler.DAGScheduler]Job 2 finished: foreach at SalesDefectNetByNetCommunity.scala:129, took 2.943386 s
[INFO][2018-06-06 15:59:56,932][com.seven.spark.rdd.SalesDefectNetByNetCommunity$]SalesDefectNetByNetCommunity$ is success . . .
[INFO][2018-06-06 15:59:56,935][org.apache.spark.SparkContext]Invoking stop() from shutdown hook
[INFO][2018-06-06 15:59:56,943][org.spark_project.jetty.server.AbstractConnector]Stopped Spark@89c10b7{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-06-06 15:59:56,945][org.apache.spark.ui.SparkUI]Stopped Spark web UI at http://10.194.32.157:4040
[INFO][2018-06-06 15:59:56,955][org.apache.spark.MapOutputTrackerMasterEndpoint]MapOutputTrackerMasterEndpoint stopped!
[INFO][2018-06-06 15:59:56,977][org.apache.spark.storage.memory.MemoryStore]MemoryStore cleared
[INFO][2018-06-06 15:59:56,977][org.apache.spark.storage.BlockManager]BlockManager stopped
[INFO][2018-06-06 15:59:56,979][org.apache.spark.storage.BlockManagerMaster]BlockManagerMaster stopped
[INFO][2018-06-06 15:59:56,982][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint]OutputCommitCoordinator stopped!
[INFO][2018-06-06 15:59:56,984][org.apache.spark.SparkContext]Successfully stopped SparkContext
[INFO][2018-06-06 15:59:56,984][org.apache.spark.util.ShutdownHookManager]Shutdown hook called
[INFO][2018-06-06 15:59:56,985][org.apache.spark.util.ShutdownHookManager]Deleting directory /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/spark-28cb8127-9b47-4693-a30f-3b7950030373
[INFO][2018-06-06 16:02:25,452][org.apache.spark.SparkContext]Running Spark version 2.2.0
[INFO][2018-06-06 16:02:26,484][org.apache.spark.SparkContext]Submitted application: SalesDefectNetByNetCommunity$
[INFO][2018-06-06 16:02:26,502][org.apache.spark.SecurityManager]Changing view acls to: seven
[INFO][2018-06-06 16:02:26,502][org.apache.spark.SecurityManager]Changing modify acls to: seven
[INFO][2018-06-06 16:02:26,503][org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO][2018-06-06 16:02:26,503][org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO][2018-06-06 16:02:26,504][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(seven); groups with view permissions: Set(); users  with modify permissions: Set(seven); groups with modify permissions: Set()
[INFO][2018-06-06 16:02:26,811][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 55975.
[INFO][2018-06-06 16:02:26,841][org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO][2018-06-06 16:02:26,866][org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO][2018-06-06 16:02:26,869][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2018-06-06 16:02:26,869][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
[INFO][2018-06-06 16:02:26,885][org.apache.spark.storage.DiskBlockManager]Created local directory at /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/blockmgr-21c7dfcd-b5e8-4a9f-8987-40a43a2f2210
[INFO][2018-06-06 16:02:26,909][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 912.3 MB
[INFO][2018-06-06 16:02:27,072][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO][2018-06-06 16:02:27,172][org.spark_project.jetty.util.log]Logging initialized @2632ms
[INFO][2018-06-06 16:02:27,236][org.spark_project.jetty.server.Server]jetty-9.3.z-SNAPSHOT
[INFO][2018-06-06 16:02:27,250][org.spark_project.jetty.server.Server]Started @2711ms
[INFO][2018-06-06 16:02:27,267][org.spark_project.jetty.server.AbstractConnector]Started ServerConnector@89c10b7{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-06-06 16:02:27,267][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4040.
[INFO][2018-06-06 16:02:27,288][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4a9f80d3{/jobs,null,AVAILABLE,@Spark}
[INFO][2018-06-06 16:02:27,289][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3e2822{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 16:02:27,290][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@29a60c27{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2018-06-06 16:02:27,291][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1ca25c47{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 16:02:27,291][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@533b266e{/stages,null,AVAILABLE,@Spark}
[INFO][2018-06-06 16:02:27,292][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@89ff02e{/stages/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 16:02:27,293][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@62679465{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2018-06-06 16:02:27,294][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5f2606b{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 16:02:27,295][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3ebff828{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2018-06-06 16:02:27,296][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@33352f32{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 16:02:27,296][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1e044120{/storage,null,AVAILABLE,@Spark}
[INFO][2018-06-06 16:02:27,297][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3624da92{/storage/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 16:02:27,298][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@94f6bfb{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2018-06-06 16:02:27,299][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2484f433{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 16:02:27,300][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1255b1d1{/environment,null,AVAILABLE,@Spark}
[INFO][2018-06-06 16:02:27,300][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7c22d4f{/environment/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 16:02:27,301][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@60bdf15d{/executors,null,AVAILABLE,@Spark}
[INFO][2018-06-06 16:02:27,302][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@51e4ccb3{/executors/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 16:02:27,303][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@495083a0{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2018-06-06 16:02:27,303][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@28a0fd6c{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 16:02:27,320][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@66629f63{/static,null,AVAILABLE,@Spark}
[INFO][2018-06-06 16:02:27,321][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1d9bec4d{/,null,AVAILABLE,@Spark}
[INFO][2018-06-06 16:02:27,334][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@10c8f62{/api,null,AVAILABLE,@Spark}
[INFO][2018-06-06 16:02:27,334][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@439a8f59{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2018-06-06 16:02:27,335][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@31024624{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2018-06-06 16:02:27,338][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://10.194.32.157:4040
[INFO][2018-06-06 16:02:27,431][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
[INFO][2018-06-06 16:02:27,449][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 55978.
[INFO][2018-06-06 16:02:27,450][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 10.194.32.157:55978
[INFO][2018-06-06 16:02:27,451][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2018-06-06 16:02:27,453][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 10.194.32.157, 55978, None)
[INFO][2018-06-06 16:02:27,456][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 10.194.32.157:55978 with 912.3 MB RAM, BlockManagerId(driver, 10.194.32.157, 55978, None)
[INFO][2018-06-06 16:02:27,461][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 10.194.32.157, 55978, None)
[INFO][2018-06-06 16:02:27,461][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 10.194.32.157, 55978, None)
[INFO][2018-06-06 16:02:27,705][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6df20ade{/metrics/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 16:02:27,733][com.seven.spark.rdd.SalesDefectNetByNetCommunity$]SalesDefectNetByNetCommunity$ is start . . .
[INFO][2018-06-06 16:02:28,309][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0 stored as values in memory (estimated size 228.1 KB, free 912.1 MB)
[INFO][2018-06-06 16:02:28,522][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0_piece0 stored as bytes in memory (estimated size 22.1 KB, free 912.1 MB)
[INFO][2018-06-06 16:02:28,524][org.apache.spark.storage.BlockManagerInfo]Added broadcast_0_piece0 in memory on 10.194.32.157:55978 (size: 22.1 KB, free: 912.3 MB)
[INFO][2018-06-06 16:02:28,530][org.apache.spark.SparkContext]Created broadcast 0 from textFile at SalesDefectNetByNetCommunity.scala:53
[WARN][2018-06-06 16:02:29,944][org.apache.hadoop.util.NativeCodeLoader]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO][2018-06-06 16:02:30,072][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 1
[INFO][2018-06-06 16:02:30,157][org.apache.spark.SparkContext]Starting job: collect at SalesDefectNetByNetCommunity.scala:64
[INFO][2018-06-06 16:02:30,170][org.apache.spark.scheduler.DAGScheduler]Got job 0 (collect at SalesDefectNetByNetCommunity.scala:64) with 2 output partitions
[INFO][2018-06-06 16:02:30,170][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 0 (collect at SalesDefectNetByNetCommunity.scala:64)
[INFO][2018-06-06 16:02:30,171][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-06-06 16:02:30,172][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-06-06 16:02:30,179][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 0 (MapPartitionsRDD[3] at mapPartitions at SalesDefectNetByNetCommunity.scala:56), which has no missing parents
[INFO][2018-06-06 16:02:30,202][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1 stored as values in memory (estimated size 3.7 KB, free 912.1 MB)
[INFO][2018-06-06 16:02:30,212][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.1 KB, free 912.1 MB)
[INFO][2018-06-06 16:02:30,213][org.apache.spark.storage.BlockManagerInfo]Added broadcast_1_piece0 in memory on 10.194.32.157:55978 (size: 2.1 KB, free: 912.3 MB)
[INFO][2018-06-06 16:02:30,213][org.apache.spark.SparkContext]Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO][2018-06-06 16:02:30,229][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at mapPartitions at SalesDefectNetByNetCommunity.scala:56) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2018-06-06 16:02:30,229][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 0.0 with 2 tasks
[INFO][2018-06-06 16:02:30,265][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 4930 bytes)
[INFO][2018-06-06 16:02:30,268][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 4930 bytes)
[INFO][2018-06-06 16:02:30,275][org.apache.spark.executor.Executor]Running task 0.0 in stage 0.0 (TID 0)
[INFO][2018-06-06 16:02:30,276][org.apache.spark.executor.Executor]Running task 1.0 in stage 0.0 (TID 1)
[INFO][2018-06-06 16:02:30,337][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/sta_vem/net_community/net_community__b4d64c39_5c6b_44c1_81be_f9aa19c55a30:278620+278620
[INFO][2018-06-06 16:02:30,337][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/sta_vem/net_community/net_community__b4d64c39_5c6b_44c1_81be_f9aa19c55a30:0+278620
[INFO][2018-06-06 16:02:30,560][org.apache.spark.executor.Executor]Finished task 1.0 in stage 0.0 (TID 1). 290550 bytes result sent to driver
[INFO][2018-06-06 16:02:30,585][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 0.0 (TID 1) in 317 ms on localhost (executor driver) (1/2)
[INFO][2018-06-06 16:02:30,636][org.apache.spark.executor.Executor]Finished task 0.0 in stage 0.0 (TID 0). 289718 bytes result sent to driver
[INFO][2018-06-06 16:02:30,646][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 0.0 (TID 0) in 392 ms on localhost (executor driver) (2/2)
[INFO][2018-06-06 16:02:30,648][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO][2018-06-06 16:02:30,648][org.apache.spark.scheduler.DAGScheduler]ResultStage 0 (collect at SalesDefectNetByNetCommunity.scala:64) finished in 0.404 s
[INFO][2018-06-06 16:02:30,653][org.apache.spark.scheduler.DAGScheduler]Job 0 finished: collect at SalesDefectNetByNetCommunity.scala:64, took 0.496073 s
[INFO][2018-06-06 16:02:30,660][org.apache.spark.storage.memory.MemoryStore]Block broadcast_2 stored as values in memory (estimated size 1060.4 KB, free 911.0 MB)
[INFO][2018-06-06 16:02:30,674][org.apache.spark.storage.memory.MemoryStore]Block broadcast_2_piece0 stored as bytes in memory (estimated size 203.6 KB, free 910.8 MB)
[INFO][2018-06-06 16:02:30,675][org.apache.spark.storage.BlockManagerInfo]Added broadcast_2_piece0 in memory on 10.194.32.157:55978 (size: 203.6 KB, free: 912.1 MB)
[INFO][2018-06-06 16:02:30,675][org.apache.spark.SparkContext]Created broadcast 2 from broadcast at SalesDefectNetByNetCommunity.scala:34
[INFO][2018-06-06 16:02:30,681][org.apache.spark.storage.memory.MemoryStore]Block broadcast_3 stored as values in memory (estimated size 228.1 KB, free 910.6 MB)
[INFO][2018-06-06 16:02:30,697][org.apache.spark.storage.memory.MemoryStore]Block broadcast_3_piece0 stored as bytes in memory (estimated size 22.1 KB, free 910.6 MB)
[INFO][2018-06-06 16:02:30,698][org.apache.spark.storage.BlockManagerInfo]Added broadcast_3_piece0 in memory on 10.194.32.157:55978 (size: 22.1 KB, free: 912.1 MB)
[INFO][2018-06-06 16:02:30,699][org.apache.spark.SparkContext]Created broadcast 3 from textFile at SalesDefectNetByNetCommunity.scala:79
[INFO][2018-06-06 16:02:30,725][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 1
[INFO][2018-06-06 16:02:30,741][org.apache.spark.SparkContext]Starting job: collect at SalesDefectNetByNetCommunity.scala:89
[INFO][2018-06-06 16:02:30,743][org.apache.spark.scheduler.DAGScheduler]Got job 1 (collect at SalesDefectNetByNetCommunity.scala:89) with 2 output partitions
[INFO][2018-06-06 16:02:30,743][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 1 (collect at SalesDefectNetByNetCommunity.scala:89)
[INFO][2018-06-06 16:02:30,743][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-06-06 16:02:30,743][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-06-06 16:02:30,744][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 1 (MapPartitionsRDD[7] at mapPartitions at SalesDefectNetByNetCommunity.scala:82), which has no missing parents
[INFO][2018-06-06 16:02:30,747][org.apache.spark.storage.memory.MemoryStore]Block broadcast_4 stored as values in memory (estimated size 3.7 KB, free 910.6 MB)
[INFO][2018-06-06 16:02:30,753][org.apache.spark.storage.memory.MemoryStore]Block broadcast_4_piece0 stored as bytes in memory (estimated size 2.0 KB, free 910.6 MB)
[INFO][2018-06-06 16:02:30,754][org.apache.spark.storage.BlockManagerInfo]Added broadcast_4_piece0 in memory on 10.194.32.157:55978 (size: 2.0 KB, free: 912.1 MB)
[INFO][2018-06-06 16:02:30,755][org.apache.spark.SparkContext]Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[INFO][2018-06-06 16:02:30,756][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[7] at mapPartitions at SalesDefectNetByNetCommunity.scala:82) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2018-06-06 16:02:30,756][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 1.0 with 2 tasks
[INFO][2018-06-06 16:02:30,757][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 1.0 (TID 2, localhost, executor driver, partition 0, ANY, 4926 bytes)
[INFO][2018-06-06 16:02:30,757][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 1.0 (TID 3, localhost, executor driver, partition 1, ANY, 4926 bytes)
[INFO][2018-06-06 16:02:30,758][org.apache.spark.executor.Executor]Running task 1.0 in stage 1.0 (TID 3)
[INFO][2018-06-06 16:02:30,759][org.apache.spark.executor.Executor]Running task 0.0 in stage 1.0 (TID 2)
[INFO][2018-06-06 16:02:30,762][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/sta_vem/vem_nettype/vem_nettype__5b80085f_0bae_4bd8_97d4_919813b3ce8b:6079084+6079084
[INFO][2018-06-06 16:02:30,765][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/sta_vem/vem_nettype/vem_nettype__5b80085f_0bae_4bd8_97d4_919813b3ce8b:0+6079084
[INFO][2018-06-06 16:02:34,972][org.apache.spark.executor.Executor]Finished task 1.0 in stage 1.0 (TID 3). 14530 bytes result sent to driver
[INFO][2018-06-06 16:02:34,987][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 1.0 (TID 3) in 4230 ms on localhost (executor driver) (1/2)
[INFO][2018-06-06 16:02:35,033][org.apache.spark.executor.Executor]Finished task 0.0 in stage 1.0 (TID 2). 240458 bytes result sent to driver
[INFO][2018-06-06 16:02:35,054][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 1.0 (TID 2) in 4298 ms on localhost (executor driver) (2/2)
[INFO][2018-06-06 16:02:35,054][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO][2018-06-06 16:02:35,055][org.apache.spark.scheduler.DAGScheduler]ResultStage 1 (collect at SalesDefectNetByNetCommunity.scala:89) finished in 4.299 s
[INFO][2018-06-06 16:02:35,055][org.apache.spark.scheduler.DAGScheduler]Job 1 finished: collect at SalesDefectNetByNetCommunity.scala:89, took 4.313728 s
[INFO][2018-06-06 16:02:35,064][org.apache.spark.storage.memory.MemoryStore]Block broadcast_5 stored as values in memory (estimated size 1924.3 KB, free 908.7 MB)
[INFO][2018-06-06 16:02:35,087][org.apache.spark.storage.memory.MemoryStore]Block broadcast_5_piece0 stored as bytes in memory (estimated size 111.1 KB, free 908.6 MB)
[INFO][2018-06-06 16:02:35,088][org.apache.spark.storage.BlockManagerInfo]Added broadcast_5_piece0 in memory on 10.194.32.157:55978 (size: 111.1 KB, free: 911.9 MB)
[INFO][2018-06-06 16:02:35,088][org.apache.spark.SparkContext]Created broadcast 5 from broadcast at SalesDefectNetByNetCommunity.scala:38
[INFO][2018-06-06 16:02:35,094][org.apache.spark.storage.memory.MemoryStore]Block broadcast_6 stored as values in memory (estimated size 228.1 KB, free 908.4 MB)
[INFO][2018-06-06 16:02:35,110][org.apache.spark.storage.memory.MemoryStore]Block broadcast_6_piece0 stored as bytes in memory (estimated size 22.1 KB, free 908.3 MB)
[INFO][2018-06-06 16:02:35,111][org.apache.spark.storage.BlockManagerInfo]Added broadcast_6_piece0 in memory on 10.194.32.157:55978 (size: 22.1 KB, free: 911.9 MB)
[INFO][2018-06-06 16:02:35,111][org.apache.spark.SparkContext]Created broadcast 6 from textFile at SalesDefectNetByNetCommunity.scala:104
[INFO][2018-06-06 16:02:35,137][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 1
[INFO][2018-06-06 16:02:35,171][org.apache.spark.SparkContext]Starting job: foreach at SalesDefectNetByNetCommunity.scala:131
[INFO][2018-06-06 16:02:35,185][org.apache.spark.scheduler.DAGScheduler]Registering RDD 10 (mapPartitions at SalesDefectNetByNetCommunity.scala:109)
[INFO][2018-06-06 16:02:35,186][org.apache.spark.scheduler.DAGScheduler]Got job 2 (foreach at SalesDefectNetByNetCommunity.scala:131) with 2 output partitions
[INFO][2018-06-06 16:02:35,186][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 3 (foreach at SalesDefectNetByNetCommunity.scala:131)
[INFO][2018-06-06 16:02:35,186][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 2)
[INFO][2018-06-06 16:02:35,190][org.apache.spark.scheduler.DAGScheduler]Missing parents: List(ShuffleMapStage 2)
[INFO][2018-06-06 16:02:35,191][org.apache.spark.scheduler.DAGScheduler]Submitting ShuffleMapStage 2 (MapPartitionsRDD[10] at mapPartitions at SalesDefectNetByNetCommunity.scala:109), which has no missing parents
[INFO][2018-06-06 16:02:35,201][org.apache.spark.storage.memory.MemoryStore]Block broadcast_7 stored as values in memory (estimated size 4.6 KB, free 908.3 MB)
[INFO][2018-06-06 16:02:35,204][org.apache.spark.storage.memory.MemoryStore]Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.7 KB, free 908.3 MB)
[INFO][2018-06-06 16:02:35,205][org.apache.spark.storage.BlockManagerInfo]Added broadcast_7_piece0 in memory on 10.194.32.157:55978 (size: 2.7 KB, free: 911.9 MB)
[INFO][2018-06-06 16:02:35,205][org.apache.spark.SparkContext]Created broadcast 7 from broadcast at DAGScheduler.scala:1006
[INFO][2018-06-06 16:02:35,208][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[10] at mapPartitions at SalesDefectNetByNetCommunity.scala:109) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2018-06-06 16:02:35,208][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 2.0 with 2 tasks
[INFO][2018-06-06 16:02:35,210][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 2.0 (TID 4, localhost, executor driver, partition 0, ANY, 4872 bytes)
[INFO][2018-06-06 16:02:35,211][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 2.0 (TID 5, localhost, executor driver, partition 1, ANY, 4872 bytes)
[INFO][2018-06-06 16:02:35,211][org.apache.spark.executor.Executor]Running task 0.0 in stage 2.0 (TID 4)
[INFO][2018-06-06 16:02:35,211][org.apache.spark.executor.Executor]Running task 1.0 in stage 2.0 (TID 5)
[INFO][2018-06-06 16:02:35,216][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/part-00000:14417720+14417721
[INFO][2018-06-06 16:02:35,217][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/part-00000:0+14417720
[INFO][2018-06-06 16:02:38,610][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_4_piece0 on 10.194.32.157:55978 in memory (size: 2.0 KB, free: 911.9 MB)
[INFO][2018-06-06 16:02:38,614][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_3_piece0 on 10.194.32.157:55978 in memory (size: 22.1 KB, free: 911.9 MB)
[INFO][2018-06-06 16:02:45,045][org.apache.spark.executor.Executor]Finished task 1.0 in stage 2.0 (TID 5). 1129 bytes result sent to driver
[INFO][2018-06-06 16:02:45,068][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 2.0 (TID 5) in 9858 ms on localhost (executor driver) (1/2)
[INFO][2018-06-06 16:02:45,771][org.apache.spark.executor.Executor]Finished task 0.0 in stage 2.0 (TID 4). 1086 bytes result sent to driver
[INFO][2018-06-06 16:02:45,772][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 2.0 (TID 4) in 10563 ms on localhost (executor driver) (2/2)
[INFO][2018-06-06 16:02:45,772][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO][2018-06-06 16:02:45,773][org.apache.spark.scheduler.DAGScheduler]ShuffleMapStage 2 (mapPartitions at SalesDefectNetByNetCommunity.scala:109) finished in 10.564 s
[INFO][2018-06-06 16:02:45,774][org.apache.spark.scheduler.DAGScheduler]looking for newly runnable stages
[INFO][2018-06-06 16:02:45,774][org.apache.spark.scheduler.DAGScheduler]running: Set()
[INFO][2018-06-06 16:02:45,775][org.apache.spark.scheduler.DAGScheduler]waiting: Set(ResultStage 3)
[INFO][2018-06-06 16:02:45,775][org.apache.spark.scheduler.DAGScheduler]failed: Set()
[INFO][2018-06-06 16:02:45,780][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 3 (MapPartitionsRDD[13] at filter at SalesDefectNetByNetCommunity.scala:125), which has no missing parents
[INFO][2018-06-06 16:02:45,784][org.apache.spark.storage.memory.MemoryStore]Block broadcast_8 stored as values in memory (estimated size 4.6 KB, free 908.6 MB)
[INFO][2018-06-06 16:02:45,791][org.apache.spark.storage.memory.MemoryStore]Block broadcast_8_piece0 stored as bytes in memory (estimated size 2.6 KB, free 908.6 MB)
[INFO][2018-06-06 16:02:45,792][org.apache.spark.storage.BlockManagerInfo]Added broadcast_8_piece0 in memory on 10.194.32.157:55978 (size: 2.6 KB, free: 911.9 MB)
[INFO][2018-06-06 16:02:45,793][org.apache.spark.SparkContext]Created broadcast 8 from broadcast at DAGScheduler.scala:1006
[INFO][2018-06-06 16:02:45,794][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ResultStage 3 (MapPartitionsRDD[13] at filter at SalesDefectNetByNetCommunity.scala:125) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2018-06-06 16:02:45,794][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 3.0 with 2 tasks
[INFO][2018-06-06 16:02:45,796][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 3.0 (TID 6, localhost, executor driver, partition 0, ANY, 4621 bytes)
[INFO][2018-06-06 16:02:45,796][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 3.0 (TID 7, localhost, executor driver, partition 1, ANY, 4621 bytes)
[INFO][2018-06-06 16:02:45,796][org.apache.spark.executor.Executor]Running task 0.0 in stage 3.0 (TID 6)
[INFO][2018-06-06 16:02:45,803][org.apache.spark.executor.Executor]Running task 1.0 in stage 3.0 (TID 7)
[INFO][2018-06-06 16:02:45,818][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-06-06 16:02:45,818][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-06-06 16:02:45,824][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 10 ms
[INFO][2018-06-06 16:02:45,824][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 10 ms
[INFO][2018-06-06 16:02:45,880][org.apache.spark.storage.memory.MemoryStore]Block rdd_13_1 stored as values in memory (estimated size 152.0 B, free 908.6 MB)
[INFO][2018-06-06 16:02:45,880][org.apache.spark.storage.BlockManagerInfo]Added rdd_13_1 in memory on 10.194.32.157:55978 (size: 152.0 B, free: 911.9 MB)
[INFO][2018-06-06 16:02:45,897][org.apache.spark.storage.memory.MemoryStore]Block rdd_13_0 stored as values in memory (estimated size 216.0 B, free 908.6 MB)
[INFO][2018-06-06 16:02:45,897][org.apache.spark.storage.BlockManagerInfo]Added rdd_13_0 in memory on 10.194.32.157:55978 (size: 216.0 B, free: 911.9 MB)
[INFO][2018-06-06 16:02:45,899][org.apache.spark.executor.Executor]Finished task 1.0 in stage 3.0 (TID 7). 1750 bytes result sent to driver
[INFO][2018-06-06 16:02:45,900][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 3.0 (TID 7) in 104 ms on localhost (executor driver) (1/2)
[INFO][2018-06-06 16:02:45,913][org.apache.spark.executor.Executor]Finished task 0.0 in stage 3.0 (TID 6). 1750 bytes result sent to driver
[INFO][2018-06-06 16:02:45,914][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 3.0 (TID 6) in 119 ms on localhost (executor driver) (2/2)
[INFO][2018-06-06 16:02:45,914][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 3.0, whose tasks have all completed, from pool 
[INFO][2018-06-06 16:02:45,915][org.apache.spark.scheduler.DAGScheduler]ResultStage 3 (foreach at SalesDefectNetByNetCommunity.scala:131) finished in 0.119 s
[INFO][2018-06-06 16:02:45,915][org.apache.spark.scheduler.DAGScheduler]Job 2 finished: foreach at SalesDefectNetByNetCommunity.scala:131, took 10.743483 s
[INFO][2018-06-06 16:02:45,916][com.seven.spark.rdd.SalesDefectNetByNetCommunity$]SalesDefectNetByNetCommunity$ is success . . .
[INFO][2018-06-06 16:02:45,922][org.apache.spark.SparkContext]Invoking stop() from shutdown hook
[INFO][2018-06-06 16:02:45,929][org.spark_project.jetty.server.AbstractConnector]Stopped Spark@89c10b7{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-06-06 16:02:45,931][org.apache.spark.ui.SparkUI]Stopped Spark web UI at http://10.194.32.157:4040
[INFO][2018-06-06 16:02:45,945][org.apache.spark.MapOutputTrackerMasterEndpoint]MapOutputTrackerMasterEndpoint stopped!
[INFO][2018-06-06 16:02:45,974][org.apache.spark.storage.memory.MemoryStore]MemoryStore cleared
[INFO][2018-06-06 16:02:45,974][org.apache.spark.storage.BlockManager]BlockManager stopped
[INFO][2018-06-06 16:02:45,976][org.apache.spark.storage.BlockManagerMaster]BlockManagerMaster stopped
[INFO][2018-06-06 16:02:45,979][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint]OutputCommitCoordinator stopped!
[INFO][2018-06-06 16:02:45,982][org.apache.spark.SparkContext]Successfully stopped SparkContext
[INFO][2018-06-06 16:02:45,982][org.apache.spark.util.ShutdownHookManager]Shutdown hook called
[INFO][2018-06-06 16:02:45,984][org.apache.spark.util.ShutdownHookManager]Deleting directory /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/spark-ac37e245-c48d-4650-a12f-b06f0ced33b0
[INFO][2018-06-06 16:06:02,036][org.apache.spark.SparkContext]Running Spark version 2.2.0
[INFO][2018-06-06 16:06:02,915][org.apache.spark.SparkContext]Submitted application: SalesDefectNetByNetCommunity$
[INFO][2018-06-06 16:06:02,932][org.apache.spark.SecurityManager]Changing view acls to: seven
[INFO][2018-06-06 16:06:02,933][org.apache.spark.SecurityManager]Changing modify acls to: seven
[INFO][2018-06-06 16:06:02,933][org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO][2018-06-06 16:06:02,934][org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO][2018-06-06 16:06:02,934][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(seven); groups with view permissions: Set(); users  with modify permissions: Set(seven); groups with modify permissions: Set()
[INFO][2018-06-06 16:06:03,196][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 56019.
[INFO][2018-06-06 16:06:03,217][org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO][2018-06-06 16:06:03,235][org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO][2018-06-06 16:06:03,239][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2018-06-06 16:06:03,240][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
[INFO][2018-06-06 16:06:03,250][org.apache.spark.storage.DiskBlockManager]Created local directory at /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/blockmgr-2c548ab4-4d3c-476c-8763-cdf72aa9b13b
[INFO][2018-06-06 16:06:03,269][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 912.3 MB
[INFO][2018-06-06 16:06:03,342][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO][2018-06-06 16:06:03,426][org.spark_project.jetty.util.log]Logging initialized @2330ms
[INFO][2018-06-06 16:06:03,480][org.spark_project.jetty.server.Server]jetty-9.3.z-SNAPSHOT
[INFO][2018-06-06 16:06:03,493][org.spark_project.jetty.server.Server]Started @2398ms
[INFO][2018-06-06 16:06:03,511][org.spark_project.jetty.server.AbstractConnector]Started ServerConnector@695b90c5{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-06-06 16:06:03,511][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4040.
[INFO][2018-06-06 16:06:03,537][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@41fe9859{/jobs,null,AVAILABLE,@Spark}
[INFO][2018-06-06 16:06:03,538][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@29a60c27{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 16:06:03,538][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@69c79f09{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2018-06-06 16:06:03,539][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@533b266e{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 16:06:03,541][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@89ff02e{/stages,null,AVAILABLE,@Spark}
[INFO][2018-06-06 16:06:03,542][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@62679465{/stages/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 16:06:03,543][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1d71006f{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2018-06-06 16:06:03,547][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3ebff828{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 16:06:03,548][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@33352f32{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2018-06-06 16:06:03,551][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1e044120{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 16:06:03,557][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3624da92{/storage,null,AVAILABLE,@Spark}
[INFO][2018-06-06 16:06:03,558][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@94f6bfb{/storage/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 16:06:03,560][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2484f433{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2018-06-06 16:06:03,560][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1255b1d1{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 16:06:03,561][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7c22d4f{/environment,null,AVAILABLE,@Spark}
[INFO][2018-06-06 16:06:03,562][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@60bdf15d{/environment/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 16:06:03,563][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@51e4ccb3{/executors,null,AVAILABLE,@Spark}
[INFO][2018-06-06 16:06:03,564][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@495083a0{/executors/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 16:06:03,564][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@28a0fd6c{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2018-06-06 16:06:03,566][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@66629f63{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 16:06:03,573][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@27a5328c{/static,null,AVAILABLE,@Spark}
[INFO][2018-06-06 16:06:03,574][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@10c8f62{/,null,AVAILABLE,@Spark}
[INFO][2018-06-06 16:06:03,575][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@25f7391e{/api,null,AVAILABLE,@Spark}
[INFO][2018-06-06 16:06:03,576][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@31024624{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2018-06-06 16:06:03,576][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@32cb636e{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2018-06-06 16:06:03,578][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://10.194.32.157:4040
[INFO][2018-06-06 16:06:03,671][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
[INFO][2018-06-06 16:06:03,694][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 56020.
[INFO][2018-06-06 16:06:03,698][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 10.194.32.157:56020
[INFO][2018-06-06 16:06:03,699][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2018-06-06 16:06:03,703][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 10.194.32.157, 56020, None)
[INFO][2018-06-06 16:06:03,706][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 10.194.32.157:56020 with 912.3 MB RAM, BlockManagerId(driver, 10.194.32.157, 56020, None)
[INFO][2018-06-06 16:06:03,711][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 10.194.32.157, 56020, None)
[INFO][2018-06-06 16:06:03,712][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 10.194.32.157, 56020, None)
[INFO][2018-06-06 16:06:03,897][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4010d494{/metrics/json,null,AVAILABLE,@Spark}
[INFO][2018-06-06 16:06:03,920][com.seven.spark.rdd.SalesDefectNetByNetCommunity$]SalesDefectNetByNetCommunity$ is start . . .
[INFO][2018-06-06 16:06:04,390][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0 stored as values in memory (estimated size 228.1 KB, free 912.1 MB)
[INFO][2018-06-06 16:06:04,597][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0_piece0 stored as bytes in memory (estimated size 22.1 KB, free 912.1 MB)
[INFO][2018-06-06 16:06:04,599][org.apache.spark.storage.BlockManagerInfo]Added broadcast_0_piece0 in memory on 10.194.32.157:56020 (size: 22.1 KB, free: 912.3 MB)
[INFO][2018-06-06 16:06:04,605][org.apache.spark.SparkContext]Created broadcast 0 from textFile at SalesDefectNetByNetCommunity.scala:53
[WARN][2018-06-06 16:06:06,040][org.apache.hadoop.util.NativeCodeLoader]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO][2018-06-06 16:06:06,187][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 1
[INFO][2018-06-06 16:06:06,267][org.apache.spark.SparkContext]Starting job: collect at SalesDefectNetByNetCommunity.scala:64
[INFO][2018-06-06 16:06:06,279][org.apache.spark.scheduler.DAGScheduler]Got job 0 (collect at SalesDefectNetByNetCommunity.scala:64) with 2 output partitions
[INFO][2018-06-06 16:06:06,279][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 0 (collect at SalesDefectNetByNetCommunity.scala:64)
[INFO][2018-06-06 16:06:06,280][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-06-06 16:06:06,281][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-06-06 16:06:06,288][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 0 (MapPartitionsRDD[3] at mapPartitions at SalesDefectNetByNetCommunity.scala:56), which has no missing parents
[INFO][2018-06-06 16:06:06,309][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1 stored as values in memory (estimated size 3.7 KB, free 912.1 MB)
[INFO][2018-06-06 16:06:06,320][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.1 KB, free 912.1 MB)
[INFO][2018-06-06 16:06:06,322][org.apache.spark.storage.BlockManagerInfo]Added broadcast_1_piece0 in memory on 10.194.32.157:56020 (size: 2.1 KB, free: 912.3 MB)
[INFO][2018-06-06 16:06:06,323][org.apache.spark.SparkContext]Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO][2018-06-06 16:06:06,342][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at mapPartitions at SalesDefectNetByNetCommunity.scala:56) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2018-06-06 16:06:06,343][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 0.0 with 2 tasks
[INFO][2018-06-06 16:06:06,380][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 4930 bytes)
[INFO][2018-06-06 16:06:06,382][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 4930 bytes)
[INFO][2018-06-06 16:06:06,390][org.apache.spark.executor.Executor]Running task 1.0 in stage 0.0 (TID 1)
[INFO][2018-06-06 16:06:06,390][org.apache.spark.executor.Executor]Running task 0.0 in stage 0.0 (TID 0)
[INFO][2018-06-06 16:06:06,460][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/sta_vem/net_community/net_community__b4d64c39_5c6b_44c1_81be_f9aa19c55a30:278620+278620
[INFO][2018-06-06 16:06:06,460][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/sta_vem/net_community/net_community__b4d64c39_5c6b_44c1_81be_f9aa19c55a30:0+278620
[INFO][2018-06-06 16:06:06,722][org.apache.spark.executor.Executor]Finished task 1.0 in stage 0.0 (TID 1). 290550 bytes result sent to driver
[INFO][2018-06-06 16:06:06,722][org.apache.spark.executor.Executor]Finished task 0.0 in stage 0.0 (TID 0). 289718 bytes result sent to driver
[INFO][2018-06-06 16:06:06,753][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 0.0 (TID 0) in 382 ms on localhost (executor driver) (1/2)
[INFO][2018-06-06 16:06:06,755][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 0.0 (TID 1) in 374 ms on localhost (executor driver) (2/2)
[INFO][2018-06-06 16:06:06,757][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO][2018-06-06 16:06:06,761][org.apache.spark.scheduler.DAGScheduler]ResultStage 0 (collect at SalesDefectNetByNetCommunity.scala:64) finished in 0.400 s
[INFO][2018-06-06 16:06:06,766][org.apache.spark.scheduler.DAGScheduler]Job 0 finished: collect at SalesDefectNetByNetCommunity.scala:64, took 0.498199 s
[INFO][2018-06-06 16:06:06,777][org.apache.spark.storage.memory.MemoryStore]Block broadcast_2 stored as values in memory (estimated size 1060.4 KB, free 911.0 MB)
[INFO][2018-06-06 16:06:06,798][org.apache.spark.storage.memory.MemoryStore]Block broadcast_2_piece0 stored as bytes in memory (estimated size 203.6 KB, free 910.8 MB)
[INFO][2018-06-06 16:06:06,799][org.apache.spark.storage.BlockManagerInfo]Added broadcast_2_piece0 in memory on 10.194.32.157:56020 (size: 203.6 KB, free: 912.1 MB)
[INFO][2018-06-06 16:06:06,800][org.apache.spark.SparkContext]Created broadcast 2 from broadcast at SalesDefectNetByNetCommunity.scala:34
[INFO][2018-06-06 16:06:06,807][org.apache.spark.storage.memory.MemoryStore]Block broadcast_3 stored as values in memory (estimated size 228.1 KB, free 910.6 MB)
[INFO][2018-06-06 16:06:06,825][org.apache.spark.storage.memory.MemoryStore]Block broadcast_3_piece0 stored as bytes in memory (estimated size 22.1 KB, free 910.6 MB)
[INFO][2018-06-06 16:06:06,826][org.apache.spark.storage.BlockManagerInfo]Added broadcast_3_piece0 in memory on 10.194.32.157:56020 (size: 22.1 KB, free: 912.1 MB)
[INFO][2018-06-06 16:06:06,827][org.apache.spark.SparkContext]Created broadcast 3 from textFile at SalesDefectNetByNetCommunity.scala:79
[INFO][2018-06-06 16:06:06,855][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 1
[INFO][2018-06-06 16:06:06,872][org.apache.spark.SparkContext]Starting job: collect at SalesDefectNetByNetCommunity.scala:89
[INFO][2018-06-06 16:06:06,873][org.apache.spark.scheduler.DAGScheduler]Got job 1 (collect at SalesDefectNetByNetCommunity.scala:89) with 2 output partitions
[INFO][2018-06-06 16:06:06,873][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 1 (collect at SalesDefectNetByNetCommunity.scala:89)
[INFO][2018-06-06 16:06:06,873][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-06-06 16:06:06,873][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-06-06 16:06:06,874][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 1 (MapPartitionsRDD[7] at mapPartitions at SalesDefectNetByNetCommunity.scala:82), which has no missing parents
[INFO][2018-06-06 16:06:06,877][org.apache.spark.storage.memory.MemoryStore]Block broadcast_4 stored as values in memory (estimated size 3.7 KB, free 910.6 MB)
[INFO][2018-06-06 16:06:06,883][org.apache.spark.storage.memory.MemoryStore]Block broadcast_4_piece0 stored as bytes in memory (estimated size 2.0 KB, free 910.6 MB)
[INFO][2018-06-06 16:06:06,884][org.apache.spark.storage.BlockManagerInfo]Added broadcast_4_piece0 in memory on 10.194.32.157:56020 (size: 2.0 KB, free: 912.1 MB)
[INFO][2018-06-06 16:06:06,885][org.apache.spark.SparkContext]Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[INFO][2018-06-06 16:06:06,886][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[7] at mapPartitions at SalesDefectNetByNetCommunity.scala:82) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2018-06-06 16:06:06,886][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 1.0 with 2 tasks
[INFO][2018-06-06 16:06:06,888][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 1.0 (TID 2, localhost, executor driver, partition 0, ANY, 4926 bytes)
[INFO][2018-06-06 16:06:06,889][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 1.0 (TID 3, localhost, executor driver, partition 1, ANY, 4926 bytes)
[INFO][2018-06-06 16:06:06,889][org.apache.spark.executor.Executor]Running task 1.0 in stage 1.0 (TID 3)
[INFO][2018-06-06 16:06:06,893][org.apache.spark.executor.Executor]Running task 0.0 in stage 1.0 (TID 2)
[INFO][2018-06-06 16:06:06,894][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/sta_vem/vem_nettype/vem_nettype__5b80085f_0bae_4bd8_97d4_919813b3ce8b:6079084+6079084
[INFO][2018-06-06 16:06:06,899][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/sta_vem/vem_nettype/vem_nettype__5b80085f_0bae_4bd8_97d4_919813b3ce8b:0+6079084
[INFO][2018-06-06 16:06:10,421][org.apache.spark.executor.Executor]Finished task 1.0 in stage 1.0 (TID 3). 14530 bytes result sent to driver
[INFO][2018-06-06 16:06:10,439][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 1.0 (TID 3) in 3550 ms on localhost (executor driver) (1/2)
[INFO][2018-06-06 16:06:10,831][org.apache.spark.executor.Executor]Finished task 0.0 in stage 1.0 (TID 2). 240458 bytes result sent to driver
[INFO][2018-06-06 16:06:10,854][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 1.0 (TID 2) in 3967 ms on localhost (executor driver) (2/2)
[INFO][2018-06-06 16:06:10,855][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO][2018-06-06 16:06:10,856][org.apache.spark.scheduler.DAGScheduler]ResultStage 1 (collect at SalesDefectNetByNetCommunity.scala:89) finished in 3.968 s
[INFO][2018-06-06 16:06:10,857][org.apache.spark.scheduler.DAGScheduler]Job 1 finished: collect at SalesDefectNetByNetCommunity.scala:89, took 3.984670 s
[INFO][2018-06-06 16:06:10,864][org.apache.spark.storage.memory.MemoryStore]Block broadcast_5 stored as values in memory (estimated size 1924.3 KB, free 908.7 MB)
[INFO][2018-06-06 16:06:10,886][org.apache.spark.storage.memory.MemoryStore]Block broadcast_5_piece0 stored as bytes in memory (estimated size 111.1 KB, free 908.6 MB)
[INFO][2018-06-06 16:06:10,888][org.apache.spark.storage.BlockManagerInfo]Added broadcast_5_piece0 in memory on 10.194.32.157:56020 (size: 111.1 KB, free: 911.9 MB)
[INFO][2018-06-06 16:06:10,889][org.apache.spark.SparkContext]Created broadcast 5 from broadcast at SalesDefectNetByNetCommunity.scala:38
[INFO][2018-06-06 16:06:10,894][org.apache.spark.storage.memory.MemoryStore]Block broadcast_6 stored as values in memory (estimated size 228.1 KB, free 908.4 MB)
[INFO][2018-06-06 16:06:10,916][org.apache.spark.storage.memory.MemoryStore]Block broadcast_6_piece0 stored as bytes in memory (estimated size 22.1 KB, free 908.3 MB)
[INFO][2018-06-06 16:06:10,917][org.apache.spark.storage.BlockManagerInfo]Added broadcast_6_piece0 in memory on 10.194.32.157:56020 (size: 22.1 KB, free: 911.9 MB)
[INFO][2018-06-06 16:06:10,918][org.apache.spark.SparkContext]Created broadcast 6 from textFile at SalesDefectNetByNetCommunity.scala:104
[INFO][2018-06-06 16:06:10,947][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 1
[INFO][2018-06-06 16:06:10,983][org.apache.spark.SparkContext]Starting job: foreach at SalesDefectNetByNetCommunity.scala:131
[INFO][2018-06-06 16:06:10,996][org.apache.spark.scheduler.DAGScheduler]Registering RDD 10 (mapPartitions at SalesDefectNetByNetCommunity.scala:109)
[INFO][2018-06-06 16:06:10,997][org.apache.spark.scheduler.DAGScheduler]Got job 2 (foreach at SalesDefectNetByNetCommunity.scala:131) with 2 output partitions
[INFO][2018-06-06 16:06:10,997][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 3 (foreach at SalesDefectNetByNetCommunity.scala:131)
[INFO][2018-06-06 16:06:10,997][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 2)
[INFO][2018-06-06 16:06:11,001][org.apache.spark.scheduler.DAGScheduler]Missing parents: List(ShuffleMapStage 2)
[INFO][2018-06-06 16:06:11,002][org.apache.spark.scheduler.DAGScheduler]Submitting ShuffleMapStage 2 (MapPartitionsRDD[10] at mapPartitions at SalesDefectNetByNetCommunity.scala:109), which has no missing parents
[INFO][2018-06-06 16:06:11,016][org.apache.spark.storage.memory.MemoryStore]Block broadcast_7 stored as values in memory (estimated size 4.6 KB, free 908.3 MB)
[INFO][2018-06-06 16:06:11,023][org.apache.spark.storage.memory.MemoryStore]Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.7 KB, free 908.3 MB)
[INFO][2018-06-06 16:06:11,023][org.apache.spark.storage.BlockManagerInfo]Added broadcast_7_piece0 in memory on 10.194.32.157:56020 (size: 2.7 KB, free: 911.9 MB)
[INFO][2018-06-06 16:06:11,024][org.apache.spark.SparkContext]Created broadcast 7 from broadcast at DAGScheduler.scala:1006
[INFO][2018-06-06 16:06:11,027][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[10] at mapPartitions at SalesDefectNetByNetCommunity.scala:109) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2018-06-06 16:06:11,027][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 2.0 with 2 tasks
[INFO][2018-06-06 16:06:11,029][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 2.0 (TID 4, localhost, executor driver, partition 0, ANY, 4872 bytes)
[INFO][2018-06-06 16:06:11,029][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 2.0 (TID 5, localhost, executor driver, partition 1, ANY, 4872 bytes)
[INFO][2018-06-06 16:06:11,029][org.apache.spark.executor.Executor]Running task 1.0 in stage 2.0 (TID 5)
[INFO][2018-06-06 16:06:11,029][org.apache.spark.executor.Executor]Running task 0.0 in stage 2.0 (TID 4)
[INFO][2018-06-06 16:06:11,034][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/part-00000:0+14417720
[INFO][2018-06-06 16:06:11,035][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/part-00000:14417720+14417721
[INFO][2018-06-06 16:06:11,165][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_4_piece0 on 10.194.32.157:56020 in memory (size: 2.0 KB, free: 911.9 MB)
[INFO][2018-06-06 16:06:11,169][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_3_piece0 on 10.194.32.157:56020 in memory (size: 22.1 KB, free: 911.9 MB)
[INFO][2018-06-06 16:06:12,482][org.apache.spark.executor.Executor]Finished task 0.0 in stage 2.0 (TID 4). 1086 bytes result sent to driver
[INFO][2018-06-06 16:06:12,502][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 2.0 (TID 4) in 1475 ms on localhost (executor driver) (1/2)
[INFO][2018-06-06 16:06:21,068][org.apache.spark.executor.Executor]Finished task 1.0 in stage 2.0 (TID 5). 1086 bytes result sent to driver
[INFO][2018-06-06 16:06:21,070][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 2.0 (TID 5) in 10041 ms on localhost (executor driver) (2/2)
[INFO][2018-06-06 16:06:21,070][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO][2018-06-06 16:06:21,071][org.apache.spark.scheduler.DAGScheduler]ShuffleMapStage 2 (mapPartitions at SalesDefectNetByNetCommunity.scala:109) finished in 10.044 s
[INFO][2018-06-06 16:06:21,071][org.apache.spark.scheduler.DAGScheduler]looking for newly runnable stages
[INFO][2018-06-06 16:06:21,072][org.apache.spark.scheduler.DAGScheduler]running: Set()
[INFO][2018-06-06 16:06:21,073][org.apache.spark.scheduler.DAGScheduler]waiting: Set(ResultStage 3)
[INFO][2018-06-06 16:06:21,074][org.apache.spark.scheduler.DAGScheduler]failed: Set()
[INFO][2018-06-06 16:06:21,079][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 3 (MapPartitionsRDD[12] at mapPartitions at SalesDefectNetByNetCommunity.scala:117), which has no missing parents
[INFO][2018-06-06 16:06:21,084][org.apache.spark.storage.memory.MemoryStore]Block broadcast_8 stored as values in memory (estimated size 4.3 KB, free 908.6 MB)
[INFO][2018-06-06 16:06:21,088][org.apache.spark.storage.memory.MemoryStore]Block broadcast_8_piece0 stored as bytes in memory (estimated size 2.4 KB, free 908.6 MB)
[INFO][2018-06-06 16:06:21,089][org.apache.spark.storage.BlockManagerInfo]Added broadcast_8_piece0 in memory on 10.194.32.157:56020 (size: 2.4 KB, free: 911.9 MB)
[INFO][2018-06-06 16:06:21,089][org.apache.spark.SparkContext]Created broadcast 8 from broadcast at DAGScheduler.scala:1006
[INFO][2018-06-06 16:06:21,090][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ResultStage 3 (MapPartitionsRDD[12] at mapPartitions at SalesDefectNetByNetCommunity.scala:117) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2018-06-06 16:06:21,090][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 3.0 with 2 tasks
[INFO][2018-06-06 16:06:21,094][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 3.0 (TID 6, localhost, executor driver, partition 0, ANY, 4621 bytes)
[INFO][2018-06-06 16:06:21,095][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 3.0 (TID 7, localhost, executor driver, partition 1, ANY, 4621 bytes)
[INFO][2018-06-06 16:06:21,095][org.apache.spark.executor.Executor]Running task 1.0 in stage 3.0 (TID 7)
[INFO][2018-06-06 16:06:21,098][org.apache.spark.executor.Executor]Running task 0.0 in stage 3.0 (TID 6)
[INFO][2018-06-06 16:06:21,115][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-06-06 16:06:21,115][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2018-06-06 16:06:21,117][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 6 ms
[INFO][2018-06-06 16:06:21,118][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 7 ms
[INFO][2018-06-06 16:06:21,173][org.apache.spark.storage.memory.MemoryStore]Block rdd_12_0 stored as values in memory (estimated size 64.1 KB, free 908.5 MB)
[INFO][2018-06-06 16:06:21,173][org.apache.spark.storage.BlockManagerInfo]Added rdd_12_0 in memory on 10.194.32.157:56020 (size: 64.1 KB, free: 911.9 MB)
[INFO][2018-06-06 16:06:21,179][org.apache.spark.storage.memory.MemoryStore]Block rdd_12_1 stored as values in memory (estimated size 63.7 KB, free 908.4 MB)
[INFO][2018-06-06 16:06:21,180][org.apache.spark.storage.BlockManagerInfo]Added rdd_12_1 in memory on 10.194.32.157:56020 (size: 63.7 KB, free: 911.8 MB)
[INFO][2018-06-06 16:06:21,203][org.apache.spark.executor.Executor]Finished task 0.0 in stage 3.0 (TID 6). 1750 bytes result sent to driver
[INFO][2018-06-06 16:06:21,205][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 3.0 (TID 6) in 113 ms on localhost (executor driver) (1/2)
[INFO][2018-06-06 16:06:21,206][org.apache.spark.executor.Executor]Finished task 1.0 in stage 3.0 (TID 7). 1750 bytes result sent to driver
[INFO][2018-06-06 16:06:21,207][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 3.0 (TID 7) in 112 ms on localhost (executor driver) (2/2)
[INFO][2018-06-06 16:06:21,207][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 3.0, whose tasks have all completed, from pool 
[INFO][2018-06-06 16:06:21,208][org.apache.spark.scheduler.DAGScheduler]ResultStage 3 (foreach at SalesDefectNetByNetCommunity.scala:131) finished in 0.117 s
[INFO][2018-06-06 16:06:21,209][org.apache.spark.scheduler.DAGScheduler]Job 2 finished: foreach at SalesDefectNetByNetCommunity.scala:131, took 10.224939 s
[INFO][2018-06-06 16:06:21,210][com.seven.spark.rdd.SalesDefectNetByNetCommunity$]SalesDefectNetByNetCommunity$ is success . . .
[INFO][2018-06-06 16:06:21,212][org.apache.spark.SparkContext]Invoking stop() from shutdown hook
[INFO][2018-06-06 16:06:21,220][org.spark_project.jetty.server.AbstractConnector]Stopped Spark@695b90c5{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-06-06 16:06:21,222][org.apache.spark.ui.SparkUI]Stopped Spark web UI at http://10.194.32.157:4040
[INFO][2018-06-06 16:06:21,233][org.apache.spark.MapOutputTrackerMasterEndpoint]MapOutputTrackerMasterEndpoint stopped!
[INFO][2018-06-06 16:06:21,254][org.apache.spark.storage.memory.MemoryStore]MemoryStore cleared
[INFO][2018-06-06 16:06:21,254][org.apache.spark.storage.BlockManager]BlockManager stopped
[INFO][2018-06-06 16:06:21,255][org.apache.spark.storage.BlockManagerMaster]BlockManagerMaster stopped
[INFO][2018-06-06 16:06:21,257][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint]OutputCommitCoordinator stopped!
[INFO][2018-06-06 16:06:21,259][org.apache.spark.SparkContext]Successfully stopped SparkContext
[INFO][2018-06-06 16:06:21,259][org.apache.spark.util.ShutdownHookManager]Shutdown hook called
[INFO][2018-06-06 16:06:21,260][org.apache.spark.util.ShutdownHookManager]Deleting directory /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/spark-d2bb2dd7-398a-48ef-b09c-3ef78ec313df
[INFO][2018-06-06 16:15:19,549][org.apache.spark.SparkContext]Running Spark version 2.2.0
