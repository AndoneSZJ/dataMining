[INFO][2018-05-24 19:42:13,186][org.apache.spark.SparkContext]Running Spark version 2.2.0
[INFO][2018-05-24 19:42:14,046][org.apache.spark.SparkContext]Submitted application: ReceiveKafkaData$
[INFO][2018-05-24 19:42:14,070][org.apache.spark.SecurityManager]Changing view acls to: seven
[INFO][2018-05-24 19:42:14,071][org.apache.spark.SecurityManager]Changing modify acls to: seven
[INFO][2018-05-24 19:42:14,071][org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO][2018-05-24 19:42:14,072][org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO][2018-05-24 19:42:14,073][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(seven); groups with view permissions: Set(); users  with modify permissions: Set(seven); groups with modify permissions: Set()
[INFO][2018-05-24 19:42:14,387][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 65257.
[INFO][2018-05-24 19:42:14,406][org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO][2018-05-24 19:42:14,424][org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO][2018-05-24 19:42:14,427][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2018-05-24 19:42:14,428][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
[INFO][2018-05-24 19:42:14,438][org.apache.spark.storage.DiskBlockManager]Created local directory at /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/blockmgr-e742b4a0-c5fe-429d-a750-f035b62696ca
[INFO][2018-05-24 19:42:14,454][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 912.3 MB
[INFO][2018-05-24 19:42:14,549][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO][2018-05-24 19:42:14,637][org.spark_project.jetty.util.log]Logging initialized @2455ms
[INFO][2018-05-24 19:42:14,709][org.spark_project.jetty.server.Server]jetty-9.3.z-SNAPSHOT
[INFO][2018-05-24 19:42:14,721][org.spark_project.jetty.server.Server]Started @2541ms
[INFO][2018-05-24 19:42:14,750][org.spark_project.jetty.server.AbstractConnector]Started ServerConnector@68d6972f{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-05-24 19:42:14,750][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4040.
[INFO][2018-05-24 19:42:14,781][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7fcbe147{/jobs,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:42:14,782][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@89c10b7{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:42:14,783][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4fe89c24{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:42:14,785][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@119f1f2a{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:42:14,786][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5b970f7{/stages,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:42:14,787][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@165b8a71{/stages/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:42:14,789][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2f058b8a{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:42:14,792][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@76c7beb3{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:42:14,796][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2cf92cc7{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:42:14,797][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7b139eab{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:42:14,797][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@611df6e3{/storage,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:42:14,798][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6273c5a4{/storage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:42:14,799][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@53e211ee{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:42:14,804][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3d8bbcdc{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:42:14,806][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@117e0fe5{/environment,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:42:14,807][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@78aea4b9{/environment/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:42:14,807][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4b85880b{/executors,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:42:14,808][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4215838f{/executors/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:42:14,809][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2289aca5{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:42:14,811][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@184497d1{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:42:14,822][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6ffab045{/static,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:42:14,823][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5e8f9e2d{/,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:42:14,824][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@fd46303{/api,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:42:14,827][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3e2822{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:42:14,828][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@29a60c27{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:42:14,831][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://192.168.0.102:4040
[INFO][2018-05-24 19:42:14,978][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
[INFO][2018-05-24 19:42:15,003][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 65258.
[INFO][2018-05-24 19:42:15,004][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 192.168.0.102:65258
[INFO][2018-05-24 19:42:15,006][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2018-05-24 19:42:15,008][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 192.168.0.102, 65258, None)
[INFO][2018-05-24 19:42:15,012][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 192.168.0.102:65258 with 912.3 MB RAM, BlockManagerId(driver, 192.168.0.102, 65258, None)
[INFO][2018-05-24 19:42:15,014][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 192.168.0.102, 65258, None)
[INFO][2018-05-24 19:42:15,015][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 192.168.0.102, 65258, None)
[INFO][2018-05-24 19:42:15,219][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6b530eb9{/metrics/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:42:15,432][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 19:42:15,436][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 19:42:15,436][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 19:42:27,554][org.apache.spark.SparkContext]Running Spark version 2.2.0
[INFO][2018-05-24 19:42:28,300][org.apache.spark.SparkContext]Submitted application: SimulationKafkaSendOutData$
[INFO][2018-05-24 19:42:28,323][org.apache.spark.SecurityManager]Changing view acls to: seven
[INFO][2018-05-24 19:42:28,324][org.apache.spark.SecurityManager]Changing modify acls to: seven
[INFO][2018-05-24 19:42:28,324][org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO][2018-05-24 19:42:28,325][org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO][2018-05-24 19:42:28,326][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(seven); groups with view permissions: Set(); users  with modify permissions: Set(seven); groups with modify permissions: Set()
[INFO][2018-05-24 19:42:28,615][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 65265.
[INFO][2018-05-24 19:42:28,651][org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO][2018-05-24 19:42:28,666][org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO][2018-05-24 19:42:28,670][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2018-05-24 19:42:28,670][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
[INFO][2018-05-24 19:42:28,679][org.apache.spark.storage.DiskBlockManager]Created local directory at /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/blockmgr-0093f14b-e6e6-465f-91ec-35db0a493392
[INFO][2018-05-24 19:42:28,701][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 912.3 MB
[INFO][2018-05-24 19:42:28,800][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO][2018-05-24 19:42:28,888][org.spark_project.jetty.util.log]Logging initialized @2329ms
[INFO][2018-05-24 19:42:28,935][org.spark_project.jetty.server.Server]jetty-9.3.z-SNAPSHOT
[INFO][2018-05-24 19:42:28,946][org.spark_project.jetty.server.Server]Started @2389ms
[WARN][2018-05-24 19:42:28,959][org.apache.spark.util.Utils]Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[INFO][2018-05-24 19:42:28,963][org.spark_project.jetty.server.AbstractConnector]Started ServerConnector@1643d68f{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}
[INFO][2018-05-24 19:42:28,963][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4041.
[INFO][2018-05-24 19:42:28,982][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2a39fe6a{/jobs,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:42:28,983][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@649725e3{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:42:28,983][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4c168660{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:42:28,984][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4eed46ee{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:42:28,985][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4fad94a7{/stages,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:42:28,985][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6326d182{/stages/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:42:28,985][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@716a7124{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:42:28,986][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5cc126dc{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:42:28,987][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@72bd06ca{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:42:28,987][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5dbe30be{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:42:28,988][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@55795845{/storage,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:42:28,988][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@119f1f2a{/storage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:42:28,989][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5b970f7{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:42:28,990][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@165b8a71{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:42:28,991][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2f058b8a{/environment,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:42:28,991][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3f2ef586{/environment/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:42:28,992][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@76c7beb3{/executors,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:42:28,993][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2cf92cc7{/executors/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:42:28,994][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7b139eab{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:42:28,994][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@611df6e3{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:42:29,000][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6273c5a4{/static,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:42:29,001][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3e2943ab{/,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:42:29,002][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4a9f80d3{/api,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:42:29,003][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@191ae03f{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:42:29,003][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@53ab0286{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:42:29,006][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://192.168.0.102:4041
[INFO][2018-05-24 19:42:29,090][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
[INFO][2018-05-24 19:42:29,107][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 65266.
[INFO][2018-05-24 19:42:29,108][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 192.168.0.102:65266
[INFO][2018-05-24 19:42:29,109][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2018-05-24 19:42:29,112][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 192.168.0.102, 65266, None)
[INFO][2018-05-24 19:42:29,115][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 192.168.0.102:65266 with 912.3 MB RAM, BlockManagerId(driver, 192.168.0.102, 65266, None)
[INFO][2018-05-24 19:42:29,117][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 192.168.0.102, 65266, None)
[INFO][2018-05-24 19:42:29,118][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 192.168.0.102, 65266, None)
[INFO][2018-05-24 19:42:29,386][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6c2f1700{/metrics/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:42:30,024][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0 stored as values in memory (estimated size 228.1 KB, free 912.1 MB)
[INFO][2018-05-24 19:42:30,102][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0_piece0 stored as bytes in memory (estimated size 22.1 KB, free 912.1 MB)
[INFO][2018-05-24 19:42:30,104][org.apache.spark.storage.BlockManagerInfo]Added broadcast_0_piece0 in memory on 192.168.0.102:65266 (size: 22.1 KB, free: 912.3 MB)
[INFO][2018-05-24 19:42:30,109][org.apache.spark.SparkContext]Created broadcast 0 from textFile at SimulationKafkaSendOutData.scala:25
[INFO][2018-05-24 19:42:30,948][org.apache.spark.streaming.kafka.DirectKafkaInputDStream]Slide time = 20000 ms
[INFO][2018-05-24 19:42:30,948][org.apache.spark.streaming.kafka.DirectKafkaInputDStream]Storage level = Serialized 1x Replicated
[INFO][2018-05-24 19:42:30,949][org.apache.spark.streaming.kafka.DirectKafkaInputDStream]Checkpoint interval = null
[INFO][2018-05-24 19:42:30,950][org.apache.spark.streaming.kafka.DirectKafkaInputDStream]Remember interval = 20000 ms
[INFO][2018-05-24 19:42:30,951][org.apache.spark.streaming.kafka.DirectKafkaInputDStream]Initialized and validated org.apache.spark.streaming.kafka.DirectKafkaInputDStream@2c69745f
[INFO][2018-05-24 19:42:30,951][org.apache.spark.streaming.dstream.MappedDStream]Slide time = 20000 ms
[INFO][2018-05-24 19:42:30,951][org.apache.spark.streaming.dstream.MappedDStream]Storage level = Serialized 1x Replicated
[INFO][2018-05-24 19:42:30,951][org.apache.spark.streaming.dstream.MappedDStream]Checkpoint interval = null
[INFO][2018-05-24 19:42:30,951][org.apache.spark.streaming.dstream.MappedDStream]Remember interval = 20000 ms
[INFO][2018-05-24 19:42:30,952][org.apache.spark.streaming.dstream.MappedDStream]Initialized and validated org.apache.spark.streaming.dstream.MappedDStream@4546bf84
[INFO][2018-05-24 19:42:30,952][org.apache.spark.streaming.dstream.ForEachDStream]Slide time = 20000 ms
[INFO][2018-05-24 19:42:30,952][org.apache.spark.streaming.dstream.ForEachDStream]Storage level = Serialized 1x Replicated
[INFO][2018-05-24 19:42:30,952][org.apache.spark.streaming.dstream.ForEachDStream]Checkpoint interval = null
[INFO][2018-05-24 19:42:30,952][org.apache.spark.streaming.dstream.ForEachDStream]Remember interval = 20000 ms
[INFO][2018-05-24 19:42:30,952][org.apache.spark.streaming.dstream.ForEachDStream]Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@46f18417
[INFO][2018-05-24 19:42:30,998][org.apache.spark.streaming.util.RecurringTimer]Started timer for JobGenerator at time 1527162160000
[INFO][2018-05-24 19:42:30,998][org.apache.spark.streaming.scheduler.JobGenerator]Started JobGenerator at 1527162160000 ms
[INFO][2018-05-24 19:42:30,999][org.apache.spark.streaming.scheduler.JobScheduler]Started JobScheduler
[INFO][2018-05-24 19:42:31,004][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4eb30d44{/streaming,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:42:31,005][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6972c30a{/streaming/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:42:31,007][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@351e414e{/streaming/batch,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:42:31,008][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5109e8cf{/streaming/batch/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:42:31,009][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@327c7bea{/static/streaming,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:42:31,010][org.apache.spark.streaming.StreamingContext]StreamingContext started
[WARN][2018-05-24 19:42:35,608][org.apache.hadoop.util.NativeCodeLoader]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO][2018-05-24 19:42:35,775][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 1
[INFO][2018-05-24 19:42:35,873][org.apache.spark.SparkContext]Starting job: collect at SimulationKafkaSendOutData.scala:25
[INFO][2018-05-24 19:42:35,885][org.apache.spark.scheduler.DAGScheduler]Got job 0 (collect at SimulationKafkaSendOutData.scala:25) with 2 output partitions
[INFO][2018-05-24 19:42:35,885][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 0 (collect at SimulationKafkaSendOutData.scala:25)
[INFO][2018-05-24 19:42:35,886][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 19:42:35,888][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 19:42:35,898][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 0 (hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/* MapPartitionsRDD[1] at textFile at SimulationKafkaSendOutData.scala:25), which has no missing parents
[INFO][2018-05-24 19:42:35,919][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1 stored as values in memory (estimated size 3.3 KB, free 912.1 MB)
[INFO][2018-05-24 19:42:35,921][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1_piece0 stored as bytes in memory (estimated size 1969.0 B, free 912.1 MB)
[INFO][2018-05-24 19:42:35,921][org.apache.spark.storage.BlockManagerInfo]Added broadcast_1_piece0 in memory on 192.168.0.102:65266 (size: 1969.0 B, free: 912.3 MB)
[INFO][2018-05-24 19:42:35,922][org.apache.spark.SparkContext]Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 19:42:35,937][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ResultStage 0 (hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/* MapPartitionsRDD[1] at textFile at SimulationKafkaSendOutData.scala:25) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2018-05-24 19:42:35,938][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 0.0 with 2 tasks
[INFO][2018-05-24 19:42:35,978][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 4883 bytes)
[INFO][2018-05-24 19:42:35,981][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 4883 bytes)
[INFO][2018-05-24 19:42:35,989][org.apache.spark.executor.Executor]Running task 0.0 in stage 0.0 (TID 0)
[INFO][2018-05-24 19:42:35,989][org.apache.spark.executor.Executor]Running task 1.0 in stage 0.0 (TID 1)
[INFO][2018-05-24 19:42:36,047][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/part-00000:10717435+10717436
[INFO][2018-05-24 19:42:36,047][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/part-00000:0+10717435
[INFO][2018-05-24 19:42:40,051][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 19:42:40,052][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 19:42:40,052][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 19:42:45,177][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527162160000 ms
[INFO][2018-05-24 19:42:45,181][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527162160000 ms.0 from job set of time 1527162160000 ms
[INFO][2018-05-24 19:42:45,227][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:64
[INFO][2018-05-24 19:42:45,246][org.apache.spark.scheduler.DAGScheduler]Got job 0 (foreachPartition at ReceiveKafkaData.scala:64) with 1 output partitions
[INFO][2018-05-24 19:42:45,247][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 0 (foreachPartition at ReceiveKafkaData.scala:64)
[INFO][2018-05-24 19:42:45,248][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 19:42:45,249][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 19:42:45,257][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 0 (MapPartitionsRDD[1] at map at ReceiveKafkaData.scala:62), which has no missing parents
[INFO][2018-05-24 19:42:45,650][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-24 19:42:45,696][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0_piece0 stored as bytes in memory (estimated size 2007.0 B, free 912.3 MB)
[INFO][2018-05-24 19:42:45,698][org.apache.spark.storage.BlockManagerInfo]Added broadcast_0_piece0 in memory on 192.168.0.102:65258 (size: 2007.0 B, free: 912.3 MB)
[INFO][2018-05-24 19:42:45,703][org.apache.spark.SparkContext]Created broadcast 0 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 19:42:45,727][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at map at ReceiveKafkaData.scala:62) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 19:42:45,729][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 0.0 with 1 tasks
[INFO][2018-05-24 19:42:45,781][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 19:42:45,799][org.apache.spark.executor.Executor]Running task 0.0 in stage 0.0 (TID 0)
[INFO][2018-05-24 19:42:45,846][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 11715 is the same as ending offset skipping seven 0
[INFO][2018-05-24 19:42:46,037][org.apache.spark.storage.memory.MemoryStore]Block taskresult_1 stored as bytes in memory (estimated size 10.4 MB, free 901.7 MB)
[INFO][2018-05-24 19:42:46,038][org.apache.spark.storage.BlockManagerInfo]Added taskresult_1 in memory on 192.168.0.102:65266 (size: 10.4 MB, free: 901.9 MB)
[INFO][2018-05-24 19:42:46,039][org.apache.spark.executor.Executor]Finished task 1.0 in stage 0.0 (TID 1). 10855899 bytes result sent via BlockManager)
[INFO][2018-05-24 19:42:46,087][org.apache.spark.network.client.TransportClientFactory]Successfully created connection to /192.168.0.102:65266 after 24 ms (0 ms spent in bootstraps)
[INFO][2018-05-24 19:42:46,389][org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper]Process identifier=hconnection-0x2429615c connecting to ZooKeeper ensemble=vm-xaj-bigdata-da-d01:2181,vm-xaj-bigdata-da-d02:2181,vm-xaj-bigdata-da-d03:2181
[INFO][2018-05-24 19:42:46,395][org.apache.zookeeper.ZooKeeper]Client environment:zookeeper.version=3.4.5-cdh5.7.0--1, built on 04/06/2016 14:24 GMT
[INFO][2018-05-24 19:42:46,395][org.apache.zookeeper.ZooKeeper]Client environment:host.name=192.168.0.102
[INFO][2018-05-24 19:42:46,395][org.apache.zookeeper.ZooKeeper]Client environment:java.version=1.8.0_161
[INFO][2018-05-24 19:42:46,396][org.apache.zookeeper.ZooKeeper]Client environment:java.vendor=Oracle Corporation
[INFO][2018-05-24 19:42:46,396][org.apache.zookeeper.ZooKeeper]Client environment:java.home=/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre
[INFO][2018-05-24 19:42:46,396][org.apache.zookeeper.ZooKeeper]Client environment:java.class.path=/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/charsets.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/deploy.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/cldrdata.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/dnsns.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/jaccess.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/jfxrt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/localedata.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/nashorn.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/sunec.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/sunjce_provider.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/sunpkcs11.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/zipfs.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/javaws.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/jce.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/jfr.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/jfxswt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/jsse.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/management-agent.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/plugin.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/resources.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/rt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/ant-javafx.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/dt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/javafx-mx.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/jconsole.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/packager.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/sa-jdi.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/tools.jar:/Users/seven/project/scala/dataMining/target/classes:/Users/seven/software/maven/repository/org/scala-lang/scala-library/2.11.12/scala-library-2.11.12.jar:/Users/seven/software/maven/repository/org/scala-lang/scala-compiler/2.11.12/scala-compiler-2.11.12.jar:/Users/seven/software/maven/repository/org/scala-lang/modules/scala-xml_2.11/1.0.5/scala-xml_2.11-1.0.5.jar:/Users/seven/software/maven/repository/org/scala-lang/modules/scala-parser-combinators_2.11/1.0.4/scala-parser-combinators_2.11-1.0.4.jar:/Users/seven/software/maven/repository/org/scala-lang/scala-reflect/2.11.12/scala-reflect-2.11.12.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-client/2.6.0-cdh5.7.0/hadoop-client-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-common/2.6.0-cdh5.7.0/hadoop-common-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/xmlenc/xmlenc/0.52/xmlenc-0.52.jar:/Users/seven/software/maven/repository/commons-net/commons-net/3.1/commons-net-3.1.jar:/Users/seven/software/maven/repository/commons-configuration/commons-configuration/1.6/commons-configuration-1.6.jar:/Users/seven/software/maven/repository/commons-digester/commons-digester/1.8/commons-digester-1.8.jar:/Users/seven/software/maven/repository/commons-beanutils/commons-beanutils/1.7.0/commons-beanutils-1.7.0.jar:/Users/seven/software/maven/repository/org/slf4j/slf4j-api/1.7.5/slf4j-api-1.7.5.jar:/Users/seven/software/maven/repository/org/slf4j/slf4j-log4j12/1.7.5/slf4j-log4j12-1.7.5.jar:/Users/seven/software/maven/repository/org/apache/avro/avro/1.7.6-cdh5.7.0/avro-1.7.6-cdh5.7.0.jar:/Users/seven/software/maven/repository/com/google/code/gson/gson/2.2.4/gson-2.2.4.jar:/Users/seven/software/maven/repository/org/apache/curator/curator-client/2.7.1/curator-client-2.7.1.jar:/Users/seven/software/maven/repository/org/apache/curator/curator-recipes/2.7.1/curator-recipes-2.7.1.jar:/Users/seven/software/maven/repository/com/google/code/findbugs/jsr305/3.0.0/jsr305-3.0.0.jar:/Users/seven/software/maven/repository/org/apache/htrace/htrace-core4/4.0.1-incubating/htrace-core4-4.0.1-incubating.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-compress/1.4.1/commons-compress-1.4.1.jar:/Users/seven/software/maven/repository/org/tukaani/xz/1.0/xz-1.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-hdfs/2.6.0-cdh5.7.0/hadoop-hdfs-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/io/netty/netty/3.6.2.Final/netty-3.6.2.Final.jar:/Users/seven/software/maven/repository/xerces/xercesImpl/2.9.1/xercesImpl-2.9.1.jar:/Users/seven/software/maven/repository/xml-apis/xml-apis/1.3.04/xml-apis-1.3.04.jar:/Users/seven/software/maven/repository/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-app/2.6.0-cdh5.7.0/hadoop-mapreduce-client-app-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-common/2.6.0-cdh5.7.0/hadoop-mapreduce-client-common-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-yarn-client/2.6.0-cdh5.7.0/hadoop-yarn-client-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-yarn-server-common/2.6.0-cdh5.7.0/hadoop-yarn-server-common-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-shuffle/2.6.0-cdh5.7.0/hadoop-mapreduce-client-shuffle-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-yarn-api/2.6.0-cdh5.7.0/hadoop-yarn-api-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-core/2.6.0-cdh5.7.0/hadoop-mapreduce-client-core-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-yarn-common/2.6.0-cdh5.7.0/hadoop-yarn-common-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/javax/xml/bind/jaxb-api/2.2.2/jaxb-api-2.2.2.jar:/Users/seven/software/maven/repository/javax/xml/stream/stax-api/1.0-2/stax-api-1.0-2.jar:/Users/seven/software/maven/repository/javax/activation/activation/1.1/activation-1.1.jar:/Users/seven/software/maven/repository/com/sun/jersey/jersey-client/1.9/jersey-client-1.9.jar:/Users/seven/software/maven/repository/org/codehaus/jackson/jackson-xc/1.8.8/jackson-xc-1.8.8.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.6.0-cdh5.7.0/hadoop-mapreduce-client-jobclient-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-aws/2.6.0-cdh5.7.0/hadoop-aws-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/com/amazonaws/aws-java-sdk-s3/1.10.6/aws-java-sdk-s3-1.10.6.jar:/Users/seven/software/maven/repository/com/amazonaws/aws-java-sdk-kms/1.10.6/aws-java-sdk-kms-1.10.6.jar:/Users/seven/software/maven/repository/com/amazonaws/aws-java-sdk-core/1.10.6/aws-java-sdk-core-1.10.6.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-annotations/2.6.0-cdh5.7.0/hadoop-annotations-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-client/1.2.0-cdh5.7.0/hbase-client-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-annotations/1.2.0-cdh5.7.0/hbase-annotations-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-common/1.2.0-cdh5.7.0/hbase-common-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-protocol/1.2.0-cdh5.7.0/hbase-protocol-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/commons-codec/commons-codec/1.9/commons-codec-1.9.jar:/Users/seven/software/maven/repository/commons-io/commons-io/2.4/commons-io-2.4.jar:/Users/seven/software/maven/repository/commons-lang/commons-lang/2.6/commons-lang-2.6.jar:/Users/seven/software/maven/repository/commons-logging/commons-logging/1.2/commons-logging-1.2.jar:/Users/seven/software/maven/repository/com/google/guava/guava/12.0.1/guava-12.0.1.jar:/Users/seven/software/maven/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/Users/seven/software/maven/repository/io/netty/netty-all/4.0.23.Final/netty-all-4.0.23.Final.jar:/Users/seven/software/maven/repository/org/apache/zookeeper/zookeeper/3.4.5-cdh5.7.0/zookeeper-3.4.5-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/htrace/htrace-core/3.2.0-incubating/htrace-core-3.2.0-incubating.jar:/Users/seven/software/maven/repository/org/codehaus/jackson/jackson-mapper-asl/1.8.8/jackson-mapper-asl-1.8.8.jar:/Users/seven/software/maven/repository/org/jruby/jcodings/jcodings/1.0.8/jcodings-1.0.8.jar:/Users/seven/software/maven/repository/org/jruby/joni/joni/2.1.2/joni-2.1.2.jar:/Users/seven/software/maven/repository/com/yammer/metrics/metrics-core/2.2.0/metrics-core-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-auth/2.6.0-cdh5.7.0/hadoop-auth-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/httpcomponents/httpclient/4.2.5/httpclient-4.2.5.jar:/Users/seven/software/maven/repository/org/apache/httpcomponents/httpcore/4.2.4/httpcore-4.2.4.jar:/Users/seven/software/maven/repository/org/apache/directory/server/apacheds-kerberos-codec/2.0.0-M15/apacheds-kerberos-codec-2.0.0-M15.jar:/Users/seven/software/maven/repository/org/apache/directory/server/apacheds-i18n/2.0.0-M15/apacheds-i18n-2.0.0-M15.jar:/Users/seven/software/maven/repository/org/apache/directory/api/api-asn1-api/1.0.0-M20/api-asn1-api-1.0.0-M20.jar:/Users/seven/software/maven/repository/org/apache/directory/api/api-util/1.0.0-M20/api-util-1.0.0-M20.jar:/Users/seven/software/maven/repository/org/apache/curator/curator-framework/2.7.1/curator-framework-2.7.1.jar:/Users/seven/software/maven/repository/com/github/stephenc/findbugs/findbugs-annotations/1.3.9-1/findbugs-annotations-1.3.9-1.jar:/Users/seven/software/maven/repository/junit/junit/4.12/junit-4.12.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-server/1.2.0-cdh5.7.0/hbase-server-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-procedure/1.2.0-cdh5.7.0/hbase-procedure-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-common/1.2.0-cdh5.7.0/hbase-common-1.2.0-cdh5.7.0-tests.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-prefix-tree/1.2.0-cdh5.7.0/hbase-prefix-tree-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar:/Users/seven/software/maven/repository/commons-collections/commons-collections/3.2.2/commons-collections-3.2.2.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-hadoop-compat/1.2.0-cdh5.7.0/hbase-hadoop-compat-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-hadoop2-compat/1.2.0-cdh5.7.0/hbase-hadoop2-compat-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/com/sun/jersey/jersey-core/1.9/jersey-core-1.9.jar:/Users/seven/software/maven/repository/com/sun/jersey/jersey-server/1.9/jersey-server-1.9.jar:/Users/seven/software/maven/repository/asm/asm/3.1/asm-3.1.jar:/Users/seven/software/maven/repository/commons-cli/commons-cli/1.2/commons-cli-1.2.jar:/Users/seven/software/maven/repository/com/github/stephenc/high-scale-lib/high-scale-lib/1.1.1/high-scale-lib-1.1.1.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-math/2.1/commons-math-2.1.jar:/Users/seven/software/maven/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jetty/6.1.26.cloudera.4/jetty-6.1.26.cloudera.4.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jetty-util/6.1.26.cloudera.4/jetty-util-6.1.26.cloudera.4.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jetty-sslengine/6.1.26.cloudera.4/jetty-sslengine-6.1.26.cloudera.4.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jsp-2.1/6.1.14/jsp-2.1-6.1.14.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jsp-api-2.1/6.1.14/jsp-api-2.1-6.1.14.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/servlet-api-2.5/6.1.14/servlet-api-2.5-6.1.14.jar:/Users/seven/software/maven/repository/org/codehaus/jackson/jackson-core-asl/1.8.8/jackson-core-asl-1.8.8.jar:/Users/seven/software/maven/repository/org/codehaus/jackson/jackson-jaxrs/1.8.8/jackson-jaxrs-1.8.8.jar:/Users/seven/software/maven/repository/tomcat/jasper-compiler/5.5.23/jasper-compiler-5.5.23.jar:/Users/seven/software/maven/repository/tomcat/jasper-runtime/5.5.23/jasper-runtime-5.5.23.jar:/Users/seven/software/maven/repository/commons-el/commons-el/1.0/commons-el-1.0.jar:/Users/seven/software/maven/repository/org/jamon/jamon-runtime/2.4.1/jamon-runtime-2.4.1.jar:/Users/seven/software/maven/repository/com/lmax/disruptor/3.3.0/disruptor-3.3.0.jar:/Users/seven/software/maven/repository/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar:/Users/seven/software/maven/repository/org/owasp/esapi/esapi/2.1.0/esapi-2.1.0.jar:/Users/seven/software/maven/repository/commons-beanutils/commons-beanutils-core/1.7.0/commons-beanutils-core-1.7.0.jar:/Users/seven/software/maven/repository/commons-fileupload/commons-fileupload/1.2/commons-fileupload-1.2.jar:/Users/seven/software/maven/repository/xom/xom/1.2.5/xom-1.2.5.jar:/Users/seven/software/maven/repository/xalan/xalan/2.7.0/xalan-2.7.0.jar:/Users/seven/software/maven/repository/org/beanshell/bsh-core/2.0b4/bsh-core-2.0b4.jar:/Users/seven/software/maven/repository/org/owasp/antisamy/antisamy/1.4.3/antisamy-1.4.3.jar:/Users/seven/software/maven/repository/org/apache/xmlgraphics/batik-css/1.7/batik-css-1.7.jar:/Users/seven/software/maven/repository/org/apache/xmlgraphics/batik-ext/1.7/batik-ext-1.7.jar:/Users/seven/software/maven/repository/org/apache/xmlgraphics/batik-util/1.7/batik-util-1.7.jar:/Users/seven/software/maven/repository/xml-apis/xml-apis-ext/1.3.04/xml-apis-ext-1.3.04.jar:/Users/seven/software/maven/repository/net/sourceforge/nekohtml/nekohtml/1.9.12/nekohtml-1.9.12.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-core/2.6.0-mr1-cdh5.7.0/hadoop-core-2.6.0-mr1-cdh5.7.0.jar:/Users/seven/software/maven/repository/javax/servlet/jsp/jsp-api/2.1/jsp-api-2.1.jar:/Users/seven/software/maven/repository/net/java/dev/jets3t/jets3t/0.6.1/jets3t-0.6.1.jar:/Users/seven/software/maven/repository/javax/servlet/servlet-api/2.5/servlet-api-2.5.jar:/Users/seven/software/maven/repository/hsqldb/hsqldb/1.8.0.10/hsqldb-1.8.0.10.jar:/Users/seven/software/maven/repository/org/eclipse/jdt/core/3.1.1/core-3.1.1.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-hdfs/2.6.0-cdh5.7.0/hadoop-hdfs-2.6.0-cdh5.7.0-tests.jar:/Users/seven/software/maven/repository/commons-daemon/commons-daemon/1.0.13/commons-daemon-1.0.13.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-lang3/3.5/commons-lang3-3.5.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-pool2/2.4.2/commons-pool2-2.4.2.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-mllib_2.11/2.2.0/spark-mllib_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-core_2.11/2.2.0/spark-core_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/avro/avro-mapred/1.7.7/avro-mapred-1.7.7-hadoop2.jar:/Users/seven/software/maven/repository/org/apache/avro/avro-ipc/1.7.7/avro-ipc-1.7.7-tests.jar:/Users/seven/software/maven/repository/com/twitter/chill_2.11/0.8.0/chill_2.11-0.8.0.jar:/Users/seven/software/maven/repository/com/esotericsoftware/kryo-shaded/3.0.3/kryo-shaded-3.0.3.jar:/Users/seven/software/maven/repository/com/esotericsoftware/minlog/1.3.0/minlog-1.3.0.jar:/Users/seven/software/maven/repository/org/objenesis/objenesis/2.1/objenesis-2.1.jar:/Users/seven/software/maven/repository/com/twitter/chill-java/0.8.0/chill-java-0.8.0.jar:/Users/seven/software/maven/repository/org/apache/xbean/xbean-asm5-shaded/4.4/xbean-asm5-shaded-4.4.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-launcher_2.11/2.2.0/spark-launcher_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-network-common_2.11/2.2.0/spark-network-common_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-network-shuffle_2.11/2.2.0/spark-network-shuffle_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-unsafe_2.11/2.2.0/spark-unsafe_2.11-2.2.0.jar:/Users/seven/software/maven/repository/javax/servlet/javax.servlet-api/3.1.0/javax.servlet-api-3.1.0.jar:/Users/seven/software/maven/repository/org/slf4j/jul-to-slf4j/1.7.16/jul-to-slf4j-1.7.16.jar:/Users/seven/software/maven/repository/org/slf4j/jcl-over-slf4j/1.7.16/jcl-over-slf4j-1.7.16.jar:/Users/seven/software/maven/repository/com/ning/compress-lzf/1.0.3/compress-lzf-1.0.3.jar:/Users/seven/software/maven/repository/org/xerial/snappy/snappy-java/1.1.2.6/snappy-java-1.1.2.6.jar:/Users/seven/software/maven/repository/org/roaringbitmap/RoaringBitmap/0.5.11/RoaringBitmap-0.5.11.jar:/Users/seven/software/maven/repository/org/json4s/json4s-jackson_2.11/3.2.11/json4s-jackson_2.11-3.2.11.jar:/Users/seven/software/maven/repository/org/json4s/json4s-core_2.11/3.2.11/json4s-core_2.11-3.2.11.jar:/Users/seven/software/maven/repository/org/json4s/json4s-ast_2.11/3.2.11/json4s-ast_2.11-3.2.11.jar:/Users/seven/software/maven/repository/org/scala-lang/scalap/2.11.0/scalap-2.11.0.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/core/jersey-client/2.22.2/jersey-client-2.22.2.jar:/Users/seven/software/maven/repository/javax/ws/rs/javax.ws.rs-api/2.0.1/javax.ws.rs-api-2.0.1.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/hk2-api/2.4.0-b34/hk2-api-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/hk2-utils/2.4.0-b34/hk2-utils-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/external/aopalliance-repackaged/2.4.0-b34/aopalliance-repackaged-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/external/javax.inject/2.4.0-b34/javax.inject-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/hk2-locator/2.4.0-b34/hk2-locator-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/javassist/javassist/3.18.1-GA/javassist-3.18.1-GA.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/core/jersey-common/2.22.2/jersey-common-2.22.2.jar:/Users/seven/software/maven/repository/javax/annotation/javax.annotation-api/1.2/javax.annotation-api-1.2.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/bundles/repackaged/jersey-guava/2.22.2/jersey-guava-2.22.2.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/osgi-resource-locator/1.0.1/osgi-resource-locator-1.0.1.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/core/jersey-server/2.22.2/jersey-server-2.22.2.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/media/jersey-media-jaxb/2.22.2/jersey-media-jaxb-2.22.2.jar:/Users/seven/software/maven/repository/javax/validation/validation-api/1.1.0.Final/validation-api-1.1.0.Final.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/containers/jersey-container-servlet/2.22.2/jersey-container-servlet-2.22.2.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/containers/jersey-container-servlet-core/2.22.2/jersey-container-servlet-core-2.22.2.jar:/Users/seven/software/maven/repository/com/clearspring/analytics/stream/2.7.0/stream-2.7.0.jar:/Users/seven/software/maven/repository/io/dropwizard/metrics/metrics-core/3.1.2/metrics-core-3.1.2.jar:/Users/seven/software/maven/repository/io/dropwizard/metrics/metrics-jvm/3.1.2/metrics-jvm-3.1.2.jar:/Users/seven/software/maven/repository/io/dropwizard/metrics/metrics-json/3.1.2/metrics-json-3.1.2.jar:/Users/seven/software/maven/repository/io/dropwizard/metrics/metrics-graphite/3.1.2/metrics-graphite-3.1.2.jar:/Users/seven/software/maven/repository/org/apache/ivy/ivy/2.4.0/ivy-2.4.0.jar:/Users/seven/software/maven/repository/oro/oro/2.0.8/oro-2.0.8.jar:/Users/seven/software/maven/repository/net/razorvine/pyrolite/4.13/pyrolite-4.13.jar:/Users/seven/software/maven/repository/net/sf/py4j/py4j/0.10.4/py4j-0.10.4.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-crypto/1.0.0/commons-crypto-1.0.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-streaming_2.11/2.2.0/spark-streaming_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-sql_2.11/2.2.0/spark-sql_2.11-2.2.0.jar:/Users/seven/software/maven/repository/com/univocity/univocity-parsers/2.2.1/univocity-parsers-2.2.1.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-sketch_2.11/2.2.0/spark-sketch_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-catalyst_2.11/2.2.0/spark-catalyst_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/codehaus/janino/janino/3.0.0/janino-3.0.0.jar:/Users/seven/software/maven/repository/org/codehaus/janino/commons-compiler/3.0.0/commons-compiler-3.0.0.jar:/Users/seven/software/maven/repository/org/antlr/antlr4-runtime/4.5.3/antlr4-runtime-4.5.3.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-column/1.8.2/parquet-column-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-common/1.8.2/parquet-common-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-encoding/1.8.2/parquet-encoding-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-hadoop/1.8.2/parquet-hadoop-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-format/2.3.1/parquet-format-2.3.1.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-jackson/1.8.2/parquet-jackson-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-graphx_2.11/2.2.0/spark-graphx_2.11-2.2.0.jar:/Users/seven/software/maven/repository/com/github/fommil/netlib/core/1.1.2/core-1.1.2.jar:/Users/seven/software/maven/repository/net/sourceforge/f2j/arpack_combined_all/0.1/arpack_combined_all-0.1.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-mllib-local_2.11/2.2.0/spark-mllib-local_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/scalanlp/breeze_2.11/0.13.1/breeze_2.11-0.13.1.jar:/Users/seven/software/maven/repository/org/scalanlp/breeze-macros_2.11/0.13.1/breeze-macros_2.11-0.13.1.jar:/Users/seven/software/maven/repository/net/sf/opencsv/opencsv/2.3/opencsv-2.3.jar:/Users/seven/software/maven/repository/com/github/rwl/jtransforms/2.4.0/jtransforms-2.4.0.jar:/Users/seven/software/maven/repository/org/spire-math/spire_2.11/0.13.0/spire_2.11-0.13.0.jar:/Users/seven/software/maven/repository/org/spire-math/spire-macros_2.11/0.13.0/spire-macros_2.11-0.13.0.jar:/Users/seven/software/maven/repository/org/typelevel/machinist_2.11/0.6.1/machinist_2.11-0.6.1.jar:/Users/seven/software/maven/repository/com/chuusai/shapeless_2.11/2.3.2/shapeless_2.11-2.3.2.jar:/Users/seven/software/maven/repository/org/typelevel/macro-compat_2.11/1.1.1/macro-compat_2.11-1.1.1.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-math3/3.4.1/commons-math3-3.4.1.jar:/Users/seven/software/maven/repository/org/jpmml/pmml-model/1.2.15/pmml-model-1.2.15.jar:/Users/seven/software/maven/repository/org/jpmml/pmml-schema/1.2.15/pmml-schema-1.2.15.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-tags_2.11/2.2.0/spark-tags_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-streaming-kafka-0-8_2.11/2.2.0/spark-streaming-kafka-0-8_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/kafka/kafka_2.11/0.8.2.1/kafka_2.11-0.8.2.1.jar:/Users/seven/software/maven/repository/com/101tec/zkclient/0.3/zkclient-0.3.jar:/Users/seven/software/maven/repository/org/apache/kafka/kafka-clients/0.8.2.1/kafka-clients-0.8.2.1.jar:/Users/seven/software/maven/repository/mysql/mysql-connector-java/5.1.31/mysql-connector-java-5.1.31.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/module/jackson-module-scala_2.11/2.6.5/jackson-module-scala_2.11-2.6.5.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/core/jackson-core/2.6.5/jackson-core-2.6.5.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/core/jackson-annotations/2.6.5/jackson-annotations-2.6.5.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/core/jackson-databind/2.6.5/jackson-databind-2.6.5.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/module/jackson-module-paranamer/2.6.5/jackson-module-paranamer-2.6.5.jar:/Users/seven/software/maven/repository/com/thoughtworks/paranamer/paranamer/2.6/paranamer-2.6.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-streaming-flume_2.11/2.2.0/spark-streaming-flume_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/flume/flume-ng-core/1.6.0/flume-ng-core-1.6.0.jar:/Users/seven/software/maven/repository/org/apache/flume/flume-ng-configuration/1.6.0/flume-ng-configuration-1.6.0.jar:/Users/seven/software/maven/repository/org/apache/avro/avro-ipc/1.7.4/avro-ipc-1.7.4.jar:/Users/seven/software/maven/repository/org/apache/velocity/velocity/1.7/velocity-1.7.jar:/Users/seven/software/maven/repository/joda-time/joda-time/2.1/joda-time-2.1.jar:/Users/seven/software/maven/repository/org/apache/mina/mina-core/2.0.4/mina-core-2.0.4.jar:/Users/seven/software/maven/repository/org/apache/flume/flume-ng-sdk/1.6.0/flume-ng-sdk-1.6.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-streaming-flume-sink_2.11/2.2.0/spark-streaming-flume-sink_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/flume/flume-ng-clients/flume-ng-log4jappender/1.6.0/flume-ng-log4jappender-1.6.0.jar:/Users/seven/software/maven/repository/net/jpountz/lz4/lz4/1.3.0/lz4-1.3.0.jar:/Applications/IntelliJ IDEA.app/Contents/lib/idea_rt.jar
[INFO][2018-05-24 19:42:46,397][org.apache.zookeeper.ZooKeeper]Client environment:java.library.path=/Users/seven/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:.
[INFO][2018-05-24 19:42:46,397][org.apache.zookeeper.ZooKeeper]Client environment:java.io.tmpdir=/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/
[INFO][2018-05-24 19:42:46,397][org.apache.zookeeper.ZooKeeper]Client environment:java.compiler=<NA>
[INFO][2018-05-24 19:42:46,397][org.apache.zookeeper.ZooKeeper]Client environment:os.name=Mac OS X
[INFO][2018-05-24 19:42:46,397][org.apache.zookeeper.ZooKeeper]Client environment:os.arch=x86_64
[INFO][2018-05-24 19:42:46,397][org.apache.zookeeper.ZooKeeper]Client environment:os.version=10.13.4
[INFO][2018-05-24 19:42:46,397][org.apache.zookeeper.ZooKeeper]Client environment:user.name=seven
[INFO][2018-05-24 19:42:46,397][org.apache.zookeeper.ZooKeeper]Client environment:user.home=/Users/seven
[INFO][2018-05-24 19:42:46,397][org.apache.zookeeper.ZooKeeper]Client environment:user.dir=/Users/seven/project/scala/dataMining
[INFO][2018-05-24 19:42:46,398][org.apache.zookeeper.ZooKeeper]Initiating client connection, connectString=vm-xaj-bigdata-da-d01:2181,vm-xaj-bigdata-da-d02:2181,vm-xaj-bigdata-da-d03:2181 sessionTimeout=60000 watcher=hconnection-0x2429615c0x0, quorum=vm-xaj-bigdata-da-d01:2181,vm-xaj-bigdata-da-d02:2181,vm-xaj-bigdata-da-d03:2181, baseZNode=/hbase
[INFO][2018-05-24 19:42:46,435][org.apache.spark.storage.memory.MemoryStore]Block taskresult_0 stored as bytes in memory (estimated size 10.4 MB, free 891.3 MB)
[INFO][2018-05-24 19:42:46,435][org.apache.spark.storage.BlockManagerInfo]Added taskresult_0 in memory on 192.168.0.102:65266 (size: 10.4 MB, free: 891.6 MB)
[INFO][2018-05-24 19:42:46,436][org.apache.spark.executor.Executor]Finished task 0.0 in stage 0.0 (TID 0). 10856349 bytes result sent via BlockManager)
[INFO][2018-05-24 19:42:46,479][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 0.0 (TID 1) in 10496 ms on localhost (executor driver) (1/2)
[INFO][2018-05-24 19:42:46,480][org.apache.spark.storage.BlockManagerInfo]Removed taskresult_1 on 192.168.0.102:65266 in memory (size: 10.4 MB, free: 901.9 MB)
[INFO][2018-05-24 19:42:46,513][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 0.0 (TID 0) in 10547 ms on localhost (executor driver) (2/2)
[INFO][2018-05-24 19:42:46,514][org.apache.spark.storage.BlockManagerInfo]Removed taskresult_0 on 192.168.0.102:65266 in memory (size: 10.4 MB, free: 912.3 MB)
[INFO][2018-05-24 19:42:46,516][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 19:42:46,516][org.apache.spark.scheduler.DAGScheduler]ResultStage 0 (collect at SimulationKafkaSendOutData.scala:25) finished in 10.561 s
[INFO][2018-05-24 19:42:46,522][org.apache.spark.scheduler.DAGScheduler]Job 0 finished: collect at SimulationKafkaSendOutData.scala:25, took 10.648839 s
[INFO][2018-05-24 19:42:46,583][org.spark_project.jetty.server.AbstractConnector]Stopped Spark@1643d68f{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}
[INFO][2018-05-24 19:42:46,585][org.apache.spark.ui.SparkUI]Stopped Spark web UI at http://192.168.0.102:4041
[INFO][2018-05-24 19:42:46,595][org.apache.spark.MapOutputTrackerMasterEndpoint]MapOutputTrackerMasterEndpoint stopped!
[INFO][2018-05-24 19:42:46,613][org.apache.spark.storage.memory.MemoryStore]MemoryStore cleared
[INFO][2018-05-24 19:42:46,613][org.apache.spark.storage.BlockManager]BlockManager stopped
[INFO][2018-05-24 19:42:46,614][org.apache.spark.storage.BlockManagerMaster]BlockManagerMaster stopped
[INFO][2018-05-24 19:42:46,616][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint]OutputCommitCoordinator stopped!
[INFO][2018-05-24 19:42:46,618][org.apache.spark.SparkContext]Successfully stopped SparkContext
[INFO][2018-05-24 19:42:46,645][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	compression.type = none
	metric.reporters = []
	metadata.max.age.ms = 300000
	metadata.fetch.timeout.ms = 60000
	acks = all
	batch.size = 16384
	reconnect.backoff.ms = 10
	bootstrap.servers = [vm-xaj-bigdata-da-d01:9092, vm-xaj-bigdata-da-d02:9092, vm-xaj-bigdata-da-d03:9092, vm-xaj-bigdata-da-d04:9092, vm-xaj-bigdata-da-d05:9092, vm-xaj-bigdata-da-d06:9092, vm-xaj-bigdata-da-d07:9092]
	receive.buffer.bytes = 32768
	retry.backoff.ms = 100
	buffer.memory = 33554432
	timeout.ms = 30000
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	retries = 0
	max.request.size = 1048576
	block.on.buffer.full = true
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
	metrics.sample.window.ms = 30000
	send.buffer.bytes = 131072
	max.in.flight.requests.per.connection = 5
	metrics.num.samples = 2
	linger.ms = 1
	client.id = 

[INFO][2018-05-24 19:42:56,428][org.apache.zookeeper.ClientCnxn]Opening socket connection to server vm-xaj-bigdata-da-d03/10.213.4.27:2181. Will not attempt to authenticate using SASL (unknown error)
[INFO][2018-05-24 19:42:56,438][org.apache.zookeeper.ClientCnxn]Socket connection established, initiating session, client: /10.0.8.39:65283, server: vm-xaj-bigdata-da-d03/10.213.4.27:2181
[INFO][2018-05-24 19:42:56,459][org.apache.zookeeper.ClientCnxn]Session establishment complete on server vm-xaj-bigdata-da-d03/10.213.4.27:2181, sessionid = 0x362b50d75ec5efc, negotiated timeout = 60000
[WARN][2018-05-24 19:42:56,938][org.apache.hadoop.util.NativeCodeLoader]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO][2018-05-24 19:42:57,021][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 19:42:57,033][org.apache.spark.executor.Executor]Finished task 0.0 in stage 0.0 (TID 0). 708 bytes result sent to driver
[INFO][2018-05-24 19:42:57,040][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 0.0 (TID 0) in 11274 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 19:42:57,042][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 19:42:57,044][org.apache.spark.scheduler.DAGScheduler]ResultStage 0 (foreachPartition at ReceiveKafkaData.scala:64) finished in 11.290 s
[INFO][2018-05-24 19:42:57,051][org.apache.spark.scheduler.DAGScheduler]Job 0 finished: foreachPartition at ReceiveKafkaData.scala:64, took 11.823370 s
[INFO][2018-05-24 19:42:57,056][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527162160000 ms.0 from job set of time 1527162160000 ms
[INFO][2018-05-24 19:42:57,057][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 17.055 s for time 1527162160000 ms (execution: 11.876 s)
[INFO][2018-05-24 19:42:57,065][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 19:42:57,071][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 
[INFO][2018-05-24 19:43:00,059][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527162180000 ms
[INFO][2018-05-24 19:43:00,060][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527162180000 ms.0 from job set of time 1527162180000 ms
[INFO][2018-05-24 19:43:00,073][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:64
[INFO][2018-05-24 19:43:00,074][org.apache.spark.scheduler.DAGScheduler]Got job 1 (foreachPartition at ReceiveKafkaData.scala:64) with 1 output partitions
[INFO][2018-05-24 19:43:00,074][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 1 (foreachPartition at ReceiveKafkaData.scala:64)
[INFO][2018-05-24 19:43:00,074][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 19:43:00,074][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 19:43:00,074][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 1 (MapPartitionsRDD[3] at map at ReceiveKafkaData.scala:62), which has no missing parents
[INFO][2018-05-24 19:43:00,076][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-24 19:43:00,080][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1_piece0 stored as bytes in memory (estimated size 2008.0 B, free 912.3 MB)
[INFO][2018-05-24 19:43:00,083][org.apache.spark.storage.BlockManagerInfo]Added broadcast_1_piece0 in memory on 192.168.0.102:65258 (size: 2008.0 B, free: 912.3 MB)
[INFO][2018-05-24 19:43:00,085][org.apache.spark.SparkContext]Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 19:43:00,086][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at map at ReceiveKafkaData.scala:62) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 19:43:00,086][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 1.0 with 1 tasks
[INFO][2018-05-24 19:43:00,088][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 19:43:00,088][org.apache.spark.executor.Executor]Running task 0.0 in stage 1.0 (TID 1)
[INFO][2018-05-24 19:43:00,117][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 11715 is the same as ending offset skipping seven 0
[INFO][2018-05-24 19:43:00,117][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 19:43:00,119][org.apache.spark.executor.Executor]Finished task 0.0 in stage 1.0 (TID 1). 708 bytes result sent to driver
[INFO][2018-05-24 19:43:00,120][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 1.0 (TID 1) in 33 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 19:43:00,120][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 19:43:00,121][org.apache.spark.scheduler.DAGScheduler]ResultStage 1 (foreachPartition at ReceiveKafkaData.scala:64) finished in 0.034 s
[INFO][2018-05-24 19:43:00,123][org.apache.spark.scheduler.DAGScheduler]Job 1 finished: foreachPartition at ReceiveKafkaData.scala:64, took 0.049593 s
[INFO][2018-05-24 19:43:00,124][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527162180000 ms.0 from job set of time 1527162180000 ms
[INFO][2018-05-24 19:43:00,125][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.124 s for time 1527162180000 ms (execution: 0.065 s)
[INFO][2018-05-24 19:43:00,126][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 1 from persistence list
[INFO][2018-05-24 19:43:00,133][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 0 from persistence list
[INFO][2018-05-24 19:43:00,134][org.apache.spark.storage.BlockManager]Removing RDD 1
[INFO][2018-05-24 19:43:00,136][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 19:43:00,137][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 
[INFO][2018-05-24 19:43:00,137][org.apache.spark.storage.BlockManager]Removing RDD 0
[INFO][2018-05-24 19:43:30,065][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527162200000 ms
[INFO][2018-05-24 19:43:30,066][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527162200000 ms.0 from job set of time 1527162200000 ms
[INFO][2018-05-24 19:43:30,077][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:64
[INFO][2018-05-24 19:43:30,078][org.apache.spark.scheduler.DAGScheduler]Got job 2 (foreachPartition at ReceiveKafkaData.scala:64) with 1 output partitions
[INFO][2018-05-24 19:43:30,078][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 2 (foreachPartition at ReceiveKafkaData.scala:64)
[INFO][2018-05-24 19:43:30,078][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 19:43:30,078][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 19:43:30,079][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 2 (MapPartitionsRDD[5] at map at ReceiveKafkaData.scala:62), which has no missing parents
[INFO][2018-05-24 19:43:30,082][org.apache.spark.storage.memory.MemoryStore]Block broadcast_2 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-24 19:43:30,085][org.apache.spark.storage.memory.MemoryStore]Block broadcast_2_piece0 stored as bytes in memory (estimated size 2008.0 B, free 912.3 MB)
[INFO][2018-05-24 19:43:30,086][org.apache.spark.storage.BlockManagerInfo]Added broadcast_2_piece0 in memory on 192.168.0.102:65258 (size: 2008.0 B, free: 912.3 MB)
[INFO][2018-05-24 19:43:30,087][org.apache.spark.SparkContext]Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 19:43:30,087][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[5] at map at ReceiveKafkaData.scala:62) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 19:43:30,088][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 2.0 with 1 tasks
[INFO][2018-05-24 19:43:30,088][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 19:43:30,089][org.apache.spark.executor.Executor]Running task 0.0 in stage 2.0 (TID 2)
[INFO][2018-05-24 19:43:30,097][org.apache.spark.streaming.kafka.KafkaRDD]Computing topic seven, partition 0 offsets 11715 -> 11724
[INFO][2018-05-24 19:43:30,098][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 19:43:30,098][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 19:43:30,098][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 19:43:30,394][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_1_piece0 on 192.168.0.102:65258 in memory (size: 2008.0 B, free: 912.3 MB)
[INFO][2018-05-24 19:43:30,398][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_0_piece0 on 192.168.0.102:65258 in memory (size: 2007.0 B, free: 912.3 MB)
[INFO][2018-05-24 19:43:35,626][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 19:43:35,627][org.apache.spark.executor.Executor]Finished task 0.0 in stage 2.0 (TID 2). 794 bytes result sent to driver
[INFO][2018-05-24 19:43:35,628][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 2.0 (TID 2) in 5540 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 19:43:35,628][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 19:43:35,629][org.apache.spark.scheduler.DAGScheduler]ResultStage 2 (foreachPartition at ReceiveKafkaData.scala:64) finished in 5.541 s
[INFO][2018-05-24 19:43:35,629][org.apache.spark.scheduler.DAGScheduler]Job 2 finished: foreachPartition at ReceiveKafkaData.scala:64, took 5.552288 s
[INFO][2018-05-24 19:43:35,630][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527162200000 ms.0 from job set of time 1527162200000 ms
[INFO][2018-05-24 19:43:35,630][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 3 from persistence list
[INFO][2018-05-24 19:43:35,630][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 15.630 s for time 1527162200000 ms (execution: 5.564 s)
[INFO][2018-05-24 19:43:35,631][org.apache.spark.storage.BlockManager]Removing RDD 3
[INFO][2018-05-24 19:43:35,631][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 2 from persistence list
[INFO][2018-05-24 19:43:35,631][org.apache.spark.storage.BlockManager]Removing RDD 2
[INFO][2018-05-24 19:43:35,632][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 19:43:35,632][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527162160000 ms
[INFO][2018-05-24 19:43:45,082][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527162220000 ms
[INFO][2018-05-24 19:43:45,082][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527162220000 ms.0 from job set of time 1527162220000 ms
[INFO][2018-05-24 19:43:45,090][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:64
[INFO][2018-05-24 19:43:45,090][org.apache.spark.scheduler.DAGScheduler]Got job 3 (foreachPartition at ReceiveKafkaData.scala:64) with 1 output partitions
[INFO][2018-05-24 19:43:45,091][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 3 (foreachPartition at ReceiveKafkaData.scala:64)
[INFO][2018-05-24 19:43:45,091][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 19:43:45,092][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 19:43:45,093][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 3 (MapPartitionsRDD[7] at map at ReceiveKafkaData.scala:62), which has no missing parents
[INFO][2018-05-24 19:43:45,096][org.apache.spark.storage.memory.MemoryStore]Block broadcast_3 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-24 19:43:45,098][org.apache.spark.storage.memory.MemoryStore]Block broadcast_3_piece0 stored as bytes in memory (estimated size 2008.0 B, free 912.3 MB)
[INFO][2018-05-24 19:43:45,098][org.apache.spark.storage.BlockManagerInfo]Added broadcast_3_piece0 in memory on 192.168.0.102:65258 (size: 2008.0 B, free: 912.3 MB)
[INFO][2018-05-24 19:43:45,099][org.apache.spark.SparkContext]Created broadcast 3 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 19:43:45,100][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[7] at map at ReceiveKafkaData.scala:62) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 19:43:45,100][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 3.0 with 1 tasks
[INFO][2018-05-24 19:43:45,101][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 3.0 (TID 3, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 19:43:45,101][org.apache.spark.executor.Executor]Running task 0.0 in stage 3.0 (TID 3)
[INFO][2018-05-24 19:43:45,104][org.apache.spark.streaming.kafka.KafkaRDD]Computing topic seven, partition 0 offsets 11724 -> 11738
[INFO][2018-05-24 19:43:45,104][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 19:43:45,104][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 19:43:45,104][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 19:43:45,205][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 19:43:45,206][org.apache.spark.executor.Executor]Finished task 0.0 in stage 3.0 (TID 3). 708 bytes result sent to driver
[INFO][2018-05-24 19:43:45,208][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 3.0 (TID 3) in 106 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 19:43:45,208][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 3.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 19:43:45,208][org.apache.spark.scheduler.DAGScheduler]ResultStage 3 (foreachPartition at ReceiveKafkaData.scala:64) finished in 0.108 s
[INFO][2018-05-24 19:43:45,209][org.apache.spark.scheduler.DAGScheduler]Job 3 finished: foreachPartition at ReceiveKafkaData.scala:64, took 0.119045 s
[INFO][2018-05-24 19:43:45,209][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527162220000 ms.0 from job set of time 1527162220000 ms
[INFO][2018-05-24 19:43:45,210][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 5 from persistence list
[INFO][2018-05-24 19:43:45,210][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 5.209 s for time 1527162220000 ms (execution: 0.127 s)
[INFO][2018-05-24 19:43:45,210][org.apache.spark.storage.BlockManager]Removing RDD 5
[INFO][2018-05-24 19:43:45,210][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 4 from persistence list
[INFO][2018-05-24 19:43:45,210][org.apache.spark.storage.BlockManager]Removing RDD 4
[INFO][2018-05-24 19:43:45,211][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 19:43:45,211][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527162180000 ms
[INFO][2018-05-24 19:44:05,097][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527162240000 ms
[INFO][2018-05-24 19:44:05,097][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527162240000 ms.0 from job set of time 1527162240000 ms
[INFO][2018-05-24 19:44:05,107][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:64
[INFO][2018-05-24 19:44:05,108][org.apache.spark.scheduler.DAGScheduler]Got job 4 (foreachPartition at ReceiveKafkaData.scala:64) with 1 output partitions
[INFO][2018-05-24 19:44:05,109][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 4 (foreachPartition at ReceiveKafkaData.scala:64)
[INFO][2018-05-24 19:44:05,109][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 19:44:05,109][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 19:44:05,109][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 4 (MapPartitionsRDD[9] at map at ReceiveKafkaData.scala:62), which has no missing parents
[INFO][2018-05-24 19:44:05,112][org.apache.spark.storage.memory.MemoryStore]Block broadcast_4 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-24 19:44:05,114][org.apache.spark.storage.memory.MemoryStore]Block broadcast_4_piece0 stored as bytes in memory (estimated size 2008.0 B, free 912.3 MB)
[INFO][2018-05-24 19:44:05,115][org.apache.spark.storage.BlockManagerInfo]Added broadcast_4_piece0 in memory on 192.168.0.102:65258 (size: 2008.0 B, free: 912.3 MB)
[INFO][2018-05-24 19:44:05,116][org.apache.spark.SparkContext]Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 19:44:05,116][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[9] at map at ReceiveKafkaData.scala:62) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 19:44:05,117][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 4.0 with 1 tasks
[INFO][2018-05-24 19:44:05,118][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 4.0 (TID 4, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 19:44:05,118][org.apache.spark.executor.Executor]Running task 0.0 in stage 4.0 (TID 4)
[INFO][2018-05-24 19:44:05,121][org.apache.spark.streaming.kafka.KafkaRDD]Computing topic seven, partition 0 offsets 11738 -> 11758
[INFO][2018-05-24 19:44:05,122][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 19:44:05,122][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 19:44:05,122][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 19:44:05,215][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 19:44:05,216][org.apache.spark.executor.Executor]Finished task 0.0 in stage 4.0 (TID 4). 708 bytes result sent to driver
[INFO][2018-05-24 19:44:05,218][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 4.0 (TID 4) in 101 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 19:44:05,218][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 4.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 19:44:05,219][org.apache.spark.scheduler.DAGScheduler]ResultStage 4 (foreachPartition at ReceiveKafkaData.scala:64) finished in 0.102 s
[INFO][2018-05-24 19:44:05,219][org.apache.spark.scheduler.DAGScheduler]Job 4 finished: foreachPartition at ReceiveKafkaData.scala:64, took 0.112135 s
[INFO][2018-05-24 19:44:05,220][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527162240000 ms.0 from job set of time 1527162240000 ms
[INFO][2018-05-24 19:44:05,220][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 7 from persistence list
[INFO][2018-05-24 19:44:05,220][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 5.220 s for time 1527162240000 ms (execution: 0.123 s)
[INFO][2018-05-24 19:44:05,221][org.apache.spark.storage.BlockManager]Removing RDD 7
[INFO][2018-05-24 19:44:05,221][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 6 from persistence list
[INFO][2018-05-24 19:44:05,221][org.apache.spark.storage.BlockManager]Removing RDD 6
[INFO][2018-05-24 19:44:05,221][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 19:44:05,221][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527162200000 ms
[INFO][2018-05-24 19:44:25,065][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527162260000 ms
[INFO][2018-05-24 19:44:25,065][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527162260000 ms.0 from job set of time 1527162260000 ms
[INFO][2018-05-24 19:44:25,073][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:64
[INFO][2018-05-24 19:44:25,074][org.apache.spark.scheduler.DAGScheduler]Got job 5 (foreachPartition at ReceiveKafkaData.scala:64) with 1 output partitions
[INFO][2018-05-24 19:44:25,074][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 5 (foreachPartition at ReceiveKafkaData.scala:64)
[INFO][2018-05-24 19:44:25,074][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 19:44:25,074][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 19:44:25,074][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 5 (MapPartitionsRDD[11] at map at ReceiveKafkaData.scala:62), which has no missing parents
[INFO][2018-05-24 19:44:25,076][org.apache.spark.storage.memory.MemoryStore]Block broadcast_5 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-24 19:44:25,077][org.apache.spark.storage.memory.MemoryStore]Block broadcast_5_piece0 stored as bytes in memory (estimated size 2008.0 B, free 912.3 MB)
[INFO][2018-05-24 19:44:25,078][org.apache.spark.storage.BlockManagerInfo]Added broadcast_5_piece0 in memory on 192.168.0.102:65258 (size: 2008.0 B, free: 912.3 MB)
[INFO][2018-05-24 19:44:25,079][org.apache.spark.SparkContext]Created broadcast 5 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 19:44:25,080][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[11] at map at ReceiveKafkaData.scala:62) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 19:44:25,080][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 5.0 with 1 tasks
[INFO][2018-05-24 19:44:25,081][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 5.0 (TID 5, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 19:44:25,081][org.apache.spark.executor.Executor]Running task 0.0 in stage 5.0 (TID 5)
[INFO][2018-05-24 19:44:25,083][org.apache.spark.streaming.kafka.KafkaRDD]Computing topic seven, partition 0 offsets 11758 -> 11778
[INFO][2018-05-24 19:44:25,083][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 19:44:25,083][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 19:44:25,083][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 19:44:25,163][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 19:44:25,164][org.apache.spark.executor.Executor]Finished task 0.0 in stage 5.0 (TID 5). 708 bytes result sent to driver
[INFO][2018-05-24 19:44:25,165][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 5.0 (TID 5) in 85 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 19:44:25,165][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 5.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 19:44:25,166][org.apache.spark.scheduler.DAGScheduler]ResultStage 5 (foreachPartition at ReceiveKafkaData.scala:64) finished in 0.085 s
[INFO][2018-05-24 19:44:25,166][org.apache.spark.scheduler.DAGScheduler]Job 5 finished: foreachPartition at ReceiveKafkaData.scala:64, took 0.092669 s
[INFO][2018-05-24 19:44:25,166][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527162260000 ms.0 from job set of time 1527162260000 ms
[INFO][2018-05-24 19:44:25,167][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 5.166 s for time 1527162260000 ms (execution: 0.101 s)
[INFO][2018-05-24 19:44:25,167][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 9 from persistence list
[INFO][2018-05-24 19:44:25,167][org.apache.spark.storage.BlockManager]Removing RDD 9
[INFO][2018-05-24 19:44:25,167][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 8 from persistence list
[INFO][2018-05-24 19:44:25,168][org.apache.spark.storage.BlockManager]Removing RDD 8
[INFO][2018-05-24 19:44:25,168][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 19:44:25,168][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527162220000 ms
[INFO][2018-05-24 19:44:50,080][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527162280000 ms
[INFO][2018-05-24 19:44:50,080][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527162280000 ms.0 from job set of time 1527162280000 ms
[INFO][2018-05-24 19:44:50,088][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:64
[INFO][2018-05-24 19:44:50,089][org.apache.spark.scheduler.DAGScheduler]Got job 6 (foreachPartition at ReceiveKafkaData.scala:64) with 1 output partitions
[INFO][2018-05-24 19:44:50,090][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 6 (foreachPartition at ReceiveKafkaData.scala:64)
[INFO][2018-05-24 19:44:50,090][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 19:44:50,090][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 19:44:50,090][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 6 (MapPartitionsRDD[13] at map at ReceiveKafkaData.scala:62), which has no missing parents
[INFO][2018-05-24 19:44:50,093][org.apache.spark.storage.memory.MemoryStore]Block broadcast_6 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-24 19:44:50,095][org.apache.spark.storage.memory.MemoryStore]Block broadcast_6_piece0 stored as bytes in memory (estimated size 2008.0 B, free 912.3 MB)
[INFO][2018-05-24 19:44:50,096][org.apache.spark.storage.BlockManagerInfo]Added broadcast_6_piece0 in memory on 192.168.0.102:65258 (size: 2008.0 B, free: 912.3 MB)
[INFO][2018-05-24 19:44:50,096][org.apache.spark.SparkContext]Created broadcast 6 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 19:44:50,097][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[13] at map at ReceiveKafkaData.scala:62) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 19:44:50,097][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 6.0 with 1 tasks
[INFO][2018-05-24 19:44:50,098][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 6.0 (TID 6, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 19:44:50,098][org.apache.spark.executor.Executor]Running task 0.0 in stage 6.0 (TID 6)
[INFO][2018-05-24 19:44:50,101][org.apache.spark.streaming.kafka.KafkaRDD]Computing topic seven, partition 0 offsets 11778 -> 11803
[INFO][2018-05-24 19:44:50,101][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 19:44:50,101][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 19:44:50,101][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 19:44:50,196][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 19:44:50,197][org.apache.spark.executor.Executor]Finished task 0.0 in stage 6.0 (TID 6). 708 bytes result sent to driver
[INFO][2018-05-24 19:44:50,198][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 6.0 (TID 6) in 101 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 19:44:50,199][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 6.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 19:44:50,199][org.apache.spark.scheduler.DAGScheduler]ResultStage 6 (foreachPartition at ReceiveKafkaData.scala:64) finished in 0.102 s
[INFO][2018-05-24 19:44:50,200][org.apache.spark.scheduler.DAGScheduler]Job 6 finished: foreachPartition at ReceiveKafkaData.scala:64, took 0.110921 s
[INFO][2018-05-24 19:44:50,200][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527162280000 ms.0 from job set of time 1527162280000 ms
[INFO][2018-05-24 19:44:50,201][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 11 from persistence list
[INFO][2018-05-24 19:44:50,201][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 10.200 s for time 1527162280000 ms (execution: 0.120 s)
[INFO][2018-05-24 19:44:50,201][org.apache.spark.storage.BlockManager]Removing RDD 11
[INFO][2018-05-24 19:44:50,201][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 10 from persistence list
[INFO][2018-05-24 19:44:50,201][org.apache.spark.storage.BlockManager]Removing RDD 10
[INFO][2018-05-24 19:44:50,202][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 19:44:50,202][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527162240000 ms
[INFO][2018-05-24 19:44:50,809][org.apache.spark.util.ShutdownHookManager]Shutdown hook called
[INFO][2018-05-24 19:44:50,824][org.apache.spark.util.ShutdownHookManager]Deleting directory /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/spark-99af3a70-1ecb-4ade-808f-51565fd0a471
[INFO][2018-05-24 19:44:50,837][org.apache.spark.streaming.StreamingContext]Invoking stop(stopGracefully=false) from shutdown hook
[INFO][2018-05-24 19:44:50,841][org.apache.spark.streaming.scheduler.ReceiverTracker]ReceiverTracker stopped
[INFO][2018-05-24 19:44:50,842][org.apache.spark.streaming.scheduler.JobGenerator]Stopping JobGenerator immediately
[INFO][2018-05-24 19:44:50,844][org.apache.spark.streaming.util.RecurringTimer]Stopped timer for JobGenerator after time 1527162280000
[INFO][2018-05-24 19:44:50,847][org.apache.spark.streaming.scheduler.JobGenerator]Stopped JobGenerator
[INFO][2018-05-24 19:44:50,851][org.apache.spark.streaming.scheduler.JobScheduler]Stopped JobScheduler
[INFO][2018-05-24 19:44:50,857][org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@4eb30d44{/streaming,null,UNAVAILABLE,@Spark}
[INFO][2018-05-24 19:44:50,858][org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@351e414e{/streaming/batch,null,UNAVAILABLE,@Spark}
[INFO][2018-05-24 19:44:50,861][org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@327c7bea{/static/streaming,null,UNAVAILABLE,@Spark}
[INFO][2018-05-24 19:44:50,863][org.apache.spark.streaming.StreamingContext]StreamingContext stopped successfully
[INFO][2018-05-24 19:44:50,864][org.apache.spark.SparkContext]Invoking stop() from shutdown hook
[INFO][2018-05-24 19:44:50,874][org.spark_project.jetty.server.AbstractConnector]Stopped Spark@68d6972f{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-05-24 19:44:50,875][org.apache.spark.ui.SparkUI]Stopped Spark web UI at http://192.168.0.102:4040
[INFO][2018-05-24 19:44:50,883][org.apache.spark.MapOutputTrackerMasterEndpoint]MapOutputTrackerMasterEndpoint stopped!
[INFO][2018-05-24 19:44:50,901][org.apache.spark.storage.memory.MemoryStore]MemoryStore cleared
[INFO][2018-05-24 19:44:50,901][org.apache.spark.storage.BlockManager]BlockManager stopped
[INFO][2018-05-24 19:44:50,902][org.apache.spark.storage.BlockManagerMaster]BlockManagerMaster stopped
[INFO][2018-05-24 19:44:50,904][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint]OutputCommitCoordinator stopped!
[INFO][2018-05-24 19:44:50,905][org.apache.spark.SparkContext]Successfully stopped SparkContext
[INFO][2018-05-24 19:44:50,906][org.apache.spark.util.ShutdownHookManager]Shutdown hook called
[INFO][2018-05-24 19:44:50,907][org.apache.spark.util.ShutdownHookManager]Deleting directory /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/spark-d62e3a13-4a24-4f08-a643-44b98a9d5ee1
[INFO][2018-05-24 19:46:27,765][org.apache.spark.SparkContext]Running Spark version 2.2.0
[INFO][2018-05-24 19:46:28,443][org.apache.spark.SparkContext]Submitted application: ReceiveKafkaData$
[INFO][2018-05-24 19:46:28,461][org.apache.spark.SecurityManager]Changing view acls to: seven
[INFO][2018-05-24 19:46:28,462][org.apache.spark.SecurityManager]Changing modify acls to: seven
[INFO][2018-05-24 19:46:28,463][org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO][2018-05-24 19:46:28,463][org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO][2018-05-24 19:46:28,464][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(seven); groups with view permissions: Set(); users  with modify permissions: Set(seven); groups with modify permissions: Set()
[INFO][2018-05-24 19:46:28,734][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 65342.
[INFO][2018-05-24 19:46:28,756][org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO][2018-05-24 19:46:28,772][org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO][2018-05-24 19:46:28,775][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2018-05-24 19:46:28,775][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
[INFO][2018-05-24 19:46:28,785][org.apache.spark.storage.DiskBlockManager]Created local directory at /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/blockmgr-11348ce1-31b7-48fd-a838-232d0892714b
[INFO][2018-05-24 19:46:28,799][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 912.3 MB
[INFO][2018-05-24 19:46:28,877][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO][2018-05-24 19:46:28,944][org.spark_project.jetty.util.log]Logging initialized @2099ms
[INFO][2018-05-24 19:46:29,009][org.spark_project.jetty.server.Server]jetty-9.3.z-SNAPSHOT
[INFO][2018-05-24 19:46:29,022][org.spark_project.jetty.server.Server]Started @2178ms
[INFO][2018-05-24 19:46:29,041][org.spark_project.jetty.server.AbstractConnector]Started ServerConnector@2090b9b9{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-05-24 19:46:29,042][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4040.
[INFO][2018-05-24 19:46:29,067][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@757d6814{/jobs,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:46:29,068][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@119f1f2a{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:46:29,068][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5b970f7{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:46:29,069][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6175619b{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:46:29,071][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@756cf158{/stages,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:46:29,072][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@751d3241{/stages/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:46:29,073][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@64337702{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:46:29,074][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4e76dac{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:46:29,075][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5f2f577{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:46:29,076][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5d465e4b{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:46:29,076][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@41a90fa8{/storage,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:46:29,077][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@52500920{/storage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:46:29,078][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@18a3962d{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:46:29,078][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2a65bb85{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:46:29,079][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4f936da8{/environment,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:46:29,081][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@452ba1db{/environment/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:46:29,083][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@76a36b71{/executors,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:46:29,084][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@f9d87b{/executors/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:46:29,084][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@26fb628{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:46:29,085][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@70dd7e15{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:46:29,100][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@35beb15e{/static,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:46:29,100][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6a62689d{/,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:46:29,104][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@60fa3495{/api,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:46:29,105][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1ca25c47{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:46:29,105][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@533b266e{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:46:29,107][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://192.168.0.102:4040
[INFO][2018-05-24 19:46:29,217][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
[INFO][2018-05-24 19:46:29,239][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 65343.
[INFO][2018-05-24 19:46:29,244][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 192.168.0.102:65343
[INFO][2018-05-24 19:46:29,246][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2018-05-24 19:46:29,248][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 192.168.0.102, 65343, None)
[INFO][2018-05-24 19:46:29,257][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 192.168.0.102:65343 with 912.3 MB RAM, BlockManagerId(driver, 192.168.0.102, 65343, None)
[INFO][2018-05-24 19:46:29,263][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 192.168.0.102, 65343, None)
[INFO][2018-05-24 19:46:29,265][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 192.168.0.102, 65343, None)
[INFO][2018-05-24 19:46:29,464][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7d2a6eac{/metrics/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:46:29,611][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 19:46:29,614][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 19:46:29,614][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 19:46:40,019][org.apache.spark.streaming.kafka.DirectKafkaInputDStream]Slide time = 20000 ms
[INFO][2018-05-24 19:46:40,020][org.apache.spark.streaming.kafka.DirectKafkaInputDStream]Storage level = Serialized 1x Replicated
[INFO][2018-05-24 19:46:40,020][org.apache.spark.streaming.kafka.DirectKafkaInputDStream]Checkpoint interval = null
[INFO][2018-05-24 19:46:40,021][org.apache.spark.streaming.kafka.DirectKafkaInputDStream]Remember interval = 20000 ms
[INFO][2018-05-24 19:46:40,022][org.apache.spark.streaming.kafka.DirectKafkaInputDStream]Initialized and validated org.apache.spark.streaming.kafka.DirectKafkaInputDStream@72c23843
[INFO][2018-05-24 19:46:40,022][org.apache.spark.streaming.dstream.MappedDStream]Slide time = 20000 ms
[INFO][2018-05-24 19:46:40,023][org.apache.spark.streaming.dstream.MappedDStream]Storage level = Serialized 1x Replicated
[INFO][2018-05-24 19:46:40,023][org.apache.spark.streaming.dstream.MappedDStream]Checkpoint interval = null
[INFO][2018-05-24 19:46:40,023][org.apache.spark.streaming.dstream.MappedDStream]Remember interval = 20000 ms
[INFO][2018-05-24 19:46:40,023][org.apache.spark.streaming.dstream.MappedDStream]Initialized and validated org.apache.spark.streaming.dstream.MappedDStream@2794e4a7
[INFO][2018-05-24 19:46:40,023][org.apache.spark.streaming.dstream.ForEachDStream]Slide time = 20000 ms
[INFO][2018-05-24 19:46:40,023][org.apache.spark.streaming.dstream.ForEachDStream]Storage level = Serialized 1x Replicated
[INFO][2018-05-24 19:46:40,023][org.apache.spark.streaming.dstream.ForEachDStream]Checkpoint interval = null
[INFO][2018-05-24 19:46:40,024][org.apache.spark.streaming.dstream.ForEachDStream]Remember interval = 20000 ms
[INFO][2018-05-24 19:46:40,024][org.apache.spark.streaming.dstream.ForEachDStream]Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@179dc7ca
[INFO][2018-05-24 19:46:40,066][org.apache.spark.streaming.util.RecurringTimer]Started timer for JobGenerator at time 1527162420000
[INFO][2018-05-24 19:46:40,067][org.apache.spark.streaming.scheduler.JobGenerator]Started JobGenerator at 1527162420000 ms
[INFO][2018-05-24 19:46:40,069][org.apache.spark.streaming.scheduler.JobScheduler]Started JobScheduler
[INFO][2018-05-24 19:46:40,073][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3910fe11{/streaming,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:46:40,073][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@351e414e{/streaming/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:46:40,074][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2c2db130{/streaming/batch,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:46:40,074][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@348d18a3{/streaming/batch/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:46:40,077][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@20e6c4dc{/static/streaming,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:46:40,077][org.apache.spark.streaming.StreamingContext]StreamingContext started
[INFO][2018-05-24 19:47:00,045][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 19:47:00,046][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 19:47:00,046][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 19:47:00,126][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527162420000 ms
[INFO][2018-05-24 19:47:00,129][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527162420000 ms.0 from job set of time 1527162420000 ms
[INFO][2018-05-24 19:47:00,169][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:64
[INFO][2018-05-24 19:47:00,180][org.apache.spark.scheduler.DAGScheduler]Got job 0 (foreachPartition at ReceiveKafkaData.scala:64) with 1 output partitions
[INFO][2018-05-24 19:47:00,181][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 0 (foreachPartition at ReceiveKafkaData.scala:64)
[INFO][2018-05-24 19:47:00,181][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 19:47:00,182][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 19:47:00,191][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 0 (MapPartitionsRDD[1] at map at ReceiveKafkaData.scala:62), which has no missing parents
[INFO][2018-05-24 19:47:00,298][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-24 19:47:00,329][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0_piece0 stored as bytes in memory (estimated size 2007.0 B, free 912.3 MB)
[INFO][2018-05-24 19:47:00,330][org.apache.spark.storage.BlockManagerInfo]Added broadcast_0_piece0 in memory on 192.168.0.102:65343 (size: 2007.0 B, free: 912.3 MB)
[INFO][2018-05-24 19:47:00,334][org.apache.spark.SparkContext]Created broadcast 0 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 19:47:00,353][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at map at ReceiveKafkaData.scala:62) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 19:47:00,354][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 0.0 with 1 tasks
[INFO][2018-05-24 19:47:00,397][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 19:47:00,413][org.apache.spark.executor.Executor]Running task 0.0 in stage 0.0 (TID 0)
[INFO][2018-05-24 19:47:00,442][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 11804 is the same as ending offset skipping seven 0
[INFO][2018-05-24 19:47:00,617][org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper]Process identifier=hconnection-0x3ca35b55 connecting to ZooKeeper ensemble=vm-xaj-bigdata-da-d01:2181,vm-xaj-bigdata-da-d02:2181,vm-xaj-bigdata-da-d03:2181
[INFO][2018-05-24 19:47:00,631][org.apache.zookeeper.ZooKeeper]Client environment:zookeeper.version=3.4.5-cdh5.7.0--1, built on 04/06/2016 14:24 GMT
[INFO][2018-05-24 19:47:00,631][org.apache.zookeeper.ZooKeeper]Client environment:host.name=192.168.0.102
[INFO][2018-05-24 19:47:00,631][org.apache.zookeeper.ZooKeeper]Client environment:java.version=1.8.0_161
[INFO][2018-05-24 19:47:00,631][org.apache.zookeeper.ZooKeeper]Client environment:java.vendor=Oracle Corporation
[INFO][2018-05-24 19:47:00,631][org.apache.zookeeper.ZooKeeper]Client environment:java.home=/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre
[INFO][2018-05-24 19:47:00,631][org.apache.zookeeper.ZooKeeper]Client environment:java.class.path=/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/charsets.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/deploy.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/cldrdata.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/dnsns.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/jaccess.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/jfxrt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/localedata.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/nashorn.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/sunec.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/sunjce_provider.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/sunpkcs11.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/zipfs.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/javaws.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/jce.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/jfr.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/jfxswt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/jsse.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/management-agent.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/plugin.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/resources.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/rt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/ant-javafx.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/dt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/javafx-mx.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/jconsole.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/packager.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/sa-jdi.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/tools.jar:/Users/seven/project/scala/dataMining/target/classes:/Users/seven/software/maven/repository/org/scala-lang/scala-library/2.11.12/scala-library-2.11.12.jar:/Users/seven/software/maven/repository/org/scala-lang/scala-compiler/2.11.12/scala-compiler-2.11.12.jar:/Users/seven/software/maven/repository/org/scala-lang/modules/scala-xml_2.11/1.0.5/scala-xml_2.11-1.0.5.jar:/Users/seven/software/maven/repository/org/scala-lang/modules/scala-parser-combinators_2.11/1.0.4/scala-parser-combinators_2.11-1.0.4.jar:/Users/seven/software/maven/repository/org/scala-lang/scala-reflect/2.11.12/scala-reflect-2.11.12.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-client/2.6.0-cdh5.7.0/hadoop-client-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-common/2.6.0-cdh5.7.0/hadoop-common-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/xmlenc/xmlenc/0.52/xmlenc-0.52.jar:/Users/seven/software/maven/repository/commons-net/commons-net/3.1/commons-net-3.1.jar:/Users/seven/software/maven/repository/commons-configuration/commons-configuration/1.6/commons-configuration-1.6.jar:/Users/seven/software/maven/repository/commons-digester/commons-digester/1.8/commons-digester-1.8.jar:/Users/seven/software/maven/repository/commons-beanutils/commons-beanutils/1.7.0/commons-beanutils-1.7.0.jar:/Users/seven/software/maven/repository/org/slf4j/slf4j-api/1.7.5/slf4j-api-1.7.5.jar:/Users/seven/software/maven/repository/org/slf4j/slf4j-log4j12/1.7.5/slf4j-log4j12-1.7.5.jar:/Users/seven/software/maven/repository/org/apache/avro/avro/1.7.6-cdh5.7.0/avro-1.7.6-cdh5.7.0.jar:/Users/seven/software/maven/repository/com/google/code/gson/gson/2.2.4/gson-2.2.4.jar:/Users/seven/software/maven/repository/org/apache/curator/curator-client/2.7.1/curator-client-2.7.1.jar:/Users/seven/software/maven/repository/org/apache/curator/curator-recipes/2.7.1/curator-recipes-2.7.1.jar:/Users/seven/software/maven/repository/com/google/code/findbugs/jsr305/3.0.0/jsr305-3.0.0.jar:/Users/seven/software/maven/repository/org/apache/htrace/htrace-core4/4.0.1-incubating/htrace-core4-4.0.1-incubating.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-compress/1.4.1/commons-compress-1.4.1.jar:/Users/seven/software/maven/repository/org/tukaani/xz/1.0/xz-1.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-hdfs/2.6.0-cdh5.7.0/hadoop-hdfs-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/io/netty/netty/3.6.2.Final/netty-3.6.2.Final.jar:/Users/seven/software/maven/repository/xerces/xercesImpl/2.9.1/xercesImpl-2.9.1.jar:/Users/seven/software/maven/repository/xml-apis/xml-apis/1.3.04/xml-apis-1.3.04.jar:/Users/seven/software/maven/repository/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-app/2.6.0-cdh5.7.0/hadoop-mapreduce-client-app-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-common/2.6.0-cdh5.7.0/hadoop-mapreduce-client-common-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-yarn-client/2.6.0-cdh5.7.0/hadoop-yarn-client-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-yarn-server-common/2.6.0-cdh5.7.0/hadoop-yarn-server-common-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-shuffle/2.6.0-cdh5.7.0/hadoop-mapreduce-client-shuffle-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-yarn-api/2.6.0-cdh5.7.0/hadoop-yarn-api-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-core/2.6.0-cdh5.7.0/hadoop-mapreduce-client-core-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-yarn-common/2.6.0-cdh5.7.0/hadoop-yarn-common-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/javax/xml/bind/jaxb-api/2.2.2/jaxb-api-2.2.2.jar:/Users/seven/software/maven/repository/javax/xml/stream/stax-api/1.0-2/stax-api-1.0-2.jar:/Users/seven/software/maven/repository/javax/activation/activation/1.1/activation-1.1.jar:/Users/seven/software/maven/repository/com/sun/jersey/jersey-client/1.9/jersey-client-1.9.jar:/Users/seven/software/maven/repository/org/codehaus/jackson/jackson-xc/1.8.8/jackson-xc-1.8.8.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.6.0-cdh5.7.0/hadoop-mapreduce-client-jobclient-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-aws/2.6.0-cdh5.7.0/hadoop-aws-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/com/amazonaws/aws-java-sdk-s3/1.10.6/aws-java-sdk-s3-1.10.6.jar:/Users/seven/software/maven/repository/com/amazonaws/aws-java-sdk-kms/1.10.6/aws-java-sdk-kms-1.10.6.jar:/Users/seven/software/maven/repository/com/amazonaws/aws-java-sdk-core/1.10.6/aws-java-sdk-core-1.10.6.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-annotations/2.6.0-cdh5.7.0/hadoop-annotations-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-client/1.2.0-cdh5.7.0/hbase-client-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-annotations/1.2.0-cdh5.7.0/hbase-annotations-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-common/1.2.0-cdh5.7.0/hbase-common-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-protocol/1.2.0-cdh5.7.0/hbase-protocol-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/commons-codec/commons-codec/1.9/commons-codec-1.9.jar:/Users/seven/software/maven/repository/commons-io/commons-io/2.4/commons-io-2.4.jar:/Users/seven/software/maven/repository/commons-lang/commons-lang/2.6/commons-lang-2.6.jar:/Users/seven/software/maven/repository/commons-logging/commons-logging/1.2/commons-logging-1.2.jar:/Users/seven/software/maven/repository/com/google/guava/guava/12.0.1/guava-12.0.1.jar:/Users/seven/software/maven/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/Users/seven/software/maven/repository/io/netty/netty-all/4.0.23.Final/netty-all-4.0.23.Final.jar:/Users/seven/software/maven/repository/org/apache/zookeeper/zookeeper/3.4.5-cdh5.7.0/zookeeper-3.4.5-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/htrace/htrace-core/3.2.0-incubating/htrace-core-3.2.0-incubating.jar:/Users/seven/software/maven/repository/org/codehaus/jackson/jackson-mapper-asl/1.8.8/jackson-mapper-asl-1.8.8.jar:/Users/seven/software/maven/repository/org/jruby/jcodings/jcodings/1.0.8/jcodings-1.0.8.jar:/Users/seven/software/maven/repository/org/jruby/joni/joni/2.1.2/joni-2.1.2.jar:/Users/seven/software/maven/repository/com/yammer/metrics/metrics-core/2.2.0/metrics-core-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-auth/2.6.0-cdh5.7.0/hadoop-auth-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/httpcomponents/httpclient/4.2.5/httpclient-4.2.5.jar:/Users/seven/software/maven/repository/org/apache/httpcomponents/httpcore/4.2.4/httpcore-4.2.4.jar:/Users/seven/software/maven/repository/org/apache/directory/server/apacheds-kerberos-codec/2.0.0-M15/apacheds-kerberos-codec-2.0.0-M15.jar:/Users/seven/software/maven/repository/org/apache/directory/server/apacheds-i18n/2.0.0-M15/apacheds-i18n-2.0.0-M15.jar:/Users/seven/software/maven/repository/org/apache/directory/api/api-asn1-api/1.0.0-M20/api-asn1-api-1.0.0-M20.jar:/Users/seven/software/maven/repository/org/apache/directory/api/api-util/1.0.0-M20/api-util-1.0.0-M20.jar:/Users/seven/software/maven/repository/org/apache/curator/curator-framework/2.7.1/curator-framework-2.7.1.jar:/Users/seven/software/maven/repository/com/github/stephenc/findbugs/findbugs-annotations/1.3.9-1/findbugs-annotations-1.3.9-1.jar:/Users/seven/software/maven/repository/junit/junit/4.12/junit-4.12.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-server/1.2.0-cdh5.7.0/hbase-server-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-procedure/1.2.0-cdh5.7.0/hbase-procedure-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-common/1.2.0-cdh5.7.0/hbase-common-1.2.0-cdh5.7.0-tests.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-prefix-tree/1.2.0-cdh5.7.0/hbase-prefix-tree-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar:/Users/seven/software/maven/repository/commons-collections/commons-collections/3.2.2/commons-collections-3.2.2.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-hadoop-compat/1.2.0-cdh5.7.0/hbase-hadoop-compat-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-hadoop2-compat/1.2.0-cdh5.7.0/hbase-hadoop2-compat-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/com/sun/jersey/jersey-core/1.9/jersey-core-1.9.jar:/Users/seven/software/maven/repository/com/sun/jersey/jersey-server/1.9/jersey-server-1.9.jar:/Users/seven/software/maven/repository/asm/asm/3.1/asm-3.1.jar:/Users/seven/software/maven/repository/commons-cli/commons-cli/1.2/commons-cli-1.2.jar:/Users/seven/software/maven/repository/com/github/stephenc/high-scale-lib/high-scale-lib/1.1.1/high-scale-lib-1.1.1.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-math/2.1/commons-math-2.1.jar:/Users/seven/software/maven/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jetty/6.1.26.cloudera.4/jetty-6.1.26.cloudera.4.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jetty-util/6.1.26.cloudera.4/jetty-util-6.1.26.cloudera.4.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jetty-sslengine/6.1.26.cloudera.4/jetty-sslengine-6.1.26.cloudera.4.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jsp-2.1/6.1.14/jsp-2.1-6.1.14.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jsp-api-2.1/6.1.14/jsp-api-2.1-6.1.14.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/servlet-api-2.5/6.1.14/servlet-api-2.5-6.1.14.jar:/Users/seven/software/maven/repository/org/codehaus/jackson/jackson-core-asl/1.8.8/jackson-core-asl-1.8.8.jar:/Users/seven/software/maven/repository/org/codehaus/jackson/jackson-jaxrs/1.8.8/jackson-jaxrs-1.8.8.jar:/Users/seven/software/maven/repository/tomcat/jasper-compiler/5.5.23/jasper-compiler-5.5.23.jar:/Users/seven/software/maven/repository/tomcat/jasper-runtime/5.5.23/jasper-runtime-5.5.23.jar:/Users/seven/software/maven/repository/commons-el/commons-el/1.0/commons-el-1.0.jar:/Users/seven/software/maven/repository/org/jamon/jamon-runtime/2.4.1/jamon-runtime-2.4.1.jar:/Users/seven/software/maven/repository/com/lmax/disruptor/3.3.0/disruptor-3.3.0.jar:/Users/seven/software/maven/repository/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar:/Users/seven/software/maven/repository/org/owasp/esapi/esapi/2.1.0/esapi-2.1.0.jar:/Users/seven/software/maven/repository/commons-beanutils/commons-beanutils-core/1.7.0/commons-beanutils-core-1.7.0.jar:/Users/seven/software/maven/repository/commons-fileupload/commons-fileupload/1.2/commons-fileupload-1.2.jar:/Users/seven/software/maven/repository/xom/xom/1.2.5/xom-1.2.5.jar:/Users/seven/software/maven/repository/xalan/xalan/2.7.0/xalan-2.7.0.jar:/Users/seven/software/maven/repository/org/beanshell/bsh-core/2.0b4/bsh-core-2.0b4.jar:/Users/seven/software/maven/repository/org/owasp/antisamy/antisamy/1.4.3/antisamy-1.4.3.jar:/Users/seven/software/maven/repository/org/apache/xmlgraphics/batik-css/1.7/batik-css-1.7.jar:/Users/seven/software/maven/repository/org/apache/xmlgraphics/batik-ext/1.7/batik-ext-1.7.jar:/Users/seven/software/maven/repository/org/apache/xmlgraphics/batik-util/1.7/batik-util-1.7.jar:/Users/seven/software/maven/repository/xml-apis/xml-apis-ext/1.3.04/xml-apis-ext-1.3.04.jar:/Users/seven/software/maven/repository/net/sourceforge/nekohtml/nekohtml/1.9.12/nekohtml-1.9.12.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-core/2.6.0-mr1-cdh5.7.0/hadoop-core-2.6.0-mr1-cdh5.7.0.jar:/Users/seven/software/maven/repository/javax/servlet/jsp/jsp-api/2.1/jsp-api-2.1.jar:/Users/seven/software/maven/repository/net/java/dev/jets3t/jets3t/0.6.1/jets3t-0.6.1.jar:/Users/seven/software/maven/repository/javax/servlet/servlet-api/2.5/servlet-api-2.5.jar:/Users/seven/software/maven/repository/hsqldb/hsqldb/1.8.0.10/hsqldb-1.8.0.10.jar:/Users/seven/software/maven/repository/org/eclipse/jdt/core/3.1.1/core-3.1.1.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-hdfs/2.6.0-cdh5.7.0/hadoop-hdfs-2.6.0-cdh5.7.0-tests.jar:/Users/seven/software/maven/repository/commons-daemon/commons-daemon/1.0.13/commons-daemon-1.0.13.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-lang3/3.5/commons-lang3-3.5.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-pool2/2.4.2/commons-pool2-2.4.2.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-mllib_2.11/2.2.0/spark-mllib_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-core_2.11/2.2.0/spark-core_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/avro/avro-mapred/1.7.7/avro-mapred-1.7.7-hadoop2.jar:/Users/seven/software/maven/repository/org/apache/avro/avro-ipc/1.7.7/avro-ipc-1.7.7-tests.jar:/Users/seven/software/maven/repository/com/twitter/chill_2.11/0.8.0/chill_2.11-0.8.0.jar:/Users/seven/software/maven/repository/com/esotericsoftware/kryo-shaded/3.0.3/kryo-shaded-3.0.3.jar:/Users/seven/software/maven/repository/com/esotericsoftware/minlog/1.3.0/minlog-1.3.0.jar:/Users/seven/software/maven/repository/org/objenesis/objenesis/2.1/objenesis-2.1.jar:/Users/seven/software/maven/repository/com/twitter/chill-java/0.8.0/chill-java-0.8.0.jar:/Users/seven/software/maven/repository/org/apache/xbean/xbean-asm5-shaded/4.4/xbean-asm5-shaded-4.4.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-launcher_2.11/2.2.0/spark-launcher_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-network-common_2.11/2.2.0/spark-network-common_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-network-shuffle_2.11/2.2.0/spark-network-shuffle_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-unsafe_2.11/2.2.0/spark-unsafe_2.11-2.2.0.jar:/Users/seven/software/maven/repository/javax/servlet/javax.servlet-api/3.1.0/javax.servlet-api-3.1.0.jar:/Users/seven/software/maven/repository/org/slf4j/jul-to-slf4j/1.7.16/jul-to-slf4j-1.7.16.jar:/Users/seven/software/maven/repository/org/slf4j/jcl-over-slf4j/1.7.16/jcl-over-slf4j-1.7.16.jar:/Users/seven/software/maven/repository/com/ning/compress-lzf/1.0.3/compress-lzf-1.0.3.jar:/Users/seven/software/maven/repository/org/xerial/snappy/snappy-java/1.1.2.6/snappy-java-1.1.2.6.jar:/Users/seven/software/maven/repository/org/roaringbitmap/RoaringBitmap/0.5.11/RoaringBitmap-0.5.11.jar:/Users/seven/software/maven/repository/org/json4s/json4s-jackson_2.11/3.2.11/json4s-jackson_2.11-3.2.11.jar:/Users/seven/software/maven/repository/org/json4s/json4s-core_2.11/3.2.11/json4s-core_2.11-3.2.11.jar:/Users/seven/software/maven/repository/org/json4s/json4s-ast_2.11/3.2.11/json4s-ast_2.11-3.2.11.jar:/Users/seven/software/maven/repository/org/scala-lang/scalap/2.11.0/scalap-2.11.0.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/core/jersey-client/2.22.2/jersey-client-2.22.2.jar:/Users/seven/software/maven/repository/javax/ws/rs/javax.ws.rs-api/2.0.1/javax.ws.rs-api-2.0.1.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/hk2-api/2.4.0-b34/hk2-api-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/hk2-utils/2.4.0-b34/hk2-utils-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/external/aopalliance-repackaged/2.4.0-b34/aopalliance-repackaged-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/external/javax.inject/2.4.0-b34/javax.inject-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/hk2-locator/2.4.0-b34/hk2-locator-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/javassist/javassist/3.18.1-GA/javassist-3.18.1-GA.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/core/jersey-common/2.22.2/jersey-common-2.22.2.jar:/Users/seven/software/maven/repository/javax/annotation/javax.annotation-api/1.2/javax.annotation-api-1.2.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/bundles/repackaged/jersey-guava/2.22.2/jersey-guava-2.22.2.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/osgi-resource-locator/1.0.1/osgi-resource-locator-1.0.1.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/core/jersey-server/2.22.2/jersey-server-2.22.2.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/media/jersey-media-jaxb/2.22.2/jersey-media-jaxb-2.22.2.jar:/Users/seven/software/maven/repository/javax/validation/validation-api/1.1.0.Final/validation-api-1.1.0.Final.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/containers/jersey-container-servlet/2.22.2/jersey-container-servlet-2.22.2.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/containers/jersey-container-servlet-core/2.22.2/jersey-container-servlet-core-2.22.2.jar:/Users/seven/software/maven/repository/com/clearspring/analytics/stream/2.7.0/stream-2.7.0.jar:/Users/seven/software/maven/repository/io/dropwizard/metrics/metrics-core/3.1.2/metrics-core-3.1.2.jar:/Users/seven/software/maven/repository/io/dropwizard/metrics/metrics-jvm/3.1.2/metrics-jvm-3.1.2.jar:/Users/seven/software/maven/repository/io/dropwizard/metrics/metrics-json/3.1.2/metrics-json-3.1.2.jar:/Users/seven/software/maven/repository/io/dropwizard/metrics/metrics-graphite/3.1.2/metrics-graphite-3.1.2.jar:/Users/seven/software/maven/repository/org/apache/ivy/ivy/2.4.0/ivy-2.4.0.jar:/Users/seven/software/maven/repository/oro/oro/2.0.8/oro-2.0.8.jar:/Users/seven/software/maven/repository/net/razorvine/pyrolite/4.13/pyrolite-4.13.jar:/Users/seven/software/maven/repository/net/sf/py4j/py4j/0.10.4/py4j-0.10.4.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-crypto/1.0.0/commons-crypto-1.0.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-streaming_2.11/2.2.0/spark-streaming_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-sql_2.11/2.2.0/spark-sql_2.11-2.2.0.jar:/Users/seven/software/maven/repository/com/univocity/univocity-parsers/2.2.1/univocity-parsers-2.2.1.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-sketch_2.11/2.2.0/spark-sketch_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-catalyst_2.11/2.2.0/spark-catalyst_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/codehaus/janino/janino/3.0.0/janino-3.0.0.jar:/Users/seven/software/maven/repository/org/codehaus/janino/commons-compiler/3.0.0/commons-compiler-3.0.0.jar:/Users/seven/software/maven/repository/org/antlr/antlr4-runtime/4.5.3/antlr4-runtime-4.5.3.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-column/1.8.2/parquet-column-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-common/1.8.2/parquet-common-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-encoding/1.8.2/parquet-encoding-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-hadoop/1.8.2/parquet-hadoop-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-format/2.3.1/parquet-format-2.3.1.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-jackson/1.8.2/parquet-jackson-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-graphx_2.11/2.2.0/spark-graphx_2.11-2.2.0.jar:/Users/seven/software/maven/repository/com/github/fommil/netlib/core/1.1.2/core-1.1.2.jar:/Users/seven/software/maven/repository/net/sourceforge/f2j/arpack_combined_all/0.1/arpack_combined_all-0.1.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-mllib-local_2.11/2.2.0/spark-mllib-local_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/scalanlp/breeze_2.11/0.13.1/breeze_2.11-0.13.1.jar:/Users/seven/software/maven/repository/org/scalanlp/breeze-macros_2.11/0.13.1/breeze-macros_2.11-0.13.1.jar:/Users/seven/software/maven/repository/net/sf/opencsv/opencsv/2.3/opencsv-2.3.jar:/Users/seven/software/maven/repository/com/github/rwl/jtransforms/2.4.0/jtransforms-2.4.0.jar:/Users/seven/software/maven/repository/org/spire-math/spire_2.11/0.13.0/spire_2.11-0.13.0.jar:/Users/seven/software/maven/repository/org/spire-math/spire-macros_2.11/0.13.0/spire-macros_2.11-0.13.0.jar:/Users/seven/software/maven/repository/org/typelevel/machinist_2.11/0.6.1/machinist_2.11-0.6.1.jar:/Users/seven/software/maven/repository/com/chuusai/shapeless_2.11/2.3.2/shapeless_2.11-2.3.2.jar:/Users/seven/software/maven/repository/org/typelevel/macro-compat_2.11/1.1.1/macro-compat_2.11-1.1.1.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-math3/3.4.1/commons-math3-3.4.1.jar:/Users/seven/software/maven/repository/org/jpmml/pmml-model/1.2.15/pmml-model-1.2.15.jar:/Users/seven/software/maven/repository/org/jpmml/pmml-schema/1.2.15/pmml-schema-1.2.15.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-tags_2.11/2.2.0/spark-tags_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-streaming-kafka-0-8_2.11/2.2.0/spark-streaming-kafka-0-8_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/kafka/kafka_2.11/0.8.2.1/kafka_2.11-0.8.2.1.jar:/Users/seven/software/maven/repository/com/101tec/zkclient/0.3/zkclient-0.3.jar:/Users/seven/software/maven/repository/org/apache/kafka/kafka-clients/0.8.2.1/kafka-clients-0.8.2.1.jar:/Users/seven/software/maven/repository/mysql/mysql-connector-java/5.1.31/mysql-connector-java-5.1.31.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/module/jackson-module-scala_2.11/2.6.5/jackson-module-scala_2.11-2.6.5.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/core/jackson-core/2.6.5/jackson-core-2.6.5.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/core/jackson-annotations/2.6.5/jackson-annotations-2.6.5.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/core/jackson-databind/2.6.5/jackson-databind-2.6.5.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/module/jackson-module-paranamer/2.6.5/jackson-module-paranamer-2.6.5.jar:/Users/seven/software/maven/repository/com/thoughtworks/paranamer/paranamer/2.6/paranamer-2.6.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-streaming-flume_2.11/2.2.0/spark-streaming-flume_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/flume/flume-ng-core/1.6.0/flume-ng-core-1.6.0.jar:/Users/seven/software/maven/repository/org/apache/flume/flume-ng-configuration/1.6.0/flume-ng-configuration-1.6.0.jar:/Users/seven/software/maven/repository/org/apache/avro/avro-ipc/1.7.4/avro-ipc-1.7.4.jar:/Users/seven/software/maven/repository/org/apache/velocity/velocity/1.7/velocity-1.7.jar:/Users/seven/software/maven/repository/joda-time/joda-time/2.1/joda-time-2.1.jar:/Users/seven/software/maven/repository/org/apache/mina/mina-core/2.0.4/mina-core-2.0.4.jar:/Users/seven/software/maven/repository/org/apache/flume/flume-ng-sdk/1.6.0/flume-ng-sdk-1.6.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-streaming-flume-sink_2.11/2.2.0/spark-streaming-flume-sink_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/flume/flume-ng-clients/flume-ng-log4jappender/1.6.0/flume-ng-log4jappender-1.6.0.jar:/Users/seven/software/maven/repository/net/jpountz/lz4/lz4/1.3.0/lz4-1.3.0.jar:/Applications/IntelliJ IDEA.app/Contents/lib/idea_rt.jar
[INFO][2018-05-24 19:47:00,632][org.apache.zookeeper.ZooKeeper]Client environment:java.library.path=/Users/seven/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:.
[INFO][2018-05-24 19:47:00,632][org.apache.zookeeper.ZooKeeper]Client environment:java.io.tmpdir=/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/
[INFO][2018-05-24 19:47:00,632][org.apache.zookeeper.ZooKeeper]Client environment:java.compiler=<NA>
[INFO][2018-05-24 19:47:00,632][org.apache.zookeeper.ZooKeeper]Client environment:os.name=Mac OS X
[INFO][2018-05-24 19:47:00,632][org.apache.zookeeper.ZooKeeper]Client environment:os.arch=x86_64
[INFO][2018-05-24 19:47:00,632][org.apache.zookeeper.ZooKeeper]Client environment:os.version=10.13.4
[INFO][2018-05-24 19:47:00,633][org.apache.zookeeper.ZooKeeper]Client environment:user.name=seven
[INFO][2018-05-24 19:47:00,633][org.apache.zookeeper.ZooKeeper]Client environment:user.home=/Users/seven
[INFO][2018-05-24 19:47:00,633][org.apache.zookeeper.ZooKeeper]Client environment:user.dir=/Users/seven/project/scala/dataMining
[INFO][2018-05-24 19:47:00,634][org.apache.zookeeper.ZooKeeper]Initiating client connection, connectString=vm-xaj-bigdata-da-d01:2181,vm-xaj-bigdata-da-d02:2181,vm-xaj-bigdata-da-d03:2181 sessionTimeout=60000 watcher=hconnection-0x3ca35b550x0, quorum=vm-xaj-bigdata-da-d01:2181,vm-xaj-bigdata-da-d02:2181,vm-xaj-bigdata-da-d03:2181, baseZNode=/hbase
[INFO][2018-05-24 19:47:15,836][org.apache.zookeeper.ClientCnxn]Opening socket connection to server vm-xaj-bigdata-da-d02/10.213.4.26:2181. Will not attempt to authenticate using SASL (unknown error)
[INFO][2018-05-24 19:47:15,848][org.apache.zookeeper.ClientCnxn]Socket connection established, initiating session, client: /10.0.8.39:65359, server: vm-xaj-bigdata-da-d02/10.213.4.26:2181
[INFO][2018-05-24 19:47:15,870][org.apache.zookeeper.ClientCnxn]Session establishment complete on server vm-xaj-bigdata-da-d02/10.213.4.26:2181, sessionid = 0x262b4dc569b5f07, negotiated timeout = 60000
[WARN][2018-05-24 19:47:16,318][org.apache.hadoop.util.NativeCodeLoader]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO][2018-05-24 19:47:16,393][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 19:47:16,402][org.apache.spark.executor.Executor]Finished task 0.0 in stage 0.0 (TID 0). 794 bytes result sent to driver
[INFO][2018-05-24 19:47:16,409][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 0.0 (TID 0) in 16022 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 19:47:16,410][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 19:47:16,413][org.apache.spark.scheduler.DAGScheduler]ResultStage 0 (foreachPartition at ReceiveKafkaData.scala:64) finished in 16.034 s
[INFO][2018-05-24 19:47:16,418][org.apache.spark.scheduler.DAGScheduler]Job 0 finished: foreachPartition at ReceiveKafkaData.scala:64, took 16.247880 s
[INFO][2018-05-24 19:47:16,422][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527162420000 ms.0 from job set of time 1527162420000 ms
[INFO][2018-05-24 19:47:16,423][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 16.421 s for time 1527162420000 ms (execution: 16.292 s)
[INFO][2018-05-24 19:47:16,427][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 19:47:16,432][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 
[INFO][2018-05-24 19:47:21,486][org.apache.spark.SparkContext]Running Spark version 2.2.0
[INFO][2018-05-24 19:47:22,255][org.apache.spark.SparkContext]Submitted application: SimulationKafkaSendOutData$
[INFO][2018-05-24 19:47:22,279][org.apache.spark.SecurityManager]Changing view acls to: seven
[INFO][2018-05-24 19:47:22,280][org.apache.spark.SecurityManager]Changing modify acls to: seven
[INFO][2018-05-24 19:47:22,281][org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO][2018-05-24 19:47:22,281][org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO][2018-05-24 19:47:22,282][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(seven); groups with view permissions: Set(); users  with modify permissions: Set(seven); groups with modify permissions: Set()
[INFO][2018-05-24 19:47:22,618][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 65364.
[INFO][2018-05-24 19:47:22,641][org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO][2018-05-24 19:47:22,659][org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO][2018-05-24 19:47:22,662][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2018-05-24 19:47:22,663][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
[INFO][2018-05-24 19:47:22,674][org.apache.spark.storage.DiskBlockManager]Created local directory at /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/blockmgr-166a1f15-baab-4d90-a78d-6430a73e3f40
[INFO][2018-05-24 19:47:22,697][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 912.3 MB
[INFO][2018-05-24 19:47:22,794][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO][2018-05-24 19:47:22,906][org.spark_project.jetty.util.log]Logging initialized @2557ms
[INFO][2018-05-24 19:47:22,973][org.spark_project.jetty.server.Server]jetty-9.3.z-SNAPSHOT
[INFO][2018-05-24 19:47:22,987][org.spark_project.jetty.server.Server]Started @2641ms
[WARN][2018-05-24 19:47:23,004][org.apache.spark.util.Utils]Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[INFO][2018-05-24 19:47:23,009][org.spark_project.jetty.server.AbstractConnector]Started ServerConnector@346a361{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}
[INFO][2018-05-24 19:47:23,010][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4041.
[INFO][2018-05-24 19:47:23,033][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@42b64ab8{/jobs,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:47:23,034][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2c1b9e4b{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:47:23,034][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@649725e3{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:47:23,035][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@52b56a3e{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:47:23,036][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4eed46ee{/stages,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:47:23,036][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4fad94a7{/stages/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:47:23,037][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6326d182{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:47:23,038][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@226642a5{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:47:23,039][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5cc126dc{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:47:23,039][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@72bd06ca{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:47:23,040][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5dbe30be{/storage,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:47:23,040][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@55795845{/storage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:47:23,041][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@119f1f2a{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:47:23,042][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5b970f7{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:47:23,042][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@165b8a71{/environment,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:47:23,043][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2f058b8a{/environment/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:47:23,044][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3f2ef586{/executors,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:47:23,044][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@76c7beb3{/executors/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:47:23,045][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2cf92cc7{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:47:23,046][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7b139eab{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:47:23,057][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@611df6e3{/static,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:47:23,058][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6ffab045{/,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:47:23,063][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3e2943ab{/api,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:47:23,064][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6c67e137{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:47:23,066][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@191ae03f{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:47:23,074][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://192.168.0.102:4041
[INFO][2018-05-24 19:47:23,208][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
[INFO][2018-05-24 19:47:23,245][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 65365.
[INFO][2018-05-24 19:47:23,246][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 192.168.0.102:65365
[INFO][2018-05-24 19:47:23,247][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2018-05-24 19:47:23,249][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 192.168.0.102, 65365, None)
[INFO][2018-05-24 19:47:23,256][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 192.168.0.102:65365 with 912.3 MB RAM, BlockManagerId(driver, 192.168.0.102, 65365, None)
[INFO][2018-05-24 19:47:23,259][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 192.168.0.102, 65365, None)
[INFO][2018-05-24 19:47:23,260][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 192.168.0.102, 65365, None)
[INFO][2018-05-24 19:47:23,451][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3e1162e7{/metrics/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:47:23,954][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0 stored as values in memory (estimated size 228.1 KB, free 912.1 MB)
[INFO][2018-05-24 19:47:24,022][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0_piece0 stored as bytes in memory (estimated size 22.1 KB, free 912.1 MB)
[INFO][2018-05-24 19:47:24,024][org.apache.spark.storage.BlockManagerInfo]Added broadcast_0_piece0 in memory on 192.168.0.102:65365 (size: 22.1 KB, free: 912.3 MB)
[INFO][2018-05-24 19:47:24,034][org.apache.spark.SparkContext]Created broadcast 0 from textFile at SimulationKafkaSendOutData.scala:25
[INFO][2018-05-24 19:47:25,068][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527162440000 ms
[INFO][2018-05-24 19:47:25,068][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527162440000 ms.0 from job set of time 1527162440000 ms
[INFO][2018-05-24 19:47:25,077][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:64
[INFO][2018-05-24 19:47:25,078][org.apache.spark.scheduler.DAGScheduler]Got job 1 (foreachPartition at ReceiveKafkaData.scala:64) with 1 output partitions
[INFO][2018-05-24 19:47:25,078][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 1 (foreachPartition at ReceiveKafkaData.scala:64)
[INFO][2018-05-24 19:47:25,078][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 19:47:25,078][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 19:47:25,079][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 1 (MapPartitionsRDD[3] at map at ReceiveKafkaData.scala:62), which has no missing parents
[INFO][2018-05-24 19:47:25,081][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-24 19:47:25,082][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1_piece0 stored as bytes in memory (estimated size 2008.0 B, free 912.3 MB)
[INFO][2018-05-24 19:47:25,083][org.apache.spark.storage.BlockManagerInfo]Added broadcast_1_piece0 in memory on 192.168.0.102:65343 (size: 2008.0 B, free: 912.3 MB)
[INFO][2018-05-24 19:47:25,083][org.apache.spark.SparkContext]Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 19:47:25,084][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at map at ReceiveKafkaData.scala:62) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 19:47:25,084][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 1.0 with 1 tasks
[INFO][2018-05-24 19:47:25,085][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 19:47:25,086][org.apache.spark.executor.Executor]Running task 0.0 in stage 1.0 (TID 1)
[INFO][2018-05-24 19:47:25,112][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 11804 is the same as ending offset skipping seven 0
[INFO][2018-05-24 19:47:25,112][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 19:47:25,115][org.apache.spark.executor.Executor]Finished task 0.0 in stage 1.0 (TID 1). 751 bytes result sent to driver
[INFO][2018-05-24 19:47:25,118][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 1.0 (TID 1) in 33 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 19:47:25,120][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 19:47:25,123][org.apache.spark.scheduler.DAGScheduler]ResultStage 1 (foreachPartition at ReceiveKafkaData.scala:64) finished in 0.038 s
[INFO][2018-05-24 19:47:25,124][org.apache.spark.scheduler.DAGScheduler]Job 1 finished: foreachPartition at ReceiveKafkaData.scala:64, took 0.046921 s
[INFO][2018-05-24 19:47:25,126][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527162440000 ms.0 from job set of time 1527162440000 ms
[INFO][2018-05-24 19:47:25,126][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 5.125 s for time 1527162440000 ms (execution: 0.057 s)
[INFO][2018-05-24 19:47:25,128][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 1 from persistence list
[INFO][2018-05-24 19:47:25,142][org.apache.spark.storage.BlockManager]Removing RDD 1
[INFO][2018-05-24 19:47:25,142][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 0 from persistence list
[INFO][2018-05-24 19:47:25,148][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 19:47:25,148][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 
[INFO][2018-05-24 19:47:25,148][org.apache.spark.storage.BlockManager]Removing RDD 0
[WARN][2018-05-24 19:47:29,444][org.apache.hadoop.util.NativeCodeLoader]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO][2018-05-24 19:47:29,577][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 1
[INFO][2018-05-24 19:47:29,659][org.apache.spark.SparkContext]Starting job: collect at SimulationKafkaSendOutData.scala:25
[INFO][2018-05-24 19:47:29,671][org.apache.spark.scheduler.DAGScheduler]Got job 0 (collect at SimulationKafkaSendOutData.scala:25) with 2 output partitions
[INFO][2018-05-24 19:47:29,671][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 0 (collect at SimulationKafkaSendOutData.scala:25)
[INFO][2018-05-24 19:47:29,672][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 19:47:29,673][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 19:47:29,680][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 0 (hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/* MapPartitionsRDD[1] at textFile at SimulationKafkaSendOutData.scala:25), which has no missing parents
[INFO][2018-05-24 19:47:29,708][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1 stored as values in memory (estimated size 3.3 KB, free 912.1 MB)
[INFO][2018-05-24 19:47:29,710][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1_piece0 stored as bytes in memory (estimated size 1969.0 B, free 912.1 MB)
[INFO][2018-05-24 19:47:29,710][org.apache.spark.storage.BlockManagerInfo]Added broadcast_1_piece0 in memory on 192.168.0.102:65365 (size: 1969.0 B, free: 912.3 MB)
[INFO][2018-05-24 19:47:29,711][org.apache.spark.SparkContext]Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 19:47:29,725][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ResultStage 0 (hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/* MapPartitionsRDD[1] at textFile at SimulationKafkaSendOutData.scala:25) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2018-05-24 19:47:29,726][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 0.0 with 2 tasks
[INFO][2018-05-24 19:47:29,761][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 4883 bytes)
[INFO][2018-05-24 19:47:29,763][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 4883 bytes)
[INFO][2018-05-24 19:47:29,771][org.apache.spark.executor.Executor]Running task 0.0 in stage 0.0 (TID 0)
[INFO][2018-05-24 19:47:29,771][org.apache.spark.executor.Executor]Running task 1.0 in stage 0.0 (TID 1)
[INFO][2018-05-24 19:47:29,836][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/part-00000:0+10717435
[INFO][2018-05-24 19:47:29,836][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/part-00000:10717435+10717436
[INFO][2018-05-24 19:47:32,479][org.apache.spark.storage.memory.MemoryStore]Block taskresult_0 stored as bytes in memory (estimated size 10.4 MB, free 901.7 MB)
[INFO][2018-05-24 19:47:32,482][org.apache.spark.storage.BlockManagerInfo]Added taskresult_0 in memory on 192.168.0.102:65365 (size: 10.4 MB, free: 901.9 MB)
[INFO][2018-05-24 19:47:32,483][org.apache.spark.executor.Executor]Finished task 0.0 in stage 0.0 (TID 0). 10856349 bytes result sent via BlockManager)
[INFO][2018-05-24 19:47:32,521][org.apache.spark.network.client.TransportClientFactory]Successfully created connection to /192.168.0.102:65365 after 21 ms (0 ms spent in bootstraps)
[INFO][2018-05-24 19:47:32,915][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 0.0 (TID 0) in 3165 ms on localhost (executor driver) (1/2)
[INFO][2018-05-24 19:47:32,918][org.apache.spark.storage.BlockManagerInfo]Removed taskresult_0 on 192.168.0.102:65365 in memory (size: 10.4 MB, free: 912.3 MB)
[INFO][2018-05-24 19:47:40,786][org.apache.spark.storage.memory.MemoryStore]Block taskresult_1 stored as bytes in memory (estimated size 10.4 MB, free 901.7 MB)
[INFO][2018-05-24 19:47:40,787][org.apache.spark.storage.BlockManagerInfo]Added taskresult_1 in memory on 192.168.0.102:65365 (size: 10.4 MB, free: 901.9 MB)
[INFO][2018-05-24 19:47:40,788][org.apache.spark.executor.Executor]Finished task 1.0 in stage 0.0 (TID 1). 10855899 bytes result sent via BlockManager)
[INFO][2018-05-24 19:47:40,852][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 0.0 (TID 1) in 11089 ms on localhost (executor driver) (2/2)
[INFO][2018-05-24 19:47:40,853][org.apache.spark.storage.BlockManagerInfo]Removed taskresult_1 on 192.168.0.102:65365 in memory (size: 10.4 MB, free: 912.3 MB)
[INFO][2018-05-24 19:47:40,854][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 19:47:40,854][org.apache.spark.scheduler.DAGScheduler]ResultStage 0 (collect at SimulationKafkaSendOutData.scala:25) finished in 11.115 s
[INFO][2018-05-24 19:47:40,859][org.apache.spark.scheduler.DAGScheduler]Job 0 finished: collect at SimulationKafkaSendOutData.scala:25, took 11.199230 s
[INFO][2018-05-24 19:47:41,025][org.spark_project.jetty.server.AbstractConnector]Stopped Spark@346a361{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}
[INFO][2018-05-24 19:47:41,029][org.apache.spark.ui.SparkUI]Stopped Spark web UI at http://192.168.0.102:4041
[INFO][2018-05-24 19:47:41,037][org.apache.spark.MapOutputTrackerMasterEndpoint]MapOutputTrackerMasterEndpoint stopped!
[INFO][2018-05-24 19:47:41,059][org.apache.spark.storage.memory.MemoryStore]MemoryStore cleared
[INFO][2018-05-24 19:47:41,059][org.apache.spark.storage.BlockManager]BlockManager stopped
[INFO][2018-05-24 19:47:41,060][org.apache.spark.storage.BlockManagerMaster]BlockManagerMaster stopped
[INFO][2018-05-24 19:47:41,062][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint]OutputCommitCoordinator stopped!
[INFO][2018-05-24 19:47:41,064][org.apache.spark.SparkContext]Successfully stopped SparkContext
[INFO][2018-05-24 19:47:41,079][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	compression.type = none
	metric.reporters = []
	metadata.max.age.ms = 300000
	metadata.fetch.timeout.ms = 60000
	acks = all
	batch.size = 16384
	reconnect.backoff.ms = 10
	bootstrap.servers = [vm-xaj-bigdata-da-d01:9092, vm-xaj-bigdata-da-d02:9092, vm-xaj-bigdata-da-d03:9092, vm-xaj-bigdata-da-d04:9092, vm-xaj-bigdata-da-d05:9092, vm-xaj-bigdata-da-d06:9092, vm-xaj-bigdata-da-d07:9092]
	receive.buffer.bytes = 32768
	retry.backoff.ms = 100
	buffer.memory = 33554432
	timeout.ms = 30000
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	retries = 0
	max.request.size = 1048576
	block.on.buffer.full = true
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
	metrics.sample.window.ms = 30000
	send.buffer.bytes = 131072
	max.in.flight.requests.per.connection = 5
	metrics.num.samples = 2
	linger.ms = 1
	client.id = 

[INFO][2018-05-24 19:47:45,084][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527162460000 ms
[INFO][2018-05-24 19:47:45,085][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527162460000 ms.0 from job set of time 1527162460000 ms
[INFO][2018-05-24 19:47:45,095][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:64
[INFO][2018-05-24 19:47:45,096][org.apache.spark.scheduler.DAGScheduler]Got job 2 (foreachPartition at ReceiveKafkaData.scala:64) with 1 output partitions
[INFO][2018-05-24 19:47:45,096][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 2 (foreachPartition at ReceiveKafkaData.scala:64)
[INFO][2018-05-24 19:47:45,096][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 19:47:45,096][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 19:47:45,097][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 2 (MapPartitionsRDD[5] at map at ReceiveKafkaData.scala:62), which has no missing parents
[INFO][2018-05-24 19:47:45,101][org.apache.spark.storage.memory.MemoryStore]Block broadcast_2 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-24 19:47:45,103][org.apache.spark.storage.memory.MemoryStore]Block broadcast_2_piece0 stored as bytes in memory (estimated size 2008.0 B, free 912.3 MB)
[INFO][2018-05-24 19:47:45,104][org.apache.spark.storage.BlockManagerInfo]Added broadcast_2_piece0 in memory on 192.168.0.102:65343 (size: 2008.0 B, free: 912.3 MB)
[INFO][2018-05-24 19:47:45,105][org.apache.spark.SparkContext]Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 19:47:45,106][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[5] at map at ReceiveKafkaData.scala:62) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 19:47:45,106][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 2.0 with 1 tasks
[INFO][2018-05-24 19:47:45,107][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 19:47:45,108][org.apache.spark.executor.Executor]Running task 0.0 in stage 2.0 (TID 2)
[INFO][2018-05-24 19:47:45,111][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 11804 is the same as ending offset skipping seven 0
[INFO][2018-05-24 19:47:45,112][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 19:47:45,113][org.apache.spark.executor.Executor]Finished task 0.0 in stage 2.0 (TID 2). 708 bytes result sent to driver
[INFO][2018-05-24 19:47:45,114][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 2.0 (TID 2) in 7 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 19:47:45,115][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 19:47:45,115][org.apache.spark.scheduler.DAGScheduler]ResultStage 2 (foreachPartition at ReceiveKafkaData.scala:64) finished in 0.009 s
[INFO][2018-05-24 19:47:45,116][org.apache.spark.scheduler.DAGScheduler]Job 2 finished: foreachPartition at ReceiveKafkaData.scala:64, took 0.020975 s
[INFO][2018-05-24 19:47:45,116][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527162460000 ms.0 from job set of time 1527162460000 ms
[INFO][2018-05-24 19:47:45,117][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 3 from persistence list
[INFO][2018-05-24 19:47:45,117][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 5.116 s for time 1527162460000 ms (execution: 0.031 s)
[INFO][2018-05-24 19:47:45,117][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 2 from persistence list
[INFO][2018-05-24 19:47:45,117][org.apache.spark.storage.BlockManager]Removing RDD 3
[INFO][2018-05-24 19:47:45,118][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 19:47:45,118][org.apache.spark.storage.BlockManager]Removing RDD 2
[INFO][2018-05-24 19:47:45,118][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527162420000 ms
[INFO][2018-05-24 19:48:05,079][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527162480000 ms
[INFO][2018-05-24 19:48:05,080][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527162480000 ms.0 from job set of time 1527162480000 ms
[INFO][2018-05-24 19:48:05,089][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:64
[INFO][2018-05-24 19:48:05,090][org.apache.spark.scheduler.DAGScheduler]Got job 3 (foreachPartition at ReceiveKafkaData.scala:64) with 1 output partitions
[INFO][2018-05-24 19:48:05,090][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 3 (foreachPartition at ReceiveKafkaData.scala:64)
[INFO][2018-05-24 19:48:05,090][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 19:48:05,090][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 19:48:05,090][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 3 (MapPartitionsRDD[7] at map at ReceiveKafkaData.scala:62), which has no missing parents
[INFO][2018-05-24 19:48:05,093][org.apache.spark.storage.memory.MemoryStore]Block broadcast_3 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-24 19:48:05,099][org.apache.spark.storage.memory.MemoryStore]Block broadcast_3_piece0 stored as bytes in memory (estimated size 2008.0 B, free 912.3 MB)
[INFO][2018-05-24 19:48:05,100][org.apache.spark.storage.BlockManagerInfo]Added broadcast_3_piece0 in memory on 192.168.0.102:65343 (size: 2008.0 B, free: 912.3 MB)
[INFO][2018-05-24 19:48:05,100][org.apache.spark.SparkContext]Created broadcast 3 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 19:48:05,101][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[7] at map at ReceiveKafkaData.scala:62) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 19:48:05,101][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 3.0 with 1 tasks
[INFO][2018-05-24 19:48:05,102][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 3.0 (TID 3, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 19:48:05,102][org.apache.spark.executor.Executor]Running task 0.0 in stage 3.0 (TID 3)
[INFO][2018-05-24 19:48:05,106][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 11804 is the same as ending offset skipping seven 0
[INFO][2018-05-24 19:48:05,106][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 19:48:05,107][org.apache.spark.executor.Executor]Finished task 0.0 in stage 3.0 (TID 3). 751 bytes result sent to driver
[INFO][2018-05-24 19:48:05,108][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 3.0 (TID 3) in 6 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 19:48:05,108][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 3.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 19:48:05,109][org.apache.spark.scheduler.DAGScheduler]ResultStage 3 (foreachPartition at ReceiveKafkaData.scala:64) finished in 0.007 s
[INFO][2018-05-24 19:48:05,109][org.apache.spark.scheduler.DAGScheduler]Job 3 finished: foreachPartition at ReceiveKafkaData.scala:64, took 0.020318 s
[INFO][2018-05-24 19:48:05,110][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527162480000 ms.0 from job set of time 1527162480000 ms
[INFO][2018-05-24 19:48:05,110][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 5 from persistence list
[INFO][2018-05-24 19:48:05,110][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 5.110 s for time 1527162480000 ms (execution: 0.030 s)
[INFO][2018-05-24 19:48:05,110][org.apache.spark.storage.BlockManager]Removing RDD 5
[INFO][2018-05-24 19:48:05,111][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 4 from persistence list
[INFO][2018-05-24 19:48:05,111][org.apache.spark.storage.BlockManager]Removing RDD 4
[INFO][2018-05-24 19:48:05,111][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 19:48:05,111][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527162440000 ms
[INFO][2018-05-24 19:48:25,070][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527162500000 ms
[INFO][2018-05-24 19:48:25,070][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527162500000 ms.0 from job set of time 1527162500000 ms
[INFO][2018-05-24 19:48:25,084][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:64
[INFO][2018-05-24 19:48:25,086][org.apache.spark.scheduler.DAGScheduler]Got job 4 (foreachPartition at ReceiveKafkaData.scala:64) with 1 output partitions
[INFO][2018-05-24 19:48:25,087][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 4 (foreachPartition at ReceiveKafkaData.scala:64)
[INFO][2018-05-24 19:48:25,087][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 19:48:25,087][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 19:48:25,087][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 4 (MapPartitionsRDD[9] at map at ReceiveKafkaData.scala:62), which has no missing parents
[INFO][2018-05-24 19:48:25,089][org.apache.spark.storage.memory.MemoryStore]Block broadcast_4 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-24 19:48:25,099][org.apache.spark.storage.memory.MemoryStore]Block broadcast_4_piece0 stored as bytes in memory (estimated size 2008.0 B, free 912.3 MB)
[INFO][2018-05-24 19:48:25,101][org.apache.spark.storage.BlockManagerInfo]Added broadcast_4_piece0 in memory on 192.168.0.102:65343 (size: 2008.0 B, free: 912.3 MB)
[INFO][2018-05-24 19:48:25,101][org.apache.spark.SparkContext]Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 19:48:25,102][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[9] at map at ReceiveKafkaData.scala:62) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 19:48:25,102][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 4.0 with 1 tasks
[INFO][2018-05-24 19:48:25,103][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 4.0 (TID 4, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 19:48:25,103][org.apache.spark.executor.Executor]Running task 0.0 in stage 4.0 (TID 4)
[INFO][2018-05-24 19:48:25,111][org.apache.spark.streaming.kafka.KafkaRDD]Computing topic seven, partition 0 offsets 11804 -> 11813
[INFO][2018-05-24 19:48:25,111][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 19:48:25,111][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 19:48:25,111][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 19:48:25,377][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_0_piece0 on 192.168.0.102:65343 in memory (size: 2007.0 B, free: 912.3 MB)
[INFO][2018-05-24 19:48:25,380][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_2_piece0 on 192.168.0.102:65343 in memory (size: 2008.0 B, free: 912.3 MB)
[INFO][2018-05-24 19:48:25,382][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_1_piece0 on 192.168.0.102:65343 in memory (size: 2008.0 B, free: 912.3 MB)
[INFO][2018-05-24 19:48:25,383][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_3_piece0 on 192.168.0.102:65343 in memory (size: 2008.0 B, free: 912.3 MB)
[INFO][2018-05-24 19:48:25,613][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 19:48:25,614][org.apache.spark.executor.Executor]Finished task 0.0 in stage 4.0 (TID 4). 751 bytes result sent to driver
[INFO][2018-05-24 19:48:25,615][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 4.0 (TID 4) in 512 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 19:48:25,615][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 4.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 19:48:25,616][org.apache.spark.scheduler.DAGScheduler]ResultStage 4 (foreachPartition at ReceiveKafkaData.scala:64) finished in 0.513 s
[INFO][2018-05-24 19:48:25,616][org.apache.spark.scheduler.DAGScheduler]Job 4 finished: foreachPartition at ReceiveKafkaData.scala:64, took 0.531305 s
[INFO][2018-05-24 19:48:25,617][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527162500000 ms.0 from job set of time 1527162500000 ms
[INFO][2018-05-24 19:48:25,617][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 7 from persistence list
[INFO][2018-05-24 19:48:25,617][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 5.617 s for time 1527162500000 ms (execution: 0.547 s)
[INFO][2018-05-24 19:48:25,617][org.apache.spark.storage.BlockManager]Removing RDD 7
[INFO][2018-05-24 19:48:25,618][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 6 from persistence list
[INFO][2018-05-24 19:48:25,618][org.apache.spark.storage.BlockManager]Removing RDD 6
[INFO][2018-05-24 19:48:25,618][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 19:48:25,618][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527162460000 ms
[INFO][2018-05-24 19:48:50,068][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527162520000 ms
[INFO][2018-05-24 19:48:50,068][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527162520000 ms.0 from job set of time 1527162520000 ms
[INFO][2018-05-24 19:48:50,075][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:64
[INFO][2018-05-24 19:48:50,076][org.apache.spark.scheduler.DAGScheduler]Got job 5 (foreachPartition at ReceiveKafkaData.scala:64) with 1 output partitions
[INFO][2018-05-24 19:48:50,076][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 5 (foreachPartition at ReceiveKafkaData.scala:64)
[INFO][2018-05-24 19:48:50,076][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 19:48:50,076][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 19:48:50,077][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 5 (MapPartitionsRDD[11] at map at ReceiveKafkaData.scala:62), which has no missing parents
[INFO][2018-05-24 19:48:50,079][org.apache.spark.storage.memory.MemoryStore]Block broadcast_5 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-24 19:48:50,080][org.apache.spark.storage.memory.MemoryStore]Block broadcast_5_piece0 stored as bytes in memory (estimated size 2008.0 B, free 912.3 MB)
[INFO][2018-05-24 19:48:50,081][org.apache.spark.storage.BlockManagerInfo]Added broadcast_5_piece0 in memory on 192.168.0.102:65343 (size: 2008.0 B, free: 912.3 MB)
[INFO][2018-05-24 19:48:50,081][org.apache.spark.SparkContext]Created broadcast 5 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 19:48:50,082][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[11] at map at ReceiveKafkaData.scala:62) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 19:48:50,082][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 5.0 with 1 tasks
[INFO][2018-05-24 19:48:50,083][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 5.0 (TID 5, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 19:48:50,083][org.apache.spark.executor.Executor]Running task 0.0 in stage 5.0 (TID 5)
[INFO][2018-05-24 19:48:50,086][org.apache.spark.streaming.kafka.KafkaRDD]Computing topic seven, partition 0 offsets 11813 -> 11838
[INFO][2018-05-24 19:48:50,086][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 19:48:50,086][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 19:48:50,086][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 19:48:50,179][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 19:48:50,180][org.apache.spark.executor.Executor]Finished task 0.0 in stage 5.0 (TID 5). 708 bytes result sent to driver
[INFO][2018-05-24 19:48:50,181][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 5.0 (TID 5) in 99 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 19:48:50,181][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 5.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 19:48:50,182][org.apache.spark.scheduler.DAGScheduler]ResultStage 5 (foreachPartition at ReceiveKafkaData.scala:64) finished in 0.100 s
[INFO][2018-05-24 19:48:50,182][org.apache.spark.scheduler.DAGScheduler]Job 5 finished: foreachPartition at ReceiveKafkaData.scala:64, took 0.106778 s
[INFO][2018-05-24 19:48:50,183][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527162520000 ms.0 from job set of time 1527162520000 ms
[INFO][2018-05-24 19:48:50,183][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 9 from persistence list
[INFO][2018-05-24 19:48:50,183][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 10.183 s for time 1527162520000 ms (execution: 0.115 s)
[INFO][2018-05-24 19:48:50,184][org.apache.spark.storage.BlockManager]Removing RDD 9
[INFO][2018-05-24 19:48:50,184][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 8 from persistence list
[INFO][2018-05-24 19:48:50,184][org.apache.spark.storage.BlockManager]Removing RDD 8
[INFO][2018-05-24 19:48:50,184][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 19:48:50,184][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527162480000 ms
[INFO][2018-05-24 19:49:05,082][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527162540000 ms
[INFO][2018-05-24 19:49:05,083][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527162540000 ms.0 from job set of time 1527162540000 ms
[INFO][2018-05-24 19:49:05,090][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:64
[INFO][2018-05-24 19:49:05,091][org.apache.spark.scheduler.DAGScheduler]Got job 6 (foreachPartition at ReceiveKafkaData.scala:64) with 1 output partitions
[INFO][2018-05-24 19:49:05,091][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 6 (foreachPartition at ReceiveKafkaData.scala:64)
[INFO][2018-05-24 19:49:05,091][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 19:49:05,092][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 19:49:05,092][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 6 (MapPartitionsRDD[13] at map at ReceiveKafkaData.scala:62), which has no missing parents
[INFO][2018-05-24 19:49:05,094][org.apache.spark.storage.memory.MemoryStore]Block broadcast_6 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-24 19:49:05,096][org.apache.spark.storage.memory.MemoryStore]Block broadcast_6_piece0 stored as bytes in memory (estimated size 2008.0 B, free 912.3 MB)
[INFO][2018-05-24 19:49:05,096][org.apache.spark.storage.BlockManagerInfo]Added broadcast_6_piece0 in memory on 192.168.0.102:65343 (size: 2008.0 B, free: 912.3 MB)
[INFO][2018-05-24 19:49:05,097][org.apache.spark.SparkContext]Created broadcast 6 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 19:49:05,098][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[13] at map at ReceiveKafkaData.scala:62) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 19:49:05,098][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 6.0 with 1 tasks
[INFO][2018-05-24 19:49:05,099][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 6.0 (TID 6, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 19:49:05,099][org.apache.spark.executor.Executor]Running task 0.0 in stage 6.0 (TID 6)
[INFO][2018-05-24 19:49:05,101][org.apache.spark.streaming.kafka.KafkaRDD]Computing topic seven, partition 0 offsets 11838 -> 11853
[INFO][2018-05-24 19:49:05,101][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 19:49:05,101][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 19:49:05,102][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 19:49:05,197][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 19:49:05,198][org.apache.spark.executor.Executor]Finished task 0.0 in stage 6.0 (TID 6). 708 bytes result sent to driver
[INFO][2018-05-24 19:49:05,199][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 6.0 (TID 6) in 101 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 19:49:05,200][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 6.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 19:49:05,200][org.apache.spark.scheduler.DAGScheduler]ResultStage 6 (foreachPartition at ReceiveKafkaData.scala:64) finished in 0.102 s
[INFO][2018-05-24 19:49:05,201][org.apache.spark.scheduler.DAGScheduler]Job 6 finished: foreachPartition at ReceiveKafkaData.scala:64, took 0.110559 s
[INFO][2018-05-24 19:49:05,202][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527162540000 ms.0 from job set of time 1527162540000 ms
[INFO][2018-05-24 19:49:05,202][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 5.202 s for time 1527162540000 ms (execution: 0.119 s)
[INFO][2018-05-24 19:49:05,202][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 11 from persistence list
[INFO][2018-05-24 19:49:05,203][org.apache.spark.storage.BlockManager]Removing RDD 11
[INFO][2018-05-24 19:49:05,203][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 10 from persistence list
[INFO][2018-05-24 19:49:05,203][org.apache.spark.storage.BlockManager]Removing RDD 10
[INFO][2018-05-24 19:49:05,203][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 19:49:05,203][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527162500000 ms
[INFO][2018-05-24 19:49:28,187][org.apache.spark.util.ShutdownHookManager]Shutdown hook called
[INFO][2018-05-24 19:49:28,188][org.apache.spark.util.ShutdownHookManager]Deleting directory /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/spark-cc706d72-3efd-4cbb-b4e1-e405b51fdbc5
[INFO][2018-05-24 19:49:28,220][org.apache.spark.streaming.StreamingContext]Invoking stop(stopGracefully=false) from shutdown hook
[INFO][2018-05-24 19:49:28,221][org.apache.spark.streaming.scheduler.ReceiverTracker]ReceiverTracker stopped
[INFO][2018-05-24 19:49:28,223][org.apache.spark.streaming.scheduler.JobGenerator]Stopping JobGenerator immediately
[INFO][2018-05-24 19:49:28,225][org.apache.spark.streaming.util.RecurringTimer]Stopped timer for JobGenerator after time 1527162560000
[INFO][2018-05-24 19:49:30,073][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527162560000 ms
[INFO][2018-05-24 19:49:30,074][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527162560000 ms.0 from job set of time 1527162560000 ms
[INFO][2018-05-24 19:49:30,077][org.apache.spark.streaming.scheduler.JobGenerator]Stopped JobGenerator
[INFO][2018-05-24 19:49:30,083][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:64
[INFO][2018-05-24 19:49:30,084][org.apache.spark.scheduler.DAGScheduler]Got job 7 (foreachPartition at ReceiveKafkaData.scala:64) with 1 output partitions
[INFO][2018-05-24 19:49:30,085][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 7 (foreachPartition at ReceiveKafkaData.scala:64)
[INFO][2018-05-24 19:49:30,085][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 19:49:30,085][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 19:49:30,085][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 7 (MapPartitionsRDD[15] at map at ReceiveKafkaData.scala:62), which has no missing parents
[INFO][2018-05-24 19:49:30,090][org.apache.spark.storage.memory.MemoryStore]Block broadcast_7 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-24 19:49:30,101][org.apache.spark.storage.memory.MemoryStore]Block broadcast_7_piece0 stored as bytes in memory (estimated size 2008.0 B, free 912.3 MB)
[INFO][2018-05-24 19:49:30,102][org.apache.spark.storage.BlockManagerInfo]Added broadcast_7_piece0 in memory on 192.168.0.102:65343 (size: 2008.0 B, free: 912.3 MB)
[INFO][2018-05-24 19:49:30,102][org.apache.spark.SparkContext]Created broadcast 7 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 19:49:30,104][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[15] at map at ReceiveKafkaData.scala:62) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 19:49:30,104][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 7.0 with 1 tasks
[INFO][2018-05-24 19:49:30,109][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 7.0 (TID 7, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 19:49:30,109][org.apache.spark.executor.Executor]Running task 0.0 in stage 7.0 (TID 7)
[INFO][2018-05-24 19:49:30,112][org.apache.spark.streaming.kafka.KafkaRDD]Computing topic seven, partition 0 offsets 11853 -> 11876
[INFO][2018-05-24 19:49:30,112][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 19:49:30,112][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 19:49:30,112][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 19:49:32,085][org.apache.spark.streaming.scheduler.JobScheduler]Stopped JobScheduler
[INFO][2018-05-24 19:49:32,098][org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@3910fe11{/streaming,null,UNAVAILABLE,@Spark}
[INFO][2018-05-24 19:49:32,099][org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@2c2db130{/streaming/batch,null,UNAVAILABLE,@Spark}
[INFO][2018-05-24 19:49:32,100][org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@20e6c4dc{/static/streaming,null,UNAVAILABLE,@Spark}
[INFO][2018-05-24 19:49:32,101][org.apache.spark.streaming.StreamingContext]StreamingContext stopped successfully
[INFO][2018-05-24 19:49:32,102][org.apache.spark.SparkContext]Invoking stop() from shutdown hook
[INFO][2018-05-24 19:49:32,113][org.spark_project.jetty.server.AbstractConnector]Stopped Spark@2090b9b9{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-05-24 19:49:32,115][org.apache.spark.ui.SparkUI]Stopped Spark web UI at http://192.168.0.102:4040
[INFO][2018-05-24 19:49:32,122][org.apache.spark.scheduler.DAGScheduler]ResultStage 7 (foreachPartition at ReceiveKafkaData.scala:64) failed in 2.016 s due to Stage cancelled because SparkContext was shut down
[ERROR][2018-05-24 19:49:32,123][org.apache.spark.scheduler.LiveListenerBus]SparkListenerBus has already stopped! Dropping event SparkListenerStageCompleted(org.apache.spark.scheduler.StageInfo@4db38d3e)
[ERROR][2018-05-24 19:49:32,124][org.apache.spark.scheduler.LiveListenerBus]SparkListenerBus has already stopped! Dropping event SparkListenerJobEnd(7,1527162572123,JobFailed(org.apache.spark.SparkException: Job 7 cancelled because SparkContext was shut down))
[INFO][2018-05-24 19:49:32,129][org.apache.spark.MapOutputTrackerMasterEndpoint]MapOutputTrackerMasterEndpoint stopped!
[INFO][2018-05-24 19:49:32,158][org.apache.spark.storage.memory.MemoryStore]MemoryStore cleared
[INFO][2018-05-24 19:49:32,158][org.apache.spark.storage.BlockManager]BlockManager stopped
[INFO][2018-05-24 19:49:32,160][org.apache.spark.storage.BlockManagerMaster]BlockManagerMaster stopped
[INFO][2018-05-24 19:49:32,162][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint]OutputCommitCoordinator stopped!
[INFO][2018-05-24 19:49:32,166][org.apache.spark.SparkContext]Successfully stopped SparkContext
[INFO][2018-05-24 19:49:32,166][org.apache.spark.util.ShutdownHookManager]Shutdown hook called
[INFO][2018-05-24 19:49:32,167][org.apache.spark.util.ShutdownHookManager]Deleting directory /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/spark-9f9af923-9145-46d8-86ab-6a18577734f3
[INFO][2018-05-24 19:50:04,633][org.apache.spark.SparkContext]Running Spark version 2.2.0
[INFO][2018-05-24 19:50:05,296][org.apache.spark.SparkContext]Submitted application: ReceiveKafkaData$
[INFO][2018-05-24 19:50:05,325][org.apache.spark.SecurityManager]Changing view acls to: seven
[INFO][2018-05-24 19:50:05,326][org.apache.spark.SecurityManager]Changing modify acls to: seven
[INFO][2018-05-24 19:50:05,327][org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO][2018-05-24 19:50:05,327][org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO][2018-05-24 19:50:05,328][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(seven); groups with view permissions: Set(); users  with modify permissions: Set(seven); groups with modify permissions: Set()
[INFO][2018-05-24 19:50:05,637][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 65422.
[INFO][2018-05-24 19:50:05,652][org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO][2018-05-24 19:50:05,668][org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO][2018-05-24 19:50:05,672][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2018-05-24 19:50:05,672][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
[INFO][2018-05-24 19:50:05,681][org.apache.spark.storage.DiskBlockManager]Created local directory at /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/blockmgr-f697e5c8-7c31-41c5-b73e-dc0a54dc5103
[INFO][2018-05-24 19:50:05,696][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 912.3 MB
[INFO][2018-05-24 19:50:05,767][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO][2018-05-24 19:50:05,842][org.spark_project.jetty.util.log]Logging initialized @2146ms
[INFO][2018-05-24 19:50:05,901][org.spark_project.jetty.server.Server]jetty-9.3.z-SNAPSHOT
[INFO][2018-05-24 19:50:05,913][org.spark_project.jetty.server.Server]Started @2218ms
[INFO][2018-05-24 19:50:05,931][org.spark_project.jetty.server.AbstractConnector]Started ServerConnector@6d366c9b{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-05-24 19:50:05,932][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4040.
[INFO][2018-05-24 19:50:05,954][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2c1b9e4b{/jobs,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:50:05,955][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3d08f3f5{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:50:05,956][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1a1da881{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:50:05,958][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@165b8a71{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:50:05,960][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2f058b8a{/stages,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:50:05,961][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3f2ef586{/stages/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:50:05,962][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@76c7beb3{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:50:05,964][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7b139eab{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:50:05,965][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@611df6e3{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:50:05,966][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6273c5a4{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:50:05,967][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@53e211ee{/storage,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:50:05,968][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3d8bbcdc{/storage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:50:05,968][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@117e0fe5{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:50:05,969][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@78aea4b9{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:50:05,970][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4b85880b{/environment,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:50:05,970][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4215838f{/environment/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:50:05,971][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2289aca5{/executors,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:50:05,972][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@184497d1{/executors/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:50:05,974][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6ffab045{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:50:05,975][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3e2943ab{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:50:05,985][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4a9f80d3{/static,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:50:05,986][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4204541c{/,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:50:05,987][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4602c2a9{/api,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:50:05,988][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@69c79f09{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:50:05,989][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5fcacc0{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:50:05,991][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://192.168.0.102:4040
[INFO][2018-05-24 19:50:06,116][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
[INFO][2018-05-24 19:50:06,150][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 65423.
[INFO][2018-05-24 19:50:06,152][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 192.168.0.102:65423
[INFO][2018-05-24 19:50:06,154][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2018-05-24 19:50:06,155][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 192.168.0.102, 65423, None)
[INFO][2018-05-24 19:50:06,163][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 192.168.0.102:65423 with 912.3 MB RAM, BlockManagerId(driver, 192.168.0.102, 65423, None)
[INFO][2018-05-24 19:50:06,168][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 192.168.0.102, 65423, None)
[INFO][2018-05-24 19:50:06,168][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 192.168.0.102, 65423, None)
[INFO][2018-05-24 19:50:06,385][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@64a1923a{/metrics/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:50:06,529][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 19:50:06,532][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 19:50:06,532][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 19:50:22,007][org.apache.spark.streaming.kafka.DirectKafkaInputDStream]Slide time = 20000 ms
[INFO][2018-05-24 19:50:22,007][org.apache.spark.streaming.kafka.DirectKafkaInputDStream]Storage level = Serialized 1x Replicated
[INFO][2018-05-24 19:50:22,008][org.apache.spark.streaming.kafka.DirectKafkaInputDStream]Checkpoint interval = null
[INFO][2018-05-24 19:50:22,008][org.apache.spark.streaming.kafka.DirectKafkaInputDStream]Remember interval = 20000 ms
[INFO][2018-05-24 19:50:22,009][org.apache.spark.streaming.kafka.DirectKafkaInputDStream]Initialized and validated org.apache.spark.streaming.kafka.DirectKafkaInputDStream@635f60ff
[INFO][2018-05-24 19:50:22,009][org.apache.spark.streaming.dstream.MappedDStream]Slide time = 20000 ms
[INFO][2018-05-24 19:50:22,009][org.apache.spark.streaming.dstream.MappedDStream]Storage level = Serialized 1x Replicated
[INFO][2018-05-24 19:50:22,009][org.apache.spark.streaming.dstream.MappedDStream]Checkpoint interval = null
[INFO][2018-05-24 19:50:22,010][org.apache.spark.streaming.dstream.MappedDStream]Remember interval = 20000 ms
[INFO][2018-05-24 19:50:22,010][org.apache.spark.streaming.dstream.MappedDStream]Initialized and validated org.apache.spark.streaming.dstream.MappedDStream@128240cb
[INFO][2018-05-24 19:50:22,010][org.apache.spark.streaming.dstream.ForEachDStream]Slide time = 20000 ms
[INFO][2018-05-24 19:50:22,010][org.apache.spark.streaming.dstream.ForEachDStream]Storage level = Serialized 1x Replicated
[INFO][2018-05-24 19:50:22,010][org.apache.spark.streaming.dstream.ForEachDStream]Checkpoint interval = null
[INFO][2018-05-24 19:50:22,010][org.apache.spark.streaming.dstream.ForEachDStream]Remember interval = 20000 ms
[INFO][2018-05-24 19:50:22,010][org.apache.spark.streaming.dstream.ForEachDStream]Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@7a7836e0
[INFO][2018-05-24 19:50:22,051][org.apache.spark.streaming.util.RecurringTimer]Started timer for JobGenerator at time 1527162640000
[INFO][2018-05-24 19:50:22,052][org.apache.spark.streaming.scheduler.JobGenerator]Started JobGenerator at 1527162640000 ms
[INFO][2018-05-24 19:50:22,053][org.apache.spark.streaming.scheduler.JobScheduler]Started JobScheduler
[INFO][2018-05-24 19:50:22,056][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6972c30a{/streaming,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:50:22,057][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1ab6718{/streaming/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:50:22,058][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5109e8cf{/streaming/batch,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:50:22,058][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@78b41097{/streaming/batch/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:50:22,059][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6c65860d{/static/streaming,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:50:22,060][org.apache.spark.streaming.StreamingContext]StreamingContext started
[INFO][2018-05-24 19:50:40,043][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 19:50:40,043][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 19:50:40,043][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 19:50:40,128][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527162640000 ms
[INFO][2018-05-24 19:50:40,130][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527162640000 ms.0 from job set of time 1527162640000 ms
[INFO][2018-05-24 19:50:40,176][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:64
[INFO][2018-05-24 19:50:40,190][org.apache.spark.scheduler.DAGScheduler]Got job 0 (foreachPartition at ReceiveKafkaData.scala:64) with 1 output partitions
[INFO][2018-05-24 19:50:40,190][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 0 (foreachPartition at ReceiveKafkaData.scala:64)
[INFO][2018-05-24 19:50:40,190][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 19:50:40,192][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 19:50:40,205][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 0 (MapPartitionsRDD[1] at map at ReceiveKafkaData.scala:62), which has no missing parents
[INFO][2018-05-24 19:50:40,338][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-24 19:50:40,361][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0_piece0 stored as bytes in memory (estimated size 2007.0 B, free 912.3 MB)
[INFO][2018-05-24 19:50:40,363][org.apache.spark.storage.BlockManagerInfo]Added broadcast_0_piece0 in memory on 192.168.0.102:65423 (size: 2007.0 B, free: 912.3 MB)
[INFO][2018-05-24 19:50:40,365][org.apache.spark.SparkContext]Created broadcast 0 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 19:50:40,377][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at map at ReceiveKafkaData.scala:62) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 19:50:40,377][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 0.0 with 1 tasks
[INFO][2018-05-24 19:50:40,414][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 19:50:40,424][org.apache.spark.executor.Executor]Running task 0.0 in stage 0.0 (TID 0)
[INFO][2018-05-24 19:50:40,460][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 11876 is the same as ending offset skipping seven 0
[INFO][2018-05-24 19:50:40,622][org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper]Process identifier=hconnection-0x2071f062 connecting to ZooKeeper ensemble=vm-xaj-bigdata-da-d01:2181,vm-xaj-bigdata-da-d02:2181,vm-xaj-bigdata-da-d03:2181
[INFO][2018-05-24 19:50:40,629][org.apache.zookeeper.ZooKeeper]Client environment:zookeeper.version=3.4.5-cdh5.7.0--1, built on 04/06/2016 14:24 GMT
[INFO][2018-05-24 19:50:40,629][org.apache.zookeeper.ZooKeeper]Client environment:host.name=192.168.0.102
[INFO][2018-05-24 19:50:40,629][org.apache.zookeeper.ZooKeeper]Client environment:java.version=1.8.0_161
[INFO][2018-05-24 19:50:40,629][org.apache.zookeeper.ZooKeeper]Client environment:java.vendor=Oracle Corporation
[INFO][2018-05-24 19:50:40,629][org.apache.zookeeper.ZooKeeper]Client environment:java.home=/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre
[INFO][2018-05-24 19:50:40,629][org.apache.zookeeper.ZooKeeper]Client environment:java.class.path=/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/charsets.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/deploy.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/cldrdata.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/dnsns.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/jaccess.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/jfxrt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/localedata.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/nashorn.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/sunec.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/sunjce_provider.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/sunpkcs11.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/zipfs.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/javaws.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/jce.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/jfr.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/jfxswt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/jsse.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/management-agent.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/plugin.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/resources.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/rt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/ant-javafx.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/dt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/javafx-mx.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/jconsole.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/packager.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/sa-jdi.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/tools.jar:/Users/seven/project/scala/dataMining/target/classes:/Users/seven/software/maven/repository/org/scala-lang/scala-library/2.11.12/scala-library-2.11.12.jar:/Users/seven/software/maven/repository/org/scala-lang/scala-compiler/2.11.12/scala-compiler-2.11.12.jar:/Users/seven/software/maven/repository/org/scala-lang/modules/scala-xml_2.11/1.0.5/scala-xml_2.11-1.0.5.jar:/Users/seven/software/maven/repository/org/scala-lang/modules/scala-parser-combinators_2.11/1.0.4/scala-parser-combinators_2.11-1.0.4.jar:/Users/seven/software/maven/repository/org/scala-lang/scala-reflect/2.11.12/scala-reflect-2.11.12.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-client/2.6.0-cdh5.7.0/hadoop-client-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-common/2.6.0-cdh5.7.0/hadoop-common-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/xmlenc/xmlenc/0.52/xmlenc-0.52.jar:/Users/seven/software/maven/repository/commons-net/commons-net/3.1/commons-net-3.1.jar:/Users/seven/software/maven/repository/commons-configuration/commons-configuration/1.6/commons-configuration-1.6.jar:/Users/seven/software/maven/repository/commons-digester/commons-digester/1.8/commons-digester-1.8.jar:/Users/seven/software/maven/repository/commons-beanutils/commons-beanutils/1.7.0/commons-beanutils-1.7.0.jar:/Users/seven/software/maven/repository/org/slf4j/slf4j-api/1.7.5/slf4j-api-1.7.5.jar:/Users/seven/software/maven/repository/org/slf4j/slf4j-log4j12/1.7.5/slf4j-log4j12-1.7.5.jar:/Users/seven/software/maven/repository/org/apache/avro/avro/1.7.6-cdh5.7.0/avro-1.7.6-cdh5.7.0.jar:/Users/seven/software/maven/repository/com/google/code/gson/gson/2.2.4/gson-2.2.4.jar:/Users/seven/software/maven/repository/org/apache/curator/curator-client/2.7.1/curator-client-2.7.1.jar:/Users/seven/software/maven/repository/org/apache/curator/curator-recipes/2.7.1/curator-recipes-2.7.1.jar:/Users/seven/software/maven/repository/com/google/code/findbugs/jsr305/3.0.0/jsr305-3.0.0.jar:/Users/seven/software/maven/repository/org/apache/htrace/htrace-core4/4.0.1-incubating/htrace-core4-4.0.1-incubating.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-compress/1.4.1/commons-compress-1.4.1.jar:/Users/seven/software/maven/repository/org/tukaani/xz/1.0/xz-1.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-hdfs/2.6.0-cdh5.7.0/hadoop-hdfs-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/io/netty/netty/3.6.2.Final/netty-3.6.2.Final.jar:/Users/seven/software/maven/repository/xerces/xercesImpl/2.9.1/xercesImpl-2.9.1.jar:/Users/seven/software/maven/repository/xml-apis/xml-apis/1.3.04/xml-apis-1.3.04.jar:/Users/seven/software/maven/repository/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-app/2.6.0-cdh5.7.0/hadoop-mapreduce-client-app-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-common/2.6.0-cdh5.7.0/hadoop-mapreduce-client-common-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-yarn-client/2.6.0-cdh5.7.0/hadoop-yarn-client-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-yarn-server-common/2.6.0-cdh5.7.0/hadoop-yarn-server-common-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-shuffle/2.6.0-cdh5.7.0/hadoop-mapreduce-client-shuffle-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-yarn-api/2.6.0-cdh5.7.0/hadoop-yarn-api-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-core/2.6.0-cdh5.7.0/hadoop-mapreduce-client-core-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-yarn-common/2.6.0-cdh5.7.0/hadoop-yarn-common-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/javax/xml/bind/jaxb-api/2.2.2/jaxb-api-2.2.2.jar:/Users/seven/software/maven/repository/javax/xml/stream/stax-api/1.0-2/stax-api-1.0-2.jar:/Users/seven/software/maven/repository/javax/activation/activation/1.1/activation-1.1.jar:/Users/seven/software/maven/repository/com/sun/jersey/jersey-client/1.9/jersey-client-1.9.jar:/Users/seven/software/maven/repository/org/codehaus/jackson/jackson-xc/1.8.8/jackson-xc-1.8.8.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.6.0-cdh5.7.0/hadoop-mapreduce-client-jobclient-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-aws/2.6.0-cdh5.7.0/hadoop-aws-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/com/amazonaws/aws-java-sdk-s3/1.10.6/aws-java-sdk-s3-1.10.6.jar:/Users/seven/software/maven/repository/com/amazonaws/aws-java-sdk-kms/1.10.6/aws-java-sdk-kms-1.10.6.jar:/Users/seven/software/maven/repository/com/amazonaws/aws-java-sdk-core/1.10.6/aws-java-sdk-core-1.10.6.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-annotations/2.6.0-cdh5.7.0/hadoop-annotations-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-client/1.2.0-cdh5.7.0/hbase-client-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-annotations/1.2.0-cdh5.7.0/hbase-annotations-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-common/1.2.0-cdh5.7.0/hbase-common-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-protocol/1.2.0-cdh5.7.0/hbase-protocol-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/commons-codec/commons-codec/1.9/commons-codec-1.9.jar:/Users/seven/software/maven/repository/commons-io/commons-io/2.4/commons-io-2.4.jar:/Users/seven/software/maven/repository/commons-lang/commons-lang/2.6/commons-lang-2.6.jar:/Users/seven/software/maven/repository/commons-logging/commons-logging/1.2/commons-logging-1.2.jar:/Users/seven/software/maven/repository/com/google/guava/guava/12.0.1/guava-12.0.1.jar:/Users/seven/software/maven/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/Users/seven/software/maven/repository/io/netty/netty-all/4.0.23.Final/netty-all-4.0.23.Final.jar:/Users/seven/software/maven/repository/org/apache/zookeeper/zookeeper/3.4.5-cdh5.7.0/zookeeper-3.4.5-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/htrace/htrace-core/3.2.0-incubating/htrace-core-3.2.0-incubating.jar:/Users/seven/software/maven/repository/org/codehaus/jackson/jackson-mapper-asl/1.8.8/jackson-mapper-asl-1.8.8.jar:/Users/seven/software/maven/repository/org/jruby/jcodings/jcodings/1.0.8/jcodings-1.0.8.jar:/Users/seven/software/maven/repository/org/jruby/joni/joni/2.1.2/joni-2.1.2.jar:/Users/seven/software/maven/repository/com/yammer/metrics/metrics-core/2.2.0/metrics-core-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-auth/2.6.0-cdh5.7.0/hadoop-auth-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/httpcomponents/httpclient/4.2.5/httpclient-4.2.5.jar:/Users/seven/software/maven/repository/org/apache/httpcomponents/httpcore/4.2.4/httpcore-4.2.4.jar:/Users/seven/software/maven/repository/org/apache/directory/server/apacheds-kerberos-codec/2.0.0-M15/apacheds-kerberos-codec-2.0.0-M15.jar:/Users/seven/software/maven/repository/org/apache/directory/server/apacheds-i18n/2.0.0-M15/apacheds-i18n-2.0.0-M15.jar:/Users/seven/software/maven/repository/org/apache/directory/api/api-asn1-api/1.0.0-M20/api-asn1-api-1.0.0-M20.jar:/Users/seven/software/maven/repository/org/apache/directory/api/api-util/1.0.0-M20/api-util-1.0.0-M20.jar:/Users/seven/software/maven/repository/org/apache/curator/curator-framework/2.7.1/curator-framework-2.7.1.jar:/Users/seven/software/maven/repository/com/github/stephenc/findbugs/findbugs-annotations/1.3.9-1/findbugs-annotations-1.3.9-1.jar:/Users/seven/software/maven/repository/junit/junit/4.12/junit-4.12.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-server/1.2.0-cdh5.7.0/hbase-server-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-procedure/1.2.0-cdh5.7.0/hbase-procedure-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-common/1.2.0-cdh5.7.0/hbase-common-1.2.0-cdh5.7.0-tests.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-prefix-tree/1.2.0-cdh5.7.0/hbase-prefix-tree-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar:/Users/seven/software/maven/repository/commons-collections/commons-collections/3.2.2/commons-collections-3.2.2.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-hadoop-compat/1.2.0-cdh5.7.0/hbase-hadoop-compat-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-hadoop2-compat/1.2.0-cdh5.7.0/hbase-hadoop2-compat-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/com/sun/jersey/jersey-core/1.9/jersey-core-1.9.jar:/Users/seven/software/maven/repository/com/sun/jersey/jersey-server/1.9/jersey-server-1.9.jar:/Users/seven/software/maven/repository/asm/asm/3.1/asm-3.1.jar:/Users/seven/software/maven/repository/commons-cli/commons-cli/1.2/commons-cli-1.2.jar:/Users/seven/software/maven/repository/com/github/stephenc/high-scale-lib/high-scale-lib/1.1.1/high-scale-lib-1.1.1.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-math/2.1/commons-math-2.1.jar:/Users/seven/software/maven/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jetty/6.1.26.cloudera.4/jetty-6.1.26.cloudera.4.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jetty-util/6.1.26.cloudera.4/jetty-util-6.1.26.cloudera.4.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jetty-sslengine/6.1.26.cloudera.4/jetty-sslengine-6.1.26.cloudera.4.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jsp-2.1/6.1.14/jsp-2.1-6.1.14.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jsp-api-2.1/6.1.14/jsp-api-2.1-6.1.14.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/servlet-api-2.5/6.1.14/servlet-api-2.5-6.1.14.jar:/Users/seven/software/maven/repository/org/codehaus/jackson/jackson-core-asl/1.8.8/jackson-core-asl-1.8.8.jar:/Users/seven/software/maven/repository/org/codehaus/jackson/jackson-jaxrs/1.8.8/jackson-jaxrs-1.8.8.jar:/Users/seven/software/maven/repository/tomcat/jasper-compiler/5.5.23/jasper-compiler-5.5.23.jar:/Users/seven/software/maven/repository/tomcat/jasper-runtime/5.5.23/jasper-runtime-5.5.23.jar:/Users/seven/software/maven/repository/commons-el/commons-el/1.0/commons-el-1.0.jar:/Users/seven/software/maven/repository/org/jamon/jamon-runtime/2.4.1/jamon-runtime-2.4.1.jar:/Users/seven/software/maven/repository/com/lmax/disruptor/3.3.0/disruptor-3.3.0.jar:/Users/seven/software/maven/repository/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar:/Users/seven/software/maven/repository/org/owasp/esapi/esapi/2.1.0/esapi-2.1.0.jar:/Users/seven/software/maven/repository/commons-beanutils/commons-beanutils-core/1.7.0/commons-beanutils-core-1.7.0.jar:/Users/seven/software/maven/repository/commons-fileupload/commons-fileupload/1.2/commons-fileupload-1.2.jar:/Users/seven/software/maven/repository/xom/xom/1.2.5/xom-1.2.5.jar:/Users/seven/software/maven/repository/xalan/xalan/2.7.0/xalan-2.7.0.jar:/Users/seven/software/maven/repository/org/beanshell/bsh-core/2.0b4/bsh-core-2.0b4.jar:/Users/seven/software/maven/repository/org/owasp/antisamy/antisamy/1.4.3/antisamy-1.4.3.jar:/Users/seven/software/maven/repository/org/apache/xmlgraphics/batik-css/1.7/batik-css-1.7.jar:/Users/seven/software/maven/repository/org/apache/xmlgraphics/batik-ext/1.7/batik-ext-1.7.jar:/Users/seven/software/maven/repository/org/apache/xmlgraphics/batik-util/1.7/batik-util-1.7.jar:/Users/seven/software/maven/repository/xml-apis/xml-apis-ext/1.3.04/xml-apis-ext-1.3.04.jar:/Users/seven/software/maven/repository/net/sourceforge/nekohtml/nekohtml/1.9.12/nekohtml-1.9.12.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-core/2.6.0-mr1-cdh5.7.0/hadoop-core-2.6.0-mr1-cdh5.7.0.jar:/Users/seven/software/maven/repository/javax/servlet/jsp/jsp-api/2.1/jsp-api-2.1.jar:/Users/seven/software/maven/repository/net/java/dev/jets3t/jets3t/0.6.1/jets3t-0.6.1.jar:/Users/seven/software/maven/repository/javax/servlet/servlet-api/2.5/servlet-api-2.5.jar:/Users/seven/software/maven/repository/hsqldb/hsqldb/1.8.0.10/hsqldb-1.8.0.10.jar:/Users/seven/software/maven/repository/org/eclipse/jdt/core/3.1.1/core-3.1.1.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-hdfs/2.6.0-cdh5.7.0/hadoop-hdfs-2.6.0-cdh5.7.0-tests.jar:/Users/seven/software/maven/repository/commons-daemon/commons-daemon/1.0.13/commons-daemon-1.0.13.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-lang3/3.5/commons-lang3-3.5.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-pool2/2.4.2/commons-pool2-2.4.2.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-mllib_2.11/2.2.0/spark-mllib_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-core_2.11/2.2.0/spark-core_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/avro/avro-mapred/1.7.7/avro-mapred-1.7.7-hadoop2.jar:/Users/seven/software/maven/repository/org/apache/avro/avro-ipc/1.7.7/avro-ipc-1.7.7-tests.jar:/Users/seven/software/maven/repository/com/twitter/chill_2.11/0.8.0/chill_2.11-0.8.0.jar:/Users/seven/software/maven/repository/com/esotericsoftware/kryo-shaded/3.0.3/kryo-shaded-3.0.3.jar:/Users/seven/software/maven/repository/com/esotericsoftware/minlog/1.3.0/minlog-1.3.0.jar:/Users/seven/software/maven/repository/org/objenesis/objenesis/2.1/objenesis-2.1.jar:/Users/seven/software/maven/repository/com/twitter/chill-java/0.8.0/chill-java-0.8.0.jar:/Users/seven/software/maven/repository/org/apache/xbean/xbean-asm5-shaded/4.4/xbean-asm5-shaded-4.4.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-launcher_2.11/2.2.0/spark-launcher_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-network-common_2.11/2.2.0/spark-network-common_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-network-shuffle_2.11/2.2.0/spark-network-shuffle_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-unsafe_2.11/2.2.0/spark-unsafe_2.11-2.2.0.jar:/Users/seven/software/maven/repository/javax/servlet/javax.servlet-api/3.1.0/javax.servlet-api-3.1.0.jar:/Users/seven/software/maven/repository/org/slf4j/jul-to-slf4j/1.7.16/jul-to-slf4j-1.7.16.jar:/Users/seven/software/maven/repository/org/slf4j/jcl-over-slf4j/1.7.16/jcl-over-slf4j-1.7.16.jar:/Users/seven/software/maven/repository/com/ning/compress-lzf/1.0.3/compress-lzf-1.0.3.jar:/Users/seven/software/maven/repository/org/xerial/snappy/snappy-java/1.1.2.6/snappy-java-1.1.2.6.jar:/Users/seven/software/maven/repository/org/roaringbitmap/RoaringBitmap/0.5.11/RoaringBitmap-0.5.11.jar:/Users/seven/software/maven/repository/org/json4s/json4s-jackson_2.11/3.2.11/json4s-jackson_2.11-3.2.11.jar:/Users/seven/software/maven/repository/org/json4s/json4s-core_2.11/3.2.11/json4s-core_2.11-3.2.11.jar:/Users/seven/software/maven/repository/org/json4s/json4s-ast_2.11/3.2.11/json4s-ast_2.11-3.2.11.jar:/Users/seven/software/maven/repository/org/scala-lang/scalap/2.11.0/scalap-2.11.0.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/core/jersey-client/2.22.2/jersey-client-2.22.2.jar:/Users/seven/software/maven/repository/javax/ws/rs/javax.ws.rs-api/2.0.1/javax.ws.rs-api-2.0.1.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/hk2-api/2.4.0-b34/hk2-api-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/hk2-utils/2.4.0-b34/hk2-utils-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/external/aopalliance-repackaged/2.4.0-b34/aopalliance-repackaged-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/external/javax.inject/2.4.0-b34/javax.inject-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/hk2-locator/2.4.0-b34/hk2-locator-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/javassist/javassist/3.18.1-GA/javassist-3.18.1-GA.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/core/jersey-common/2.22.2/jersey-common-2.22.2.jar:/Users/seven/software/maven/repository/javax/annotation/javax.annotation-api/1.2/javax.annotation-api-1.2.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/bundles/repackaged/jersey-guava/2.22.2/jersey-guava-2.22.2.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/osgi-resource-locator/1.0.1/osgi-resource-locator-1.0.1.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/core/jersey-server/2.22.2/jersey-server-2.22.2.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/media/jersey-media-jaxb/2.22.2/jersey-media-jaxb-2.22.2.jar:/Users/seven/software/maven/repository/javax/validation/validation-api/1.1.0.Final/validation-api-1.1.0.Final.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/containers/jersey-container-servlet/2.22.2/jersey-container-servlet-2.22.2.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/containers/jersey-container-servlet-core/2.22.2/jersey-container-servlet-core-2.22.2.jar:/Users/seven/software/maven/repository/com/clearspring/analytics/stream/2.7.0/stream-2.7.0.jar:/Users/seven/software/maven/repository/io/dropwizard/metrics/metrics-core/3.1.2/metrics-core-3.1.2.jar:/Users/seven/software/maven/repository/io/dropwizard/metrics/metrics-jvm/3.1.2/metrics-jvm-3.1.2.jar:/Users/seven/software/maven/repository/io/dropwizard/metrics/metrics-json/3.1.2/metrics-json-3.1.2.jar:/Users/seven/software/maven/repository/io/dropwizard/metrics/metrics-graphite/3.1.2/metrics-graphite-3.1.2.jar:/Users/seven/software/maven/repository/org/apache/ivy/ivy/2.4.0/ivy-2.4.0.jar:/Users/seven/software/maven/repository/oro/oro/2.0.8/oro-2.0.8.jar:/Users/seven/software/maven/repository/net/razorvine/pyrolite/4.13/pyrolite-4.13.jar:/Users/seven/software/maven/repository/net/sf/py4j/py4j/0.10.4/py4j-0.10.4.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-crypto/1.0.0/commons-crypto-1.0.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-streaming_2.11/2.2.0/spark-streaming_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-sql_2.11/2.2.0/spark-sql_2.11-2.2.0.jar:/Users/seven/software/maven/repository/com/univocity/univocity-parsers/2.2.1/univocity-parsers-2.2.1.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-sketch_2.11/2.2.0/spark-sketch_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-catalyst_2.11/2.2.0/spark-catalyst_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/codehaus/janino/janino/3.0.0/janino-3.0.0.jar:/Users/seven/software/maven/repository/org/codehaus/janino/commons-compiler/3.0.0/commons-compiler-3.0.0.jar:/Users/seven/software/maven/repository/org/antlr/antlr4-runtime/4.5.3/antlr4-runtime-4.5.3.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-column/1.8.2/parquet-column-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-common/1.8.2/parquet-common-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-encoding/1.8.2/parquet-encoding-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-hadoop/1.8.2/parquet-hadoop-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-format/2.3.1/parquet-format-2.3.1.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-jackson/1.8.2/parquet-jackson-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-graphx_2.11/2.2.0/spark-graphx_2.11-2.2.0.jar:/Users/seven/software/maven/repository/com/github/fommil/netlib/core/1.1.2/core-1.1.2.jar:/Users/seven/software/maven/repository/net/sourceforge/f2j/arpack_combined_all/0.1/arpack_combined_all-0.1.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-mllib-local_2.11/2.2.0/spark-mllib-local_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/scalanlp/breeze_2.11/0.13.1/breeze_2.11-0.13.1.jar:/Users/seven/software/maven/repository/org/scalanlp/breeze-macros_2.11/0.13.1/breeze-macros_2.11-0.13.1.jar:/Users/seven/software/maven/repository/net/sf/opencsv/opencsv/2.3/opencsv-2.3.jar:/Users/seven/software/maven/repository/com/github/rwl/jtransforms/2.4.0/jtransforms-2.4.0.jar:/Users/seven/software/maven/repository/org/spire-math/spire_2.11/0.13.0/spire_2.11-0.13.0.jar:/Users/seven/software/maven/repository/org/spire-math/spire-macros_2.11/0.13.0/spire-macros_2.11-0.13.0.jar:/Users/seven/software/maven/repository/org/typelevel/machinist_2.11/0.6.1/machinist_2.11-0.6.1.jar:/Users/seven/software/maven/repository/com/chuusai/shapeless_2.11/2.3.2/shapeless_2.11-2.3.2.jar:/Users/seven/software/maven/repository/org/typelevel/macro-compat_2.11/1.1.1/macro-compat_2.11-1.1.1.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-math3/3.4.1/commons-math3-3.4.1.jar:/Users/seven/software/maven/repository/org/jpmml/pmml-model/1.2.15/pmml-model-1.2.15.jar:/Users/seven/software/maven/repository/org/jpmml/pmml-schema/1.2.15/pmml-schema-1.2.15.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-tags_2.11/2.2.0/spark-tags_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-streaming-kafka-0-8_2.11/2.2.0/spark-streaming-kafka-0-8_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/kafka/kafka_2.11/0.8.2.1/kafka_2.11-0.8.2.1.jar:/Users/seven/software/maven/repository/com/101tec/zkclient/0.3/zkclient-0.3.jar:/Users/seven/software/maven/repository/org/apache/kafka/kafka-clients/0.8.2.1/kafka-clients-0.8.2.1.jar:/Users/seven/software/maven/repository/mysql/mysql-connector-java/5.1.31/mysql-connector-java-5.1.31.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/module/jackson-module-scala_2.11/2.6.5/jackson-module-scala_2.11-2.6.5.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/core/jackson-core/2.6.5/jackson-core-2.6.5.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/core/jackson-annotations/2.6.5/jackson-annotations-2.6.5.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/core/jackson-databind/2.6.5/jackson-databind-2.6.5.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/module/jackson-module-paranamer/2.6.5/jackson-module-paranamer-2.6.5.jar:/Users/seven/software/maven/repository/com/thoughtworks/paranamer/paranamer/2.6/paranamer-2.6.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-streaming-flume_2.11/2.2.0/spark-streaming-flume_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/flume/flume-ng-core/1.6.0/flume-ng-core-1.6.0.jar:/Users/seven/software/maven/repository/org/apache/flume/flume-ng-configuration/1.6.0/flume-ng-configuration-1.6.0.jar:/Users/seven/software/maven/repository/org/apache/avro/avro-ipc/1.7.4/avro-ipc-1.7.4.jar:/Users/seven/software/maven/repository/org/apache/velocity/velocity/1.7/velocity-1.7.jar:/Users/seven/software/maven/repository/joda-time/joda-time/2.1/joda-time-2.1.jar:/Users/seven/software/maven/repository/org/apache/mina/mina-core/2.0.4/mina-core-2.0.4.jar:/Users/seven/software/maven/repository/org/apache/flume/flume-ng-sdk/1.6.0/flume-ng-sdk-1.6.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-streaming-flume-sink_2.11/2.2.0/spark-streaming-flume-sink_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/flume/flume-ng-clients/flume-ng-log4jappender/1.6.0/flume-ng-log4jappender-1.6.0.jar:/Users/seven/software/maven/repository/net/jpountz/lz4/lz4/1.3.0/lz4-1.3.0.jar:/Applications/IntelliJ IDEA.app/Contents/lib/idea_rt.jar
[INFO][2018-05-24 19:50:40,630][org.apache.zookeeper.ZooKeeper]Client environment:java.library.path=/Users/seven/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:.
[INFO][2018-05-24 19:50:40,631][org.apache.zookeeper.ZooKeeper]Client environment:java.io.tmpdir=/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/
[INFO][2018-05-24 19:50:40,631][org.apache.zookeeper.ZooKeeper]Client environment:java.compiler=<NA>
[INFO][2018-05-24 19:50:40,631][org.apache.zookeeper.ZooKeeper]Client environment:os.name=Mac OS X
[INFO][2018-05-24 19:50:40,631][org.apache.zookeeper.ZooKeeper]Client environment:os.arch=x86_64
[INFO][2018-05-24 19:50:40,631][org.apache.zookeeper.ZooKeeper]Client environment:os.version=10.13.4
[INFO][2018-05-24 19:50:40,631][org.apache.zookeeper.ZooKeeper]Client environment:user.name=seven
[INFO][2018-05-24 19:50:40,631][org.apache.zookeeper.ZooKeeper]Client environment:user.home=/Users/seven
[INFO][2018-05-24 19:50:40,631][org.apache.zookeeper.ZooKeeper]Client environment:user.dir=/Users/seven/project/scala/dataMining
[INFO][2018-05-24 19:50:40,633][org.apache.zookeeper.ZooKeeper]Initiating client connection, connectString=vm-xaj-bigdata-da-d01:2181,vm-xaj-bigdata-da-d02:2181,vm-xaj-bigdata-da-d03:2181 sessionTimeout=60000 watcher=hconnection-0x2071f0620x0, quorum=vm-xaj-bigdata-da-d01:2181,vm-xaj-bigdata-da-d02:2181,vm-xaj-bigdata-da-d03:2181, baseZNode=/hbase
[INFO][2018-05-24 19:50:55,668][org.apache.zookeeper.ClientCnxn]Opening socket connection to server master/10.213.4.25:2181. Will not attempt to authenticate using SASL (unknown error)
[INFO][2018-05-24 19:50:55,684][org.apache.zookeeper.ClientCnxn]Socket connection established, initiating session, client: /10.0.8.39:65438, server: master/10.213.4.25:2181
[INFO][2018-05-24 19:50:55,709][org.apache.zookeeper.ClientCnxn]Session establishment complete on server master/10.213.4.25:2181, sessionid = 0x162b4dc56095e73, negotiated timeout = 60000
[WARN][2018-05-24 19:50:56,141][org.apache.hadoop.util.NativeCodeLoader]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO][2018-05-24 19:50:56,234][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 19:50:56,247][org.apache.spark.executor.Executor]Finished task 0.0 in stage 0.0 (TID 0). 708 bytes result sent to driver
[INFO][2018-05-24 19:50:56,254][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 0.0 (TID 0) in 15850 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 19:50:56,256][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 19:50:56,259][org.apache.spark.scheduler.DAGScheduler]ResultStage 0 (foreachPartition at ReceiveKafkaData.scala:64) finished in 15.867 s
[INFO][2018-05-24 19:50:56,265][org.apache.spark.scheduler.DAGScheduler]Job 0 finished: foreachPartition at ReceiveKafkaData.scala:64, took 16.088680 s
[INFO][2018-05-24 19:50:56,269][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527162640000 ms.0 from job set of time 1527162640000 ms
[INFO][2018-05-24 19:50:56,270][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 16.269 s for time 1527162640000 ms (execution: 16.139 s)
[INFO][2018-05-24 19:50:56,276][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 19:50:56,279][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 
[INFO][2018-05-24 19:51:01,585][org.apache.spark.SparkContext]Running Spark version 2.2.0
[INFO][2018-05-24 19:51:02,258][org.apache.spark.SparkContext]Submitted application: SimulationKafkaSendOutData$
[INFO][2018-05-24 19:51:02,283][org.apache.spark.SecurityManager]Changing view acls to: seven
[INFO][2018-05-24 19:51:02,284][org.apache.spark.SecurityManager]Changing modify acls to: seven
[INFO][2018-05-24 19:51:02,284][org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO][2018-05-24 19:51:02,285][org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO][2018-05-24 19:51:02,286][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(seven); groups with view permissions: Set(); users  with modify permissions: Set(seven); groups with modify permissions: Set()
[INFO][2018-05-24 19:51:02,559][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 65442.
[INFO][2018-05-24 19:51:02,588][org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO][2018-05-24 19:51:02,607][org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO][2018-05-24 19:51:02,610][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2018-05-24 19:51:02,611][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
[INFO][2018-05-24 19:51:02,620][org.apache.spark.storage.DiskBlockManager]Created local directory at /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/blockmgr-99d8a8aa-f2b9-4f07-b002-adbc02407d6a
[INFO][2018-05-24 19:51:02,639][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 912.3 MB
[INFO][2018-05-24 19:51:02,690][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO][2018-05-24 19:51:02,809][org.spark_project.jetty.util.log]Logging initialized @2106ms
[INFO][2018-05-24 19:51:02,864][org.spark_project.jetty.server.Server]jetty-9.3.z-SNAPSHOT
[INFO][2018-05-24 19:51:02,876][org.spark_project.jetty.server.Server]Started @2174ms
[WARN][2018-05-24 19:51:02,891][org.apache.spark.util.Utils]Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[INFO][2018-05-24 19:51:02,896][org.spark_project.jetty.server.AbstractConnector]Started ServerConnector@346a361{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}
[INFO][2018-05-24 19:51:02,896][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4041.
[INFO][2018-05-24 19:51:02,917][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@42b64ab8{/jobs,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:51:02,917][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2c1b9e4b{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:51:02,918][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@649725e3{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:51:02,919][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@52b56a3e{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:51:02,920][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4eed46ee{/stages,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:51:02,920][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4fad94a7{/stages/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:51:02,921][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6326d182{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:51:02,922][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@226642a5{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:51:02,923][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5cc126dc{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:51:02,923][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@72bd06ca{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:51:02,924][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5dbe30be{/storage,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:51:02,925][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@55795845{/storage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:51:02,926][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@119f1f2a{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:51:02,926][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5b970f7{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:51:02,927][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@165b8a71{/environment,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:51:02,928][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2f058b8a{/environment/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:51:02,929][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3f2ef586{/executors,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:51:02,929][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@76c7beb3{/executors/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:51:02,930][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2cf92cc7{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:51:02,931][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7b139eab{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:51:02,937][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@611df6e3{/static,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:51:02,938][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6ffab045{/,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:51:02,939][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3e2943ab{/api,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:51:02,941][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6c67e137{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:51:02,944][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@191ae03f{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:51:02,947][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://192.168.0.102:4041
[INFO][2018-05-24 19:51:03,068][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
[INFO][2018-05-24 19:51:03,113][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 65443.
[INFO][2018-05-24 19:51:03,114][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 192.168.0.102:65443
[INFO][2018-05-24 19:51:03,115][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2018-05-24 19:51:03,117][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 192.168.0.102, 65443, None)
[INFO][2018-05-24 19:51:03,121][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 192.168.0.102:65443 with 912.3 MB RAM, BlockManagerId(driver, 192.168.0.102, 65443, None)
[INFO][2018-05-24 19:51:03,127][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 192.168.0.102, 65443, None)
[INFO][2018-05-24 19:51:03,129][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 192.168.0.102, 65443, None)
[INFO][2018-05-24 19:51:03,364][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3e1162e7{/metrics/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:51:03,906][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0 stored as values in memory (estimated size 228.1 KB, free 912.1 MB)
[INFO][2018-05-24 19:51:03,979][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0_piece0 stored as bytes in memory (estimated size 22.1 KB, free 912.1 MB)
[INFO][2018-05-24 19:51:03,981][org.apache.spark.storage.BlockManagerInfo]Added broadcast_0_piece0 in memory on 192.168.0.102:65443 (size: 22.1 KB, free: 912.3 MB)
[INFO][2018-05-24 19:51:03,986][org.apache.spark.SparkContext]Created broadcast 0 from textFile at SimulationKafkaSendOutData.scala:25
[WARN][2018-05-24 19:51:09,414][org.apache.hadoop.util.NativeCodeLoader]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO][2018-05-24 19:51:09,547][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 1
[INFO][2018-05-24 19:51:09,646][org.apache.spark.SparkContext]Starting job: collect at SimulationKafkaSendOutData.scala:25
[INFO][2018-05-24 19:51:09,664][org.apache.spark.scheduler.DAGScheduler]Got job 0 (collect at SimulationKafkaSendOutData.scala:25) with 2 output partitions
[INFO][2018-05-24 19:51:09,665][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 0 (collect at SimulationKafkaSendOutData.scala:25)
[INFO][2018-05-24 19:51:09,665][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 19:51:09,667][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 19:51:09,676][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 0 (hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/* MapPartitionsRDD[1] at textFile at SimulationKafkaSendOutData.scala:25), which has no missing parents
[INFO][2018-05-24 19:51:09,691][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1 stored as values in memory (estimated size 3.3 KB, free 912.1 MB)
[INFO][2018-05-24 19:51:09,693][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1_piece0 stored as bytes in memory (estimated size 1969.0 B, free 912.1 MB)
[INFO][2018-05-24 19:51:09,693][org.apache.spark.storage.BlockManagerInfo]Added broadcast_1_piece0 in memory on 192.168.0.102:65443 (size: 1969.0 B, free: 912.3 MB)
[INFO][2018-05-24 19:51:09,694][org.apache.spark.SparkContext]Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 19:51:09,707][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ResultStage 0 (hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/* MapPartitionsRDD[1] at textFile at SimulationKafkaSendOutData.scala:25) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2018-05-24 19:51:09,708][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 0.0 with 2 tasks
[INFO][2018-05-24 19:51:09,751][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 4883 bytes)
[INFO][2018-05-24 19:51:09,763][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 4883 bytes)
[INFO][2018-05-24 19:51:09,775][org.apache.spark.executor.Executor]Running task 1.0 in stage 0.0 (TID 1)
[INFO][2018-05-24 19:51:09,775][org.apache.spark.executor.Executor]Running task 0.0 in stage 0.0 (TID 0)
[INFO][2018-05-24 19:51:09,868][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/part-00000:10717435+10717436
[INFO][2018-05-24 19:51:09,869][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/part-00000:0+10717435
[INFO][2018-05-24 19:51:10,320][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527162660000 ms
[INFO][2018-05-24 19:51:10,321][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527162660000 ms.0 from job set of time 1527162660000 ms
[INFO][2018-05-24 19:51:10,333][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:64
[INFO][2018-05-24 19:51:10,334][org.apache.spark.scheduler.DAGScheduler]Got job 1 (foreachPartition at ReceiveKafkaData.scala:64) with 1 output partitions
[INFO][2018-05-24 19:51:10,334][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 1 (foreachPartition at ReceiveKafkaData.scala:64)
[INFO][2018-05-24 19:51:10,334][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 19:51:10,335][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 19:51:10,335][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 1 (MapPartitionsRDD[3] at map at ReceiveKafkaData.scala:62), which has no missing parents
[INFO][2018-05-24 19:51:10,338][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-24 19:51:10,346][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1_piece0 stored as bytes in memory (estimated size 2008.0 B, free 912.3 MB)
[INFO][2018-05-24 19:51:10,347][org.apache.spark.storage.BlockManagerInfo]Added broadcast_1_piece0 in memory on 192.168.0.102:65423 (size: 2008.0 B, free: 912.3 MB)
[INFO][2018-05-24 19:51:10,348][org.apache.spark.SparkContext]Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 19:51:10,350][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at map at ReceiveKafkaData.scala:62) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 19:51:10,350][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 1.0 with 1 tasks
[INFO][2018-05-24 19:51:10,352][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 19:51:10,353][org.apache.spark.executor.Executor]Running task 0.0 in stage 1.0 (TID 1)
[INFO][2018-05-24 19:51:10,394][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 11876 is the same as ending offset skipping seven 0
[INFO][2018-05-24 19:51:10,394][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 19:51:10,396][org.apache.spark.executor.Executor]Finished task 0.0 in stage 1.0 (TID 1). 708 bytes result sent to driver
[INFO][2018-05-24 19:51:10,397][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 1.0 (TID 1) in 46 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 19:51:10,397][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 19:51:10,398][org.apache.spark.scheduler.DAGScheduler]ResultStage 1 (foreachPartition at ReceiveKafkaData.scala:64) finished in 0.047 s
[INFO][2018-05-24 19:51:10,398][org.apache.spark.scheduler.DAGScheduler]Job 1 finished: foreachPartition at ReceiveKafkaData.scala:64, took 0.065096 s
[INFO][2018-05-24 19:51:10,399][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527162660000 ms.0 from job set of time 1527162660000 ms
[INFO][2018-05-24 19:51:10,400][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 10.399 s for time 1527162660000 ms (execution: 0.078 s)
[INFO][2018-05-24 19:51:10,400][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 1 from persistence list
[INFO][2018-05-24 19:51:10,406][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 0 from persistence list
[INFO][2018-05-24 19:51:10,406][org.apache.spark.storage.BlockManager]Removing RDD 1
[INFO][2018-05-24 19:51:10,407][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 19:51:10,407][org.apache.spark.storage.BlockManager]Removing RDD 0
[INFO][2018-05-24 19:51:10,407][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 
[INFO][2018-05-24 19:51:12,232][org.apache.spark.storage.memory.MemoryStore]Block taskresult_0 stored as bytes in memory (estimated size 10.4 MB, free 901.7 MB)
[INFO][2018-05-24 19:51:12,234][org.apache.spark.storage.BlockManagerInfo]Added taskresult_0 in memory on 192.168.0.102:65443 (size: 10.4 MB, free: 901.9 MB)
[INFO][2018-05-24 19:51:12,235][org.apache.spark.executor.Executor]Finished task 0.0 in stage 0.0 (TID 0). 10856349 bytes result sent via BlockManager)
[INFO][2018-05-24 19:51:12,280][org.apache.spark.network.client.TransportClientFactory]Successfully created connection to /192.168.0.102:65443 after 30 ms (0 ms spent in bootstraps)
[INFO][2018-05-24 19:51:12,611][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 0.0 (TID 0) in 2874 ms on localhost (executor driver) (1/2)
[INFO][2018-05-24 19:51:12,613][org.apache.spark.storage.BlockManagerInfo]Removed taskresult_0 on 192.168.0.102:65443 in memory (size: 10.4 MB, free: 912.3 MB)
[INFO][2018-05-24 19:51:22,190][org.apache.spark.storage.memory.MemoryStore]Block taskresult_1 stored as bytes in memory (estimated size 10.4 MB, free 901.7 MB)
[INFO][2018-05-24 19:51:22,192][org.apache.spark.storage.BlockManagerInfo]Added taskresult_1 in memory on 192.168.0.102:65443 (size: 10.4 MB, free: 901.9 MB)
[INFO][2018-05-24 19:51:22,193][org.apache.spark.executor.Executor]Finished task 1.0 in stage 0.0 (TID 1). 10855899 bytes result sent via BlockManager)
[INFO][2018-05-24 19:51:22,252][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 0.0 (TID 1) in 12490 ms on localhost (executor driver) (2/2)
[INFO][2018-05-24 19:51:22,253][org.apache.spark.storage.BlockManagerInfo]Removed taskresult_1 on 192.168.0.102:65443 in memory (size: 10.4 MB, free: 912.3 MB)
[INFO][2018-05-24 19:51:22,254][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 19:51:22,255][org.apache.spark.scheduler.DAGScheduler]ResultStage 0 (collect at SimulationKafkaSendOutData.scala:25) finished in 12.532 s
[INFO][2018-05-24 19:51:22,259][org.apache.spark.scheduler.DAGScheduler]Job 0 finished: collect at SimulationKafkaSendOutData.scala:25, took 12.612373 s
[INFO][2018-05-24 19:51:22,394][org.spark_project.jetty.server.AbstractConnector]Stopped Spark@346a361{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}
[INFO][2018-05-24 19:51:22,396][org.apache.spark.ui.SparkUI]Stopped Spark web UI at http://192.168.0.102:4041
[INFO][2018-05-24 19:51:22,404][org.apache.spark.MapOutputTrackerMasterEndpoint]MapOutputTrackerMasterEndpoint stopped!
[INFO][2018-05-24 19:51:22,417][org.apache.spark.storage.memory.MemoryStore]MemoryStore cleared
[INFO][2018-05-24 19:51:22,418][org.apache.spark.storage.BlockManager]BlockManager stopped
[INFO][2018-05-24 19:51:22,419][org.apache.spark.storage.BlockManagerMaster]BlockManagerMaster stopped
[INFO][2018-05-24 19:51:22,420][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint]OutputCommitCoordinator stopped!
[INFO][2018-05-24 19:51:22,422][org.apache.spark.SparkContext]Successfully stopped SparkContext
[INFO][2018-05-24 19:51:22,433][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	compression.type = none
	metric.reporters = []
	metadata.max.age.ms = 300000
	metadata.fetch.timeout.ms = 60000
	acks = all
	batch.size = 16384
	reconnect.backoff.ms = 10
	bootstrap.servers = [vm-xaj-bigdata-da-d01:9092, vm-xaj-bigdata-da-d02:9092, vm-xaj-bigdata-da-d03:9092, vm-xaj-bigdata-da-d04:9092, vm-xaj-bigdata-da-d05:9092, vm-xaj-bigdata-da-d06:9092, vm-xaj-bigdata-da-d07:9092]
	receive.buffer.bytes = 32768
	retry.backoff.ms = 100
	buffer.memory = 33554432
	timeout.ms = 30000
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	retries = 0
	max.request.size = 1048576
	block.on.buffer.full = true
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
	metrics.sample.window.ms = 30000
	send.buffer.bytes = 131072
	max.in.flight.requests.per.connection = 5
	metrics.num.samples = 2
	linger.ms = 1
	client.id = 

[INFO][2018-05-24 19:51:25,067][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527162680000 ms
[INFO][2018-05-24 19:51:25,067][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527162680000 ms.0 from job set of time 1527162680000 ms
[INFO][2018-05-24 19:51:25,078][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:64
[INFO][2018-05-24 19:51:25,079][org.apache.spark.scheduler.DAGScheduler]Got job 2 (foreachPartition at ReceiveKafkaData.scala:64) with 1 output partitions
[INFO][2018-05-24 19:51:25,080][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 2 (foreachPartition at ReceiveKafkaData.scala:64)
[INFO][2018-05-24 19:51:25,080][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 19:51:25,080][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 19:51:25,082][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 2 (MapPartitionsRDD[5] at map at ReceiveKafkaData.scala:62), which has no missing parents
[INFO][2018-05-24 19:51:25,087][org.apache.spark.storage.memory.MemoryStore]Block broadcast_2 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-24 19:51:25,091][org.apache.spark.storage.memory.MemoryStore]Block broadcast_2_piece0 stored as bytes in memory (estimated size 2008.0 B, free 912.3 MB)
[INFO][2018-05-24 19:51:25,092][org.apache.spark.storage.BlockManagerInfo]Added broadcast_2_piece0 in memory on 192.168.0.102:65423 (size: 2008.0 B, free: 912.3 MB)
[INFO][2018-05-24 19:51:25,093][org.apache.spark.SparkContext]Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 19:51:25,094][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[5] at map at ReceiveKafkaData.scala:62) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 19:51:25,094][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 2.0 with 1 tasks
[INFO][2018-05-24 19:51:25,095][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 19:51:25,096][org.apache.spark.executor.Executor]Running task 0.0 in stage 2.0 (TID 2)
[INFO][2018-05-24 19:51:25,100][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 11876 is the same as ending offset skipping seven 0
[INFO][2018-05-24 19:51:25,101][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 19:51:25,104][org.apache.spark.executor.Executor]Finished task 0.0 in stage 2.0 (TID 2). 708 bytes result sent to driver
[INFO][2018-05-24 19:51:25,105][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 2.0 (TID 2) in 10 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 19:51:25,106][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 19:51:25,106][org.apache.spark.scheduler.DAGScheduler]ResultStage 2 (foreachPartition at ReceiveKafkaData.scala:64) finished in 0.011 s
[INFO][2018-05-24 19:51:25,107][org.apache.spark.scheduler.DAGScheduler]Job 2 finished: foreachPartition at ReceiveKafkaData.scala:64, took 0.028137 s
[INFO][2018-05-24 19:51:25,107][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527162680000 ms.0 from job set of time 1527162680000 ms
[INFO][2018-05-24 19:51:25,108][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 3 from persistence list
[INFO][2018-05-24 19:51:25,108][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 5.107 s for time 1527162680000 ms (execution: 0.040 s)
[INFO][2018-05-24 19:51:25,108][org.apache.spark.storage.BlockManager]Removing RDD 3
[INFO][2018-05-24 19:51:25,108][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 2 from persistence list
[INFO][2018-05-24 19:51:25,109][org.apache.spark.storage.BlockManager]Removing RDD 2
[INFO][2018-05-24 19:51:25,109][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 19:51:25,109][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527162640000 ms
[INFO][2018-05-24 19:51:50,077][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527162700000 ms
[INFO][2018-05-24 19:51:50,077][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527162700000 ms.0 from job set of time 1527162700000 ms
[INFO][2018-05-24 19:51:50,088][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:64
[INFO][2018-05-24 19:51:50,090][org.apache.spark.scheduler.DAGScheduler]Got job 3 (foreachPartition at ReceiveKafkaData.scala:64) with 1 output partitions
[INFO][2018-05-24 19:51:50,090][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 3 (foreachPartition at ReceiveKafkaData.scala:64)
[INFO][2018-05-24 19:51:50,090][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 19:51:50,090][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 19:51:50,092][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 3 (MapPartitionsRDD[7] at map at ReceiveKafkaData.scala:62), which has no missing parents
[INFO][2018-05-24 19:51:50,096][org.apache.spark.storage.memory.MemoryStore]Block broadcast_3 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-24 19:51:50,100][org.apache.spark.storage.memory.MemoryStore]Block broadcast_3_piece0 stored as bytes in memory (estimated size 2008.0 B, free 912.3 MB)
[INFO][2018-05-24 19:51:50,100][org.apache.spark.storage.BlockManagerInfo]Added broadcast_3_piece0 in memory on 192.168.0.102:65423 (size: 2008.0 B, free: 912.3 MB)
[INFO][2018-05-24 19:51:50,101][org.apache.spark.SparkContext]Created broadcast 3 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 19:51:50,102][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[7] at map at ReceiveKafkaData.scala:62) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 19:51:50,102][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 3.0 with 1 tasks
[INFO][2018-05-24 19:51:50,103][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 3.0 (TID 3, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 19:51:50,104][org.apache.spark.executor.Executor]Running task 0.0 in stage 3.0 (TID 3)
[INFO][2018-05-24 19:51:50,108][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 11876 is the same as ending offset skipping seven 0
[INFO][2018-05-24 19:51:50,109][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 19:51:50,110][org.apache.spark.executor.Executor]Finished task 0.0 in stage 3.0 (TID 3). 708 bytes result sent to driver
[INFO][2018-05-24 19:51:50,111][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 3.0 (TID 3) in 8 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 19:51:50,112][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 3.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 19:51:50,112][org.apache.spark.scheduler.DAGScheduler]ResultStage 3 (foreachPartition at ReceiveKafkaData.scala:64) finished in 0.010 s
[INFO][2018-05-24 19:51:50,113][org.apache.spark.scheduler.DAGScheduler]Job 3 finished: foreachPartition at ReceiveKafkaData.scala:64, took 0.024024 s
[INFO][2018-05-24 19:51:50,113][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527162700000 ms.0 from job set of time 1527162700000 ms
[INFO][2018-05-24 19:51:50,114][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 5 from persistence list
[INFO][2018-05-24 19:51:50,114][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 10.113 s for time 1527162700000 ms (execution: 0.036 s)
[INFO][2018-05-24 19:51:50,114][org.apache.spark.storage.BlockManager]Removing RDD 5
[INFO][2018-05-24 19:51:50,115][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 4 from persistence list
[INFO][2018-05-24 19:51:50,115][org.apache.spark.storage.BlockManager]Removing RDD 4
[INFO][2018-05-24 19:51:50,116][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 19:51:50,116][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527162660000 ms
[INFO][2018-05-24 19:52:05,078][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527162720000 ms
[INFO][2018-05-24 19:52:05,079][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527162720000 ms.0 from job set of time 1527162720000 ms
[INFO][2018-05-24 19:52:05,086][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:64
[INFO][2018-05-24 19:52:05,087][org.apache.spark.scheduler.DAGScheduler]Got job 4 (foreachPartition at ReceiveKafkaData.scala:64) with 1 output partitions
[INFO][2018-05-24 19:52:05,088][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 4 (foreachPartition at ReceiveKafkaData.scala:64)
[INFO][2018-05-24 19:52:05,088][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 19:52:05,088][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 19:52:05,088][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 4 (MapPartitionsRDD[9] at map at ReceiveKafkaData.scala:62), which has no missing parents
[INFO][2018-05-24 19:52:05,091][org.apache.spark.storage.memory.MemoryStore]Block broadcast_4 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-24 19:52:05,095][org.apache.spark.storage.memory.MemoryStore]Block broadcast_4_piece0 stored as bytes in memory (estimated size 2008.0 B, free 912.3 MB)
[INFO][2018-05-24 19:52:05,095][org.apache.spark.storage.BlockManagerInfo]Added broadcast_4_piece0 in memory on 192.168.0.102:65423 (size: 2008.0 B, free: 912.3 MB)
[INFO][2018-05-24 19:52:05,096][org.apache.spark.SparkContext]Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 19:52:05,097][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[9] at map at ReceiveKafkaData.scala:62) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 19:52:05,097][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 4.0 with 1 tasks
[INFO][2018-05-24 19:52:05,098][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 4.0 (TID 4, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 19:52:05,099][org.apache.spark.executor.Executor]Running task 0.0 in stage 4.0 (TID 4)
[INFO][2018-05-24 19:52:05,107][org.apache.spark.streaming.kafka.KafkaRDD]Computing topic seven, partition 0 offsets 11876 -> 11889
[INFO][2018-05-24 19:52:05,108][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 19:52:05,108][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 19:52:05,108][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 19:52:05,280][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_1_piece0 on 192.168.0.102:65423 in memory (size: 2008.0 B, free: 912.3 MB)
[INFO][2018-05-24 19:52:05,283][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_3_piece0 on 192.168.0.102:65423 in memory (size: 2008.0 B, free: 912.3 MB)
[INFO][2018-05-24 19:52:05,285][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_0_piece0 on 192.168.0.102:65423 in memory (size: 2007.0 B, free: 912.3 MB)
[INFO][2018-05-24 19:52:05,287][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_2_piece0 on 192.168.0.102:65423 in memory (size: 2008.0 B, free: 912.3 MB)
[INFO][2018-05-24 19:52:10,683][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 19:52:10,684][org.apache.spark.executor.Executor]Finished task 0.0 in stage 4.0 (TID 4). 751 bytes result sent to driver
[INFO][2018-05-24 19:52:10,685][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 4.0 (TID 4) in 5587 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 19:52:10,685][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 4.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 19:52:10,686][org.apache.spark.scheduler.DAGScheduler]ResultStage 4 (foreachPartition at ReceiveKafkaData.scala:64) finished in 5.588 s
[INFO][2018-05-24 19:52:10,686][org.apache.spark.scheduler.DAGScheduler]Job 4 finished: foreachPartition at ReceiveKafkaData.scala:64, took 5.600137 s
[INFO][2018-05-24 19:52:10,687][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527162720000 ms.0 from job set of time 1527162720000 ms
[INFO][2018-05-24 19:52:10,687][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 10.687 s for time 1527162720000 ms (execution: 5.608 s)
[INFO][2018-05-24 19:52:10,687][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 7 from persistence list
[INFO][2018-05-24 19:52:10,688][org.apache.spark.storage.BlockManager]Removing RDD 7
[INFO][2018-05-24 19:52:10,688][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 6 from persistence list
[INFO][2018-05-24 19:52:10,688][org.apache.spark.storage.BlockManager]Removing RDD 6
[INFO][2018-05-24 19:52:10,688][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 19:52:10,688][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527162680000 ms
[INFO][2018-05-24 19:52:30,082][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527162740000 ms
[INFO][2018-05-24 19:52:30,083][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527162740000 ms.0 from job set of time 1527162740000 ms
[INFO][2018-05-24 19:52:30,090][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:64
[INFO][2018-05-24 19:52:30,091][org.apache.spark.scheduler.DAGScheduler]Got job 5 (foreachPartition at ReceiveKafkaData.scala:64) with 1 output partitions
[INFO][2018-05-24 19:52:30,091][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 5 (foreachPartition at ReceiveKafkaData.scala:64)
[INFO][2018-05-24 19:52:30,091][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 19:52:30,091][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 19:52:30,092][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 5 (MapPartitionsRDD[11] at map at ReceiveKafkaData.scala:62), which has no missing parents
[INFO][2018-05-24 19:52:30,094][org.apache.spark.storage.memory.MemoryStore]Block broadcast_5 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-24 19:52:30,100][org.apache.spark.storage.memory.MemoryStore]Block broadcast_5_piece0 stored as bytes in memory (estimated size 2008.0 B, free 912.3 MB)
[INFO][2018-05-24 19:52:30,100][org.apache.spark.storage.BlockManagerInfo]Added broadcast_5_piece0 in memory on 192.168.0.102:65423 (size: 2008.0 B, free: 912.3 MB)
[INFO][2018-05-24 19:52:30,102][org.apache.spark.SparkContext]Created broadcast 5 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 19:52:30,104][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[11] at map at ReceiveKafkaData.scala:62) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 19:52:30,104][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 5.0 with 1 tasks
[INFO][2018-05-24 19:52:30,106][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 5.0 (TID 5, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 19:52:30,106][org.apache.spark.executor.Executor]Running task 0.0 in stage 5.0 (TID 5)
[INFO][2018-05-24 19:52:30,110][org.apache.spark.streaming.kafka.KafkaRDD]Computing topic seven, partition 0 offsets 11889 -> 11914
[INFO][2018-05-24 19:52:30,111][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 19:52:30,111][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 19:52:30,111][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 19:52:30,269][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 19:52:30,270][org.apache.spark.executor.Executor]Finished task 0.0 in stage 5.0 (TID 5). 708 bytes result sent to driver
[INFO][2018-05-24 19:52:30,271][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 5.0 (TID 5) in 166 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 19:52:30,271][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 5.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 19:52:30,272][org.apache.spark.scheduler.DAGScheduler]ResultStage 5 (foreachPartition at ReceiveKafkaData.scala:64) finished in 0.167 s
[INFO][2018-05-24 19:52:30,273][org.apache.spark.scheduler.DAGScheduler]Job 5 finished: foreachPartition at ReceiveKafkaData.scala:64, took 0.181987 s
[INFO][2018-05-24 19:52:30,273][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527162740000 ms.0 from job set of time 1527162740000 ms
[INFO][2018-05-24 19:52:30,273][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 10.273 s for time 1527162740000 ms (execution: 0.190 s)
[INFO][2018-05-24 19:52:30,273][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 9 from persistence list
[INFO][2018-05-24 19:52:30,274][org.apache.spark.storage.BlockManager]Removing RDD 9
[INFO][2018-05-24 19:52:30,274][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 8 from persistence list
[INFO][2018-05-24 19:52:30,275][org.apache.spark.storage.BlockManager]Removing RDD 8
[INFO][2018-05-24 19:52:30,275][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 19:52:30,275][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527162700000 ms
[INFO][2018-05-24 19:52:40,052][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527162760000 ms
[INFO][2018-05-24 19:52:40,053][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527162760000 ms.0 from job set of time 1527162760000 ms
[INFO][2018-05-24 19:52:40,065][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:64
[INFO][2018-05-24 19:52:40,066][org.apache.spark.scheduler.DAGScheduler]Got job 6 (foreachPartition at ReceiveKafkaData.scala:64) with 1 output partitions
[INFO][2018-05-24 19:52:40,066][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 6 (foreachPartition at ReceiveKafkaData.scala:64)
[INFO][2018-05-24 19:52:40,066][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 19:52:40,066][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 19:52:40,067][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 6 (MapPartitionsRDD[13] at map at ReceiveKafkaData.scala:62), which has no missing parents
[INFO][2018-05-24 19:52:40,069][org.apache.spark.storage.memory.MemoryStore]Block broadcast_6 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-24 19:52:40,075][org.apache.spark.storage.memory.MemoryStore]Block broadcast_6_piece0 stored as bytes in memory (estimated size 2008.0 B, free 912.3 MB)
[INFO][2018-05-24 19:52:40,076][org.apache.spark.storage.BlockManagerInfo]Added broadcast_6_piece0 in memory on 192.168.0.102:65423 (size: 2008.0 B, free: 912.3 MB)
[INFO][2018-05-24 19:52:40,077][org.apache.spark.SparkContext]Created broadcast 6 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 19:52:40,079][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[13] at map at ReceiveKafkaData.scala:62) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 19:52:40,079][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 6.0 with 1 tasks
[INFO][2018-05-24 19:52:40,080][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 6.0 (TID 6, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 19:52:40,080][org.apache.spark.executor.Executor]Running task 0.0 in stage 6.0 (TID 6)
[INFO][2018-05-24 19:52:40,083][org.apache.spark.streaming.kafka.KafkaRDD]Computing topic seven, partition 0 offsets 11914 -> 11924
[INFO][2018-05-24 19:52:40,084][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 19:52:40,084][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 19:52:40,084][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 19:52:40,156][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 19:52:40,157][org.apache.spark.executor.Executor]Finished task 0.0 in stage 6.0 (TID 6). 708 bytes result sent to driver
[INFO][2018-05-24 19:52:40,158][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 6.0 (TID 6) in 79 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 19:52:40,159][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 6.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 19:52:40,160][org.apache.spark.scheduler.DAGScheduler]ResultStage 6 (foreachPartition at ReceiveKafkaData.scala:64) finished in 0.080 s
[INFO][2018-05-24 19:52:40,161][org.apache.spark.scheduler.DAGScheduler]Job 6 finished: foreachPartition at ReceiveKafkaData.scala:64, took 0.094869 s
[INFO][2018-05-24 19:52:40,161][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527162760000 ms.0 from job set of time 1527162760000 ms
[INFO][2018-05-24 19:52:40,162][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.161 s for time 1527162760000 ms (execution: 0.108 s)
[INFO][2018-05-24 19:52:40,162][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 11 from persistence list
[INFO][2018-05-24 19:52:40,162][org.apache.spark.storage.BlockManager]Removing RDD 11
[INFO][2018-05-24 19:52:40,162][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 10 from persistence list
[INFO][2018-05-24 19:52:40,163][org.apache.spark.storage.BlockManager]Removing RDD 10
[INFO][2018-05-24 19:52:40,163][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 19:52:40,164][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527162720000 ms
[INFO][2018-05-24 19:53:10,080][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527162780000 ms
[INFO][2018-05-24 19:53:10,081][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527162780000 ms.0 from job set of time 1527162780000 ms
[INFO][2018-05-24 19:53:10,088][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:64
[INFO][2018-05-24 19:53:10,089][org.apache.spark.scheduler.DAGScheduler]Got job 7 (foreachPartition at ReceiveKafkaData.scala:64) with 1 output partitions
[INFO][2018-05-24 19:53:10,089][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 7 (foreachPartition at ReceiveKafkaData.scala:64)
[INFO][2018-05-24 19:53:10,089][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 19:53:10,089][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 19:53:10,090][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 7 (MapPartitionsRDD[15] at map at ReceiveKafkaData.scala:62), which has no missing parents
[INFO][2018-05-24 19:53:10,094][org.apache.spark.storage.memory.MemoryStore]Block broadcast_7 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-24 19:53:10,100][org.apache.spark.storage.memory.MemoryStore]Block broadcast_7_piece0 stored as bytes in memory (estimated size 2008.0 B, free 912.3 MB)
[INFO][2018-05-24 19:53:10,101][org.apache.spark.storage.BlockManagerInfo]Added broadcast_7_piece0 in memory on 192.168.0.102:65423 (size: 2008.0 B, free: 912.3 MB)
[INFO][2018-05-24 19:53:10,101][org.apache.spark.SparkContext]Created broadcast 7 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 19:53:10,102][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[15] at map at ReceiveKafkaData.scala:62) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 19:53:10,102][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 7.0 with 1 tasks
[INFO][2018-05-24 19:53:10,103][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 7.0 (TID 7, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 19:53:10,103][org.apache.spark.executor.Executor]Running task 0.0 in stage 7.0 (TID 7)
[INFO][2018-05-24 19:53:10,107][org.apache.spark.streaming.kafka.KafkaRDD]Computing topic seven, partition 0 offsets 11924 -> 11954
[INFO][2018-05-24 19:53:10,107][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 19:53:10,108][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 19:53:10,108][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 19:53:15,209][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 19:53:15,210][org.apache.spark.executor.Executor]Finished task 0.0 in stage 7.0 (TID 7). 708 bytes result sent to driver
[INFO][2018-05-24 19:53:15,211][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 7.0 (TID 7) in 5108 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 19:53:15,211][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 7.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 19:53:15,211][org.apache.spark.scheduler.DAGScheduler]ResultStage 7 (foreachPartition at ReceiveKafkaData.scala:64) finished in 5.109 s
[INFO][2018-05-24 19:53:15,212][org.apache.spark.scheduler.DAGScheduler]Job 7 finished: foreachPartition at ReceiveKafkaData.scala:64, took 5.123324 s
[INFO][2018-05-24 19:53:15,212][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527162780000 ms.0 from job set of time 1527162780000 ms
[INFO][2018-05-24 19:53:15,213][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 13 from persistence list
[INFO][2018-05-24 19:53:15,213][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 15.212 s for time 1527162780000 ms (execution: 5.131 s)
[INFO][2018-05-24 19:53:15,213][org.apache.spark.storage.BlockManager]Removing RDD 13
[INFO][2018-05-24 19:53:15,213][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 12 from persistence list
[INFO][2018-05-24 19:53:15,214][org.apache.spark.storage.BlockManager]Removing RDD 12
[INFO][2018-05-24 19:53:15,214][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 19:53:15,214][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527162740000 ms
[INFO][2018-05-24 19:53:25,071][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527162800000 ms
[INFO][2018-05-24 19:53:25,072][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527162800000 ms.0 from job set of time 1527162800000 ms
[INFO][2018-05-24 19:53:25,080][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:64
[INFO][2018-05-24 19:53:25,081][org.apache.spark.scheduler.DAGScheduler]Got job 8 (foreachPartition at ReceiveKafkaData.scala:64) with 1 output partitions
[INFO][2018-05-24 19:53:25,081][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 8 (foreachPartition at ReceiveKafkaData.scala:64)
[INFO][2018-05-24 19:53:25,081][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 19:53:25,081][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 19:53:25,081][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 8 (MapPartitionsRDD[17] at map at ReceiveKafkaData.scala:62), which has no missing parents
[INFO][2018-05-24 19:53:25,083][org.apache.spark.storage.memory.MemoryStore]Block broadcast_8 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-24 19:53:25,089][org.apache.spark.storage.memory.MemoryStore]Block broadcast_8_piece0 stored as bytes in memory (estimated size 2008.0 B, free 912.3 MB)
[INFO][2018-05-24 19:53:25,089][org.apache.spark.storage.BlockManagerInfo]Added broadcast_8_piece0 in memory on 192.168.0.102:65423 (size: 2008.0 B, free: 912.3 MB)
[INFO][2018-05-24 19:53:25,090][org.apache.spark.SparkContext]Created broadcast 8 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 19:53:25,091][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[17] at map at ReceiveKafkaData.scala:62) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 19:53:25,091][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 8.0 with 1 tasks
[INFO][2018-05-24 19:53:25,092][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 8.0 (TID 8, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 19:53:25,093][org.apache.spark.executor.Executor]Running task 0.0 in stage 8.0 (TID 8)
[INFO][2018-05-24 19:53:25,095][org.apache.spark.streaming.kafka.KafkaRDD]Computing topic seven, partition 0 offsets 11954 -> 11968
[INFO][2018-05-24 19:53:25,095][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 19:53:25,096][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 19:53:25,096][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 19:53:25,161][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 19:53:25,162][org.apache.spark.executor.Executor]Finished task 0.0 in stage 8.0 (TID 8). 708 bytes result sent to driver
[INFO][2018-05-24 19:53:25,163][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 8.0 (TID 8) in 71 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 19:53:25,163][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 8.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 19:53:25,166][org.apache.spark.scheduler.DAGScheduler]ResultStage 8 (foreachPartition at ReceiveKafkaData.scala:64) finished in 0.074 s
[INFO][2018-05-24 19:53:25,171][org.apache.spark.scheduler.DAGScheduler]Job 8 finished: foreachPartition at ReceiveKafkaData.scala:64, took 0.090033 s
[INFO][2018-05-24 19:53:25,172][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527162800000 ms.0 from job set of time 1527162800000 ms
[INFO][2018-05-24 19:53:25,173][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 5.172 s for time 1527162800000 ms (execution: 0.100 s)
[INFO][2018-05-24 19:53:25,173][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 15 from persistence list
[INFO][2018-05-24 19:53:25,175][org.apache.spark.storage.BlockManager]Removing RDD 15
[INFO][2018-05-24 19:53:25,177][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 14 from persistence list
[INFO][2018-05-24 19:53:25,178][org.apache.spark.storage.BlockManager]Removing RDD 14
[INFO][2018-05-24 19:53:25,179][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 19:53:25,179][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527162760000 ms
[INFO][2018-05-24 19:53:33,135][org.apache.spark.util.ShutdownHookManager]Shutdown hook called
[INFO][2018-05-24 19:53:33,141][org.apache.spark.util.ShutdownHookManager]Deleting directory /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/spark-2852bfa3-446b-4bf6-81ff-a449c8a04522
[INFO][2018-05-24 19:53:50,094][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527162820000 ms
[INFO][2018-05-24 19:53:50,095][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527162820000 ms.0 from job set of time 1527162820000 ms
[INFO][2018-05-24 19:53:50,110][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:64
[INFO][2018-05-24 19:53:50,111][org.apache.spark.scheduler.DAGScheduler]Got job 9 (foreachPartition at ReceiveKafkaData.scala:64) with 1 output partitions
[INFO][2018-05-24 19:53:50,111][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 9 (foreachPartition at ReceiveKafkaData.scala:64)
[INFO][2018-05-24 19:53:50,111][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 19:53:50,111][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 19:53:50,112][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 9 (MapPartitionsRDD[19] at map at ReceiveKafkaData.scala:62), which has no missing parents
[INFO][2018-05-24 19:53:50,114][org.apache.spark.storage.memory.MemoryStore]Block broadcast_9 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-24 19:53:50,120][org.apache.spark.storage.memory.MemoryStore]Block broadcast_9_piece0 stored as bytes in memory (estimated size 2008.0 B, free 912.3 MB)
[INFO][2018-05-24 19:53:50,120][org.apache.spark.storage.BlockManagerInfo]Added broadcast_9_piece0 in memory on 192.168.0.102:65423 (size: 2008.0 B, free: 912.3 MB)
[INFO][2018-05-24 19:53:50,121][org.apache.spark.SparkContext]Created broadcast 9 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 19:53:50,122][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[19] at map at ReceiveKafkaData.scala:62) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 19:53:50,122][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 9.0 with 1 tasks
[INFO][2018-05-24 19:53:50,122][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 9.0 (TID 9, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 19:53:50,123][org.apache.spark.executor.Executor]Running task 0.0 in stage 9.0 (TID 9)
[INFO][2018-05-24 19:53:50,125][org.apache.spark.streaming.kafka.KafkaRDD]Computing topic seven, partition 0 offsets 11968 -> 11976
[INFO][2018-05-24 19:53:50,125][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 19:53:50,125][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 19:53:50,125][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 19:53:55,194][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 19:53:55,229][org.apache.spark.executor.Executor]Finished task 0.0 in stage 9.0 (TID 9). 665 bytes result sent to driver
[INFO][2018-05-24 19:53:55,230][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 9.0 (TID 9) in 5108 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 19:53:55,231][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 9.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 19:53:55,231][org.apache.spark.scheduler.DAGScheduler]ResultStage 9 (foreachPartition at ReceiveKafkaData.scala:64) finished in 5.109 s
[INFO][2018-05-24 19:53:55,232][org.apache.spark.scheduler.DAGScheduler]Job 9 finished: foreachPartition at ReceiveKafkaData.scala:64, took 5.121371 s
[INFO][2018-05-24 19:53:55,233][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527162820000 ms.0 from job set of time 1527162820000 ms
[INFO][2018-05-24 19:53:55,233][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 17 from persistence list
[INFO][2018-05-24 19:53:55,233][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 15.233 s for time 1527162820000 ms (execution: 5.138 s)
[INFO][2018-05-24 19:53:55,234][org.apache.spark.storage.BlockManager]Removing RDD 17
[INFO][2018-05-24 19:53:55,234][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 16 from persistence list
[INFO][2018-05-24 19:53:55,234][org.apache.spark.storage.BlockManager]Removing RDD 16
[INFO][2018-05-24 19:53:55,234][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 19:53:55,234][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527162780000 ms
[INFO][2018-05-24 19:54:00,058][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527162840000 ms
[INFO][2018-05-24 19:54:00,060][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527162840000 ms.0 from job set of time 1527162840000 ms
[INFO][2018-05-24 19:54:00,072][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:64
[INFO][2018-05-24 19:54:00,073][org.apache.spark.scheduler.DAGScheduler]Got job 10 (foreachPartition at ReceiveKafkaData.scala:64) with 1 output partitions
[INFO][2018-05-24 19:54:00,073][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 10 (foreachPartition at ReceiveKafkaData.scala:64)
[INFO][2018-05-24 19:54:00,073][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 19:54:00,073][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 19:54:00,074][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 10 (MapPartitionsRDD[21] at map at ReceiveKafkaData.scala:62), which has no missing parents
[INFO][2018-05-24 19:54:00,075][org.apache.spark.storage.memory.MemoryStore]Block broadcast_10 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-24 19:54:00,083][org.apache.spark.storage.memory.MemoryStore]Block broadcast_10_piece0 stored as bytes in memory (estimated size 2008.0 B, free 912.3 MB)
[INFO][2018-05-24 19:54:00,083][org.apache.spark.storage.BlockManagerInfo]Added broadcast_10_piece0 in memory on 192.168.0.102:65423 (size: 2008.0 B, free: 912.3 MB)
[INFO][2018-05-24 19:54:00,084][org.apache.spark.SparkContext]Created broadcast 10 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 19:54:00,084][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[21] at map at ReceiveKafkaData.scala:62) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 19:54:00,084][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 10.0 with 1 tasks
[INFO][2018-05-24 19:54:00,085][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 10.0 (TID 10, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 19:54:00,085][org.apache.spark.executor.Executor]Running task 0.0 in stage 10.0 (TID 10)
[INFO][2018-05-24 19:54:00,087][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 11976 is the same as ending offset skipping seven 0
[INFO][2018-05-24 19:54:00,087][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 19:54:00,088][org.apache.spark.executor.Executor]Finished task 0.0 in stage 10.0 (TID 10). 708 bytes result sent to driver
[INFO][2018-05-24 19:54:00,089][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 10.0 (TID 10) in 4 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 19:54:00,089][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 10.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 19:54:00,090][org.apache.spark.scheduler.DAGScheduler]ResultStage 10 (foreachPartition at ReceiveKafkaData.scala:64) finished in 0.005 s
[INFO][2018-05-24 19:54:00,090][org.apache.spark.scheduler.DAGScheduler]Job 10 finished: foreachPartition at ReceiveKafkaData.scala:64, took 0.017870 s
[INFO][2018-05-24 19:54:00,091][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527162840000 ms.0 from job set of time 1527162840000 ms
[INFO][2018-05-24 19:54:00,091][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.091 s for time 1527162840000 ms (execution: 0.031 s)
[INFO][2018-05-24 19:54:00,091][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 19 from persistence list
[INFO][2018-05-24 19:54:00,091][org.apache.spark.storage.BlockManager]Removing RDD 19
[INFO][2018-05-24 19:54:00,092][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 18 from persistence list
[INFO][2018-05-24 19:54:00,092][org.apache.spark.storage.BlockManager]Removing RDD 18
[INFO][2018-05-24 19:54:00,093][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 19:54:00,093][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527162800000 ms
[INFO][2018-05-24 19:54:30,068][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527162860000 ms
[INFO][2018-05-24 19:54:30,068][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527162860000 ms.0 from job set of time 1527162860000 ms
[INFO][2018-05-24 19:54:30,076][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:64
[INFO][2018-05-24 19:54:30,076][org.apache.spark.scheduler.DAGScheduler]Got job 11 (foreachPartition at ReceiveKafkaData.scala:64) with 1 output partitions
[INFO][2018-05-24 19:54:30,077][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 11 (foreachPartition at ReceiveKafkaData.scala:64)
[INFO][2018-05-24 19:54:30,077][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 19:54:30,077][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 19:54:30,077][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 11 (MapPartitionsRDD[23] at map at ReceiveKafkaData.scala:62), which has no missing parents
[INFO][2018-05-24 19:54:30,079][org.apache.spark.storage.memory.MemoryStore]Block broadcast_11 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-24 19:54:30,085][org.apache.spark.storage.memory.MemoryStore]Block broadcast_11_piece0 stored as bytes in memory (estimated size 2008.0 B, free 912.3 MB)
[INFO][2018-05-24 19:54:30,086][org.apache.spark.storage.BlockManagerInfo]Added broadcast_11_piece0 in memory on 192.168.0.102:65423 (size: 2008.0 B, free: 912.3 MB)
[INFO][2018-05-24 19:54:30,086][org.apache.spark.SparkContext]Created broadcast 11 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 19:54:30,087][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[23] at map at ReceiveKafkaData.scala:62) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 19:54:30,087][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 11.0 with 1 tasks
[INFO][2018-05-24 19:54:30,088][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 11.0 (TID 11, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 19:54:30,088][org.apache.spark.executor.Executor]Running task 0.0 in stage 11.0 (TID 11)
[INFO][2018-05-24 19:54:30,090][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 11976 is the same as ending offset skipping seven 0
[INFO][2018-05-24 19:54:30,090][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 19:54:30,091][org.apache.spark.executor.Executor]Finished task 0.0 in stage 11.0 (TID 11). 708 bytes result sent to driver
[INFO][2018-05-24 19:54:30,092][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 11.0 (TID 11) in 5 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 19:54:30,092][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 11.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 19:54:30,093][org.apache.spark.scheduler.DAGScheduler]ResultStage 11 (foreachPartition at ReceiveKafkaData.scala:64) finished in 0.005 s
[INFO][2018-05-24 19:54:30,093][org.apache.spark.scheduler.DAGScheduler]Job 11 finished: foreachPartition at ReceiveKafkaData.scala:64, took 0.017130 s
[INFO][2018-05-24 19:54:30,094][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527162860000 ms.0 from job set of time 1527162860000 ms
[INFO][2018-05-24 19:54:30,094][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 21 from persistence list
[INFO][2018-05-24 19:54:30,094][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 10.093 s for time 1527162860000 ms (execution: 0.025 s)
[INFO][2018-05-24 19:54:30,094][org.apache.spark.storage.BlockManager]Removing RDD 21
[INFO][2018-05-24 19:54:30,094][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 20 from persistence list
[INFO][2018-05-24 19:54:30,095][org.apache.spark.storage.BlockManager]Removing RDD 20
[INFO][2018-05-24 19:54:30,095][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 19:54:30,095][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527162820000 ms
[INFO][2018-05-24 19:54:35,625][org.apache.spark.streaming.StreamingContext]Invoking stop(stopGracefully=false) from shutdown hook
[INFO][2018-05-24 19:54:35,629][org.apache.spark.streaming.scheduler.ReceiverTracker]ReceiverTracker stopped
[INFO][2018-05-24 19:54:35,630][org.apache.spark.streaming.scheduler.JobGenerator]Stopping JobGenerator immediately
[INFO][2018-05-24 19:54:35,631][org.apache.spark.streaming.util.RecurringTimer]Stopped timer for JobGenerator after time 1527162860000
[INFO][2018-05-24 19:54:35,634][org.apache.spark.streaming.scheduler.JobGenerator]Stopped JobGenerator
[INFO][2018-05-24 19:54:35,635][org.apache.spark.streaming.scheduler.JobScheduler]Stopped JobScheduler
[INFO][2018-05-24 19:54:35,644][org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@6972c30a{/streaming,null,UNAVAILABLE,@Spark}
[INFO][2018-05-24 19:54:35,645][org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@5109e8cf{/streaming/batch,null,UNAVAILABLE,@Spark}
[INFO][2018-05-24 19:54:35,647][org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@6c65860d{/static/streaming,null,UNAVAILABLE,@Spark}
[INFO][2018-05-24 19:54:35,648][org.apache.spark.streaming.StreamingContext]StreamingContext stopped successfully
[INFO][2018-05-24 19:54:35,649][org.apache.spark.SparkContext]Invoking stop() from shutdown hook
[INFO][2018-05-24 19:54:35,668][org.spark_project.jetty.server.AbstractConnector]Stopped Spark@6d366c9b{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-05-24 19:54:35,670][org.apache.spark.ui.SparkUI]Stopped Spark web UI at http://192.168.0.102:4040
[INFO][2018-05-24 19:54:35,691][org.apache.spark.MapOutputTrackerMasterEndpoint]MapOutputTrackerMasterEndpoint stopped!
[INFO][2018-05-24 19:54:35,711][org.apache.spark.storage.memory.MemoryStore]MemoryStore cleared
[INFO][2018-05-24 19:54:35,711][org.apache.spark.storage.BlockManager]BlockManager stopped
[INFO][2018-05-24 19:54:35,712][org.apache.spark.storage.BlockManagerMaster]BlockManagerMaster stopped
[INFO][2018-05-24 19:54:35,714][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint]OutputCommitCoordinator stopped!
[INFO][2018-05-24 19:54:35,716][org.apache.spark.SparkContext]Successfully stopped SparkContext
[INFO][2018-05-24 19:54:35,716][org.apache.spark.util.ShutdownHookManager]Shutdown hook called
[INFO][2018-05-24 19:54:35,717][org.apache.spark.util.ShutdownHookManager]Deleting directory /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/spark-b8eaa8ee-cee4-408c-8f33-4b3df1f9f3dc
[INFO][2018-05-24 19:57:07,753][org.apache.spark.SparkContext]Running Spark version 2.2.0
[INFO][2018-05-24 19:57:08,510][org.apache.spark.SparkContext]Submitted application: ReceiveKafkaData$
[INFO][2018-05-24 19:57:08,529][org.apache.spark.SecurityManager]Changing view acls to: seven
[INFO][2018-05-24 19:57:08,530][org.apache.spark.SecurityManager]Changing modify acls to: seven
[INFO][2018-05-24 19:57:08,530][org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO][2018-05-24 19:57:08,531][org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO][2018-05-24 19:57:08,531][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(seven); groups with view permissions: Set(); users  with modify permissions: Set(seven); groups with modify permissions: Set()
[INFO][2018-05-24 19:57:08,785][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 49170.
[INFO][2018-05-24 19:57:08,805][org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO][2018-05-24 19:57:08,822][org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO][2018-05-24 19:57:08,826][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2018-05-24 19:57:08,826][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
[INFO][2018-05-24 19:57:08,836][org.apache.spark.storage.DiskBlockManager]Created local directory at /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/blockmgr-e194edcd-7adf-400b-bbc2-116ed05536c2
[INFO][2018-05-24 19:57:08,849][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 912.3 MB
[INFO][2018-05-24 19:57:08,925][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO][2018-05-24 19:57:08,995][org.spark_project.jetty.util.log]Logging initialized @2181ms
[INFO][2018-05-24 19:57:09,070][org.spark_project.jetty.server.Server]jetty-9.3.z-SNAPSHOT
[INFO][2018-05-24 19:57:09,083][org.spark_project.jetty.server.Server]Started @2270ms
[INFO][2018-05-24 19:57:09,107][org.spark_project.jetty.server.AbstractConnector]Started ServerConnector@2090b9b9{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-05-24 19:57:09,107][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4040.
[INFO][2018-05-24 19:57:09,136][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@757d6814{/jobs,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:57:09,137][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@119f1f2a{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:57:09,138][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5b970f7{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:57:09,142][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6175619b{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:57:09,147][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@756cf158{/stages,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:57:09,152][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@751d3241{/stages/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:57:09,153][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@64337702{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:57:09,193][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4e76dac{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:57:09,195][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5f2f577{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:57:09,196][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5d465e4b{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:57:09,200][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@41a90fa8{/storage,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:57:09,207][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@52500920{/storage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:57:09,209][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@18a3962d{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:57:09,210][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2a65bb85{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:57:09,211][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4f936da8{/environment,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:57:09,211][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@452ba1db{/environment/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:57:09,212][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@76a36b71{/executors,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:57:09,216][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@f9d87b{/executors/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:57:09,225][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@26fb628{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:57:09,226][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@70dd7e15{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:57:09,261][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@35beb15e{/static,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:57:09,262][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6a62689d{/,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:57:09,276][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@60fa3495{/api,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:57:09,278][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1ca25c47{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:57:09,279][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@533b266e{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:57:09,283][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://192.168.0.102:4040
[INFO][2018-05-24 19:57:09,411][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
[INFO][2018-05-24 19:57:09,433][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 49171.
[INFO][2018-05-24 19:57:09,434][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 192.168.0.102:49171
[INFO][2018-05-24 19:57:09,436][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2018-05-24 19:57:09,440][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 192.168.0.102, 49171, None)
[INFO][2018-05-24 19:57:09,448][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 192.168.0.102:49171 with 912.3 MB RAM, BlockManagerId(driver, 192.168.0.102, 49171, None)
[INFO][2018-05-24 19:57:09,451][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 192.168.0.102, 49171, None)
[INFO][2018-05-24 19:57:09,452][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 192.168.0.102, 49171, None)
[INFO][2018-05-24 19:57:09,653][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7d2a6eac{/metrics/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:57:09,833][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 19:57:09,836][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 19:57:09,836][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 19:57:25,325][org.apache.spark.streaming.kafka.DirectKafkaInputDStream]Slide time = 60000 ms
[INFO][2018-05-24 19:57:25,326][org.apache.spark.streaming.kafka.DirectKafkaInputDStream]Storage level = Serialized 1x Replicated
[INFO][2018-05-24 19:57:25,326][org.apache.spark.streaming.kafka.DirectKafkaInputDStream]Checkpoint interval = null
[INFO][2018-05-24 19:57:25,327][org.apache.spark.streaming.kafka.DirectKafkaInputDStream]Remember interval = 60000 ms
[INFO][2018-05-24 19:57:25,327][org.apache.spark.streaming.kafka.DirectKafkaInputDStream]Initialized and validated org.apache.spark.streaming.kafka.DirectKafkaInputDStream@1c380177
[INFO][2018-05-24 19:57:25,328][org.apache.spark.streaming.dstream.MappedDStream]Slide time = 60000 ms
[INFO][2018-05-24 19:57:25,328][org.apache.spark.streaming.dstream.MappedDStream]Storage level = Serialized 1x Replicated
[INFO][2018-05-24 19:57:25,328][org.apache.spark.streaming.dstream.MappedDStream]Checkpoint interval = null
[INFO][2018-05-24 19:57:25,328][org.apache.spark.streaming.dstream.MappedDStream]Remember interval = 60000 ms
[INFO][2018-05-24 19:57:25,328][org.apache.spark.streaming.dstream.MappedDStream]Initialized and validated org.apache.spark.streaming.dstream.MappedDStream@2ed804fc
[INFO][2018-05-24 19:57:25,328][org.apache.spark.streaming.dstream.ForEachDStream]Slide time = 60000 ms
[INFO][2018-05-24 19:57:25,328][org.apache.spark.streaming.dstream.ForEachDStream]Storage level = Serialized 1x Replicated
[INFO][2018-05-24 19:57:25,328][org.apache.spark.streaming.dstream.ForEachDStream]Checkpoint interval = null
[INFO][2018-05-24 19:57:25,328][org.apache.spark.streaming.dstream.ForEachDStream]Remember interval = 60000 ms
[INFO][2018-05-24 19:57:25,328][org.apache.spark.streaming.dstream.ForEachDStream]Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@49b81857
[INFO][2018-05-24 19:57:25,362][org.apache.spark.streaming.util.RecurringTimer]Started timer for JobGenerator at time 1527163080000
[INFO][2018-05-24 19:57:25,363][org.apache.spark.streaming.scheduler.JobGenerator]Started JobGenerator at 1527163080000 ms
[INFO][2018-05-24 19:57:25,364][org.apache.spark.streaming.scheduler.JobScheduler]Started JobScheduler
[INFO][2018-05-24 19:57:25,366][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@57a48985{/streaming,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:57:25,367][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3910fe11{/streaming/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:57:25,368][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3f672204{/streaming/batch,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:57:25,369][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2c2db130{/streaming/batch/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:57:25,372][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2d000e80{/static/streaming,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:57:25,372][org.apache.spark.streaming.StreamingContext]StreamingContext started
[INFO][2018-05-24 19:58:00,063][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 19:58:00,064][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 19:58:00,064][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 19:58:10,163][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527163080000 ms
[INFO][2018-05-24 19:58:10,166][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527163080000 ms.0 from job set of time 1527163080000 ms
[INFO][2018-05-24 19:58:10,212][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:64
[INFO][2018-05-24 19:58:10,243][org.apache.spark.scheduler.DAGScheduler]Got job 0 (foreachPartition at ReceiveKafkaData.scala:64) with 1 output partitions
[INFO][2018-05-24 19:58:10,243][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 0 (foreachPartition at ReceiveKafkaData.scala:64)
[INFO][2018-05-24 19:58:10,243][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 19:58:10,245][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 19:58:10,253][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 0 (MapPartitionsRDD[1] at map at ReceiveKafkaData.scala:62), which has no missing parents
[INFO][2018-05-24 19:58:10,371][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-24 19:58:10,413][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0_piece0 stored as bytes in memory (estimated size 2007.0 B, free 912.3 MB)
[INFO][2018-05-24 19:58:10,414][org.apache.spark.storage.BlockManagerInfo]Added broadcast_0_piece0 in memory on 192.168.0.102:49171 (size: 2007.0 B, free: 912.3 MB)
[INFO][2018-05-24 19:58:10,417][org.apache.spark.SparkContext]Created broadcast 0 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 19:58:10,433][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at map at ReceiveKafkaData.scala:62) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 19:58:10,434][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 0.0 with 1 tasks
[INFO][2018-05-24 19:58:10,467][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 19:58:10,476][org.apache.spark.executor.Executor]Running task 0.0 in stage 0.0 (TID 0)
[INFO][2018-05-24 19:58:10,505][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 11976 is the same as ending offset skipping seven 0
[INFO][2018-05-24 19:58:10,669][org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper]Process identifier=hconnection-0x68404ee2 connecting to ZooKeeper ensemble=vm-xaj-bigdata-da-d01:2181,vm-xaj-bigdata-da-d02:2181,vm-xaj-bigdata-da-d03:2181
[INFO][2018-05-24 19:58:10,674][org.apache.zookeeper.ZooKeeper]Client environment:zookeeper.version=3.4.5-cdh5.7.0--1, built on 04/06/2016 14:24 GMT
[INFO][2018-05-24 19:58:10,675][org.apache.zookeeper.ZooKeeper]Client environment:host.name=192.168.0.102
[INFO][2018-05-24 19:58:10,675][org.apache.zookeeper.ZooKeeper]Client environment:java.version=1.8.0_161
[INFO][2018-05-24 19:58:10,675][org.apache.zookeeper.ZooKeeper]Client environment:java.vendor=Oracle Corporation
[INFO][2018-05-24 19:58:10,675][org.apache.zookeeper.ZooKeeper]Client environment:java.home=/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre
[INFO][2018-05-24 19:58:10,675][org.apache.zookeeper.ZooKeeper]Client environment:java.class.path=/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/charsets.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/deploy.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/cldrdata.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/dnsns.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/jaccess.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/jfxrt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/localedata.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/nashorn.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/sunec.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/sunjce_provider.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/sunpkcs11.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/zipfs.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/javaws.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/jce.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/jfr.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/jfxswt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/jsse.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/management-agent.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/plugin.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/resources.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/rt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/ant-javafx.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/dt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/javafx-mx.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/jconsole.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/packager.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/sa-jdi.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/tools.jar:/Users/seven/project/scala/dataMining/target/classes:/Users/seven/software/maven/repository/org/scala-lang/scala-library/2.11.12/scala-library-2.11.12.jar:/Users/seven/software/maven/repository/org/scala-lang/scala-compiler/2.11.12/scala-compiler-2.11.12.jar:/Users/seven/software/maven/repository/org/scala-lang/modules/scala-xml_2.11/1.0.5/scala-xml_2.11-1.0.5.jar:/Users/seven/software/maven/repository/org/scala-lang/modules/scala-parser-combinators_2.11/1.0.4/scala-parser-combinators_2.11-1.0.4.jar:/Users/seven/software/maven/repository/org/scala-lang/scala-reflect/2.11.12/scala-reflect-2.11.12.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-client/2.6.0-cdh5.7.0/hadoop-client-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-common/2.6.0-cdh5.7.0/hadoop-common-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/xmlenc/xmlenc/0.52/xmlenc-0.52.jar:/Users/seven/software/maven/repository/commons-net/commons-net/3.1/commons-net-3.1.jar:/Users/seven/software/maven/repository/commons-configuration/commons-configuration/1.6/commons-configuration-1.6.jar:/Users/seven/software/maven/repository/commons-digester/commons-digester/1.8/commons-digester-1.8.jar:/Users/seven/software/maven/repository/commons-beanutils/commons-beanutils/1.7.0/commons-beanutils-1.7.0.jar:/Users/seven/software/maven/repository/org/slf4j/slf4j-api/1.7.5/slf4j-api-1.7.5.jar:/Users/seven/software/maven/repository/org/slf4j/slf4j-log4j12/1.7.5/slf4j-log4j12-1.7.5.jar:/Users/seven/software/maven/repository/org/apache/avro/avro/1.7.6-cdh5.7.0/avro-1.7.6-cdh5.7.0.jar:/Users/seven/software/maven/repository/com/google/code/gson/gson/2.2.4/gson-2.2.4.jar:/Users/seven/software/maven/repository/org/apache/curator/curator-client/2.7.1/curator-client-2.7.1.jar:/Users/seven/software/maven/repository/org/apache/curator/curator-recipes/2.7.1/curator-recipes-2.7.1.jar:/Users/seven/software/maven/repository/com/google/code/findbugs/jsr305/3.0.0/jsr305-3.0.0.jar:/Users/seven/software/maven/repository/org/apache/htrace/htrace-core4/4.0.1-incubating/htrace-core4-4.0.1-incubating.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-compress/1.4.1/commons-compress-1.4.1.jar:/Users/seven/software/maven/repository/org/tukaani/xz/1.0/xz-1.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-hdfs/2.6.0-cdh5.7.0/hadoop-hdfs-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/io/netty/netty/3.6.2.Final/netty-3.6.2.Final.jar:/Users/seven/software/maven/repository/xerces/xercesImpl/2.9.1/xercesImpl-2.9.1.jar:/Users/seven/software/maven/repository/xml-apis/xml-apis/1.3.04/xml-apis-1.3.04.jar:/Users/seven/software/maven/repository/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-app/2.6.0-cdh5.7.0/hadoop-mapreduce-client-app-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-common/2.6.0-cdh5.7.0/hadoop-mapreduce-client-common-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-yarn-client/2.6.0-cdh5.7.0/hadoop-yarn-client-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-yarn-server-common/2.6.0-cdh5.7.0/hadoop-yarn-server-common-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-shuffle/2.6.0-cdh5.7.0/hadoop-mapreduce-client-shuffle-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-yarn-api/2.6.0-cdh5.7.0/hadoop-yarn-api-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-core/2.6.0-cdh5.7.0/hadoop-mapreduce-client-core-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-yarn-common/2.6.0-cdh5.7.0/hadoop-yarn-common-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/javax/xml/bind/jaxb-api/2.2.2/jaxb-api-2.2.2.jar:/Users/seven/software/maven/repository/javax/xml/stream/stax-api/1.0-2/stax-api-1.0-2.jar:/Users/seven/software/maven/repository/javax/activation/activation/1.1/activation-1.1.jar:/Users/seven/software/maven/repository/com/sun/jersey/jersey-client/1.9/jersey-client-1.9.jar:/Users/seven/software/maven/repository/org/codehaus/jackson/jackson-xc/1.8.8/jackson-xc-1.8.8.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.6.0-cdh5.7.0/hadoop-mapreduce-client-jobclient-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-aws/2.6.0-cdh5.7.0/hadoop-aws-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/com/amazonaws/aws-java-sdk-s3/1.10.6/aws-java-sdk-s3-1.10.6.jar:/Users/seven/software/maven/repository/com/amazonaws/aws-java-sdk-kms/1.10.6/aws-java-sdk-kms-1.10.6.jar:/Users/seven/software/maven/repository/com/amazonaws/aws-java-sdk-core/1.10.6/aws-java-sdk-core-1.10.6.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-annotations/2.6.0-cdh5.7.0/hadoop-annotations-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-client/1.2.0-cdh5.7.0/hbase-client-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-annotations/1.2.0-cdh5.7.0/hbase-annotations-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-common/1.2.0-cdh5.7.0/hbase-common-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-protocol/1.2.0-cdh5.7.0/hbase-protocol-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/commons-codec/commons-codec/1.9/commons-codec-1.9.jar:/Users/seven/software/maven/repository/commons-io/commons-io/2.4/commons-io-2.4.jar:/Users/seven/software/maven/repository/commons-lang/commons-lang/2.6/commons-lang-2.6.jar:/Users/seven/software/maven/repository/commons-logging/commons-logging/1.2/commons-logging-1.2.jar:/Users/seven/software/maven/repository/com/google/guava/guava/12.0.1/guava-12.0.1.jar:/Users/seven/software/maven/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/Users/seven/software/maven/repository/io/netty/netty-all/4.0.23.Final/netty-all-4.0.23.Final.jar:/Users/seven/software/maven/repository/org/apache/zookeeper/zookeeper/3.4.5-cdh5.7.0/zookeeper-3.4.5-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/htrace/htrace-core/3.2.0-incubating/htrace-core-3.2.0-incubating.jar:/Users/seven/software/maven/repository/org/codehaus/jackson/jackson-mapper-asl/1.8.8/jackson-mapper-asl-1.8.8.jar:/Users/seven/software/maven/repository/org/jruby/jcodings/jcodings/1.0.8/jcodings-1.0.8.jar:/Users/seven/software/maven/repository/org/jruby/joni/joni/2.1.2/joni-2.1.2.jar:/Users/seven/software/maven/repository/com/yammer/metrics/metrics-core/2.2.0/metrics-core-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-auth/2.6.0-cdh5.7.0/hadoop-auth-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/httpcomponents/httpclient/4.2.5/httpclient-4.2.5.jar:/Users/seven/software/maven/repository/org/apache/httpcomponents/httpcore/4.2.4/httpcore-4.2.4.jar:/Users/seven/software/maven/repository/org/apache/directory/server/apacheds-kerberos-codec/2.0.0-M15/apacheds-kerberos-codec-2.0.0-M15.jar:/Users/seven/software/maven/repository/org/apache/directory/server/apacheds-i18n/2.0.0-M15/apacheds-i18n-2.0.0-M15.jar:/Users/seven/software/maven/repository/org/apache/directory/api/api-asn1-api/1.0.0-M20/api-asn1-api-1.0.0-M20.jar:/Users/seven/software/maven/repository/org/apache/directory/api/api-util/1.0.0-M20/api-util-1.0.0-M20.jar:/Users/seven/software/maven/repository/org/apache/curator/curator-framework/2.7.1/curator-framework-2.7.1.jar:/Users/seven/software/maven/repository/com/github/stephenc/findbugs/findbugs-annotations/1.3.9-1/findbugs-annotations-1.3.9-1.jar:/Users/seven/software/maven/repository/junit/junit/4.12/junit-4.12.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-server/1.2.0-cdh5.7.0/hbase-server-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-procedure/1.2.0-cdh5.7.0/hbase-procedure-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-common/1.2.0-cdh5.7.0/hbase-common-1.2.0-cdh5.7.0-tests.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-prefix-tree/1.2.0-cdh5.7.0/hbase-prefix-tree-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar:/Users/seven/software/maven/repository/commons-collections/commons-collections/3.2.2/commons-collections-3.2.2.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-hadoop-compat/1.2.0-cdh5.7.0/hbase-hadoop-compat-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-hadoop2-compat/1.2.0-cdh5.7.0/hbase-hadoop2-compat-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/com/sun/jersey/jersey-core/1.9/jersey-core-1.9.jar:/Users/seven/software/maven/repository/com/sun/jersey/jersey-server/1.9/jersey-server-1.9.jar:/Users/seven/software/maven/repository/asm/asm/3.1/asm-3.1.jar:/Users/seven/software/maven/repository/commons-cli/commons-cli/1.2/commons-cli-1.2.jar:/Users/seven/software/maven/repository/com/github/stephenc/high-scale-lib/high-scale-lib/1.1.1/high-scale-lib-1.1.1.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-math/2.1/commons-math-2.1.jar:/Users/seven/software/maven/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jetty/6.1.26.cloudera.4/jetty-6.1.26.cloudera.4.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jetty-util/6.1.26.cloudera.4/jetty-util-6.1.26.cloudera.4.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jetty-sslengine/6.1.26.cloudera.4/jetty-sslengine-6.1.26.cloudera.4.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jsp-2.1/6.1.14/jsp-2.1-6.1.14.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jsp-api-2.1/6.1.14/jsp-api-2.1-6.1.14.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/servlet-api-2.5/6.1.14/servlet-api-2.5-6.1.14.jar:/Users/seven/software/maven/repository/org/codehaus/jackson/jackson-core-asl/1.8.8/jackson-core-asl-1.8.8.jar:/Users/seven/software/maven/repository/org/codehaus/jackson/jackson-jaxrs/1.8.8/jackson-jaxrs-1.8.8.jar:/Users/seven/software/maven/repository/tomcat/jasper-compiler/5.5.23/jasper-compiler-5.5.23.jar:/Users/seven/software/maven/repository/tomcat/jasper-runtime/5.5.23/jasper-runtime-5.5.23.jar:/Users/seven/software/maven/repository/commons-el/commons-el/1.0/commons-el-1.0.jar:/Users/seven/software/maven/repository/org/jamon/jamon-runtime/2.4.1/jamon-runtime-2.4.1.jar:/Users/seven/software/maven/repository/com/lmax/disruptor/3.3.0/disruptor-3.3.0.jar:/Users/seven/software/maven/repository/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar:/Users/seven/software/maven/repository/org/owasp/esapi/esapi/2.1.0/esapi-2.1.0.jar:/Users/seven/software/maven/repository/commons-beanutils/commons-beanutils-core/1.7.0/commons-beanutils-core-1.7.0.jar:/Users/seven/software/maven/repository/commons-fileupload/commons-fileupload/1.2/commons-fileupload-1.2.jar:/Users/seven/software/maven/repository/xom/xom/1.2.5/xom-1.2.5.jar:/Users/seven/software/maven/repository/xalan/xalan/2.7.0/xalan-2.7.0.jar:/Users/seven/software/maven/repository/org/beanshell/bsh-core/2.0b4/bsh-core-2.0b4.jar:/Users/seven/software/maven/repository/org/owasp/antisamy/antisamy/1.4.3/antisamy-1.4.3.jar:/Users/seven/software/maven/repository/org/apache/xmlgraphics/batik-css/1.7/batik-css-1.7.jar:/Users/seven/software/maven/repository/org/apache/xmlgraphics/batik-ext/1.7/batik-ext-1.7.jar:/Users/seven/software/maven/repository/org/apache/xmlgraphics/batik-util/1.7/batik-util-1.7.jar:/Users/seven/software/maven/repository/xml-apis/xml-apis-ext/1.3.04/xml-apis-ext-1.3.04.jar:/Users/seven/software/maven/repository/net/sourceforge/nekohtml/nekohtml/1.9.12/nekohtml-1.9.12.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-core/2.6.0-mr1-cdh5.7.0/hadoop-core-2.6.0-mr1-cdh5.7.0.jar:/Users/seven/software/maven/repository/javax/servlet/jsp/jsp-api/2.1/jsp-api-2.1.jar:/Users/seven/software/maven/repository/net/java/dev/jets3t/jets3t/0.6.1/jets3t-0.6.1.jar:/Users/seven/software/maven/repository/javax/servlet/servlet-api/2.5/servlet-api-2.5.jar:/Users/seven/software/maven/repository/hsqldb/hsqldb/1.8.0.10/hsqldb-1.8.0.10.jar:/Users/seven/software/maven/repository/org/eclipse/jdt/core/3.1.1/core-3.1.1.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-hdfs/2.6.0-cdh5.7.0/hadoop-hdfs-2.6.0-cdh5.7.0-tests.jar:/Users/seven/software/maven/repository/commons-daemon/commons-daemon/1.0.13/commons-daemon-1.0.13.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-lang3/3.5/commons-lang3-3.5.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-pool2/2.4.2/commons-pool2-2.4.2.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-mllib_2.11/2.2.0/spark-mllib_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-core_2.11/2.2.0/spark-core_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/avro/avro-mapred/1.7.7/avro-mapred-1.7.7-hadoop2.jar:/Users/seven/software/maven/repository/org/apache/avro/avro-ipc/1.7.7/avro-ipc-1.7.7-tests.jar:/Users/seven/software/maven/repository/com/twitter/chill_2.11/0.8.0/chill_2.11-0.8.0.jar:/Users/seven/software/maven/repository/com/esotericsoftware/kryo-shaded/3.0.3/kryo-shaded-3.0.3.jar:/Users/seven/software/maven/repository/com/esotericsoftware/minlog/1.3.0/minlog-1.3.0.jar:/Users/seven/software/maven/repository/org/objenesis/objenesis/2.1/objenesis-2.1.jar:/Users/seven/software/maven/repository/com/twitter/chill-java/0.8.0/chill-java-0.8.0.jar:/Users/seven/software/maven/repository/org/apache/xbean/xbean-asm5-shaded/4.4/xbean-asm5-shaded-4.4.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-launcher_2.11/2.2.0/spark-launcher_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-network-common_2.11/2.2.0/spark-network-common_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-network-shuffle_2.11/2.2.0/spark-network-shuffle_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-unsafe_2.11/2.2.0/spark-unsafe_2.11-2.2.0.jar:/Users/seven/software/maven/repository/javax/servlet/javax.servlet-api/3.1.0/javax.servlet-api-3.1.0.jar:/Users/seven/software/maven/repository/org/slf4j/jul-to-slf4j/1.7.16/jul-to-slf4j-1.7.16.jar:/Users/seven/software/maven/repository/org/slf4j/jcl-over-slf4j/1.7.16/jcl-over-slf4j-1.7.16.jar:/Users/seven/software/maven/repository/com/ning/compress-lzf/1.0.3/compress-lzf-1.0.3.jar:/Users/seven/software/maven/repository/org/xerial/snappy/snappy-java/1.1.2.6/snappy-java-1.1.2.6.jar:/Users/seven/software/maven/repository/org/roaringbitmap/RoaringBitmap/0.5.11/RoaringBitmap-0.5.11.jar:/Users/seven/software/maven/repository/org/json4s/json4s-jackson_2.11/3.2.11/json4s-jackson_2.11-3.2.11.jar:/Users/seven/software/maven/repository/org/json4s/json4s-core_2.11/3.2.11/json4s-core_2.11-3.2.11.jar:/Users/seven/software/maven/repository/org/json4s/json4s-ast_2.11/3.2.11/json4s-ast_2.11-3.2.11.jar:/Users/seven/software/maven/repository/org/scala-lang/scalap/2.11.0/scalap-2.11.0.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/core/jersey-client/2.22.2/jersey-client-2.22.2.jar:/Users/seven/software/maven/repository/javax/ws/rs/javax.ws.rs-api/2.0.1/javax.ws.rs-api-2.0.1.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/hk2-api/2.4.0-b34/hk2-api-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/hk2-utils/2.4.0-b34/hk2-utils-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/external/aopalliance-repackaged/2.4.0-b34/aopalliance-repackaged-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/external/javax.inject/2.4.0-b34/javax.inject-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/hk2-locator/2.4.0-b34/hk2-locator-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/javassist/javassist/3.18.1-GA/javassist-3.18.1-GA.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/core/jersey-common/2.22.2/jersey-common-2.22.2.jar:/Users/seven/software/maven/repository/javax/annotation/javax.annotation-api/1.2/javax.annotation-api-1.2.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/bundles/repackaged/jersey-guava/2.22.2/jersey-guava-2.22.2.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/osgi-resource-locator/1.0.1/osgi-resource-locator-1.0.1.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/core/jersey-server/2.22.2/jersey-server-2.22.2.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/media/jersey-media-jaxb/2.22.2/jersey-media-jaxb-2.22.2.jar:/Users/seven/software/maven/repository/javax/validation/validation-api/1.1.0.Final/validation-api-1.1.0.Final.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/containers/jersey-container-servlet/2.22.2/jersey-container-servlet-2.22.2.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/containers/jersey-container-servlet-core/2.22.2/jersey-container-servlet-core-2.22.2.jar:/Users/seven/software/maven/repository/com/clearspring/analytics/stream/2.7.0/stream-2.7.0.jar:/Users/seven/software/maven/repository/io/dropwizard/metrics/metrics-core/3.1.2/metrics-core-3.1.2.jar:/Users/seven/software/maven/repository/io/dropwizard/metrics/metrics-jvm/3.1.2/metrics-jvm-3.1.2.jar:/Users/seven/software/maven/repository/io/dropwizard/metrics/metrics-json/3.1.2/metrics-json-3.1.2.jar:/Users/seven/software/maven/repository/io/dropwizard/metrics/metrics-graphite/3.1.2/metrics-graphite-3.1.2.jar:/Users/seven/software/maven/repository/org/apache/ivy/ivy/2.4.0/ivy-2.4.0.jar:/Users/seven/software/maven/repository/oro/oro/2.0.8/oro-2.0.8.jar:/Users/seven/software/maven/repository/net/razorvine/pyrolite/4.13/pyrolite-4.13.jar:/Users/seven/software/maven/repository/net/sf/py4j/py4j/0.10.4/py4j-0.10.4.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-crypto/1.0.0/commons-crypto-1.0.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-streaming_2.11/2.2.0/spark-streaming_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-sql_2.11/2.2.0/spark-sql_2.11-2.2.0.jar:/Users/seven/software/maven/repository/com/univocity/univocity-parsers/2.2.1/univocity-parsers-2.2.1.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-sketch_2.11/2.2.0/spark-sketch_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-catalyst_2.11/2.2.0/spark-catalyst_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/codehaus/janino/janino/3.0.0/janino-3.0.0.jar:/Users/seven/software/maven/repository/org/codehaus/janino/commons-compiler/3.0.0/commons-compiler-3.0.0.jar:/Users/seven/software/maven/repository/org/antlr/antlr4-runtime/4.5.3/antlr4-runtime-4.5.3.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-column/1.8.2/parquet-column-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-common/1.8.2/parquet-common-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-encoding/1.8.2/parquet-encoding-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-hadoop/1.8.2/parquet-hadoop-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-format/2.3.1/parquet-format-2.3.1.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-jackson/1.8.2/parquet-jackson-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-graphx_2.11/2.2.0/spark-graphx_2.11-2.2.0.jar:/Users/seven/software/maven/repository/com/github/fommil/netlib/core/1.1.2/core-1.1.2.jar:/Users/seven/software/maven/repository/net/sourceforge/f2j/arpack_combined_all/0.1/arpack_combined_all-0.1.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-mllib-local_2.11/2.2.0/spark-mllib-local_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/scalanlp/breeze_2.11/0.13.1/breeze_2.11-0.13.1.jar:/Users/seven/software/maven/repository/org/scalanlp/breeze-macros_2.11/0.13.1/breeze-macros_2.11-0.13.1.jar:/Users/seven/software/maven/repository/net/sf/opencsv/opencsv/2.3/opencsv-2.3.jar:/Users/seven/software/maven/repository/com/github/rwl/jtransforms/2.4.0/jtransforms-2.4.0.jar:/Users/seven/software/maven/repository/org/spire-math/spire_2.11/0.13.0/spire_2.11-0.13.0.jar:/Users/seven/software/maven/repository/org/spire-math/spire-macros_2.11/0.13.0/spire-macros_2.11-0.13.0.jar:/Users/seven/software/maven/repository/org/typelevel/machinist_2.11/0.6.1/machinist_2.11-0.6.1.jar:/Users/seven/software/maven/repository/com/chuusai/shapeless_2.11/2.3.2/shapeless_2.11-2.3.2.jar:/Users/seven/software/maven/repository/org/typelevel/macro-compat_2.11/1.1.1/macro-compat_2.11-1.1.1.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-math3/3.4.1/commons-math3-3.4.1.jar:/Users/seven/software/maven/repository/org/jpmml/pmml-model/1.2.15/pmml-model-1.2.15.jar:/Users/seven/software/maven/repository/org/jpmml/pmml-schema/1.2.15/pmml-schema-1.2.15.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-tags_2.11/2.2.0/spark-tags_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-streaming-kafka-0-8_2.11/2.2.0/spark-streaming-kafka-0-8_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/kafka/kafka_2.11/0.8.2.1/kafka_2.11-0.8.2.1.jar:/Users/seven/software/maven/repository/com/101tec/zkclient/0.3/zkclient-0.3.jar:/Users/seven/software/maven/repository/org/apache/kafka/kafka-clients/0.8.2.1/kafka-clients-0.8.2.1.jar:/Users/seven/software/maven/repository/mysql/mysql-connector-java/5.1.31/mysql-connector-java-5.1.31.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/module/jackson-module-scala_2.11/2.6.5/jackson-module-scala_2.11-2.6.5.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/core/jackson-core/2.6.5/jackson-core-2.6.5.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/core/jackson-annotations/2.6.5/jackson-annotations-2.6.5.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/core/jackson-databind/2.6.5/jackson-databind-2.6.5.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/module/jackson-module-paranamer/2.6.5/jackson-module-paranamer-2.6.5.jar:/Users/seven/software/maven/repository/com/thoughtworks/paranamer/paranamer/2.6/paranamer-2.6.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-streaming-flume_2.11/2.2.0/spark-streaming-flume_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/flume/flume-ng-core/1.6.0/flume-ng-core-1.6.0.jar:/Users/seven/software/maven/repository/org/apache/flume/flume-ng-configuration/1.6.0/flume-ng-configuration-1.6.0.jar:/Users/seven/software/maven/repository/org/apache/avro/avro-ipc/1.7.4/avro-ipc-1.7.4.jar:/Users/seven/software/maven/repository/org/apache/velocity/velocity/1.7/velocity-1.7.jar:/Users/seven/software/maven/repository/joda-time/joda-time/2.1/joda-time-2.1.jar:/Users/seven/software/maven/repository/org/apache/mina/mina-core/2.0.4/mina-core-2.0.4.jar:/Users/seven/software/maven/repository/org/apache/flume/flume-ng-sdk/1.6.0/flume-ng-sdk-1.6.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-streaming-flume-sink_2.11/2.2.0/spark-streaming-flume-sink_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/flume/flume-ng-clients/flume-ng-log4jappender/1.6.0/flume-ng-log4jappender-1.6.0.jar:/Users/seven/software/maven/repository/net/jpountz/lz4/lz4/1.3.0/lz4-1.3.0.jar:/Applications/IntelliJ IDEA.app/Contents/lib/idea_rt.jar
[INFO][2018-05-24 19:58:10,676][org.apache.zookeeper.ZooKeeper]Client environment:java.library.path=/Users/seven/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:.
[INFO][2018-05-24 19:58:10,676][org.apache.zookeeper.ZooKeeper]Client environment:java.io.tmpdir=/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/
[INFO][2018-05-24 19:58:10,676][org.apache.zookeeper.ZooKeeper]Client environment:java.compiler=<NA>
[INFO][2018-05-24 19:58:10,677][org.apache.zookeeper.ZooKeeper]Client environment:os.name=Mac OS X
[INFO][2018-05-24 19:58:10,677][org.apache.zookeeper.ZooKeeper]Client environment:os.arch=x86_64
[INFO][2018-05-24 19:58:10,677][org.apache.zookeeper.ZooKeeper]Client environment:os.version=10.13.4
[INFO][2018-05-24 19:58:10,677][org.apache.zookeeper.ZooKeeper]Client environment:user.name=seven
[INFO][2018-05-24 19:58:10,677][org.apache.zookeeper.ZooKeeper]Client environment:user.home=/Users/seven
[INFO][2018-05-24 19:58:10,677][org.apache.zookeeper.ZooKeeper]Client environment:user.dir=/Users/seven/project/scala/dataMining
[INFO][2018-05-24 19:58:10,678][org.apache.zookeeper.ZooKeeper]Initiating client connection, connectString=vm-xaj-bigdata-da-d01:2181,vm-xaj-bigdata-da-d02:2181,vm-xaj-bigdata-da-d03:2181 sessionTimeout=60000 watcher=hconnection-0x68404ee20x0, quorum=vm-xaj-bigdata-da-d01:2181,vm-xaj-bigdata-da-d02:2181,vm-xaj-bigdata-da-d03:2181, baseZNode=/hbase
[INFO][2018-05-24 19:58:15,703][org.apache.zookeeper.ClientCnxn]Opening socket connection to server vm-xaj-bigdata-da-d03/10.213.4.27:2181. Will not attempt to authenticate using SASL (unknown error)
[INFO][2018-05-24 19:58:15,733][org.apache.zookeeper.ClientCnxn]Socket connection established, initiating session, client: /10.0.8.39:49190, server: vm-xaj-bigdata-da-d03/10.213.4.27:2181
[INFO][2018-05-24 19:58:15,765][org.apache.zookeeper.ClientCnxn]Session establishment complete on server vm-xaj-bigdata-da-d03/10.213.4.27:2181, sessionid = 0x362b50d75ec5f18, negotiated timeout = 60000
[INFO][2018-05-24 19:58:15,814][org.apache.spark.SparkContext]Running Spark version 2.2.0
[WARN][2018-05-24 19:58:16,804][org.apache.hadoop.util.NativeCodeLoader]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO][2018-05-24 19:58:16,953][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 19:58:16,957][org.apache.spark.SparkContext]Submitted application: SimulationKafkaSendOutData$
[INFO][2018-05-24 19:58:16,973][org.apache.spark.executor.Executor]Finished task 0.0 in stage 0.0 (TID 0). 794 bytes result sent to driver
[INFO][2018-05-24 19:58:16,980][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 0.0 (TID 0) in 6522 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 19:58:16,982][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 19:58:16,985][org.apache.spark.scheduler.DAGScheduler]ResultStage 0 (foreachPartition at ReceiveKafkaData.scala:64) finished in 6.535 s
[INFO][2018-05-24 19:58:16,990][org.apache.spark.scheduler.DAGScheduler]Job 0 finished: foreachPartition at ReceiveKafkaData.scala:64, took 6.777496 s
[INFO][2018-05-24 19:58:16,994][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527163080000 ms.0 from job set of time 1527163080000 ms
[INFO][2018-05-24 19:58:16,995][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 16.994 s for time 1527163080000 ms (execution: 6.829 s)
[INFO][2018-05-24 19:58:17,000][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 19:58:17,004][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 
[INFO][2018-05-24 19:58:17,008][org.apache.spark.SecurityManager]Changing view acls to: seven
[INFO][2018-05-24 19:58:17,009][org.apache.spark.SecurityManager]Changing modify acls to: seven
[INFO][2018-05-24 19:58:17,009][org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO][2018-05-24 19:58:17,010][org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO][2018-05-24 19:58:17,011][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(seven); groups with view permissions: Set(); users  with modify permissions: Set(seven); groups with modify permissions: Set()
[INFO][2018-05-24 19:58:17,326][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 49191.
[INFO][2018-05-24 19:58:17,348][org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO][2018-05-24 19:58:17,363][org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO][2018-05-24 19:58:17,365][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2018-05-24 19:58:17,365][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
[INFO][2018-05-24 19:58:17,375][org.apache.spark.storage.DiskBlockManager]Created local directory at /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/blockmgr-54a13771-ec4c-4e7e-89a9-74630e394be9
[INFO][2018-05-24 19:58:17,393][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 912.3 MB
[INFO][2018-05-24 19:58:17,466][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO][2018-05-24 19:58:17,555][org.spark_project.jetty.util.log]Logging initialized @2904ms
[INFO][2018-05-24 19:58:17,642][org.spark_project.jetty.server.Server]jetty-9.3.z-SNAPSHOT
[INFO][2018-05-24 19:58:17,660][org.spark_project.jetty.server.Server]Started @3011ms
[WARN][2018-05-24 19:58:17,678][org.apache.spark.util.Utils]Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[INFO][2018-05-24 19:58:17,684][org.spark_project.jetty.server.AbstractConnector]Started ServerConnector@60ca0db6{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}
[INFO][2018-05-24 19:58:17,684][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4041.
[INFO][2018-05-24 19:58:17,706][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@180e6ac4{/jobs,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:58:17,707][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@c7a975a{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:58:17,707][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@757d6814{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:58:17,708][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4c168660{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:58:17,709][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@fd0e5b6{/stages,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:58:17,709][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@36b0fcd5{/stages/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:58:17,710][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@475835b1{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:58:17,711][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@77192705{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:58:17,712][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7e809b79{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:58:17,712][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@625e134e{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:58:17,713][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@89c10b7{/storage,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:58:17,714][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4fe89c24{/storage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:58:17,714][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3d08f3f5{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:58:17,715][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1a1da881{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:58:17,716][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7fd4acee{/environment,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:58:17,717][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6175619b{/environment/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:58:17,718][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@756cf158{/executors,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:58:17,719][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@751d3241{/executors/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:58:17,720][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@64337702{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:58:17,720][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@30ea8c23{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:58:17,727][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4e76dac{/static,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:58:17,728][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@f9d87b{/,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:58:17,729][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@26fb628{/api,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:58:17,730][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5ac86ba5{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:58:17,731][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2c9399a4{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:58:17,737][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://192.168.0.102:4041
[INFO][2018-05-24 19:58:17,860][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
[INFO][2018-05-24 19:58:17,881][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 49192.
[INFO][2018-05-24 19:58:17,884][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 192.168.0.102:49192
[INFO][2018-05-24 19:58:17,887][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2018-05-24 19:58:17,888][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 192.168.0.102, 49192, None)
[INFO][2018-05-24 19:58:17,891][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 192.168.0.102:49192 with 912.3 MB RAM, BlockManagerId(driver, 192.168.0.102, 49192, None)
[INFO][2018-05-24 19:58:17,895][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 192.168.0.102, 49192, None)
[INFO][2018-05-24 19:58:17,895][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 192.168.0.102, 49192, None)
[INFO][2018-05-24 19:58:18,101][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@405325cf{/metrics/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 19:58:18,687][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0 stored as values in memory (estimated size 228.1 KB, free 912.1 MB)
[INFO][2018-05-24 19:58:18,753][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0_piece0 stored as bytes in memory (estimated size 22.1 KB, free 912.1 MB)
[INFO][2018-05-24 19:58:18,755][org.apache.spark.storage.BlockManagerInfo]Added broadcast_0_piece0 in memory on 192.168.0.102:49192 (size: 22.1 KB, free: 912.3 MB)
[INFO][2018-05-24 19:58:18,761][org.apache.spark.SparkContext]Created broadcast 0 from textFile at SimulationKafkaSendOutData.scala:25
[WARN][2018-05-24 19:58:24,178][org.apache.hadoop.util.NativeCodeLoader]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO][2018-05-24 19:58:24,327][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 1
[INFO][2018-05-24 19:58:24,418][org.apache.spark.SparkContext]Starting job: collect at SimulationKafkaSendOutData.scala:25
[INFO][2018-05-24 19:58:24,430][org.apache.spark.scheduler.DAGScheduler]Got job 0 (collect at SimulationKafkaSendOutData.scala:25) with 2 output partitions
[INFO][2018-05-24 19:58:24,430][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 0 (collect at SimulationKafkaSendOutData.scala:25)
[INFO][2018-05-24 19:58:24,431][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 19:58:24,432][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 19:58:24,439][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 0 (hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/* MapPartitionsRDD[1] at textFile at SimulationKafkaSendOutData.scala:25), which has no missing parents
[INFO][2018-05-24 19:58:24,456][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1 stored as values in memory (estimated size 3.3 KB, free 912.1 MB)
[INFO][2018-05-24 19:58:24,457][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1_piece0 stored as bytes in memory (estimated size 1969.0 B, free 912.1 MB)
[INFO][2018-05-24 19:58:24,458][org.apache.spark.storage.BlockManagerInfo]Added broadcast_1_piece0 in memory on 192.168.0.102:49192 (size: 1969.0 B, free: 912.3 MB)
[INFO][2018-05-24 19:58:24,459][org.apache.spark.SparkContext]Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 19:58:24,473][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ResultStage 0 (hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/* MapPartitionsRDD[1] at textFile at SimulationKafkaSendOutData.scala:25) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2018-05-24 19:58:24,474][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 0.0 with 2 tasks
[INFO][2018-05-24 19:58:24,512][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 4883 bytes)
[INFO][2018-05-24 19:58:24,517][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 4883 bytes)
[INFO][2018-05-24 19:58:24,524][org.apache.spark.executor.Executor]Running task 0.0 in stage 0.0 (TID 0)
[INFO][2018-05-24 19:58:24,524][org.apache.spark.executor.Executor]Running task 1.0 in stage 0.0 (TID 1)
[INFO][2018-05-24 19:58:24,586][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/part-00000:0+10717435
[INFO][2018-05-24 19:58:24,587][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/part-00000:10717435+10717436
[INFO][2018-05-24 19:58:34,831][org.apache.spark.storage.memory.MemoryStore]Block taskresult_0 stored as bytes in memory (estimated size 10.4 MB, free 901.7 MB)
[INFO][2018-05-24 19:58:34,833][org.apache.spark.storage.BlockManagerInfo]Added taskresult_0 in memory on 192.168.0.102:49192 (size: 10.4 MB, free: 901.9 MB)
[INFO][2018-05-24 19:58:34,834][org.apache.spark.executor.Executor]Finished task 0.0 in stage 0.0 (TID 0). 10856349 bytes result sent via BlockManager)
[INFO][2018-05-24 19:58:34,866][org.apache.spark.network.client.TransportClientFactory]Successfully created connection to /192.168.0.102:49192 after 18 ms (0 ms spent in bootstraps)
[INFO][2018-05-24 19:58:35,139][org.apache.spark.storage.BlockManagerInfo]Removed taskresult_0 on 192.168.0.102:49192 in memory (size: 10.4 MB, free: 912.3 MB)
[INFO][2018-05-24 19:58:35,139][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 0.0 (TID 0) in 10638 ms on localhost (executor driver) (1/2)
[INFO][2018-05-24 19:58:35,736][org.apache.spark.storage.memory.MemoryStore]Block taskresult_1 stored as bytes in memory (estimated size 10.4 MB, free 901.7 MB)
[INFO][2018-05-24 19:58:35,737][org.apache.spark.storage.BlockManagerInfo]Added taskresult_1 in memory on 192.168.0.102:49192 (size: 10.4 MB, free: 901.9 MB)
[INFO][2018-05-24 19:58:35,737][org.apache.spark.executor.Executor]Finished task 1.0 in stage 0.0 (TID 1). 10855899 bytes result sent via BlockManager)
[INFO][2018-05-24 19:58:35,839][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 0.0 (TID 1) in 11322 ms on localhost (executor driver) (2/2)
[INFO][2018-05-24 19:58:35,839][org.apache.spark.storage.BlockManagerInfo]Removed taskresult_1 on 192.168.0.102:49192 in memory (size: 10.4 MB, free: 912.3 MB)
[INFO][2018-05-24 19:58:35,840][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 19:58:35,841][org.apache.spark.scheduler.DAGScheduler]ResultStage 0 (collect at SimulationKafkaSendOutData.scala:25) finished in 11.351 s
[INFO][2018-05-24 19:58:35,845][org.apache.spark.scheduler.DAGScheduler]Job 0 finished: collect at SimulationKafkaSendOutData.scala:25, took 11.426325 s
[INFO][2018-05-24 19:58:35,904][org.spark_project.jetty.server.AbstractConnector]Stopped Spark@60ca0db6{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}
[INFO][2018-05-24 19:58:35,907][org.apache.spark.ui.SparkUI]Stopped Spark web UI at http://192.168.0.102:4041
[INFO][2018-05-24 19:58:35,915][org.apache.spark.MapOutputTrackerMasterEndpoint]MapOutputTrackerMasterEndpoint stopped!
[INFO][2018-05-24 19:58:35,927][org.apache.spark.storage.memory.MemoryStore]MemoryStore cleared
[INFO][2018-05-24 19:58:35,928][org.apache.spark.storage.BlockManager]BlockManager stopped
[INFO][2018-05-24 19:58:35,987][org.apache.spark.storage.BlockManagerMaster]BlockManagerMaster stopped
[INFO][2018-05-24 19:58:35,989][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint]OutputCommitCoordinator stopped!
[INFO][2018-05-24 19:58:35,990][org.apache.spark.SparkContext]Successfully stopped SparkContext
[INFO][2018-05-24 19:58:36,005][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	compression.type = none
	metric.reporters = []
	metadata.max.age.ms = 300000
	metadata.fetch.timeout.ms = 60000
	acks = all
	batch.size = 16384
	reconnect.backoff.ms = 10
	bootstrap.servers = [vm-xaj-bigdata-da-d01:9092, vm-xaj-bigdata-da-d02:9092, vm-xaj-bigdata-da-d03:9092, vm-xaj-bigdata-da-d04:9092, vm-xaj-bigdata-da-d05:9092, vm-xaj-bigdata-da-d06:9092, vm-xaj-bigdata-da-d07:9092]
	receive.buffer.bytes = 32768
	retry.backoff.ms = 100
	buffer.memory = 33554432
	timeout.ms = 30000
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	retries = 0
	max.request.size = 1048576
	block.on.buffer.full = true
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
	metrics.sample.window.ms = 30000
	send.buffer.bytes = 131072
	max.in.flight.requests.per.connection = 5
	metrics.num.samples = 2
	linger.ms = 1
	client.id = 

[INFO][2018-05-24 19:59:10,098][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527163140000 ms
[INFO][2018-05-24 19:59:10,100][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527163140000 ms.0 from job set of time 1527163140000 ms
[INFO][2018-05-24 19:59:10,120][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:64
[INFO][2018-05-24 19:59:10,122][org.apache.spark.scheduler.DAGScheduler]Got job 1 (foreachPartition at ReceiveKafkaData.scala:64) with 1 output partitions
[INFO][2018-05-24 19:59:10,122][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 1 (foreachPartition at ReceiveKafkaData.scala:64)
[INFO][2018-05-24 19:59:10,122][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 19:59:10,123][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 19:59:10,123][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 1 (MapPartitionsRDD[3] at map at ReceiveKafkaData.scala:62), which has no missing parents
[INFO][2018-05-24 19:59:10,129][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-24 19:59:10,137][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1_piece0 stored as bytes in memory (estimated size 2008.0 B, free 912.3 MB)
[INFO][2018-05-24 19:59:10,138][org.apache.spark.storage.BlockManagerInfo]Added broadcast_1_piece0 in memory on 192.168.0.102:49171 (size: 2008.0 B, free: 912.3 MB)
[INFO][2018-05-24 19:59:10,139][org.apache.spark.SparkContext]Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 19:59:10,141][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at map at ReceiveKafkaData.scala:62) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 19:59:10,141][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 1.0 with 1 tasks
[INFO][2018-05-24 19:59:10,144][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 19:59:10,145][org.apache.spark.executor.Executor]Running task 0.0 in stage 1.0 (TID 1)
[INFO][2018-05-24 19:59:10,217][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 11976 is the same as ending offset skipping seven 0
[INFO][2018-05-24 19:59:10,219][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 19:59:10,221][org.apache.spark.executor.Executor]Finished task 0.0 in stage 1.0 (TID 1). 708 bytes result sent to driver
[INFO][2018-05-24 19:59:10,224][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 1.0 (TID 1) in 81 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 19:59:10,224][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 19:59:10,228][org.apache.spark.scheduler.DAGScheduler]ResultStage 1 (foreachPartition at ReceiveKafkaData.scala:64) finished in 0.084 s
[INFO][2018-05-24 19:59:10,232][org.apache.spark.scheduler.DAGScheduler]Job 1 finished: foreachPartition at ReceiveKafkaData.scala:64, took 0.111857 s
[INFO][2018-05-24 19:59:10,234][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527163140000 ms.0 from job set of time 1527163140000 ms
[INFO][2018-05-24 19:59:10,235][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 10.234 s for time 1527163140000 ms (execution: 0.135 s)
[INFO][2018-05-24 19:59:10,236][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 1 from persistence list
[INFO][2018-05-24 19:59:10,251][org.apache.spark.storage.BlockManager]Removing RDD 1
[INFO][2018-05-24 19:59:10,256][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 0 from persistence list
[INFO][2018-05-24 19:59:10,260][org.apache.spark.storage.BlockManager]Removing RDD 0
[INFO][2018-05-24 19:59:10,261][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 19:59:10,261][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 
[INFO][2018-05-24 20:00:05,074][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527163200000 ms
[INFO][2018-05-24 20:00:05,075][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527163200000 ms.0 from job set of time 1527163200000 ms
[INFO][2018-05-24 20:00:05,095][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:64
[INFO][2018-05-24 20:00:05,096][org.apache.spark.scheduler.DAGScheduler]Got job 2 (foreachPartition at ReceiveKafkaData.scala:64) with 1 output partitions
[INFO][2018-05-24 20:00:05,096][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 2 (foreachPartition at ReceiveKafkaData.scala:64)
[INFO][2018-05-24 20:00:05,096][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:00:05,096][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:00:05,099][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 2 (MapPartitionsRDD[5] at map at ReceiveKafkaData.scala:62), which has no missing parents
[INFO][2018-05-24 20:00:05,103][org.apache.spark.storage.memory.MemoryStore]Block broadcast_2 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-24 20:00:05,113][org.apache.spark.storage.memory.MemoryStore]Block broadcast_2_piece0 stored as bytes in memory (estimated size 2008.0 B, free 912.3 MB)
[INFO][2018-05-24 20:00:05,115][org.apache.spark.storage.BlockManagerInfo]Added broadcast_2_piece0 in memory on 192.168.0.102:49171 (size: 2008.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:00:05,116][org.apache.spark.SparkContext]Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:00:05,117][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[5] at map at ReceiveKafkaData.scala:62) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:00:05,117][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 2.0 with 1 tasks
[INFO][2018-05-24 20:00:05,118][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:00:05,118][org.apache.spark.executor.Executor]Running task 0.0 in stage 2.0 (TID 2)
[INFO][2018-05-24 20:00:05,127][org.apache.spark.streaming.kafka.KafkaRDD]Computing topic seven, partition 0 offsets 11976 -> 12030
[INFO][2018-05-24 20:00:05,128][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 20:00:05,128][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 20:00:05,128][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 20:00:10,726][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:00:10,727][org.apache.spark.executor.Executor]Finished task 0.0 in stage 2.0 (TID 2). 751 bytes result sent to driver
[INFO][2018-05-24 20:00:10,730][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 2.0 (TID 2) in 5612 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:00:10,730][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:00:10,732][org.apache.spark.scheduler.DAGScheduler]ResultStage 2 (foreachPartition at ReceiveKafkaData.scala:64) finished in 5.614 s
[INFO][2018-05-24 20:00:10,734][org.apache.spark.scheduler.DAGScheduler]Job 2 finished: foreachPartition at ReceiveKafkaData.scala:64, took 5.638829 s
[INFO][2018-05-24 20:00:10,742][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527163200000 ms.0 from job set of time 1527163200000 ms
[INFO][2018-05-24 20:00:10,743][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 10.742 s for time 1527163200000 ms (execution: 5.667 s)
[INFO][2018-05-24 20:00:10,743][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 3 from persistence list
[INFO][2018-05-24 20:00:10,745][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_1_piece0 on 192.168.0.102:49171 in memory (size: 2008.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:00:10,747][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 2 from persistence list
[INFO][2018-05-24 20:00:10,747][org.apache.spark.storage.BlockManager]Removing RDD 3
[INFO][2018-05-24 20:00:10,748][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:00:10,748][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527163080000 ms
[INFO][2018-05-24 20:00:10,749][org.apache.spark.storage.BlockManager]Removing RDD 2
[INFO][2018-05-24 20:00:51,726][org.apache.spark.util.ShutdownHookManager]Shutdown hook called
[INFO][2018-05-24 20:00:51,729][org.apache.spark.util.ShutdownHookManager]Deleting directory /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/spark-629a02a3-0cd7-4fef-989d-b0b80b9c30d6
[INFO][2018-05-24 20:01:10,078][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527163260000 ms
[INFO][2018-05-24 20:01:10,079][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527163260000 ms.0 from job set of time 1527163260000 ms
[INFO][2018-05-24 20:01:10,089][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:64
[INFO][2018-05-24 20:01:10,090][org.apache.spark.scheduler.DAGScheduler]Got job 3 (foreachPartition at ReceiveKafkaData.scala:64) with 1 output partitions
[INFO][2018-05-24 20:01:10,091][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 3 (foreachPartition at ReceiveKafkaData.scala:64)
[INFO][2018-05-24 20:01:10,091][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:01:10,091][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:01:10,091][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 3 (MapPartitionsRDD[7] at map at ReceiveKafkaData.scala:62), which has no missing parents
[INFO][2018-05-24 20:01:10,095][org.apache.spark.storage.memory.MemoryStore]Block broadcast_3 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-24 20:01:10,099][org.apache.spark.storage.memory.MemoryStore]Block broadcast_3_piece0 stored as bytes in memory (estimated size 2008.0 B, free 912.3 MB)
[INFO][2018-05-24 20:01:10,100][org.apache.spark.storage.BlockManagerInfo]Added broadcast_3_piece0 in memory on 192.168.0.102:49171 (size: 2008.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:01:10,100][org.apache.spark.SparkContext]Created broadcast 3 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:01:10,101][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[7] at map at ReceiveKafkaData.scala:62) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:01:10,101][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 3.0 with 1 tasks
[INFO][2018-05-24 20:01:10,102][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 3.0 (TID 3, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:01:10,102][org.apache.spark.executor.Executor]Running task 0.0 in stage 3.0 (TID 3)
[INFO][2018-05-24 20:01:10,105][org.apache.spark.streaming.kafka.KafkaRDD]Computing topic seven, partition 0 offsets 12030 -> 12076
[INFO][2018-05-24 20:01:10,106][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 20:01:10,106][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 20:01:10,106][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 20:01:15,228][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:01:15,229][org.apache.spark.executor.Executor]Finished task 0.0 in stage 3.0 (TID 3). 708 bytes result sent to driver
[INFO][2018-05-24 20:01:15,231][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 3.0 (TID 3) in 5129 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:01:15,231][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 3.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:01:15,232][org.apache.spark.scheduler.DAGScheduler]ResultStage 3 (foreachPartition at ReceiveKafkaData.scala:64) finished in 5.131 s
[INFO][2018-05-24 20:01:15,232][org.apache.spark.scheduler.DAGScheduler]Job 3 finished: foreachPartition at ReceiveKafkaData.scala:64, took 5.142500 s
[INFO][2018-05-24 20:01:15,233][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527163260000 ms.0 from job set of time 1527163260000 ms
[INFO][2018-05-24 20:01:15,233][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 15.233 s for time 1527163260000 ms (execution: 5.154 s)
[INFO][2018-05-24 20:01:15,233][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 5 from persistence list
[INFO][2018-05-24 20:01:15,234][org.apache.spark.storage.BlockManager]Removing RDD 5
[INFO][2018-05-24 20:01:15,234][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 4 from persistence list
[INFO][2018-05-24 20:01:15,235][org.apache.spark.storage.BlockManager]Removing RDD 4
[INFO][2018-05-24 20:01:15,235][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:01:15,235][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527163140000 ms
[INFO][2018-05-24 20:02:10,091][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527163320000 ms
[INFO][2018-05-24 20:02:10,091][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527163320000 ms.0 from job set of time 1527163320000 ms
[INFO][2018-05-24 20:02:10,099][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:64
[INFO][2018-05-24 20:02:10,101][org.apache.spark.scheduler.DAGScheduler]Got job 4 (foreachPartition at ReceiveKafkaData.scala:64) with 1 output partitions
[INFO][2018-05-24 20:02:10,101][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 4 (foreachPartition at ReceiveKafkaData.scala:64)
[INFO][2018-05-24 20:02:10,101][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:02:10,102][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:02:10,102][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 4 (MapPartitionsRDD[9] at map at ReceiveKafkaData.scala:62), which has no missing parents
[INFO][2018-05-24 20:02:10,104][org.apache.spark.storage.memory.MemoryStore]Block broadcast_4 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-24 20:02:10,109][org.apache.spark.storage.memory.MemoryStore]Block broadcast_4_piece0 stored as bytes in memory (estimated size 2008.0 B, free 912.3 MB)
[INFO][2018-05-24 20:02:10,109][org.apache.spark.storage.BlockManagerInfo]Added broadcast_4_piece0 in memory on 192.168.0.102:49171 (size: 2008.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:02:10,110][org.apache.spark.SparkContext]Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:02:10,110][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[9] at map at ReceiveKafkaData.scala:62) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:02:10,111][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 4.0 with 1 tasks
[INFO][2018-05-24 20:02:10,111][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 4.0 (TID 4, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:02:10,112][org.apache.spark.executor.Executor]Running task 0.0 in stage 4.0 (TID 4)
[INFO][2018-05-24 20:02:10,115][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12076 is the same as ending offset skipping seven 0
[INFO][2018-05-24 20:02:10,115][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:02:10,116][org.apache.spark.executor.Executor]Finished task 0.0 in stage 4.0 (TID 4). 708 bytes result sent to driver
[INFO][2018-05-24 20:02:10,117][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 4.0 (TID 4) in 6 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:02:10,117][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 4.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:02:10,117][org.apache.spark.scheduler.DAGScheduler]ResultStage 4 (foreachPartition at ReceiveKafkaData.scala:64) finished in 0.006 s
[INFO][2018-05-24 20:02:10,118][org.apache.spark.scheduler.DAGScheduler]Job 4 finished: foreachPartition at ReceiveKafkaData.scala:64, took 0.018234 s
[INFO][2018-05-24 20:02:10,118][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527163320000 ms.0 from job set of time 1527163320000 ms
[INFO][2018-05-24 20:02:10,119][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 10.118 s for time 1527163320000 ms (execution: 0.027 s)
[INFO][2018-05-24 20:02:10,119][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 7 from persistence list
[INFO][2018-05-24 20:02:10,119][org.apache.spark.storage.BlockManager]Removing RDD 7
[INFO][2018-05-24 20:02:10,119][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 6 from persistence list
[INFO][2018-05-24 20:02:10,119][org.apache.spark.storage.BlockManager]Removing RDD 6
[INFO][2018-05-24 20:02:10,119][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:02:10,120][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527163200000 ms
[INFO][2018-05-24 20:02:45,885][org.apache.spark.streaming.StreamingContext]Invoking stop(stopGracefully=false) from shutdown hook
[INFO][2018-05-24 20:02:45,889][org.apache.spark.streaming.scheduler.ReceiverTracker]ReceiverTracker stopped
[INFO][2018-05-24 20:02:45,890][org.apache.spark.streaming.scheduler.JobGenerator]Stopping JobGenerator immediately
[INFO][2018-05-24 20:02:45,891][org.apache.spark.streaming.util.RecurringTimer]Stopped timer for JobGenerator after time 1527163320000
[INFO][2018-05-24 20:02:45,893][org.apache.spark.streaming.scheduler.JobGenerator]Stopped JobGenerator
[INFO][2018-05-24 20:02:45,894][org.apache.spark.streaming.scheduler.JobScheduler]Stopped JobScheduler
[INFO][2018-05-24 20:02:45,907][org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@57a48985{/streaming,null,UNAVAILABLE,@Spark}
[INFO][2018-05-24 20:02:45,908][org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@3f672204{/streaming/batch,null,UNAVAILABLE,@Spark}
[INFO][2018-05-24 20:02:45,909][org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@2d000e80{/static/streaming,null,UNAVAILABLE,@Spark}
[INFO][2018-05-24 20:02:45,910][org.apache.spark.streaming.StreamingContext]StreamingContext stopped successfully
[INFO][2018-05-24 20:02:45,910][org.apache.spark.SparkContext]Invoking stop() from shutdown hook
[INFO][2018-05-24 20:02:45,919][org.spark_project.jetty.server.AbstractConnector]Stopped Spark@2090b9b9{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-05-24 20:02:45,922][org.apache.spark.ui.SparkUI]Stopped Spark web UI at http://192.168.0.102:4040
[INFO][2018-05-24 20:02:45,933][org.apache.spark.MapOutputTrackerMasterEndpoint]MapOutputTrackerMasterEndpoint stopped!
[INFO][2018-05-24 20:02:45,947][org.apache.spark.storage.memory.MemoryStore]MemoryStore cleared
[INFO][2018-05-24 20:02:45,947][org.apache.spark.storage.BlockManager]BlockManager stopped
[INFO][2018-05-24 20:02:45,948][org.apache.spark.storage.BlockManagerMaster]BlockManagerMaster stopped
[INFO][2018-05-24 20:02:45,950][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint]OutputCommitCoordinator stopped!
[INFO][2018-05-24 20:02:45,951][org.apache.spark.SparkContext]Successfully stopped SparkContext
[INFO][2018-05-24 20:02:45,952][org.apache.spark.util.ShutdownHookManager]Shutdown hook called
[INFO][2018-05-24 20:02:45,952][org.apache.spark.util.ShutdownHookManager]Deleting directory /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/spark-d6cfb298-1773-407a-bfe4-3e85d59fe166
[INFO][2018-05-24 20:10:13,570][org.apache.spark.SparkContext]Running Spark version 2.2.0
[INFO][2018-05-24 20:10:14,564][org.apache.spark.SparkContext]Submitted application: ReceiveKafkaData$
[INFO][2018-05-24 20:10:14,591][org.apache.spark.SecurityManager]Changing view acls to: seven
[INFO][2018-05-24 20:10:14,592][org.apache.spark.SecurityManager]Changing modify acls to: seven
[INFO][2018-05-24 20:10:14,592][org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO][2018-05-24 20:10:14,593][org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO][2018-05-24 20:10:14,600][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(seven); groups with view permissions: Set(); users  with modify permissions: Set(seven); groups with modify permissions: Set()
[INFO][2018-05-24 20:10:14,987][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 49457.
[INFO][2018-05-24 20:10:15,018][org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO][2018-05-24 20:10:15,053][org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO][2018-05-24 20:10:15,056][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2018-05-24 20:10:15,057][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
[INFO][2018-05-24 20:10:15,070][org.apache.spark.storage.DiskBlockManager]Created local directory at /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/blockmgr-fe1d0352-8474-4361-8dbb-40c093df3b65
[INFO][2018-05-24 20:10:15,085][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 912.3 MB
[INFO][2018-05-24 20:10:15,158][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO][2018-05-24 20:10:15,259][org.spark_project.jetty.util.log]Logging initialized @2960ms
[INFO][2018-05-24 20:10:15,330][org.spark_project.jetty.server.Server]jetty-9.3.z-SNAPSHOT
[INFO][2018-05-24 20:10:15,353][org.spark_project.jetty.server.Server]Started @3056ms
[INFO][2018-05-24 20:10:15,388][org.spark_project.jetty.server.AbstractConnector]Started ServerConnector@24faea88{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-05-24 20:10:15,388][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4040.
[INFO][2018-05-24 20:10:15,437][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@649725e3{/jobs,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:10:15,438][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1a1da881{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:10:15,443][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7fd4acee{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:10:15,446][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2f058b8a{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:10:15,447][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3f2ef586{/stages,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:10:15,448][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@76c7beb3{/stages/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:10:15,450][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2cf92cc7{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:10:15,454][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@611df6e3{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:10:15,454][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6273c5a4{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:10:15,455][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@53e211ee{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:10:15,471][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3d8bbcdc{/storage,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:10:15,480][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@117e0fe5{/storage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:10:15,481][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@78aea4b9{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:10:15,482][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4b85880b{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:10:15,483][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4215838f{/environment,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:10:15,487][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2289aca5{/environment/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:10:15,488][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@184497d1{/executors,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:10:15,488][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6ffab045{/executors/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:10:15,492][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3e2943ab{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:10:15,493][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4a9f80d3{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:10:15,508][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@41fe9859{/static,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:10:15,512][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4602c2a9{/,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:10:15,514][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3e2822{/api,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:10:15,518][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5fcacc0{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:10:15,520][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6d1d4d7{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:10:15,526][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://192.168.0.102:4040
[INFO][2018-05-24 20:10:15,662][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
[INFO][2018-05-24 20:10:15,731][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 49458.
[INFO][2018-05-24 20:10:15,733][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 192.168.0.102:49458
[INFO][2018-05-24 20:10:15,735][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2018-05-24 20:10:15,739][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 192.168.0.102, 49458, None)
[INFO][2018-05-24 20:10:15,745][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 192.168.0.102:49458 with 912.3 MB RAM, BlockManagerId(driver, 192.168.0.102, 49458, None)
[INFO][2018-05-24 20:10:15,754][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 192.168.0.102, 49458, None)
[INFO][2018-05-24 20:10:15,755][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 192.168.0.102, 49458, None)
[INFO][2018-05-24 20:10:16,085][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@18ca3c62{/metrics/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:10:16,244][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 20:10:16,250][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 20:10:16,251][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 20:10:31,715][org.apache.spark.streaming.kafka.DirectKafkaInputDStream]Slide time = 10000 ms
[INFO][2018-05-24 20:10:31,716][org.apache.spark.streaming.kafka.DirectKafkaInputDStream]Storage level = Serialized 1x Replicated
[INFO][2018-05-24 20:10:31,716][org.apache.spark.streaming.kafka.DirectKafkaInputDStream]Checkpoint interval = null
[INFO][2018-05-24 20:10:31,717][org.apache.spark.streaming.kafka.DirectKafkaInputDStream]Remember interval = 10000 ms
[INFO][2018-05-24 20:10:31,718][org.apache.spark.streaming.kafka.DirectKafkaInputDStream]Initialized and validated org.apache.spark.streaming.kafka.DirectKafkaInputDStream@748e565d
[INFO][2018-05-24 20:10:31,718][org.apache.spark.streaming.dstream.MappedDStream]Slide time = 10000 ms
[INFO][2018-05-24 20:10:31,718][org.apache.spark.streaming.dstream.MappedDStream]Storage level = Serialized 1x Replicated
[INFO][2018-05-24 20:10:31,718][org.apache.spark.streaming.dstream.MappedDStream]Checkpoint interval = null
[INFO][2018-05-24 20:10:31,718][org.apache.spark.streaming.dstream.MappedDStream]Remember interval = 10000 ms
[INFO][2018-05-24 20:10:31,718][org.apache.spark.streaming.dstream.MappedDStream]Initialized and validated org.apache.spark.streaming.dstream.MappedDStream@d3c017d
[INFO][2018-05-24 20:10:31,719][org.apache.spark.streaming.dstream.ForEachDStream]Slide time = 10000 ms
[INFO][2018-05-24 20:10:31,719][org.apache.spark.streaming.dstream.ForEachDStream]Storage level = Serialized 1x Replicated
[INFO][2018-05-24 20:10:31,719][org.apache.spark.streaming.dstream.ForEachDStream]Checkpoint interval = null
[INFO][2018-05-24 20:10:31,719][org.apache.spark.streaming.dstream.ForEachDStream]Remember interval = 10000 ms
[INFO][2018-05-24 20:10:31,719][org.apache.spark.streaming.dstream.ForEachDStream]Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@56a7fc14
[INFO][2018-05-24 20:10:31,778][org.apache.spark.streaming.util.RecurringTimer]Started timer for JobGenerator at time 1527163840000
[INFO][2018-05-24 20:10:31,779][org.apache.spark.streaming.scheduler.JobGenerator]Started JobGenerator at 1527163840000 ms
[INFO][2018-05-24 20:10:31,780][org.apache.spark.streaming.scheduler.JobScheduler]Started JobScheduler
[INFO][2018-05-24 20:10:31,784][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1ab6718{/streaming,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:10:31,785][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@460510aa{/streaming/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:10:31,786][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@78b41097{/streaming/batch,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:10:31,786][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@327c7bea{/streaming/batch/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:10:31,787][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7cf283e1{/static/streaming,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:10:31,788][org.apache.spark.streaming.StreamingContext]StreamingContext started
[INFO][2018-05-24 20:10:38,308][org.apache.spark.SparkContext]Running Spark version 2.2.0
[INFO][2018-05-24 20:10:39,124][org.apache.spark.SparkContext]Submitted application: SimulationKafkaSendOutData$
[INFO][2018-05-24 20:10:39,145][org.apache.spark.SecurityManager]Changing view acls to: seven
[INFO][2018-05-24 20:10:39,145][org.apache.spark.SecurityManager]Changing modify acls to: seven
[INFO][2018-05-24 20:10:39,146][org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO][2018-05-24 20:10:39,146][org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO][2018-05-24 20:10:39,147][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(seven); groups with view permissions: Set(); users  with modify permissions: Set(seven); groups with modify permissions: Set()
[INFO][2018-05-24 20:10:39,429][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 49472.
[INFO][2018-05-24 20:10:39,449][org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO][2018-05-24 20:10:39,464][org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO][2018-05-24 20:10:39,468][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2018-05-24 20:10:39,468][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
[INFO][2018-05-24 20:10:39,478][org.apache.spark.storage.DiskBlockManager]Created local directory at /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/blockmgr-0a997afd-72ad-4ca4-b2cc-4f5e1fa9c153
[INFO][2018-05-24 20:10:39,500][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 912.3 MB
[INFO][2018-05-24 20:10:39,603][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO][2018-05-24 20:10:39,706][org.spark_project.jetty.util.log]Logging initialized @2477ms
[INFO][2018-05-24 20:10:39,785][org.spark_project.jetty.server.Server]jetty-9.3.z-SNAPSHOT
[INFO][2018-05-24 20:10:39,799][org.spark_project.jetty.server.Server]Started @2571ms
[WARN][2018-05-24 20:10:39,827][org.apache.spark.util.Utils]Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[INFO][2018-05-24 20:10:39,834][org.spark_project.jetty.server.AbstractConnector]Started ServerConnector@1643d68f{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}
[INFO][2018-05-24 20:10:39,835][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4041.
[INFO][2018-05-24 20:10:39,863][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2a39fe6a{/jobs,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:10:39,864][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@649725e3{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:10:39,864][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4c168660{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:10:39,866][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4eed46ee{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:10:39,867][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4fad94a7{/stages,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:10:39,867][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6326d182{/stages/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:10:39,869][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@716a7124{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:10:39,871][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5cc126dc{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:10:39,871][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@72bd06ca{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:10:39,872][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5dbe30be{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:10:39,873][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@55795845{/storage,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:10:39,873][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@119f1f2a{/storage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:10:39,873][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5b970f7{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:10:39,874][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@165b8a71{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:10:39,875][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2f058b8a{/environment,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:10:39,876][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3f2ef586{/environment/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:10:39,876][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@76c7beb3{/executors,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:10:39,877][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2cf92cc7{/executors/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:10:39,877][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7b139eab{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:10:39,878][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@611df6e3{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:10:39,884][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6273c5a4{/static,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:10:39,884][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3e2943ab{/,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:10:39,886][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4a9f80d3{/api,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:10:39,886][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@191ae03f{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:10:39,887][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@53ab0286{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:10:39,895][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://192.168.0.102:4041
[INFO][2018-05-24 20:10:39,993][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
[INFO][2018-05-24 20:10:40,011][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 49473.
[INFO][2018-05-24 20:10:40,012][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 192.168.0.102:49473
[INFO][2018-05-24 20:10:40,013][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2018-05-24 20:10:40,015][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 192.168.0.102, 49473, None)
[INFO][2018-05-24 20:10:40,019][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 192.168.0.102:49473 with 912.3 MB RAM, BlockManagerId(driver, 192.168.0.102, 49473, None)
[INFO][2018-05-24 20:10:40,028][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 192.168.0.102, 49473, None)
[INFO][2018-05-24 20:10:40,030][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 192.168.0.102, 49473, None)
[INFO][2018-05-24 20:10:40,092][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 20:10:40,092][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 20:10:40,092][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 20:10:40,295][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@63034ed1{/metrics/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:10:40,849][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0 stored as values in memory (estimated size 228.1 KB, free 912.1 MB)
[INFO][2018-05-24 20:10:40,923][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0_piece0 stored as bytes in memory (estimated size 22.1 KB, free 912.1 MB)
[INFO][2018-05-24 20:10:40,925][org.apache.spark.storage.BlockManagerInfo]Added broadcast_0_piece0 in memory on 192.168.0.102:49473 (size: 22.1 KB, free: 912.3 MB)
[INFO][2018-05-24 20:10:40,931][org.apache.spark.SparkContext]Created broadcast 0 from textFile at SimulationKafkaSendOutData.scala:25
[INFO][2018-05-24 20:10:45,213][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527163840000 ms
[INFO][2018-05-24 20:10:45,219][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527163840000 ms.0 from job set of time 1527163840000 ms
[INFO][2018-05-24 20:10:45,279][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:64
[INFO][2018-05-24 20:10:45,309][org.apache.spark.scheduler.DAGScheduler]Got job 0 (foreachPartition at ReceiveKafkaData.scala:64) with 1 output partitions
[INFO][2018-05-24 20:10:45,311][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 0 (foreachPartition at ReceiveKafkaData.scala:64)
[INFO][2018-05-24 20:10:45,311][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:10:45,312][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:10:45,326][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 0 (MapPartitionsRDD[1] at map at ReceiveKafkaData.scala:62), which has no missing parents
[INFO][2018-05-24 20:10:45,522][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-24 20:10:45,565][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0_piece0 stored as bytes in memory (estimated size 2007.0 B, free 912.3 MB)
[INFO][2018-05-24 20:10:45,566][org.apache.spark.storage.BlockManagerInfo]Added broadcast_0_piece0 in memory on 192.168.0.102:49458 (size: 2007.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:10:45,572][org.apache.spark.SparkContext]Created broadcast 0 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:10:45,596][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at map at ReceiveKafkaData.scala:62) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:10:45,597][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 0.0 with 1 tasks
[INFO][2018-05-24 20:10:45,642][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:10:45,650][org.apache.spark.executor.Executor]Running task 0.0 in stage 0.0 (TID 0)
[INFO][2018-05-24 20:10:45,682][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12076 is the same as ending offset skipping seven 0
[INFO][2018-05-24 20:10:45,872][org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper]Process identifier=hconnection-0x24a26cd4 connecting to ZooKeeper ensemble=vm-xaj-bigdata-da-d01:2181,vm-xaj-bigdata-da-d02:2181,vm-xaj-bigdata-da-d03:2181
[INFO][2018-05-24 20:10:45,878][org.apache.zookeeper.ZooKeeper]Client environment:zookeeper.version=3.4.5-cdh5.7.0--1, built on 04/06/2016 14:24 GMT
[INFO][2018-05-24 20:10:45,878][org.apache.zookeeper.ZooKeeper]Client environment:host.name=192.168.0.102
[INFO][2018-05-24 20:10:45,878][org.apache.zookeeper.ZooKeeper]Client environment:java.version=1.8.0_161
[INFO][2018-05-24 20:10:45,878][org.apache.zookeeper.ZooKeeper]Client environment:java.vendor=Oracle Corporation
[INFO][2018-05-24 20:10:45,878][org.apache.zookeeper.ZooKeeper]Client environment:java.home=/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre
[INFO][2018-05-24 20:10:45,878][org.apache.zookeeper.ZooKeeper]Client environment:java.class.path=/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/charsets.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/deploy.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/cldrdata.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/dnsns.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/jaccess.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/jfxrt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/localedata.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/nashorn.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/sunec.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/sunjce_provider.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/sunpkcs11.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/zipfs.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/javaws.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/jce.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/jfr.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/jfxswt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/jsse.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/management-agent.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/plugin.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/resources.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/rt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/ant-javafx.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/dt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/javafx-mx.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/jconsole.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/packager.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/sa-jdi.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/tools.jar:/Users/seven/project/scala/dataMining/target/classes:/Users/seven/software/maven/repository/org/scala-lang/scala-library/2.11.12/scala-library-2.11.12.jar:/Users/seven/software/maven/repository/org/scala-lang/scala-compiler/2.11.12/scala-compiler-2.11.12.jar:/Users/seven/software/maven/repository/org/scala-lang/modules/scala-xml_2.11/1.0.5/scala-xml_2.11-1.0.5.jar:/Users/seven/software/maven/repository/org/scala-lang/modules/scala-parser-combinators_2.11/1.0.4/scala-parser-combinators_2.11-1.0.4.jar:/Users/seven/software/maven/repository/org/scala-lang/scala-reflect/2.11.12/scala-reflect-2.11.12.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-client/2.6.0-cdh5.7.0/hadoop-client-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-common/2.6.0-cdh5.7.0/hadoop-common-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/xmlenc/xmlenc/0.52/xmlenc-0.52.jar:/Users/seven/software/maven/repository/commons-net/commons-net/3.1/commons-net-3.1.jar:/Users/seven/software/maven/repository/commons-configuration/commons-configuration/1.6/commons-configuration-1.6.jar:/Users/seven/software/maven/repository/commons-digester/commons-digester/1.8/commons-digester-1.8.jar:/Users/seven/software/maven/repository/commons-beanutils/commons-beanutils/1.7.0/commons-beanutils-1.7.0.jar:/Users/seven/software/maven/repository/org/slf4j/slf4j-api/1.7.5/slf4j-api-1.7.5.jar:/Users/seven/software/maven/repository/org/slf4j/slf4j-log4j12/1.7.5/slf4j-log4j12-1.7.5.jar:/Users/seven/software/maven/repository/org/apache/avro/avro/1.7.6-cdh5.7.0/avro-1.7.6-cdh5.7.0.jar:/Users/seven/software/maven/repository/com/google/code/gson/gson/2.2.4/gson-2.2.4.jar:/Users/seven/software/maven/repository/org/apache/curator/curator-client/2.7.1/curator-client-2.7.1.jar:/Users/seven/software/maven/repository/org/apache/curator/curator-recipes/2.7.1/curator-recipes-2.7.1.jar:/Users/seven/software/maven/repository/com/google/code/findbugs/jsr305/3.0.0/jsr305-3.0.0.jar:/Users/seven/software/maven/repository/org/apache/htrace/htrace-core4/4.0.1-incubating/htrace-core4-4.0.1-incubating.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-compress/1.4.1/commons-compress-1.4.1.jar:/Users/seven/software/maven/repository/org/tukaani/xz/1.0/xz-1.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-hdfs/2.6.0-cdh5.7.0/hadoop-hdfs-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/io/netty/netty/3.6.2.Final/netty-3.6.2.Final.jar:/Users/seven/software/maven/repository/xerces/xercesImpl/2.9.1/xercesImpl-2.9.1.jar:/Users/seven/software/maven/repository/xml-apis/xml-apis/1.3.04/xml-apis-1.3.04.jar:/Users/seven/software/maven/repository/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-app/2.6.0-cdh5.7.0/hadoop-mapreduce-client-app-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-common/2.6.0-cdh5.7.0/hadoop-mapreduce-client-common-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-yarn-client/2.6.0-cdh5.7.0/hadoop-yarn-client-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-yarn-server-common/2.6.0-cdh5.7.0/hadoop-yarn-server-common-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-shuffle/2.6.0-cdh5.7.0/hadoop-mapreduce-client-shuffle-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-yarn-api/2.6.0-cdh5.7.0/hadoop-yarn-api-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-core/2.6.0-cdh5.7.0/hadoop-mapreduce-client-core-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-yarn-common/2.6.0-cdh5.7.0/hadoop-yarn-common-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/javax/xml/bind/jaxb-api/2.2.2/jaxb-api-2.2.2.jar:/Users/seven/software/maven/repository/javax/xml/stream/stax-api/1.0-2/stax-api-1.0-2.jar:/Users/seven/software/maven/repository/javax/activation/activation/1.1/activation-1.1.jar:/Users/seven/software/maven/repository/com/sun/jersey/jersey-client/1.9/jersey-client-1.9.jar:/Users/seven/software/maven/repository/org/codehaus/jackson/jackson-xc/1.8.8/jackson-xc-1.8.8.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.6.0-cdh5.7.0/hadoop-mapreduce-client-jobclient-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-aws/2.6.0-cdh5.7.0/hadoop-aws-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/com/amazonaws/aws-java-sdk-s3/1.10.6/aws-java-sdk-s3-1.10.6.jar:/Users/seven/software/maven/repository/com/amazonaws/aws-java-sdk-kms/1.10.6/aws-java-sdk-kms-1.10.6.jar:/Users/seven/software/maven/repository/com/amazonaws/aws-java-sdk-core/1.10.6/aws-java-sdk-core-1.10.6.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-annotations/2.6.0-cdh5.7.0/hadoop-annotations-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-client/1.2.0-cdh5.7.0/hbase-client-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-annotations/1.2.0-cdh5.7.0/hbase-annotations-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-common/1.2.0-cdh5.7.0/hbase-common-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-protocol/1.2.0-cdh5.7.0/hbase-protocol-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/commons-codec/commons-codec/1.9/commons-codec-1.9.jar:/Users/seven/software/maven/repository/commons-io/commons-io/2.4/commons-io-2.4.jar:/Users/seven/software/maven/repository/commons-lang/commons-lang/2.6/commons-lang-2.6.jar:/Users/seven/software/maven/repository/commons-logging/commons-logging/1.2/commons-logging-1.2.jar:/Users/seven/software/maven/repository/com/google/guava/guava/12.0.1/guava-12.0.1.jar:/Users/seven/software/maven/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/Users/seven/software/maven/repository/io/netty/netty-all/4.0.23.Final/netty-all-4.0.23.Final.jar:/Users/seven/software/maven/repository/org/apache/zookeeper/zookeeper/3.4.5-cdh5.7.0/zookeeper-3.4.5-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/htrace/htrace-core/3.2.0-incubating/htrace-core-3.2.0-incubating.jar:/Users/seven/software/maven/repository/org/codehaus/jackson/jackson-mapper-asl/1.8.8/jackson-mapper-asl-1.8.8.jar:/Users/seven/software/maven/repository/org/jruby/jcodings/jcodings/1.0.8/jcodings-1.0.8.jar:/Users/seven/software/maven/repository/org/jruby/joni/joni/2.1.2/joni-2.1.2.jar:/Users/seven/software/maven/repository/com/yammer/metrics/metrics-core/2.2.0/metrics-core-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-auth/2.6.0-cdh5.7.0/hadoop-auth-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/httpcomponents/httpclient/4.2.5/httpclient-4.2.5.jar:/Users/seven/software/maven/repository/org/apache/httpcomponents/httpcore/4.2.4/httpcore-4.2.4.jar:/Users/seven/software/maven/repository/org/apache/directory/server/apacheds-kerberos-codec/2.0.0-M15/apacheds-kerberos-codec-2.0.0-M15.jar:/Users/seven/software/maven/repository/org/apache/directory/server/apacheds-i18n/2.0.0-M15/apacheds-i18n-2.0.0-M15.jar:/Users/seven/software/maven/repository/org/apache/directory/api/api-asn1-api/1.0.0-M20/api-asn1-api-1.0.0-M20.jar:/Users/seven/software/maven/repository/org/apache/directory/api/api-util/1.0.0-M20/api-util-1.0.0-M20.jar:/Users/seven/software/maven/repository/org/apache/curator/curator-framework/2.7.1/curator-framework-2.7.1.jar:/Users/seven/software/maven/repository/com/github/stephenc/findbugs/findbugs-annotations/1.3.9-1/findbugs-annotations-1.3.9-1.jar:/Users/seven/software/maven/repository/junit/junit/4.12/junit-4.12.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-server/1.2.0-cdh5.7.0/hbase-server-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-procedure/1.2.0-cdh5.7.0/hbase-procedure-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-common/1.2.0-cdh5.7.0/hbase-common-1.2.0-cdh5.7.0-tests.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-prefix-tree/1.2.0-cdh5.7.0/hbase-prefix-tree-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar:/Users/seven/software/maven/repository/commons-collections/commons-collections/3.2.2/commons-collections-3.2.2.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-hadoop-compat/1.2.0-cdh5.7.0/hbase-hadoop-compat-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-hadoop2-compat/1.2.0-cdh5.7.0/hbase-hadoop2-compat-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/com/sun/jersey/jersey-core/1.9/jersey-core-1.9.jar:/Users/seven/software/maven/repository/com/sun/jersey/jersey-server/1.9/jersey-server-1.9.jar:/Users/seven/software/maven/repository/asm/asm/3.1/asm-3.1.jar:/Users/seven/software/maven/repository/commons-cli/commons-cli/1.2/commons-cli-1.2.jar:/Users/seven/software/maven/repository/com/github/stephenc/high-scale-lib/high-scale-lib/1.1.1/high-scale-lib-1.1.1.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-math/2.1/commons-math-2.1.jar:/Users/seven/software/maven/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jetty/6.1.26.cloudera.4/jetty-6.1.26.cloudera.4.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jetty-util/6.1.26.cloudera.4/jetty-util-6.1.26.cloudera.4.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jetty-sslengine/6.1.26.cloudera.4/jetty-sslengine-6.1.26.cloudera.4.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jsp-2.1/6.1.14/jsp-2.1-6.1.14.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jsp-api-2.1/6.1.14/jsp-api-2.1-6.1.14.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/servlet-api-2.5/6.1.14/servlet-api-2.5-6.1.14.jar:/Users/seven/software/maven/repository/org/codehaus/jackson/jackson-core-asl/1.8.8/jackson-core-asl-1.8.8.jar:/Users/seven/software/maven/repository/org/codehaus/jackson/jackson-jaxrs/1.8.8/jackson-jaxrs-1.8.8.jar:/Users/seven/software/maven/repository/tomcat/jasper-compiler/5.5.23/jasper-compiler-5.5.23.jar:/Users/seven/software/maven/repository/tomcat/jasper-runtime/5.5.23/jasper-runtime-5.5.23.jar:/Users/seven/software/maven/repository/commons-el/commons-el/1.0/commons-el-1.0.jar:/Users/seven/software/maven/repository/org/jamon/jamon-runtime/2.4.1/jamon-runtime-2.4.1.jar:/Users/seven/software/maven/repository/com/lmax/disruptor/3.3.0/disruptor-3.3.0.jar:/Users/seven/software/maven/repository/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar:/Users/seven/software/maven/repository/org/owasp/esapi/esapi/2.1.0/esapi-2.1.0.jar:/Users/seven/software/maven/repository/commons-beanutils/commons-beanutils-core/1.7.0/commons-beanutils-core-1.7.0.jar:/Users/seven/software/maven/repository/commons-fileupload/commons-fileupload/1.2/commons-fileupload-1.2.jar:/Users/seven/software/maven/repository/xom/xom/1.2.5/xom-1.2.5.jar:/Users/seven/software/maven/repository/xalan/xalan/2.7.0/xalan-2.7.0.jar:/Users/seven/software/maven/repository/org/beanshell/bsh-core/2.0b4/bsh-core-2.0b4.jar:/Users/seven/software/maven/repository/org/owasp/antisamy/antisamy/1.4.3/antisamy-1.4.3.jar:/Users/seven/software/maven/repository/org/apache/xmlgraphics/batik-css/1.7/batik-css-1.7.jar:/Users/seven/software/maven/repository/org/apache/xmlgraphics/batik-ext/1.7/batik-ext-1.7.jar:/Users/seven/software/maven/repository/org/apache/xmlgraphics/batik-util/1.7/batik-util-1.7.jar:/Users/seven/software/maven/repository/xml-apis/xml-apis-ext/1.3.04/xml-apis-ext-1.3.04.jar:/Users/seven/software/maven/repository/net/sourceforge/nekohtml/nekohtml/1.9.12/nekohtml-1.9.12.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-core/2.6.0-mr1-cdh5.7.0/hadoop-core-2.6.0-mr1-cdh5.7.0.jar:/Users/seven/software/maven/repository/javax/servlet/jsp/jsp-api/2.1/jsp-api-2.1.jar:/Users/seven/software/maven/repository/net/java/dev/jets3t/jets3t/0.6.1/jets3t-0.6.1.jar:/Users/seven/software/maven/repository/javax/servlet/servlet-api/2.5/servlet-api-2.5.jar:/Users/seven/software/maven/repository/hsqldb/hsqldb/1.8.0.10/hsqldb-1.8.0.10.jar:/Users/seven/software/maven/repository/org/eclipse/jdt/core/3.1.1/core-3.1.1.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-hdfs/2.6.0-cdh5.7.0/hadoop-hdfs-2.6.0-cdh5.7.0-tests.jar:/Users/seven/software/maven/repository/commons-daemon/commons-daemon/1.0.13/commons-daemon-1.0.13.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-lang3/3.5/commons-lang3-3.5.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-pool2/2.4.2/commons-pool2-2.4.2.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-mllib_2.11/2.2.0/spark-mllib_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-core_2.11/2.2.0/spark-core_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/avro/avro-mapred/1.7.7/avro-mapred-1.7.7-hadoop2.jar:/Users/seven/software/maven/repository/org/apache/avro/avro-ipc/1.7.7/avro-ipc-1.7.7-tests.jar:/Users/seven/software/maven/repository/com/twitter/chill_2.11/0.8.0/chill_2.11-0.8.0.jar:/Users/seven/software/maven/repository/com/esotericsoftware/kryo-shaded/3.0.3/kryo-shaded-3.0.3.jar:/Users/seven/software/maven/repository/com/esotericsoftware/minlog/1.3.0/minlog-1.3.0.jar:/Users/seven/software/maven/repository/org/objenesis/objenesis/2.1/objenesis-2.1.jar:/Users/seven/software/maven/repository/com/twitter/chill-java/0.8.0/chill-java-0.8.0.jar:/Users/seven/software/maven/repository/org/apache/xbean/xbean-asm5-shaded/4.4/xbean-asm5-shaded-4.4.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-launcher_2.11/2.2.0/spark-launcher_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-network-common_2.11/2.2.0/spark-network-common_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-network-shuffle_2.11/2.2.0/spark-network-shuffle_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-unsafe_2.11/2.2.0/spark-unsafe_2.11-2.2.0.jar:/Users/seven/software/maven/repository/javax/servlet/javax.servlet-api/3.1.0/javax.servlet-api-3.1.0.jar:/Users/seven/software/maven/repository/org/slf4j/jul-to-slf4j/1.7.16/jul-to-slf4j-1.7.16.jar:/Users/seven/software/maven/repository/org/slf4j/jcl-over-slf4j/1.7.16/jcl-over-slf4j-1.7.16.jar:/Users/seven/software/maven/repository/com/ning/compress-lzf/1.0.3/compress-lzf-1.0.3.jar:/Users/seven/software/maven/repository/org/xerial/snappy/snappy-java/1.1.2.6/snappy-java-1.1.2.6.jar:/Users/seven/software/maven/repository/org/roaringbitmap/RoaringBitmap/0.5.11/RoaringBitmap-0.5.11.jar:/Users/seven/software/maven/repository/org/json4s/json4s-jackson_2.11/3.2.11/json4s-jackson_2.11-3.2.11.jar:/Users/seven/software/maven/repository/org/json4s/json4s-core_2.11/3.2.11/json4s-core_2.11-3.2.11.jar:/Users/seven/software/maven/repository/org/json4s/json4s-ast_2.11/3.2.11/json4s-ast_2.11-3.2.11.jar:/Users/seven/software/maven/repository/org/scala-lang/scalap/2.11.0/scalap-2.11.0.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/core/jersey-client/2.22.2/jersey-client-2.22.2.jar:/Users/seven/software/maven/repository/javax/ws/rs/javax.ws.rs-api/2.0.1/javax.ws.rs-api-2.0.1.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/hk2-api/2.4.0-b34/hk2-api-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/hk2-utils/2.4.0-b34/hk2-utils-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/external/aopalliance-repackaged/2.4.0-b34/aopalliance-repackaged-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/external/javax.inject/2.4.0-b34/javax.inject-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/hk2-locator/2.4.0-b34/hk2-locator-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/javassist/javassist/3.18.1-GA/javassist-3.18.1-GA.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/core/jersey-common/2.22.2/jersey-common-2.22.2.jar:/Users/seven/software/maven/repository/javax/annotation/javax.annotation-api/1.2/javax.annotation-api-1.2.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/bundles/repackaged/jersey-guava/2.22.2/jersey-guava-2.22.2.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/osgi-resource-locator/1.0.1/osgi-resource-locator-1.0.1.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/core/jersey-server/2.22.2/jersey-server-2.22.2.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/media/jersey-media-jaxb/2.22.2/jersey-media-jaxb-2.22.2.jar:/Users/seven/software/maven/repository/javax/validation/validation-api/1.1.0.Final/validation-api-1.1.0.Final.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/containers/jersey-container-servlet/2.22.2/jersey-container-servlet-2.22.2.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/containers/jersey-container-servlet-core/2.22.2/jersey-container-servlet-core-2.22.2.jar:/Users/seven/software/maven/repository/com/clearspring/analytics/stream/2.7.0/stream-2.7.0.jar:/Users/seven/software/maven/repository/io/dropwizard/metrics/metrics-core/3.1.2/metrics-core-3.1.2.jar:/Users/seven/software/maven/repository/io/dropwizard/metrics/metrics-jvm/3.1.2/metrics-jvm-3.1.2.jar:/Users/seven/software/maven/repository/io/dropwizard/metrics/metrics-json/3.1.2/metrics-json-3.1.2.jar:/Users/seven/software/maven/repository/io/dropwizard/metrics/metrics-graphite/3.1.2/metrics-graphite-3.1.2.jar:/Users/seven/software/maven/repository/org/apache/ivy/ivy/2.4.0/ivy-2.4.0.jar:/Users/seven/software/maven/repository/oro/oro/2.0.8/oro-2.0.8.jar:/Users/seven/software/maven/repository/net/razorvine/pyrolite/4.13/pyrolite-4.13.jar:/Users/seven/software/maven/repository/net/sf/py4j/py4j/0.10.4/py4j-0.10.4.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-crypto/1.0.0/commons-crypto-1.0.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-streaming_2.11/2.2.0/spark-streaming_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-sql_2.11/2.2.0/spark-sql_2.11-2.2.0.jar:/Users/seven/software/maven/repository/com/univocity/univocity-parsers/2.2.1/univocity-parsers-2.2.1.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-sketch_2.11/2.2.0/spark-sketch_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-catalyst_2.11/2.2.0/spark-catalyst_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/codehaus/janino/janino/3.0.0/janino-3.0.0.jar:/Users/seven/software/maven/repository/org/codehaus/janino/commons-compiler/3.0.0/commons-compiler-3.0.0.jar:/Users/seven/software/maven/repository/org/antlr/antlr4-runtime/4.5.3/antlr4-runtime-4.5.3.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-column/1.8.2/parquet-column-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-common/1.8.2/parquet-common-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-encoding/1.8.2/parquet-encoding-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-hadoop/1.8.2/parquet-hadoop-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-format/2.3.1/parquet-format-2.3.1.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-jackson/1.8.2/parquet-jackson-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-graphx_2.11/2.2.0/spark-graphx_2.11-2.2.0.jar:/Users/seven/software/maven/repository/com/github/fommil/netlib/core/1.1.2/core-1.1.2.jar:/Users/seven/software/maven/repository/net/sourceforge/f2j/arpack_combined_all/0.1/arpack_combined_all-0.1.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-mllib-local_2.11/2.2.0/spark-mllib-local_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/scalanlp/breeze_2.11/0.13.1/breeze_2.11-0.13.1.jar:/Users/seven/software/maven/repository/org/scalanlp/breeze-macros_2.11/0.13.1/breeze-macros_2.11-0.13.1.jar:/Users/seven/software/maven/repository/net/sf/opencsv/opencsv/2.3/opencsv-2.3.jar:/Users/seven/software/maven/repository/com/github/rwl/jtransforms/2.4.0/jtransforms-2.4.0.jar:/Users/seven/software/maven/repository/org/spire-math/spire_2.11/0.13.0/spire_2.11-0.13.0.jar:/Users/seven/software/maven/repository/org/spire-math/spire-macros_2.11/0.13.0/spire-macros_2.11-0.13.0.jar:/Users/seven/software/maven/repository/org/typelevel/machinist_2.11/0.6.1/machinist_2.11-0.6.1.jar:/Users/seven/software/maven/repository/com/chuusai/shapeless_2.11/2.3.2/shapeless_2.11-2.3.2.jar:/Users/seven/software/maven/repository/org/typelevel/macro-compat_2.11/1.1.1/macro-compat_2.11-1.1.1.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-math3/3.4.1/commons-math3-3.4.1.jar:/Users/seven/software/maven/repository/org/jpmml/pmml-model/1.2.15/pmml-model-1.2.15.jar:/Users/seven/software/maven/repository/org/jpmml/pmml-schema/1.2.15/pmml-schema-1.2.15.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-tags_2.11/2.2.0/spark-tags_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-streaming-kafka-0-8_2.11/2.2.0/spark-streaming-kafka-0-8_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/kafka/kafka_2.11/0.8.2.1/kafka_2.11-0.8.2.1.jar:/Users/seven/software/maven/repository/com/101tec/zkclient/0.3/zkclient-0.3.jar:/Users/seven/software/maven/repository/org/apache/kafka/kafka-clients/0.8.2.1/kafka-clients-0.8.2.1.jar:/Users/seven/software/maven/repository/mysql/mysql-connector-java/5.1.31/mysql-connector-java-5.1.31.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/module/jackson-module-scala_2.11/2.6.5/jackson-module-scala_2.11-2.6.5.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/core/jackson-core/2.6.5/jackson-core-2.6.5.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/core/jackson-annotations/2.6.5/jackson-annotations-2.6.5.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/core/jackson-databind/2.6.5/jackson-databind-2.6.5.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/module/jackson-module-paranamer/2.6.5/jackson-module-paranamer-2.6.5.jar:/Users/seven/software/maven/repository/com/thoughtworks/paranamer/paranamer/2.6/paranamer-2.6.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-streaming-flume_2.11/2.2.0/spark-streaming-flume_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/flume/flume-ng-core/1.6.0/flume-ng-core-1.6.0.jar:/Users/seven/software/maven/repository/org/apache/flume/flume-ng-configuration/1.6.0/flume-ng-configuration-1.6.0.jar:/Users/seven/software/maven/repository/org/apache/avro/avro-ipc/1.7.4/avro-ipc-1.7.4.jar:/Users/seven/software/maven/repository/org/apache/velocity/velocity/1.7/velocity-1.7.jar:/Users/seven/software/maven/repository/joda-time/joda-time/2.1/joda-time-2.1.jar:/Users/seven/software/maven/repository/org/apache/mina/mina-core/2.0.4/mina-core-2.0.4.jar:/Users/seven/software/maven/repository/org/apache/flume/flume-ng-sdk/1.6.0/flume-ng-sdk-1.6.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-streaming-flume-sink_2.11/2.2.0/spark-streaming-flume-sink_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/flume/flume-ng-clients/flume-ng-log4jappender/1.6.0/flume-ng-log4jappender-1.6.0.jar:/Users/seven/software/maven/repository/net/jpountz/lz4/lz4/1.3.0/lz4-1.3.0.jar:/Applications/IntelliJ IDEA.app/Contents/lib/idea_rt.jar
[INFO][2018-05-24 20:10:45,879][org.apache.zookeeper.ZooKeeper]Client environment:java.library.path=/Users/seven/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:.
[INFO][2018-05-24 20:10:45,879][org.apache.zookeeper.ZooKeeper]Client environment:java.io.tmpdir=/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/
[INFO][2018-05-24 20:10:45,879][org.apache.zookeeper.ZooKeeper]Client environment:java.compiler=<NA>
[INFO][2018-05-24 20:10:45,879][org.apache.zookeeper.ZooKeeper]Client environment:os.name=Mac OS X
[INFO][2018-05-24 20:10:45,879][org.apache.zookeeper.ZooKeeper]Client environment:os.arch=x86_64
[INFO][2018-05-24 20:10:45,879][org.apache.zookeeper.ZooKeeper]Client environment:os.version=10.13.4
[INFO][2018-05-24 20:10:45,879][org.apache.zookeeper.ZooKeeper]Client environment:user.name=seven
[INFO][2018-05-24 20:10:45,879][org.apache.zookeeper.ZooKeeper]Client environment:user.home=/Users/seven
[INFO][2018-05-24 20:10:45,879][org.apache.zookeeper.ZooKeeper]Client environment:user.dir=/Users/seven/project/scala/dataMining
[INFO][2018-05-24 20:10:45,880][org.apache.zookeeper.ZooKeeper]Initiating client connection, connectString=vm-xaj-bigdata-da-d01:2181,vm-xaj-bigdata-da-d02:2181,vm-xaj-bigdata-da-d03:2181 sessionTimeout=60000 watcher=hconnection-0x24a26cd40x0, quorum=vm-xaj-bigdata-da-d01:2181,vm-xaj-bigdata-da-d02:2181,vm-xaj-bigdata-da-d03:2181, baseZNode=/hbase
[WARN][2018-05-24 20:10:46,380][org.apache.hadoop.util.NativeCodeLoader]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO][2018-05-24 20:10:46,515][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 1
[INFO][2018-05-24 20:10:46,617][org.apache.spark.SparkContext]Starting job: collect at SimulationKafkaSendOutData.scala:25
[INFO][2018-05-24 20:10:46,645][org.apache.spark.scheduler.DAGScheduler]Got job 0 (collect at SimulationKafkaSendOutData.scala:25) with 2 output partitions
[INFO][2018-05-24 20:10:46,646][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 0 (collect at SimulationKafkaSendOutData.scala:25)
[INFO][2018-05-24 20:10:46,646][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:10:46,649][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:10:46,659][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 0 (hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/* MapPartitionsRDD[1] at textFile at SimulationKafkaSendOutData.scala:25), which has no missing parents
[INFO][2018-05-24 20:10:46,680][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1 stored as values in memory (estimated size 3.3 KB, free 912.1 MB)
[INFO][2018-05-24 20:10:46,682][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1_piece0 stored as bytes in memory (estimated size 1969.0 B, free 912.1 MB)
[INFO][2018-05-24 20:10:46,683][org.apache.spark.storage.BlockManagerInfo]Added broadcast_1_piece0 in memory on 192.168.0.102:49473 (size: 1969.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:10:46,684][org.apache.spark.SparkContext]Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:10:46,699][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ResultStage 0 (hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/* MapPartitionsRDD[1] at textFile at SimulationKafkaSendOutData.scala:25) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2018-05-24 20:10:46,702][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 0.0 with 2 tasks
[INFO][2018-05-24 20:10:46,739][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 4883 bytes)
[INFO][2018-05-24 20:10:46,743][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 4883 bytes)
[INFO][2018-05-24 20:10:46,753][org.apache.spark.executor.Executor]Running task 1.0 in stage 0.0 (TID 1)
[INFO][2018-05-24 20:10:46,753][org.apache.spark.executor.Executor]Running task 0.0 in stage 0.0 (TID 0)
[INFO][2018-05-24 20:10:46,842][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/part-00000:10717435+10717436
[INFO][2018-05-24 20:10:46,843][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/part-00000:0+10717435
[INFO][2018-05-24 20:10:50,340][org.apache.spark.storage.memory.MemoryStore]Block taskresult_0 stored as bytes in memory (estimated size 10.4 MB, free 901.7 MB)
[INFO][2018-05-24 20:10:50,341][org.apache.spark.storage.BlockManagerInfo]Added taskresult_0 in memory on 192.168.0.102:49473 (size: 10.4 MB, free: 901.9 MB)
[INFO][2018-05-24 20:10:50,342][org.apache.spark.executor.Executor]Finished task 0.0 in stage 0.0 (TID 0). 10856349 bytes result sent via BlockManager)
[INFO][2018-05-24 20:10:50,423][org.apache.spark.network.client.TransportClientFactory]Successfully created connection to /192.168.0.102:49473 after 63 ms (0 ms spent in bootstraps)
[INFO][2018-05-24 20:10:50,560][org.apache.spark.storage.memory.MemoryStore]Block taskresult_1 stored as bytes in memory (estimated size 10.4 MB, free 891.3 MB)
[INFO][2018-05-24 20:10:50,561][org.apache.spark.storage.BlockManagerInfo]Added taskresult_1 in memory on 192.168.0.102:49473 (size: 10.4 MB, free: 891.6 MB)
[INFO][2018-05-24 20:10:50,575][org.apache.spark.executor.Executor]Finished task 1.0 in stage 0.0 (TID 1). 10855899 bytes result sent via BlockManager)
[INFO][2018-05-24 20:10:50,827][org.apache.spark.storage.BlockManagerInfo]Removed taskresult_0 on 192.168.0.102:49473 in memory (size: 10.4 MB, free: 901.9 MB)
[INFO][2018-05-24 20:10:50,827][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 0.0 (TID 0) in 4099 ms on localhost (executor driver) (1/2)
[INFO][2018-05-24 20:10:50,844][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 0.0 (TID 1) in 4102 ms on localhost (executor driver) (2/2)
[INFO][2018-05-24 20:10:50,845][org.apache.spark.storage.BlockManagerInfo]Removed taskresult_1 on 192.168.0.102:49473 in memory (size: 10.4 MB, free: 912.3 MB)
[INFO][2018-05-24 20:10:50,846][org.apache.spark.scheduler.DAGScheduler]ResultStage 0 (collect at SimulationKafkaSendOutData.scala:25) finished in 4.129 s
[INFO][2018-05-24 20:10:50,847][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:10:50,852][org.apache.spark.scheduler.DAGScheduler]Job 0 finished: collect at SimulationKafkaSendOutData.scala:25, took 4.234651 s
[INFO][2018-05-24 20:10:50,940][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527163850000 ms
[INFO][2018-05-24 20:10:50,992][org.spark_project.jetty.server.AbstractConnector]Stopped Spark@1643d68f{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}
[INFO][2018-05-24 20:10:50,994][org.apache.spark.ui.SparkUI]Stopped Spark web UI at http://192.168.0.102:4041
[INFO][2018-05-24 20:10:51,003][org.apache.spark.MapOutputTrackerMasterEndpoint]MapOutputTrackerMasterEndpoint stopped!
[INFO][2018-05-24 20:10:51,017][org.apache.spark.storage.memory.MemoryStore]MemoryStore cleared
[INFO][2018-05-24 20:10:51,017][org.apache.spark.storage.BlockManager]BlockManager stopped
[INFO][2018-05-24 20:10:51,018][org.apache.spark.storage.BlockManagerMaster]BlockManagerMaster stopped
[INFO][2018-05-24 20:10:51,020][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint]OutputCommitCoordinator stopped!
[INFO][2018-05-24 20:10:51,021][org.apache.spark.SparkContext]Successfully stopped SparkContext
[INFO][2018-05-24 20:10:51,052][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	compression.type = none
	metric.reporters = []
	metadata.max.age.ms = 300000
	metadata.fetch.timeout.ms = 60000
	acks = all
	batch.size = 16384
	reconnect.backoff.ms = 10
	bootstrap.servers = [vm-xaj-bigdata-da-d01:9092, vm-xaj-bigdata-da-d02:9092, vm-xaj-bigdata-da-d03:9092, vm-xaj-bigdata-da-d04:9092, vm-xaj-bigdata-da-d05:9092, vm-xaj-bigdata-da-d06:9092, vm-xaj-bigdata-da-d07:9092]
	receive.buffer.bytes = 32768
	retry.backoff.ms = 100
	buffer.memory = 33554432
	timeout.ms = 30000
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	retries = 0
	max.request.size = 1048576
	block.on.buffer.full = true
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
	metrics.sample.window.ms = 30000
	send.buffer.bytes = 131072
	max.in.flight.requests.per.connection = 5
	metrics.num.samples = 2
	linger.ms = 1
	client.id = 

[INFO][2018-05-24 20:10:55,926][org.apache.zookeeper.ClientCnxn]Opening socket connection to server master/10.213.4.25:2181. Will not attempt to authenticate using SASL (unknown error)
[INFO][2018-05-24 20:10:55,939][org.apache.zookeeper.ClientCnxn]Socket connection established, initiating session, client: /10.0.8.39:49485, server: master/10.213.4.25:2181
[INFO][2018-05-24 20:10:55,959][org.apache.zookeeper.ClientCnxn]Session establishment complete on server master/10.213.4.25:2181, sessionid = 0x162b4dc56095e8c, negotiated timeout = 60000
[WARN][2018-05-24 20:10:56,427][org.apache.hadoop.util.NativeCodeLoader]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO][2018-05-24 20:10:56,504][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:10:56,519][org.apache.spark.executor.Executor]Finished task 0.0 in stage 0.0 (TID 0). 794 bytes result sent to driver
[INFO][2018-05-24 20:10:56,526][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 0.0 (TID 0) in 10895 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:10:56,529][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:10:56,532][org.apache.spark.scheduler.DAGScheduler]ResultStage 0 (foreachPartition at ReceiveKafkaData.scala:64) finished in 10.914 s
[INFO][2018-05-24 20:10:56,538][org.apache.spark.scheduler.DAGScheduler]Job 0 finished: foreachPartition at ReceiveKafkaData.scala:64, took 11.257978 s
[INFO][2018-05-24 20:10:56,542][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527163840000 ms.0 from job set of time 1527163840000 ms
[INFO][2018-05-24 20:10:56,543][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 16.541 s for time 1527163840000 ms (execution: 11.325 s)
[INFO][2018-05-24 20:10:56,543][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527163850000 ms.0 from job set of time 1527163850000 ms
[INFO][2018-05-24 20:10:56,549][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:10:56,551][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:64
[INFO][2018-05-24 20:10:56,553][org.apache.spark.scheduler.DAGScheduler]Got job 1 (foreachPartition at ReceiveKafkaData.scala:64) with 1 output partitions
[INFO][2018-05-24 20:10:56,553][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 1 (foreachPartition at ReceiveKafkaData.scala:64)
[INFO][2018-05-24 20:10:56,553][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:10:56,553][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:10:56,553][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 1 (MapPartitionsRDD[3] at map at ReceiveKafkaData.scala:62), which has no missing parents
[INFO][2018-05-24 20:10:56,555][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 
[INFO][2018-05-24 20:10:56,559][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-24 20:10:56,561][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1_piece0 stored as bytes in memory (estimated size 2008.0 B, free 912.3 MB)
[INFO][2018-05-24 20:10:56,561][org.apache.spark.storage.BlockManagerInfo]Added broadcast_1_piece0 in memory on 192.168.0.102:49458 (size: 2008.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:10:56,562][org.apache.spark.SparkContext]Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:10:56,563][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at map at ReceiveKafkaData.scala:62) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:10:56,563][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 1.0 with 1 tasks
[INFO][2018-05-24 20:10:56,564][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:10:56,567][org.apache.spark.executor.Executor]Running task 0.0 in stage 1.0 (TID 1)
[INFO][2018-05-24 20:10:56,601][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12076 is the same as ending offset skipping seven 0
[INFO][2018-05-24 20:10:56,601][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:10:56,603][org.apache.spark.executor.Executor]Finished task 0.0 in stage 1.0 (TID 1). 708 bytes result sent to driver
[INFO][2018-05-24 20:10:56,604][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 1.0 (TID 1) in 40 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:10:56,604][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:10:56,605][org.apache.spark.scheduler.DAGScheduler]ResultStage 1 (foreachPartition at ReceiveKafkaData.scala:64) finished in 0.041 s
[INFO][2018-05-24 20:10:56,605][org.apache.spark.scheduler.DAGScheduler]Job 1 finished: foreachPartition at ReceiveKafkaData.scala:64, took 0.054102 s
[INFO][2018-05-24 20:10:56,606][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527163850000 ms.0 from job set of time 1527163850000 ms
[INFO][2018-05-24 20:10:56,607][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 6.606 s for time 1527163850000 ms (execution: 0.063 s)
[INFO][2018-05-24 20:10:56,607][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 1 from persistence list
[INFO][2018-05-24 20:10:56,612][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 0 from persistence list
[INFO][2018-05-24 20:10:56,613][org.apache.spark.storage.BlockManager]Removing RDD 1
[INFO][2018-05-24 20:10:56,613][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:10:56,613][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 
[INFO][2018-05-24 20:10:56,613][org.apache.spark.storage.BlockManager]Removing RDD 0
[INFO][2018-05-24 20:11:00,060][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527163860000 ms
[INFO][2018-05-24 20:11:00,061][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527163860000 ms.0 from job set of time 1527163860000 ms
[INFO][2018-05-24 20:11:00,070][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:64
[INFO][2018-05-24 20:11:00,071][org.apache.spark.scheduler.DAGScheduler]Got job 2 (foreachPartition at ReceiveKafkaData.scala:64) with 1 output partitions
[INFO][2018-05-24 20:11:00,071][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 2 (foreachPartition at ReceiveKafkaData.scala:64)
[INFO][2018-05-24 20:11:00,071][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:11:00,072][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:11:00,072][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 2 (MapPartitionsRDD[5] at map at ReceiveKafkaData.scala:62), which has no missing parents
[INFO][2018-05-24 20:11:00,076][org.apache.spark.storage.memory.MemoryStore]Block broadcast_2 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-24 20:11:00,078][org.apache.spark.storage.memory.MemoryStore]Block broadcast_2_piece0 stored as bytes in memory (estimated size 2008.0 B, free 912.3 MB)
[INFO][2018-05-24 20:11:00,078][org.apache.spark.storage.BlockManagerInfo]Added broadcast_2_piece0 in memory on 192.168.0.102:49458 (size: 2008.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:11:00,079][org.apache.spark.SparkContext]Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:11:00,080][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[5] at map at ReceiveKafkaData.scala:62) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:11:00,080][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 2.0 with 1 tasks
[INFO][2018-05-24 20:11:00,081][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:11:00,081][org.apache.spark.executor.Executor]Running task 0.0 in stage 2.0 (TID 2)
[INFO][2018-05-24 20:11:00,085][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12076 is the same as ending offset skipping seven 0
[INFO][2018-05-24 20:11:00,085][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:11:00,086][org.apache.spark.executor.Executor]Finished task 0.0 in stage 2.0 (TID 2). 751 bytes result sent to driver
[INFO][2018-05-24 20:11:00,087][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 2.0 (TID 2) in 6 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:11:00,087][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:11:00,088][org.apache.spark.scheduler.DAGScheduler]ResultStage 2 (foreachPartition at ReceiveKafkaData.scala:64) finished in 0.007 s
[INFO][2018-05-24 20:11:00,089][org.apache.spark.scheduler.DAGScheduler]Job 2 finished: foreachPartition at ReceiveKafkaData.scala:64, took 0.018639 s
[INFO][2018-05-24 20:11:00,089][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527163860000 ms.0 from job set of time 1527163860000 ms
[INFO][2018-05-24 20:11:00,090][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 3 from persistence list
[INFO][2018-05-24 20:11:00,090][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.089 s for time 1527163860000 ms (execution: 0.028 s)
[INFO][2018-05-24 20:11:00,090][org.apache.spark.storage.BlockManager]Removing RDD 3
[INFO][2018-05-24 20:11:00,090][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 2 from persistence list
[INFO][2018-05-24 20:11:00,090][org.apache.spark.storage.BlockManager]Removing RDD 2
[INFO][2018-05-24 20:11:00,091][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:11:00,091][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527163840000 ms
[INFO][2018-05-24 20:11:15,061][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527163870000 ms
[INFO][2018-05-24 20:11:15,061][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527163870000 ms.0 from job set of time 1527163870000 ms
[INFO][2018-05-24 20:11:15,074][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:64
[INFO][2018-05-24 20:11:15,075][org.apache.spark.scheduler.DAGScheduler]Got job 3 (foreachPartition at ReceiveKafkaData.scala:64) with 1 output partitions
[INFO][2018-05-24 20:11:15,075][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 3 (foreachPartition at ReceiveKafkaData.scala:64)
[INFO][2018-05-24 20:11:15,075][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:11:15,075][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:11:15,077][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 3 (MapPartitionsRDD[7] at map at ReceiveKafkaData.scala:62), which has no missing parents
[INFO][2018-05-24 20:11:15,080][org.apache.spark.storage.memory.MemoryStore]Block broadcast_3 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-24 20:11:15,089][org.apache.spark.storage.memory.MemoryStore]Block broadcast_3_piece0 stored as bytes in memory (estimated size 2008.0 B, free 912.3 MB)
[INFO][2018-05-24 20:11:15,090][org.apache.spark.storage.BlockManagerInfo]Added broadcast_3_piece0 in memory on 192.168.0.102:49458 (size: 2008.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:11:15,091][org.apache.spark.SparkContext]Created broadcast 3 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:11:15,093][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[7] at map at ReceiveKafkaData.scala:62) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:11:15,093][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 3.0 with 1 tasks
[INFO][2018-05-24 20:11:15,094][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 3.0 (TID 3, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:11:15,095][org.apache.spark.executor.Executor]Running task 0.0 in stage 3.0 (TID 3)
[INFO][2018-05-24 20:11:15,100][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12076 is the same as ending offset skipping seven 0
[INFO][2018-05-24 20:11:15,101][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:11:15,103][org.apache.spark.executor.Executor]Finished task 0.0 in stage 3.0 (TID 3). 708 bytes result sent to driver
[INFO][2018-05-24 20:11:15,106][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 3.0 (TID 3) in 12 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:11:15,106][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 3.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:11:15,107][org.apache.spark.scheduler.DAGScheduler]ResultStage 3 (foreachPartition at ReceiveKafkaData.scala:64) finished in 0.013 s
[INFO][2018-05-24 20:11:15,108][org.apache.spark.scheduler.DAGScheduler]Job 3 finished: foreachPartition at ReceiveKafkaData.scala:64, took 0.033555 s
[INFO][2018-05-24 20:11:15,108][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527163870000 ms.0 from job set of time 1527163870000 ms
[INFO][2018-05-24 20:11:15,108][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 5.108 s for time 1527163870000 ms (execution: 0.047 s)
[INFO][2018-05-24 20:11:15,108][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 5 from persistence list
[INFO][2018-05-24 20:11:15,109][org.apache.spark.storage.BlockManager]Removing RDD 5
[INFO][2018-05-24 20:11:15,109][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 4 from persistence list
[INFO][2018-05-24 20:11:15,109][org.apache.spark.storage.BlockManager]Removing RDD 4
[INFO][2018-05-24 20:11:15,110][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:11:15,110][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527163850000 ms
[INFO][2018-05-24 20:11:20,066][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527163880000 ms
[INFO][2018-05-24 20:11:20,066][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527163880000 ms.0 from job set of time 1527163880000 ms
[INFO][2018-05-24 20:11:20,074][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:64
[INFO][2018-05-24 20:11:20,076][org.apache.spark.scheduler.DAGScheduler]Got job 4 (foreachPartition at ReceiveKafkaData.scala:64) with 1 output partitions
[INFO][2018-05-24 20:11:20,076][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 4 (foreachPartition at ReceiveKafkaData.scala:64)
[INFO][2018-05-24 20:11:20,076][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:11:20,076][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:11:20,076][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 4 (MapPartitionsRDD[9] at map at ReceiveKafkaData.scala:62), which has no missing parents
[INFO][2018-05-24 20:11:20,078][org.apache.spark.storage.memory.MemoryStore]Block broadcast_4 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-24 20:11:20,082][org.apache.spark.storage.memory.MemoryStore]Block broadcast_4_piece0 stored as bytes in memory (estimated size 2008.0 B, free 912.3 MB)
[INFO][2018-05-24 20:11:20,083][org.apache.spark.storage.BlockManagerInfo]Added broadcast_4_piece0 in memory on 192.168.0.102:49458 (size: 2008.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:11:20,083][org.apache.spark.SparkContext]Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:11:20,084][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[9] at map at ReceiveKafkaData.scala:62) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:11:20,084][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 4.0 with 1 tasks
[INFO][2018-05-24 20:11:20,085][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 4.0 (TID 4, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:11:20,085][org.apache.spark.executor.Executor]Running task 0.0 in stage 4.0 (TID 4)
[INFO][2018-05-24 20:11:20,088][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12076 is the same as ending offset skipping seven 0
[INFO][2018-05-24 20:11:20,089][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:11:20,089][org.apache.spark.executor.Executor]Finished task 0.0 in stage 4.0 (TID 4). 708 bytes result sent to driver
[INFO][2018-05-24 20:11:20,091][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 4.0 (TID 4) in 6 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:11:20,091][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 4.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:11:20,092][org.apache.spark.scheduler.DAGScheduler]ResultStage 4 (foreachPartition at ReceiveKafkaData.scala:64) finished in 0.006 s
[INFO][2018-05-24 20:11:20,092][org.apache.spark.scheduler.DAGScheduler]Job 4 finished: foreachPartition at ReceiveKafkaData.scala:64, took 0.017755 s
[INFO][2018-05-24 20:11:20,092][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527163880000 ms.0 from job set of time 1527163880000 ms
[INFO][2018-05-24 20:11:20,093][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.092 s for time 1527163880000 ms (execution: 0.026 s)
[INFO][2018-05-24 20:11:20,093][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 7 from persistence list
[INFO][2018-05-24 20:11:20,093][org.apache.spark.storage.BlockManager]Removing RDD 7
[INFO][2018-05-24 20:11:20,094][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 6 from persistence list
[INFO][2018-05-24 20:11:20,094][org.apache.spark.storage.BlockManager]Removing RDD 6
[INFO][2018-05-24 20:11:20,094][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:11:20,095][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527163860000 ms
[INFO][2018-05-24 20:11:35,070][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527163890000 ms
[INFO][2018-05-24 20:11:35,071][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527163890000 ms.0 from job set of time 1527163890000 ms
[INFO][2018-05-24 20:11:35,079][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:64
[INFO][2018-05-24 20:11:35,080][org.apache.spark.scheduler.DAGScheduler]Got job 5 (foreachPartition at ReceiveKafkaData.scala:64) with 1 output partitions
[INFO][2018-05-24 20:11:35,080][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 5 (foreachPartition at ReceiveKafkaData.scala:64)
[INFO][2018-05-24 20:11:35,080][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:11:35,080][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:11:35,081][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 5 (MapPartitionsRDD[11] at map at ReceiveKafkaData.scala:62), which has no missing parents
[INFO][2018-05-24 20:11:35,083][org.apache.spark.storage.memory.MemoryStore]Block broadcast_5 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-24 20:11:35,087][org.apache.spark.storage.memory.MemoryStore]Block broadcast_5_piece0 stored as bytes in memory (estimated size 2008.0 B, free 912.3 MB)
[INFO][2018-05-24 20:11:35,087][org.apache.spark.storage.BlockManagerInfo]Added broadcast_5_piece0 in memory on 192.168.0.102:49458 (size: 2008.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:11:35,088][org.apache.spark.SparkContext]Created broadcast 5 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:11:35,088][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[11] at map at ReceiveKafkaData.scala:62) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:11:35,088][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 5.0 with 1 tasks
[INFO][2018-05-24 20:11:35,089][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 5.0 (TID 5, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:11:35,089][org.apache.spark.executor.Executor]Running task 0.0 in stage 5.0 (TID 5)
[INFO][2018-05-24 20:11:35,096][org.apache.spark.streaming.kafka.KafkaRDD]Computing topic seven, partition 0 offsets 12076 -> 12082
[INFO][2018-05-24 20:11:35,096][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 20:11:35,097][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 20:11:35,097][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 20:11:35,297][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_2_piece0 on 192.168.0.102:49458 in memory (size: 2008.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:11:35,300][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_3_piece0 on 192.168.0.102:49458 in memory (size: 2008.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:11:35,307][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_4_piece0 on 192.168.0.102:49458 in memory (size: 2008.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:11:35,312][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_1_piece0 on 192.168.0.102:49458 in memory (size: 2008.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:11:40,580][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:11:40,582][org.apache.spark.executor.Executor]Finished task 0.0 in stage 5.0 (TID 5). 751 bytes result sent to driver
[INFO][2018-05-24 20:11:40,583][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 5.0 (TID 5) in 5493 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:11:40,583][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 5.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:11:40,583][org.apache.spark.scheduler.DAGScheduler]ResultStage 5 (foreachPartition at ReceiveKafkaData.scala:64) finished in 5.494 s
[INFO][2018-05-24 20:11:40,584][org.apache.spark.scheduler.DAGScheduler]Job 5 finished: foreachPartition at ReceiveKafkaData.scala:64, took 5.504302 s
[INFO][2018-05-24 20:11:40,584][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527163890000 ms.0 from job set of time 1527163890000 ms
[INFO][2018-05-24 20:11:40,585][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 10.584 s for time 1527163890000 ms (execution: 5.513 s)
[INFO][2018-05-24 20:11:50,068][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527163900000 ms
[INFO][2018-05-24 20:11:50,068][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 9 from persistence list
[INFO][2018-05-24 20:11:50,069][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527163900000 ms.0 from job set of time 1527163900000 ms
[INFO][2018-05-24 20:11:50,069][org.apache.spark.storage.BlockManager]Removing RDD 9
[INFO][2018-05-24 20:11:50,069][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 8 from persistence list
[INFO][2018-05-24 20:11:50,070][org.apache.spark.storage.BlockManager]Removing RDD 8
[INFO][2018-05-24 20:11:50,070][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:11:50,070][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527163870000 ms
[INFO][2018-05-24 20:11:50,075][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:64
[INFO][2018-05-24 20:11:50,077][org.apache.spark.scheduler.DAGScheduler]Got job 6 (foreachPartition at ReceiveKafkaData.scala:64) with 1 output partitions
[INFO][2018-05-24 20:11:50,077][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 6 (foreachPartition at ReceiveKafkaData.scala:64)
[INFO][2018-05-24 20:11:50,077][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:11:50,077][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:11:50,077][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 6 (MapPartitionsRDD[13] at map at ReceiveKafkaData.scala:62), which has no missing parents
[INFO][2018-05-24 20:11:50,079][org.apache.spark.storage.memory.MemoryStore]Block broadcast_6 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-24 20:11:50,080][org.apache.spark.storage.memory.MemoryStore]Block broadcast_6_piece0 stored as bytes in memory (estimated size 2004.0 B, free 912.3 MB)
[INFO][2018-05-24 20:11:50,081][org.apache.spark.storage.BlockManagerInfo]Added broadcast_6_piece0 in memory on 192.168.0.102:49458 (size: 2004.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:11:50,082][org.apache.spark.SparkContext]Created broadcast 6 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:11:50,082][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[13] at map at ReceiveKafkaData.scala:62) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:11:50,082][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 6.0 with 1 tasks
[INFO][2018-05-24 20:11:50,083][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 6.0 (TID 6, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:11:50,084][org.apache.spark.executor.Executor]Running task 0.0 in stage 6.0 (TID 6)
[INFO][2018-05-24 20:11:50,086][org.apache.spark.streaming.kafka.KafkaRDD]Computing topic seven, partition 0 offsets 12082 -> 12088
[INFO][2018-05-24 20:11:50,086][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 20:11:50,086][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 20:11:50,086][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 20:11:50,123][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527163910000 ms
[INFO][2018-05-24 20:11:50,165][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:11:50,166][org.apache.spark.executor.Executor]Finished task 0.0 in stage 6.0 (TID 6). 708 bytes result sent to driver
[INFO][2018-05-24 20:11:50,169][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 6.0 (TID 6) in 86 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:11:50,170][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 6.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:11:50,171][org.apache.spark.scheduler.DAGScheduler]ResultStage 6 (foreachPartition at ReceiveKafkaData.scala:64) finished in 0.088 s
[INFO][2018-05-24 20:11:50,172][org.apache.spark.scheduler.DAGScheduler]Job 6 finished: foreachPartition at ReceiveKafkaData.scala:64, took 0.096566 s
[INFO][2018-05-24 20:11:50,173][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527163900000 ms.0 from job set of time 1527163900000 ms
[INFO][2018-05-24 20:11:50,173][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 10.173 s for time 1527163900000 ms (execution: 0.104 s)
[INFO][2018-05-24 20:11:50,173][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527163910000 ms.0 from job set of time 1527163910000 ms
[INFO][2018-05-24 20:11:50,174][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 11 from persistence list
[INFO][2018-05-24 20:11:50,177][org.apache.spark.storage.BlockManager]Removing RDD 11
[INFO][2018-05-24 20:11:50,177][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 10 from persistence list
[INFO][2018-05-24 20:11:50,179][org.apache.spark.storage.BlockManager]Removing RDD 10
[INFO][2018-05-24 20:11:50,179][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:11:50,179][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527163880000 ms
[INFO][2018-05-24 20:11:50,181][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:64
[INFO][2018-05-24 20:11:50,182][org.apache.spark.scheduler.DAGScheduler]Got job 7 (foreachPartition at ReceiveKafkaData.scala:64) with 1 output partitions
[INFO][2018-05-24 20:11:50,182][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 7 (foreachPartition at ReceiveKafkaData.scala:64)
[INFO][2018-05-24 20:11:50,182][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:11:50,182][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:11:50,182][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 7 (MapPartitionsRDD[15] at map at ReceiveKafkaData.scala:62), which has no missing parents
[INFO][2018-05-24 20:11:50,185][org.apache.spark.storage.memory.MemoryStore]Block broadcast_7 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-24 20:11:50,186][org.apache.spark.storage.memory.MemoryStore]Block broadcast_7_piece0 stored as bytes in memory (estimated size 2005.0 B, free 912.3 MB)
[INFO][2018-05-24 20:11:50,186][org.apache.spark.storage.BlockManagerInfo]Added broadcast_7_piece0 in memory on 192.168.0.102:49458 (size: 2005.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:11:50,187][org.apache.spark.SparkContext]Created broadcast 7 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:11:50,188][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[15] at map at ReceiveKafkaData.scala:62) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:11:50,188][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 7.0 with 1 tasks
[INFO][2018-05-24 20:11:50,189][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 7.0 (TID 7, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:11:50,190][org.apache.spark.executor.Executor]Running task 0.0 in stage 7.0 (TID 7)
[INFO][2018-05-24 20:11:50,194][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12088 is the same as ending offset skipping seven 0
[INFO][2018-05-24 20:11:50,194][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:11:50,195][org.apache.spark.executor.Executor]Finished task 0.0 in stage 7.0 (TID 7). 751 bytes result sent to driver
[INFO][2018-05-24 20:11:50,196][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 7.0 (TID 7) in 7 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:11:50,196][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 7.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:11:50,198][org.apache.spark.scheduler.DAGScheduler]ResultStage 7 (foreachPartition at ReceiveKafkaData.scala:64) finished in 0.008 s
[INFO][2018-05-24 20:11:50,198][org.apache.spark.scheduler.DAGScheduler]Job 7 finished: foreachPartition at ReceiveKafkaData.scala:64, took 0.017265 s
[INFO][2018-05-24 20:11:50,199][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527163910000 ms.0 from job set of time 1527163910000 ms
[INFO][2018-05-24 20:11:50,199][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 13 from persistence list
[INFO][2018-05-24 20:11:50,200][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.199 s for time 1527163910000 ms (execution: 0.026 s)
[INFO][2018-05-24 20:11:50,200][org.apache.spark.storage.BlockManager]Removing RDD 13
[INFO][2018-05-24 20:11:50,200][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 12 from persistence list
[INFO][2018-05-24 20:11:50,200][org.apache.spark.storage.BlockManager]Removing RDD 12
[INFO][2018-05-24 20:11:50,200][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:11:50,201][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527163890000 ms
[INFO][2018-05-24 20:12:00,065][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527163920000 ms
[INFO][2018-05-24 20:12:00,065][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527163920000 ms.0 from job set of time 1527163920000 ms
[INFO][2018-05-24 20:12:00,071][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:64
[INFO][2018-05-24 20:12:00,071][org.apache.spark.scheduler.DAGScheduler]Got job 8 (foreachPartition at ReceiveKafkaData.scala:64) with 1 output partitions
[INFO][2018-05-24 20:12:00,072][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 8 (foreachPartition at ReceiveKafkaData.scala:64)
[INFO][2018-05-24 20:12:00,072][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:12:00,072][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:12:00,072][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 8 (MapPartitionsRDD[17] at map at ReceiveKafkaData.scala:62), which has no missing parents
[INFO][2018-05-24 20:12:00,074][org.apache.spark.storage.memory.MemoryStore]Block broadcast_8 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-24 20:12:00,076][org.apache.spark.storage.memory.MemoryStore]Block broadcast_8_piece0 stored as bytes in memory (estimated size 2005.0 B, free 912.3 MB)
[INFO][2018-05-24 20:12:00,077][org.apache.spark.storage.BlockManagerInfo]Added broadcast_8_piece0 in memory on 192.168.0.102:49458 (size: 2005.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:12:00,077][org.apache.spark.SparkContext]Created broadcast 8 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:12:00,078][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[17] at map at ReceiveKafkaData.scala:62) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:12:00,078][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 8.0 with 1 tasks
[INFO][2018-05-24 20:12:00,079][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 8.0 (TID 8, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:12:00,079][org.apache.spark.executor.Executor]Running task 0.0 in stage 8.0 (TID 8)
[INFO][2018-05-24 20:12:00,081][org.apache.spark.streaming.kafka.KafkaRDD]Computing topic seven, partition 0 offsets 12088 -> 12092
[INFO][2018-05-24 20:12:00,081][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 20:12:00,082][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 20:12:00,082][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 20:12:00,150][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:12:00,151][org.apache.spark.executor.Executor]Finished task 0.0 in stage 8.0 (TID 8). 708 bytes result sent to driver
[INFO][2018-05-24 20:12:00,152][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 8.0 (TID 8) in 74 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:12:00,152][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 8.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:12:00,153][org.apache.spark.scheduler.DAGScheduler]ResultStage 8 (foreachPartition at ReceiveKafkaData.scala:64) finished in 0.074 s
[INFO][2018-05-24 20:12:00,153][org.apache.spark.scheduler.DAGScheduler]Job 8 finished: foreachPartition at ReceiveKafkaData.scala:64, took 0.082526 s
[INFO][2018-05-24 20:12:00,154][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527163920000 ms.0 from job set of time 1527163920000 ms
[INFO][2018-05-24 20:12:00,154][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 15 from persistence list
[INFO][2018-05-24 20:12:00,154][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.154 s for time 1527163920000 ms (execution: 0.089 s)
[INFO][2018-05-24 20:12:00,155][org.apache.spark.storage.BlockManager]Removing RDD 15
[INFO][2018-05-24 20:12:00,155][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 14 from persistence list
[INFO][2018-05-24 20:12:00,155][org.apache.spark.storage.BlockManager]Removing RDD 14
[INFO][2018-05-24 20:12:00,155][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:12:00,155][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527163900000 ms
[INFO][2018-05-24 20:12:10,059][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527163930000 ms
[INFO][2018-05-24 20:12:10,059][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527163930000 ms.0 from job set of time 1527163930000 ms
[INFO][2018-05-24 20:12:10,065][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:64
[INFO][2018-05-24 20:12:10,066][org.apache.spark.scheduler.DAGScheduler]Got job 9 (foreachPartition at ReceiveKafkaData.scala:64) with 1 output partitions
[INFO][2018-05-24 20:12:10,066][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 9 (foreachPartition at ReceiveKafkaData.scala:64)
[INFO][2018-05-24 20:12:10,066][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:12:10,067][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:12:10,067][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 9 (MapPartitionsRDD[19] at map at ReceiveKafkaData.scala:62), which has no missing parents
[INFO][2018-05-24 20:12:10,069][org.apache.spark.storage.memory.MemoryStore]Block broadcast_9 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-24 20:12:10,070][org.apache.spark.storage.memory.MemoryStore]Block broadcast_9_piece0 stored as bytes in memory (estimated size 2008.0 B, free 912.3 MB)
[INFO][2018-05-24 20:12:10,071][org.apache.spark.storage.BlockManagerInfo]Added broadcast_9_piece0 in memory on 192.168.0.102:49458 (size: 2008.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:12:10,071][org.apache.spark.SparkContext]Created broadcast 9 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:12:10,072][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[19] at map at ReceiveKafkaData.scala:62) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:12:10,072][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 9.0 with 1 tasks
[INFO][2018-05-24 20:12:10,072][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 9.0 (TID 9, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:12:10,073][org.apache.spark.executor.Executor]Running task 0.0 in stage 9.0 (TID 9)
[INFO][2018-05-24 20:12:10,074][org.apache.spark.streaming.kafka.KafkaRDD]Computing topic seven, partition 0 offsets 12092 -> 12096
[INFO][2018-05-24 20:12:10,075][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 20:12:10,075][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 20:12:10,075][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 20:12:10,157][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:12:10,158][org.apache.spark.executor.Executor]Finished task 0.0 in stage 9.0 (TID 9). 665 bytes result sent to driver
[INFO][2018-05-24 20:12:10,159][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 9.0 (TID 9) in 87 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:12:10,159][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 9.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:12:10,160][org.apache.spark.scheduler.DAGScheduler]ResultStage 9 (foreachPartition at ReceiveKafkaData.scala:64) finished in 0.088 s
[INFO][2018-05-24 20:12:10,160][org.apache.spark.scheduler.DAGScheduler]Job 9 finished: foreachPartition at ReceiveKafkaData.scala:64, took 0.094654 s
[INFO][2018-05-24 20:12:10,161][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527163930000 ms.0 from job set of time 1527163930000 ms
[INFO][2018-05-24 20:12:10,161][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.161 s for time 1527163930000 ms (execution: 0.102 s)
[INFO][2018-05-24 20:12:10,161][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 17 from persistence list
[INFO][2018-05-24 20:12:10,162][org.apache.spark.storage.BlockManager]Removing RDD 17
[INFO][2018-05-24 20:12:10,162][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 16 from persistence list
[INFO][2018-05-24 20:12:10,162][org.apache.spark.storage.BlockManager]Removing RDD 16
[INFO][2018-05-24 20:12:10,162][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:12:10,162][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527163910000 ms
[INFO][2018-05-24 20:12:30,074][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527163940000 ms
[INFO][2018-05-24 20:12:30,080][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527163940000 ms.0 from job set of time 1527163940000 ms
[INFO][2018-05-24 20:12:30,100][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:64
[INFO][2018-05-24 20:12:30,101][org.apache.spark.scheduler.DAGScheduler]Got job 10 (foreachPartition at ReceiveKafkaData.scala:64) with 1 output partitions
[INFO][2018-05-24 20:12:30,101][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 10 (foreachPartition at ReceiveKafkaData.scala:64)
[INFO][2018-05-24 20:12:30,101][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:12:30,101][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:12:30,102][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 10 (MapPartitionsRDD[21] at map at ReceiveKafkaData.scala:62), which has no missing parents
[INFO][2018-05-24 20:12:30,103][org.apache.spark.storage.memory.MemoryStore]Block broadcast_10 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-24 20:12:30,105][org.apache.spark.storage.memory.MemoryStore]Block broadcast_10_piece0 stored as bytes in memory (estimated size 2008.0 B, free 912.3 MB)
[INFO][2018-05-24 20:12:30,106][org.apache.spark.storage.BlockManagerInfo]Added broadcast_10_piece0 in memory on 192.168.0.102:49458 (size: 2008.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:12:30,106][org.apache.spark.SparkContext]Created broadcast 10 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:12:30,107][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[21] at map at ReceiveKafkaData.scala:62) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:12:30,107][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 10.0 with 1 tasks
[INFO][2018-05-24 20:12:30,108][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 10.0 (TID 10, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:12:30,108][org.apache.spark.executor.Executor]Running task 0.0 in stage 10.0 (TID 10)
[INFO][2018-05-24 20:12:30,110][org.apache.spark.streaming.kafka.KafkaRDD]Computing topic seven, partition 0 offsets 12096 -> 12104
[INFO][2018-05-24 20:12:30,110][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 20:12:30,110][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 20:12:30,110][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 20:12:32,383][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527163950000 ms
[INFO][2018-05-24 20:12:35,196][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:12:35,198][org.apache.spark.executor.Executor]Finished task 0.0 in stage 10.0 (TID 10). 708 bytes result sent to driver
[INFO][2018-05-24 20:12:35,198][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 10.0 (TID 10) in 5090 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:12:35,198][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 10.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:12:35,199][org.apache.spark.scheduler.DAGScheduler]ResultStage 10 (foreachPartition at ReceiveKafkaData.scala:64) finished in 5.092 s
[INFO][2018-05-24 20:12:35,200][org.apache.spark.scheduler.DAGScheduler]Job 10 finished: foreachPartition at ReceiveKafkaData.scala:64, took 5.098949 s
[INFO][2018-05-24 20:12:35,201][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527163940000 ms.0 from job set of time 1527163940000 ms
[INFO][2018-05-24 20:12:35,201][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 15.200 s for time 1527163940000 ms (execution: 5.123 s)
[INFO][2018-05-24 20:12:35,201][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 19 from persistence list
[INFO][2018-05-24 20:12:35,201][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527163950000 ms.0 from job set of time 1527163950000 ms
[INFO][2018-05-24 20:12:35,202][org.apache.spark.storage.BlockManager]Removing RDD 19
[INFO][2018-05-24 20:12:35,202][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 18 from persistence list
[INFO][2018-05-24 20:12:35,203][org.apache.spark.storage.BlockManager]Removing RDD 18
[INFO][2018-05-24 20:12:35,203][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:12:35,203][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527163920000 ms
[INFO][2018-05-24 20:12:35,207][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:64
[INFO][2018-05-24 20:12:35,208][org.apache.spark.scheduler.DAGScheduler]Got job 11 (foreachPartition at ReceiveKafkaData.scala:64) with 1 output partitions
[INFO][2018-05-24 20:12:35,208][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 11 (foreachPartition at ReceiveKafkaData.scala:64)
[INFO][2018-05-24 20:12:35,208][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:12:35,208][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:12:35,209][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 11 (MapPartitionsRDD[23] at map at ReceiveKafkaData.scala:62), which has no missing parents
[INFO][2018-05-24 20:12:35,211][org.apache.spark.storage.memory.MemoryStore]Block broadcast_11 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-24 20:12:35,212][org.apache.spark.storage.memory.MemoryStore]Block broadcast_11_piece0 stored as bytes in memory (estimated size 2008.0 B, free 912.3 MB)
[INFO][2018-05-24 20:12:35,213][org.apache.spark.storage.BlockManagerInfo]Added broadcast_11_piece0 in memory on 192.168.0.102:49458 (size: 2008.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:12:35,213][org.apache.spark.SparkContext]Created broadcast 11 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:12:35,214][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[23] at map at ReceiveKafkaData.scala:62) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:12:35,214][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 11.0 with 1 tasks
[INFO][2018-05-24 20:12:35,214][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 11.0 (TID 11, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:12:35,215][org.apache.spark.executor.Executor]Running task 0.0 in stage 11.0 (TID 11)
[INFO][2018-05-24 20:12:35,216][org.apache.spark.streaming.kafka.KafkaRDD]Computing topic seven, partition 0 offsets 12104 -> 12105
[INFO][2018-05-24 20:12:35,216][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 20:12:35,217][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 20:12:35,217][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 20:12:35,270][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:12:35,271][org.apache.spark.executor.Executor]Finished task 0.0 in stage 11.0 (TID 11). 708 bytes result sent to driver
[INFO][2018-05-24 20:12:35,271][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 11.0 (TID 11) in 57 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:12:35,272][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 11.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:12:35,274][org.apache.spark.scheduler.DAGScheduler]ResultStage 11 (foreachPartition at ReceiveKafkaData.scala:64) finished in 0.059 s
[INFO][2018-05-24 20:12:35,276][org.apache.spark.scheduler.DAGScheduler]Job 11 finished: foreachPartition at ReceiveKafkaData.scala:64, took 0.068504 s
[INFO][2018-05-24 20:12:35,277][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527163950000 ms.0 from job set of time 1527163950000 ms
[INFO][2018-05-24 20:12:35,278][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 5.277 s for time 1527163950000 ms (execution: 0.076 s)
[INFO][2018-05-24 20:12:35,278][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 21 from persistence list
[INFO][2018-05-24 20:12:35,279][org.apache.spark.storage.BlockManager]Removing RDD 21
[INFO][2018-05-24 20:12:35,280][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 20 from persistence list
[INFO][2018-05-24 20:12:35,280][org.apache.spark.storage.BlockManager]Removing RDD 20
[INFO][2018-05-24 20:12:35,283][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:12:35,283][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527163930000 ms
[INFO][2018-05-24 20:12:40,062][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527163960000 ms
[INFO][2018-05-24 20:12:40,063][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527163960000 ms.0 from job set of time 1527163960000 ms
[INFO][2018-05-24 20:12:40,068][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:64
[INFO][2018-05-24 20:12:40,069][org.apache.spark.scheduler.DAGScheduler]Got job 12 (foreachPartition at ReceiveKafkaData.scala:64) with 1 output partitions
[INFO][2018-05-24 20:12:40,069][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 12 (foreachPartition at ReceiveKafkaData.scala:64)
[INFO][2018-05-24 20:12:40,069][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:12:40,069][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:12:40,069][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 12 (MapPartitionsRDD[25] at map at ReceiveKafkaData.scala:62), which has no missing parents
[INFO][2018-05-24 20:12:40,071][org.apache.spark.storage.memory.MemoryStore]Block broadcast_12 stored as values in memory (estimated size 3.4 KB, free 912.3 MB)
[INFO][2018-05-24 20:12:40,073][org.apache.spark.storage.memory.MemoryStore]Block broadcast_12_piece0 stored as bytes in memory (estimated size 2008.0 B, free 912.3 MB)
[INFO][2018-05-24 20:12:40,073][org.apache.spark.storage.BlockManagerInfo]Added broadcast_12_piece0 in memory on 192.168.0.102:49458 (size: 2008.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:12:40,074][org.apache.spark.SparkContext]Created broadcast 12 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:12:40,074][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[25] at map at ReceiveKafkaData.scala:62) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:12:40,074][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 12.0 with 1 tasks
[INFO][2018-05-24 20:12:40,075][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 12.0 (TID 12, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:12:40,075][org.apache.spark.executor.Executor]Running task 0.0 in stage 12.0 (TID 12)
[INFO][2018-05-24 20:12:40,077][org.apache.spark.streaming.kafka.KafkaRDD]Computing topic seven, partition 0 offsets 12105 -> 12108
[INFO][2018-05-24 20:12:40,077][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 20:12:40,078][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 20:12:40,078][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 20:12:40,150][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:12:40,151][org.apache.spark.executor.Executor]Finished task 0.0 in stage 12.0 (TID 12). 708 bytes result sent to driver
[INFO][2018-05-24 20:12:40,151][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 12.0 (TID 12) in 76 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:12:40,151][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 12.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:12:40,152][org.apache.spark.scheduler.DAGScheduler]ResultStage 12 (foreachPartition at ReceiveKafkaData.scala:64) finished in 0.077 s
[INFO][2018-05-24 20:12:40,152][org.apache.spark.scheduler.DAGScheduler]Job 12 finished: foreachPartition at ReceiveKafkaData.scala:64, took 0.084080 s
[INFO][2018-05-24 20:12:40,153][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527163960000 ms.0 from job set of time 1527163960000 ms
[INFO][2018-05-24 20:12:40,153][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 23 from persistence list
[INFO][2018-05-24 20:12:40,153][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.153 s for time 1527163960000 ms (execution: 0.090 s)
[INFO][2018-05-24 20:12:40,153][org.apache.spark.storage.BlockManager]Removing RDD 23
[INFO][2018-05-24 20:12:40,153][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 22 from persistence list
[INFO][2018-05-24 20:12:40,154][org.apache.spark.storage.BlockManager]Removing RDD 22
[INFO][2018-05-24 20:12:40,154][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:12:40,154][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527163940000 ms
[INFO][2018-05-24 20:12:50,076][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527163970000 ms
[INFO][2018-05-24 20:12:50,077][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527163970000 ms.0 from job set of time 1527163970000 ms
[INFO][2018-05-24 20:12:50,084][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:64
[INFO][2018-05-24 20:12:50,084][org.apache.spark.scheduler.DAGScheduler]Got job 13 (foreachPartition at ReceiveKafkaData.scala:64) with 1 output partitions
[INFO][2018-05-24 20:12:50,084][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 13 (foreachPartition at ReceiveKafkaData.scala:64)
[INFO][2018-05-24 20:12:50,084][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:12:50,085][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:12:50,085][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 13 (MapPartitionsRDD[27] at map at ReceiveKafkaData.scala:62), which has no missing parents
[INFO][2018-05-24 20:12:50,087][org.apache.spark.storage.memory.MemoryStore]Block broadcast_13 stored as values in memory (estimated size 3.4 KB, free 912.2 MB)
[INFO][2018-05-24 20:12:50,088][org.apache.spark.storage.memory.MemoryStore]Block broadcast_13_piece0 stored as bytes in memory (estimated size 2008.0 B, free 912.2 MB)
[INFO][2018-05-24 20:12:50,089][org.apache.spark.storage.BlockManagerInfo]Added broadcast_13_piece0 in memory on 192.168.0.102:49458 (size: 2008.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:12:50,089][org.apache.spark.SparkContext]Created broadcast 13 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:12:50,090][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[27] at map at ReceiveKafkaData.scala:62) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:12:50,090][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 13.0 with 1 tasks
[INFO][2018-05-24 20:12:50,090][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 13.0 (TID 13, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:12:50,092][org.apache.spark.executor.Executor]Running task 0.0 in stage 13.0 (TID 13)
[INFO][2018-05-24 20:12:50,094][org.apache.spark.streaming.kafka.KafkaRDD]Computing topic seven, partition 0 offsets 12108 -> 12112
[INFO][2018-05-24 20:12:50,094][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 20:12:50,094][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 20:12:50,094][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 20:12:50,153][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:12:50,154][org.apache.spark.executor.Executor]Finished task 0.0 in stage 13.0 (TID 13). 708 bytes result sent to driver
[INFO][2018-05-24 20:12:50,154][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 13.0 (TID 13) in 64 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:12:50,155][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 13.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:12:50,156][org.apache.spark.scheduler.DAGScheduler]ResultStage 13 (foreachPartition at ReceiveKafkaData.scala:64) finished in 0.063 s
[INFO][2018-05-24 20:12:50,157][org.apache.spark.scheduler.DAGScheduler]Job 13 finished: foreachPartition at ReceiveKafkaData.scala:64, took 0.073166 s
[INFO][2018-05-24 20:12:50,157][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527163970000 ms.0 from job set of time 1527163970000 ms
[INFO][2018-05-24 20:12:50,157][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 25 from persistence list
[INFO][2018-05-24 20:12:50,157][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.157 s for time 1527163970000 ms (execution: 0.080 s)
[INFO][2018-05-24 20:12:50,158][org.apache.spark.storage.BlockManager]Removing RDD 25
[INFO][2018-05-24 20:12:50,158][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 24 from persistence list
[INFO][2018-05-24 20:12:50,158][org.apache.spark.storage.BlockManager]Removing RDD 24
[INFO][2018-05-24 20:12:50,159][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:12:50,159][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527163950000 ms
[INFO][2018-05-24 20:13:10,078][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527163980000 ms
[INFO][2018-05-24 20:13:10,078][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527163980000 ms.0 from job set of time 1527163980000 ms
[INFO][2018-05-24 20:13:10,086][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:64
[INFO][2018-05-24 20:13:10,087][org.apache.spark.scheduler.DAGScheduler]Got job 14 (foreachPartition at ReceiveKafkaData.scala:64) with 1 output partitions
[INFO][2018-05-24 20:13:10,087][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 14 (foreachPartition at ReceiveKafkaData.scala:64)
[INFO][2018-05-24 20:13:10,087][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:13:10,087][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:13:10,087][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 14 (MapPartitionsRDD[29] at map at ReceiveKafkaData.scala:62), which has no missing parents
[INFO][2018-05-24 20:13:10,089][org.apache.spark.storage.memory.MemoryStore]Block broadcast_14 stored as values in memory (estimated size 3.4 KB, free 912.2 MB)
[INFO][2018-05-24 20:13:10,090][org.apache.spark.storage.memory.MemoryStore]Block broadcast_14_piece0 stored as bytes in memory (estimated size 2008.0 B, free 912.2 MB)
[INFO][2018-05-24 20:13:10,091][org.apache.spark.storage.BlockManagerInfo]Added broadcast_14_piece0 in memory on 192.168.0.102:49458 (size: 2008.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:13:10,092][org.apache.spark.SparkContext]Created broadcast 14 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:13:10,093][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[29] at map at ReceiveKafkaData.scala:62) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:13:10,093][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 14.0 with 1 tasks
[INFO][2018-05-24 20:13:10,101][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 14.0 (TID 14, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:13:10,102][org.apache.spark.executor.Executor]Running task 0.0 in stage 14.0 (TID 14)
[INFO][2018-05-24 20:13:10,104][org.apache.spark.streaming.kafka.KafkaRDD]Computing topic seven, partition 0 offsets 12112 -> 12120
[INFO][2018-05-24 20:13:10,105][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 20:13:10,105][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 20:13:10,105][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 20:13:15,144][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:13:15,148][org.apache.spark.executor.Executor]Finished task 0.0 in stage 14.0 (TID 14). 708 bytes result sent to driver
[INFO][2018-05-24 20:13:15,152][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 14.0 (TID 14) in 5054 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:13:15,153][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 14.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:13:15,153][org.apache.spark.scheduler.DAGScheduler]ResultStage 14 (foreachPartition at ReceiveKafkaData.scala:64) finished in 5.059 s
[INFO][2018-05-24 20:13:15,158][org.apache.spark.scheduler.DAGScheduler]Job 14 finished: foreachPartition at ReceiveKafkaData.scala:64, took 5.071614 s
[INFO][2018-05-24 20:13:15,165][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527163980000 ms.0 from job set of time 1527163980000 ms
[INFO][2018-05-24 20:13:15,166][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 15.165 s for time 1527163980000 ms (execution: 5.087 s)
[INFO][2018-05-24 20:13:15,203][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527163990000 ms
[INFO][2018-05-24 20:13:15,203][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 27 from persistence list
[INFO][2018-05-24 20:13:15,203][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527163990000 ms.0 from job set of time 1527163990000 ms
[INFO][2018-05-24 20:13:15,204][org.apache.spark.storage.BlockManager]Removing RDD 27
[INFO][2018-05-24 20:13:15,204][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 26 from persistence list
[INFO][2018-05-24 20:13:15,205][org.apache.spark.storage.BlockManager]Removing RDD 26
[INFO][2018-05-24 20:13:15,205][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:13:15,205][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527163960000 ms
[INFO][2018-05-24 20:13:15,209][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:64
[INFO][2018-05-24 20:13:15,209][org.apache.spark.scheduler.DAGScheduler]Got job 15 (foreachPartition at ReceiveKafkaData.scala:64) with 1 output partitions
[INFO][2018-05-24 20:13:15,209][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 15 (foreachPartition at ReceiveKafkaData.scala:64)
[INFO][2018-05-24 20:13:15,209][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:13:15,209][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:13:15,210][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 15 (MapPartitionsRDD[31] at map at ReceiveKafkaData.scala:62), which has no missing parents
[INFO][2018-05-24 20:13:15,211][org.apache.spark.storage.memory.MemoryStore]Block broadcast_15 stored as values in memory (estimated size 3.4 KB, free 912.2 MB)
[INFO][2018-05-24 20:13:15,212][org.apache.spark.storage.memory.MemoryStore]Block broadcast_15_piece0 stored as bytes in memory (estimated size 2008.0 B, free 912.2 MB)
[INFO][2018-05-24 20:13:15,212][org.apache.spark.storage.BlockManagerInfo]Added broadcast_15_piece0 in memory on 192.168.0.102:49458 (size: 2008.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:13:15,213][org.apache.spark.SparkContext]Created broadcast 15 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:13:15,214][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 15 (MapPartitionsRDD[31] at map at ReceiveKafkaData.scala:62) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:13:15,214][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 15.0 with 1 tasks
[INFO][2018-05-24 20:13:15,215][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 15.0 (TID 15, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:13:15,216][org.apache.spark.executor.Executor]Running task 0.0 in stage 15.0 (TID 15)
[INFO][2018-05-24 20:13:15,224][org.apache.spark.streaming.kafka.KafkaRDD]Computing topic seven, partition 0 offsets 12120 -> 12122
[INFO][2018-05-24 20:13:15,224][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 20:13:15,224][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 20:13:15,224][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 20:13:15,287][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:13:15,288][org.apache.spark.executor.Executor]Finished task 0.0 in stage 15.0 (TID 15). 708 bytes result sent to driver
[INFO][2018-05-24 20:13:15,289][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 15.0 (TID 15) in 74 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:13:15,289][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 15.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:13:15,290][org.apache.spark.scheduler.DAGScheduler]ResultStage 15 (foreachPartition at ReceiveKafkaData.scala:64) finished in 0.075 s
[INFO][2018-05-24 20:13:15,290][org.apache.spark.scheduler.DAGScheduler]Job 15 finished: foreachPartition at ReceiveKafkaData.scala:64, took 0.081573 s
[INFO][2018-05-24 20:13:15,291][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527163990000 ms.0 from job set of time 1527163990000 ms
[INFO][2018-05-24 20:13:15,291][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 5.291 s for time 1527163990000 ms (execution: 0.088 s)
[INFO][2018-05-24 20:13:15,291][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 29 from persistence list
[INFO][2018-05-24 20:13:15,292][org.apache.spark.storage.BlockManager]Removing RDD 29
[INFO][2018-05-24 20:13:15,292][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 28 from persistence list
[INFO][2018-05-24 20:13:15,293][org.apache.spark.storage.BlockManager]Removing RDD 28
[INFO][2018-05-24 20:13:15,293][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:13:15,293][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527163970000 ms
[INFO][2018-05-24 20:13:19,153][org.apache.spark.util.ShutdownHookManager]Shutdown hook called
[INFO][2018-05-24 20:13:19,154][org.apache.spark.util.ShutdownHookManager]Deleting directory /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/spark-0eb2ed87-71f8-40b0-9e40-60e066627c4f
[INFO][2018-05-24 20:13:19,184][org.apache.spark.streaming.StreamingContext]Invoking stop(stopGracefully=false) from shutdown hook
[INFO][2018-05-24 20:13:19,186][org.apache.spark.streaming.scheduler.ReceiverTracker]ReceiverTracker stopped
[INFO][2018-05-24 20:13:19,189][org.apache.spark.streaming.scheduler.JobGenerator]Stopping JobGenerator immediately
[INFO][2018-05-24 20:13:19,190][org.apache.spark.streaming.util.RecurringTimer]Stopped timer for JobGenerator after time 1527163990000
[INFO][2018-05-24 20:13:19,191][org.apache.spark.streaming.scheduler.JobGenerator]Stopped JobGenerator
[INFO][2018-05-24 20:13:19,195][org.apache.spark.streaming.scheduler.JobScheduler]Stopped JobScheduler
[INFO][2018-05-24 20:13:19,205][org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@1ab6718{/streaming,null,UNAVAILABLE,@Spark}
[INFO][2018-05-24 20:13:19,206][org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@78b41097{/streaming/batch,null,UNAVAILABLE,@Spark}
[INFO][2018-05-24 20:13:19,207][org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@7cf283e1{/static/streaming,null,UNAVAILABLE,@Spark}
[INFO][2018-05-24 20:13:19,208][org.apache.spark.streaming.StreamingContext]StreamingContext stopped successfully
[INFO][2018-05-24 20:13:19,208][org.apache.spark.SparkContext]Invoking stop() from shutdown hook
[INFO][2018-05-24 20:13:19,220][org.spark_project.jetty.server.AbstractConnector]Stopped Spark@24faea88{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-05-24 20:13:19,222][org.apache.spark.ui.SparkUI]Stopped Spark web UI at http://192.168.0.102:4040
[INFO][2018-05-24 20:13:19,230][org.apache.spark.MapOutputTrackerMasterEndpoint]MapOutputTrackerMasterEndpoint stopped!
[INFO][2018-05-24 20:13:19,250][org.apache.spark.storage.memory.MemoryStore]MemoryStore cleared
[INFO][2018-05-24 20:13:19,250][org.apache.spark.storage.BlockManager]BlockManager stopped
[INFO][2018-05-24 20:13:19,251][org.apache.spark.storage.BlockManagerMaster]BlockManagerMaster stopped
[INFO][2018-05-24 20:13:19,253][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint]OutputCommitCoordinator stopped!
[INFO][2018-05-24 20:13:19,254][org.apache.spark.SparkContext]Successfully stopped SparkContext
[INFO][2018-05-24 20:13:19,255][org.apache.spark.util.ShutdownHookManager]Shutdown hook called
[INFO][2018-05-24 20:13:19,255][org.apache.spark.util.ShutdownHookManager]Deleting directory /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/spark-45cc9e91-7750-42f9-8a7e-77159e5673a1
[INFO][2018-05-24 20:22:58,037][org.apache.spark.SparkContext]Running Spark version 2.2.0
[INFO][2018-05-24 20:22:58,681][org.apache.spark.SparkContext]Submitted application: ReceiveKafkaData$
[INFO][2018-05-24 20:22:58,706][org.apache.spark.SecurityManager]Changing view acls to: seven
[INFO][2018-05-24 20:22:58,706][org.apache.spark.SecurityManager]Changing modify acls to: seven
[INFO][2018-05-24 20:22:58,709][org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO][2018-05-24 20:22:58,710][org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO][2018-05-24 20:22:58,711][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(seven); groups with view permissions: Set(); users  with modify permissions: Set(seven); groups with modify permissions: Set()
[INFO][2018-05-24 20:22:58,986][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 49676.
[INFO][2018-05-24 20:22:59,008][org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO][2018-05-24 20:22:59,022][org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO][2018-05-24 20:22:59,025][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2018-05-24 20:22:59,025][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
[INFO][2018-05-24 20:22:59,034][org.apache.spark.storage.DiskBlockManager]Created local directory at /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/blockmgr-1eb23118-d589-4d33-9478-e40c03e97963
[INFO][2018-05-24 20:22:59,047][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 912.3 MB
[INFO][2018-05-24 20:22:59,117][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO][2018-05-24 20:22:59,179][org.spark_project.jetty.util.log]Logging initialized @1989ms
[INFO][2018-05-24 20:22:59,236][org.spark_project.jetty.server.Server]jetty-9.3.z-SNAPSHOT
[INFO][2018-05-24 20:22:59,250][org.spark_project.jetty.server.Server]Started @2060ms
[INFO][2018-05-24 20:22:59,273][org.spark_project.jetty.server.AbstractConnector]Started ServerConnector@6efa953f{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-05-24 20:22:59,273][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4040.
[INFO][2018-05-24 20:22:59,301][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@c7a975a{/jobs,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:22:59,302][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@55795845{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:22:59,302][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@119f1f2a{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:22:59,304][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7fd4acee{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:22:59,304][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6175619b{/stages,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:22:59,305][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@756cf158{/stages/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:22:59,306][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@751d3241{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:22:59,307][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@30ea8c23{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:22:59,308][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4e76dac{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:22:59,308][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5f2f577{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:22:59,309][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5d465e4b{/storage,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:22:59,310][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@41a90fa8{/storage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:22:59,310][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@52500920{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:22:59,311][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@18a3962d{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:22:59,312][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2a65bb85{/environment,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:22:59,318][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4f936da8{/environment/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:22:59,321][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@452ba1db{/executors,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:22:59,322][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@76a36b71{/executors/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:22:59,322][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@f9d87b{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:22:59,323][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@26fb628{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:22:59,331][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@70dd7e15{/static,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:22:59,332][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@60d8c0dc{/,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:22:59,333][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6a62689d{/api,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:22:59,334][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1849db1a{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:22:59,335][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1ca25c47{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:22:59,341][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://192.168.0.102:4040
[INFO][2018-05-24 20:22:59,476][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
[INFO][2018-05-24 20:22:59,512][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 49677.
[INFO][2018-05-24 20:22:59,516][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 192.168.0.102:49677
[INFO][2018-05-24 20:22:59,522][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2018-05-24 20:22:59,524][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 192.168.0.102, 49677, None)
[INFO][2018-05-24 20:22:59,527][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 192.168.0.102:49677 with 912.3 MB RAM, BlockManagerId(driver, 192.168.0.102, 49677, None)
[INFO][2018-05-24 20:22:59,530][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 192.168.0.102, 49677, None)
[INFO][2018-05-24 20:22:59,531][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 192.168.0.102, 49677, None)
[INFO][2018-05-24 20:22:59,805][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@17f460bb{/metrics/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:22:59,917][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 20:22:59,920][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 20:22:59,920][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 20:23:10,433][org.apache.spark.streaming.kafka.DirectKafkaInputDStream]Slide time = 10000 ms
[INFO][2018-05-24 20:23:10,434][org.apache.spark.streaming.kafka.DirectKafkaInputDStream]Storage level = Serialized 1x Replicated
[INFO][2018-05-24 20:23:10,435][org.apache.spark.streaming.kafka.DirectKafkaInputDStream]Checkpoint interval = null
[INFO][2018-05-24 20:23:10,435][org.apache.spark.streaming.kafka.DirectKafkaInputDStream]Remember interval = 10000 ms
[INFO][2018-05-24 20:23:10,436][org.apache.spark.streaming.kafka.DirectKafkaInputDStream]Initialized and validated org.apache.spark.streaming.kafka.DirectKafkaInputDStream@36a8cfad
[INFO][2018-05-24 20:23:10,436][org.apache.spark.streaming.dstream.MappedDStream]Slide time = 10000 ms
[INFO][2018-05-24 20:23:10,436][org.apache.spark.streaming.dstream.MappedDStream]Storage level = Serialized 1x Replicated
[INFO][2018-05-24 20:23:10,436][org.apache.spark.streaming.dstream.MappedDStream]Checkpoint interval = null
[INFO][2018-05-24 20:23:10,436][org.apache.spark.streaming.dstream.MappedDStream]Remember interval = 10000 ms
[INFO][2018-05-24 20:23:10,436][org.apache.spark.streaming.dstream.MappedDStream]Initialized and validated org.apache.spark.streaming.dstream.MappedDStream@228e988d
[INFO][2018-05-24 20:23:10,436][org.apache.spark.streaming.dstream.ForEachDStream]Slide time = 10000 ms
[INFO][2018-05-24 20:23:10,436][org.apache.spark.streaming.dstream.ForEachDStream]Storage level = Serialized 1x Replicated
[INFO][2018-05-24 20:23:10,436][org.apache.spark.streaming.dstream.ForEachDStream]Checkpoint interval = null
[INFO][2018-05-24 20:23:10,437][org.apache.spark.streaming.dstream.ForEachDStream]Remember interval = 10000 ms
[INFO][2018-05-24 20:23:10,437][org.apache.spark.streaming.dstream.ForEachDStream]Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@3025b3ba
[INFO][2018-05-24 20:23:10,477][org.apache.spark.streaming.util.RecurringTimer]Started timer for JobGenerator at time 1527164600000
[INFO][2018-05-24 20:23:10,477][org.apache.spark.streaming.scheduler.JobGenerator]Started JobGenerator at 1527164600000 ms
[INFO][2018-05-24 20:23:10,478][org.apache.spark.streaming.scheduler.JobScheduler]Started JobScheduler
[INFO][2018-05-24 20:23:10,483][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@57a48985{/streaming,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:23:10,486][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3910fe11{/streaming/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:23:10,487][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3f672204{/streaming/batch,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:23:10,487][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2c2db130{/streaming/batch/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:23:10,488][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2d000e80{/static/streaming,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:23:10,489][org.apache.spark.streaming.StreamingContext]StreamingContext started
[INFO][2018-05-24 20:23:20,054][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 20:23:20,055][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 20:23:20,056][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 20:23:22,267][org.apache.spark.SparkContext]Running Spark version 2.2.0
[INFO][2018-05-24 20:23:23,112][org.apache.spark.SparkContext]Submitted application: SimulationKafkaSendOutData$
[INFO][2018-05-24 20:23:23,139][org.apache.spark.SecurityManager]Changing view acls to: seven
[INFO][2018-05-24 20:23:23,140][org.apache.spark.SecurityManager]Changing modify acls to: seven
[INFO][2018-05-24 20:23:23,140][org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO][2018-05-24 20:23:23,141][org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO][2018-05-24 20:23:23,141][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(seven); groups with view permissions: Set(); users  with modify permissions: Set(seven); groups with modify permissions: Set()
[INFO][2018-05-24 20:23:23,514][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 49685.
[INFO][2018-05-24 20:23:23,539][org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO][2018-05-24 20:23:23,559][org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO][2018-05-24 20:23:23,562][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2018-05-24 20:23:23,562][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
[INFO][2018-05-24 20:23:23,574][org.apache.spark.storage.DiskBlockManager]Created local directory at /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/blockmgr-400bcae9-fa25-4046-ac65-bc389d2a4d25
[INFO][2018-05-24 20:23:23,597][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 912.3 MB
[INFO][2018-05-24 20:23:23,705][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO][2018-05-24 20:23:23,818][org.spark_project.jetty.util.log]Logging initialized @2593ms
[INFO][2018-05-24 20:23:23,917][org.spark_project.jetty.server.Server]jetty-9.3.z-SNAPSHOT
[INFO][2018-05-24 20:23:23,939][org.spark_project.jetty.server.Server]Started @2714ms
[WARN][2018-05-24 20:23:23,959][org.apache.spark.util.Utils]Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[INFO][2018-05-24 20:23:23,967][org.spark_project.jetty.server.AbstractConnector]Started ServerConnector@60ca0db6{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}
[INFO][2018-05-24 20:23:23,967][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4041.
[INFO][2018-05-24 20:23:23,998][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@180e6ac4{/jobs,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:23:23,999][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@c7a975a{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:23:23,999][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@757d6814{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:23:24,003][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4c168660{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:23:24,004][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@fd0e5b6{/stages,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:23:24,004][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@36b0fcd5{/stages/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:23:24,005][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@475835b1{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:23:24,009][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@77192705{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:23:24,010][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7e809b79{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:23:24,011][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@625e134e{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:23:24,012][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@89c10b7{/storage,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:23:24,012][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4fe89c24{/storage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:23:24,013][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3d08f3f5{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:23:24,015][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1a1da881{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:23:24,016][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7fd4acee{/environment,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:23:24,019][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6175619b{/environment/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:23:24,020][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@756cf158{/executors,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:23:24,021][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@751d3241{/executors/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:23:24,021][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@64337702{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:23:24,022][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@30ea8c23{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:23:24,031][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4e76dac{/static,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:23:24,032][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@f9d87b{/,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:23:24,036][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@26fb628{/api,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:23:24,038][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5ac86ba5{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:23:24,040][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2c9399a4{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:23:24,051][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://192.168.0.102:4041
[INFO][2018-05-24 20:23:24,200][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
[INFO][2018-05-24 20:23:24,232][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 49686.
[INFO][2018-05-24 20:23:24,233][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 192.168.0.102:49686
[INFO][2018-05-24 20:23:24,269][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2018-05-24 20:23:24,271][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 192.168.0.102, 49686, None)
[INFO][2018-05-24 20:23:24,274][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 192.168.0.102:49686 with 912.3 MB RAM, BlockManagerId(driver, 192.168.0.102, 49686, None)
[INFO][2018-05-24 20:23:24,278][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 192.168.0.102, 49686, None)
[INFO][2018-05-24 20:23:24,278][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 192.168.0.102, 49686, None)
[INFO][2018-05-24 20:23:24,614][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@405325cf{/metrics/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:23:25,153][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527164600000 ms
[INFO][2018-05-24 20:23:25,156][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527164600000 ms.0 from job set of time 1527164600000 ms
[INFO][2018-05-24 20:23:25,190][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:65
[INFO][2018-05-24 20:23:25,207][org.apache.spark.scheduler.DAGScheduler]Got job 0 (foreachPartition at ReceiveKafkaData.scala:65) with 1 output partitions
[INFO][2018-05-24 20:23:25,207][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 0 (foreachPartition at ReceiveKafkaData.scala:65)
[INFO][2018-05-24 20:23:25,208][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:23:25,209][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:23:25,219][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 0 (MapPartitionsRDD[1] at map at ReceiveKafkaData.scala:63), which has no missing parents
[INFO][2018-05-24 20:23:25,323][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0 stored as values in memory (estimated size 228.1 KB, free 912.1 MB)
[INFO][2018-05-24 20:23:25,471][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 20:23:25,515][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0_piece0 stored as bytes in memory (estimated size 22.1 KB, free 912.1 MB)
[INFO][2018-05-24 20:23:25,518][org.apache.spark.storage.BlockManagerInfo]Added broadcast_0_piece0 in memory on 192.168.0.102:49686 (size: 22.1 KB, free: 912.3 MB)
[INFO][2018-05-24 20:23:25,519][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0_piece0 stored as bytes in memory (estimated size 1877.0 B, free 912.3 MB)
[INFO][2018-05-24 20:23:25,521][org.apache.spark.storage.BlockManagerInfo]Added broadcast_0_piece0 in memory on 192.168.0.102:49677 (size: 1877.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:23:25,524][org.apache.spark.SparkContext]Created broadcast 0 from textFile at SimulationKafkaSendOutData.scala:25
[INFO][2018-05-24 20:23:25,526][org.apache.spark.SparkContext]Created broadcast 0 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:23:25,555][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at map at ReceiveKafkaData.scala:63) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:23:25,556][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 0.0 with 1 tasks
[INFO][2018-05-24 20:23:25,597][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:23:25,608][org.apache.spark.executor.Executor]Running task 0.0 in stage 0.0 (TID 0)
[INFO][2018-05-24 20:23:25,639][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12124 is the same as ending offset skipping seven 0
[INFO][2018-05-24 20:23:25,918][org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper]Process identifier=hconnection-0x2020b19 connecting to ZooKeeper ensemble=vm-xaj-bigdata-da-d01:2181,vm-xaj-bigdata-da-d02:2181,vm-xaj-bigdata-da-d03:2181
[INFO][2018-05-24 20:23:25,925][org.apache.zookeeper.ZooKeeper]Client environment:zookeeper.version=3.4.5-cdh5.7.0--1, built on 04/06/2016 14:24 GMT
[INFO][2018-05-24 20:23:25,925][org.apache.zookeeper.ZooKeeper]Client environment:host.name=192.168.0.102
[INFO][2018-05-24 20:23:25,925][org.apache.zookeeper.ZooKeeper]Client environment:java.version=1.8.0_161
[INFO][2018-05-24 20:23:25,925][org.apache.zookeeper.ZooKeeper]Client environment:java.vendor=Oracle Corporation
[INFO][2018-05-24 20:23:25,925][org.apache.zookeeper.ZooKeeper]Client environment:java.home=/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre
[INFO][2018-05-24 20:23:25,925][org.apache.zookeeper.ZooKeeper]Client environment:java.class.path=/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/charsets.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/deploy.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/cldrdata.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/dnsns.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/jaccess.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/jfxrt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/localedata.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/nashorn.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/sunec.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/sunjce_provider.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/sunpkcs11.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/zipfs.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/javaws.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/jce.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/jfr.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/jfxswt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/jsse.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/management-agent.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/plugin.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/resources.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/rt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/ant-javafx.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/dt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/javafx-mx.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/jconsole.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/packager.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/sa-jdi.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/tools.jar:/Users/seven/project/scala/dataMining/target/classes:/Users/seven/software/maven/repository/org/scala-lang/scala-library/2.11.12/scala-library-2.11.12.jar:/Users/seven/software/maven/repository/org/scala-lang/scala-compiler/2.11.12/scala-compiler-2.11.12.jar:/Users/seven/software/maven/repository/org/scala-lang/modules/scala-xml_2.11/1.0.5/scala-xml_2.11-1.0.5.jar:/Users/seven/software/maven/repository/org/scala-lang/modules/scala-parser-combinators_2.11/1.0.4/scala-parser-combinators_2.11-1.0.4.jar:/Users/seven/software/maven/repository/org/scala-lang/scala-reflect/2.11.12/scala-reflect-2.11.12.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-client/2.6.0-cdh5.7.0/hadoop-client-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-common/2.6.0-cdh5.7.0/hadoop-common-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/xmlenc/xmlenc/0.52/xmlenc-0.52.jar:/Users/seven/software/maven/repository/commons-net/commons-net/3.1/commons-net-3.1.jar:/Users/seven/software/maven/repository/commons-configuration/commons-configuration/1.6/commons-configuration-1.6.jar:/Users/seven/software/maven/repository/commons-digester/commons-digester/1.8/commons-digester-1.8.jar:/Users/seven/software/maven/repository/commons-beanutils/commons-beanutils/1.7.0/commons-beanutils-1.7.0.jar:/Users/seven/software/maven/repository/org/slf4j/slf4j-api/1.7.5/slf4j-api-1.7.5.jar:/Users/seven/software/maven/repository/org/slf4j/slf4j-log4j12/1.7.5/slf4j-log4j12-1.7.5.jar:/Users/seven/software/maven/repository/org/apache/avro/avro/1.7.6-cdh5.7.0/avro-1.7.6-cdh5.7.0.jar:/Users/seven/software/maven/repository/com/google/code/gson/gson/2.2.4/gson-2.2.4.jar:/Users/seven/software/maven/repository/org/apache/curator/curator-client/2.7.1/curator-client-2.7.1.jar:/Users/seven/software/maven/repository/org/apache/curator/curator-recipes/2.7.1/curator-recipes-2.7.1.jar:/Users/seven/software/maven/repository/com/google/code/findbugs/jsr305/3.0.0/jsr305-3.0.0.jar:/Users/seven/software/maven/repository/org/apache/htrace/htrace-core4/4.0.1-incubating/htrace-core4-4.0.1-incubating.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-compress/1.4.1/commons-compress-1.4.1.jar:/Users/seven/software/maven/repository/org/tukaani/xz/1.0/xz-1.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-hdfs/2.6.0-cdh5.7.0/hadoop-hdfs-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/io/netty/netty/3.6.2.Final/netty-3.6.2.Final.jar:/Users/seven/software/maven/repository/xerces/xercesImpl/2.9.1/xercesImpl-2.9.1.jar:/Users/seven/software/maven/repository/xml-apis/xml-apis/1.3.04/xml-apis-1.3.04.jar:/Users/seven/software/maven/repository/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-app/2.6.0-cdh5.7.0/hadoop-mapreduce-client-app-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-common/2.6.0-cdh5.7.0/hadoop-mapreduce-client-common-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-yarn-client/2.6.0-cdh5.7.0/hadoop-yarn-client-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-yarn-server-common/2.6.0-cdh5.7.0/hadoop-yarn-server-common-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-shuffle/2.6.0-cdh5.7.0/hadoop-mapreduce-client-shuffle-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-yarn-api/2.6.0-cdh5.7.0/hadoop-yarn-api-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-core/2.6.0-cdh5.7.0/hadoop-mapreduce-client-core-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-yarn-common/2.6.0-cdh5.7.0/hadoop-yarn-common-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/javax/xml/bind/jaxb-api/2.2.2/jaxb-api-2.2.2.jar:/Users/seven/software/maven/repository/javax/xml/stream/stax-api/1.0-2/stax-api-1.0-2.jar:/Users/seven/software/maven/repository/javax/activation/activation/1.1/activation-1.1.jar:/Users/seven/software/maven/repository/com/sun/jersey/jersey-client/1.9/jersey-client-1.9.jar:/Users/seven/software/maven/repository/org/codehaus/jackson/jackson-xc/1.8.8/jackson-xc-1.8.8.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.6.0-cdh5.7.0/hadoop-mapreduce-client-jobclient-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-aws/2.6.0-cdh5.7.0/hadoop-aws-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/com/amazonaws/aws-java-sdk-s3/1.10.6/aws-java-sdk-s3-1.10.6.jar:/Users/seven/software/maven/repository/com/amazonaws/aws-java-sdk-kms/1.10.6/aws-java-sdk-kms-1.10.6.jar:/Users/seven/software/maven/repository/com/amazonaws/aws-java-sdk-core/1.10.6/aws-java-sdk-core-1.10.6.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-annotations/2.6.0-cdh5.7.0/hadoop-annotations-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-client/1.2.0-cdh5.7.0/hbase-client-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-annotations/1.2.0-cdh5.7.0/hbase-annotations-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-common/1.2.0-cdh5.7.0/hbase-common-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-protocol/1.2.0-cdh5.7.0/hbase-protocol-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/commons-codec/commons-codec/1.9/commons-codec-1.9.jar:/Users/seven/software/maven/repository/commons-io/commons-io/2.4/commons-io-2.4.jar:/Users/seven/software/maven/repository/commons-lang/commons-lang/2.6/commons-lang-2.6.jar:/Users/seven/software/maven/repository/commons-logging/commons-logging/1.2/commons-logging-1.2.jar:/Users/seven/software/maven/repository/com/google/guava/guava/12.0.1/guava-12.0.1.jar:/Users/seven/software/maven/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/Users/seven/software/maven/repository/io/netty/netty-all/4.0.23.Final/netty-all-4.0.23.Final.jar:/Users/seven/software/maven/repository/org/apache/zookeeper/zookeeper/3.4.5-cdh5.7.0/zookeeper-3.4.5-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/htrace/htrace-core/3.2.0-incubating/htrace-core-3.2.0-incubating.jar:/Users/seven/software/maven/repository/org/codehaus/jackson/jackson-mapper-asl/1.8.8/jackson-mapper-asl-1.8.8.jar:/Users/seven/software/maven/repository/org/jruby/jcodings/jcodings/1.0.8/jcodings-1.0.8.jar:/Users/seven/software/maven/repository/org/jruby/joni/joni/2.1.2/joni-2.1.2.jar:/Users/seven/software/maven/repository/com/yammer/metrics/metrics-core/2.2.0/metrics-core-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-auth/2.6.0-cdh5.7.0/hadoop-auth-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/httpcomponents/httpclient/4.2.5/httpclient-4.2.5.jar:/Users/seven/software/maven/repository/org/apache/httpcomponents/httpcore/4.2.4/httpcore-4.2.4.jar:/Users/seven/software/maven/repository/org/apache/directory/server/apacheds-kerberos-codec/2.0.0-M15/apacheds-kerberos-codec-2.0.0-M15.jar:/Users/seven/software/maven/repository/org/apache/directory/server/apacheds-i18n/2.0.0-M15/apacheds-i18n-2.0.0-M15.jar:/Users/seven/software/maven/repository/org/apache/directory/api/api-asn1-api/1.0.0-M20/api-asn1-api-1.0.0-M20.jar:/Users/seven/software/maven/repository/org/apache/directory/api/api-util/1.0.0-M20/api-util-1.0.0-M20.jar:/Users/seven/software/maven/repository/org/apache/curator/curator-framework/2.7.1/curator-framework-2.7.1.jar:/Users/seven/software/maven/repository/com/github/stephenc/findbugs/findbugs-annotations/1.3.9-1/findbugs-annotations-1.3.9-1.jar:/Users/seven/software/maven/repository/junit/junit/4.12/junit-4.12.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-server/1.2.0-cdh5.7.0/hbase-server-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-procedure/1.2.0-cdh5.7.0/hbase-procedure-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-common/1.2.0-cdh5.7.0/hbase-common-1.2.0-cdh5.7.0-tests.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-prefix-tree/1.2.0-cdh5.7.0/hbase-prefix-tree-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar:/Users/seven/software/maven/repository/commons-collections/commons-collections/3.2.2/commons-collections-3.2.2.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-hadoop-compat/1.2.0-cdh5.7.0/hbase-hadoop-compat-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-hadoop2-compat/1.2.0-cdh5.7.0/hbase-hadoop2-compat-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/com/sun/jersey/jersey-core/1.9/jersey-core-1.9.jar:/Users/seven/software/maven/repository/com/sun/jersey/jersey-server/1.9/jersey-server-1.9.jar:/Users/seven/software/maven/repository/asm/asm/3.1/asm-3.1.jar:/Users/seven/software/maven/repository/commons-cli/commons-cli/1.2/commons-cli-1.2.jar:/Users/seven/software/maven/repository/com/github/stephenc/high-scale-lib/high-scale-lib/1.1.1/high-scale-lib-1.1.1.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-math/2.1/commons-math-2.1.jar:/Users/seven/software/maven/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jetty/6.1.26.cloudera.4/jetty-6.1.26.cloudera.4.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jetty-util/6.1.26.cloudera.4/jetty-util-6.1.26.cloudera.4.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jetty-sslengine/6.1.26.cloudera.4/jetty-sslengine-6.1.26.cloudera.4.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jsp-2.1/6.1.14/jsp-2.1-6.1.14.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jsp-api-2.1/6.1.14/jsp-api-2.1-6.1.14.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/servlet-api-2.5/6.1.14/servlet-api-2.5-6.1.14.jar:/Users/seven/software/maven/repository/org/codehaus/jackson/jackson-core-asl/1.8.8/jackson-core-asl-1.8.8.jar:/Users/seven/software/maven/repository/org/codehaus/jackson/jackson-jaxrs/1.8.8/jackson-jaxrs-1.8.8.jar:/Users/seven/software/maven/repository/tomcat/jasper-compiler/5.5.23/jasper-compiler-5.5.23.jar:/Users/seven/software/maven/repository/tomcat/jasper-runtime/5.5.23/jasper-runtime-5.5.23.jar:/Users/seven/software/maven/repository/commons-el/commons-el/1.0/commons-el-1.0.jar:/Users/seven/software/maven/repository/org/jamon/jamon-runtime/2.4.1/jamon-runtime-2.4.1.jar:/Users/seven/software/maven/repository/com/lmax/disruptor/3.3.0/disruptor-3.3.0.jar:/Users/seven/software/maven/repository/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar:/Users/seven/software/maven/repository/org/owasp/esapi/esapi/2.1.0/esapi-2.1.0.jar:/Users/seven/software/maven/repository/commons-beanutils/commons-beanutils-core/1.7.0/commons-beanutils-core-1.7.0.jar:/Users/seven/software/maven/repository/commons-fileupload/commons-fileupload/1.2/commons-fileupload-1.2.jar:/Users/seven/software/maven/repository/xom/xom/1.2.5/xom-1.2.5.jar:/Users/seven/software/maven/repository/xalan/xalan/2.7.0/xalan-2.7.0.jar:/Users/seven/software/maven/repository/org/beanshell/bsh-core/2.0b4/bsh-core-2.0b4.jar:/Users/seven/software/maven/repository/org/owasp/antisamy/antisamy/1.4.3/antisamy-1.4.3.jar:/Users/seven/software/maven/repository/org/apache/xmlgraphics/batik-css/1.7/batik-css-1.7.jar:/Users/seven/software/maven/repository/org/apache/xmlgraphics/batik-ext/1.7/batik-ext-1.7.jar:/Users/seven/software/maven/repository/org/apache/xmlgraphics/batik-util/1.7/batik-util-1.7.jar:/Users/seven/software/maven/repository/xml-apis/xml-apis-ext/1.3.04/xml-apis-ext-1.3.04.jar:/Users/seven/software/maven/repository/net/sourceforge/nekohtml/nekohtml/1.9.12/nekohtml-1.9.12.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-core/2.6.0-mr1-cdh5.7.0/hadoop-core-2.6.0-mr1-cdh5.7.0.jar:/Users/seven/software/maven/repository/javax/servlet/jsp/jsp-api/2.1/jsp-api-2.1.jar:/Users/seven/software/maven/repository/net/java/dev/jets3t/jets3t/0.6.1/jets3t-0.6.1.jar:/Users/seven/software/maven/repository/javax/servlet/servlet-api/2.5/servlet-api-2.5.jar:/Users/seven/software/maven/repository/hsqldb/hsqldb/1.8.0.10/hsqldb-1.8.0.10.jar:/Users/seven/software/maven/repository/org/eclipse/jdt/core/3.1.1/core-3.1.1.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-hdfs/2.6.0-cdh5.7.0/hadoop-hdfs-2.6.0-cdh5.7.0-tests.jar:/Users/seven/software/maven/repository/commons-daemon/commons-daemon/1.0.13/commons-daemon-1.0.13.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-lang3/3.5/commons-lang3-3.5.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-pool2/2.4.2/commons-pool2-2.4.2.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-mllib_2.11/2.2.0/spark-mllib_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-core_2.11/2.2.0/spark-core_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/avro/avro-mapred/1.7.7/avro-mapred-1.7.7-hadoop2.jar:/Users/seven/software/maven/repository/org/apache/avro/avro-ipc/1.7.7/avro-ipc-1.7.7-tests.jar:/Users/seven/software/maven/repository/com/twitter/chill_2.11/0.8.0/chill_2.11-0.8.0.jar:/Users/seven/software/maven/repository/com/esotericsoftware/kryo-shaded/3.0.3/kryo-shaded-3.0.3.jar:/Users/seven/software/maven/repository/com/esotericsoftware/minlog/1.3.0/minlog-1.3.0.jar:/Users/seven/software/maven/repository/org/objenesis/objenesis/2.1/objenesis-2.1.jar:/Users/seven/software/maven/repository/com/twitter/chill-java/0.8.0/chill-java-0.8.0.jar:/Users/seven/software/maven/repository/org/apache/xbean/xbean-asm5-shaded/4.4/xbean-asm5-shaded-4.4.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-launcher_2.11/2.2.0/spark-launcher_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-network-common_2.11/2.2.0/spark-network-common_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-network-shuffle_2.11/2.2.0/spark-network-shuffle_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-unsafe_2.11/2.2.0/spark-unsafe_2.11-2.2.0.jar:/Users/seven/software/maven/repository/javax/servlet/javax.servlet-api/3.1.0/javax.servlet-api-3.1.0.jar:/Users/seven/software/maven/repository/org/slf4j/jul-to-slf4j/1.7.16/jul-to-slf4j-1.7.16.jar:/Users/seven/software/maven/repository/org/slf4j/jcl-over-slf4j/1.7.16/jcl-over-slf4j-1.7.16.jar:/Users/seven/software/maven/repository/com/ning/compress-lzf/1.0.3/compress-lzf-1.0.3.jar:/Users/seven/software/maven/repository/org/xerial/snappy/snappy-java/1.1.2.6/snappy-java-1.1.2.6.jar:/Users/seven/software/maven/repository/org/roaringbitmap/RoaringBitmap/0.5.11/RoaringBitmap-0.5.11.jar:/Users/seven/software/maven/repository/org/json4s/json4s-jackson_2.11/3.2.11/json4s-jackson_2.11-3.2.11.jar:/Users/seven/software/maven/repository/org/json4s/json4s-core_2.11/3.2.11/json4s-core_2.11-3.2.11.jar:/Users/seven/software/maven/repository/org/json4s/json4s-ast_2.11/3.2.11/json4s-ast_2.11-3.2.11.jar:/Users/seven/software/maven/repository/org/scala-lang/scalap/2.11.0/scalap-2.11.0.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/core/jersey-client/2.22.2/jersey-client-2.22.2.jar:/Users/seven/software/maven/repository/javax/ws/rs/javax.ws.rs-api/2.0.1/javax.ws.rs-api-2.0.1.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/hk2-api/2.4.0-b34/hk2-api-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/hk2-utils/2.4.0-b34/hk2-utils-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/external/aopalliance-repackaged/2.4.0-b34/aopalliance-repackaged-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/external/javax.inject/2.4.0-b34/javax.inject-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/hk2-locator/2.4.0-b34/hk2-locator-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/javassist/javassist/3.18.1-GA/javassist-3.18.1-GA.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/core/jersey-common/2.22.2/jersey-common-2.22.2.jar:/Users/seven/software/maven/repository/javax/annotation/javax.annotation-api/1.2/javax.annotation-api-1.2.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/bundles/repackaged/jersey-guava/2.22.2/jersey-guava-2.22.2.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/osgi-resource-locator/1.0.1/osgi-resource-locator-1.0.1.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/core/jersey-server/2.22.2/jersey-server-2.22.2.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/media/jersey-media-jaxb/2.22.2/jersey-media-jaxb-2.22.2.jar:/Users/seven/software/maven/repository/javax/validation/validation-api/1.1.0.Final/validation-api-1.1.0.Final.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/containers/jersey-container-servlet/2.22.2/jersey-container-servlet-2.22.2.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/containers/jersey-container-servlet-core/2.22.2/jersey-container-servlet-core-2.22.2.jar:/Users/seven/software/maven/repository/com/clearspring/analytics/stream/2.7.0/stream-2.7.0.jar:/Users/seven/software/maven/repository/io/dropwizard/metrics/metrics-core/3.1.2/metrics-core-3.1.2.jar:/Users/seven/software/maven/repository/io/dropwizard/metrics/metrics-jvm/3.1.2/metrics-jvm-3.1.2.jar:/Users/seven/software/maven/repository/io/dropwizard/metrics/metrics-json/3.1.2/metrics-json-3.1.2.jar:/Users/seven/software/maven/repository/io/dropwizard/metrics/metrics-graphite/3.1.2/metrics-graphite-3.1.2.jar:/Users/seven/software/maven/repository/org/apache/ivy/ivy/2.4.0/ivy-2.4.0.jar:/Users/seven/software/maven/repository/oro/oro/2.0.8/oro-2.0.8.jar:/Users/seven/software/maven/repository/net/razorvine/pyrolite/4.13/pyrolite-4.13.jar:/Users/seven/software/maven/repository/net/sf/py4j/py4j/0.10.4/py4j-0.10.4.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-crypto/1.0.0/commons-crypto-1.0.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-streaming_2.11/2.2.0/spark-streaming_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-sql_2.11/2.2.0/spark-sql_2.11-2.2.0.jar:/Users/seven/software/maven/repository/com/univocity/univocity-parsers/2.2.1/univocity-parsers-2.2.1.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-sketch_2.11/2.2.0/spark-sketch_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-catalyst_2.11/2.2.0/spark-catalyst_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/codehaus/janino/janino/3.0.0/janino-3.0.0.jar:/Users/seven/software/maven/repository/org/codehaus/janino/commons-compiler/3.0.0/commons-compiler-3.0.0.jar:/Users/seven/software/maven/repository/org/antlr/antlr4-runtime/4.5.3/antlr4-runtime-4.5.3.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-column/1.8.2/parquet-column-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-common/1.8.2/parquet-common-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-encoding/1.8.2/parquet-encoding-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-hadoop/1.8.2/parquet-hadoop-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-format/2.3.1/parquet-format-2.3.1.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-jackson/1.8.2/parquet-jackson-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-graphx_2.11/2.2.0/spark-graphx_2.11-2.2.0.jar:/Users/seven/software/maven/repository/com/github/fommil/netlib/core/1.1.2/core-1.1.2.jar:/Users/seven/software/maven/repository/net/sourceforge/f2j/arpack_combined_all/0.1/arpack_combined_all-0.1.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-mllib-local_2.11/2.2.0/spark-mllib-local_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/scalanlp/breeze_2.11/0.13.1/breeze_2.11-0.13.1.jar:/Users/seven/software/maven/repository/org/scalanlp/breeze-macros_2.11/0.13.1/breeze-macros_2.11-0.13.1.jar:/Users/seven/software/maven/repository/net/sf/opencsv/opencsv/2.3/opencsv-2.3.jar:/Users/seven/software/maven/repository/com/github/rwl/jtransforms/2.4.0/jtransforms-2.4.0.jar:/Users/seven/software/maven/repository/org/spire-math/spire_2.11/0.13.0/spire_2.11-0.13.0.jar:/Users/seven/software/maven/repository/org/spire-math/spire-macros_2.11/0.13.0/spire-macros_2.11-0.13.0.jar:/Users/seven/software/maven/repository/org/typelevel/machinist_2.11/0.6.1/machinist_2.11-0.6.1.jar:/Users/seven/software/maven/repository/com/chuusai/shapeless_2.11/2.3.2/shapeless_2.11-2.3.2.jar:/Users/seven/software/maven/repository/org/typelevel/macro-compat_2.11/1.1.1/macro-compat_2.11-1.1.1.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-math3/3.4.1/commons-math3-3.4.1.jar:/Users/seven/software/maven/repository/org/jpmml/pmml-model/1.2.15/pmml-model-1.2.15.jar:/Users/seven/software/maven/repository/org/jpmml/pmml-schema/1.2.15/pmml-schema-1.2.15.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-tags_2.11/2.2.0/spark-tags_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-streaming-kafka-0-8_2.11/2.2.0/spark-streaming-kafka-0-8_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/kafka/kafka_2.11/0.8.2.1/kafka_2.11-0.8.2.1.jar:/Users/seven/software/maven/repository/com/101tec/zkclient/0.3/zkclient-0.3.jar:/Users/seven/software/maven/repository/org/apache/kafka/kafka-clients/0.8.2.1/kafka-clients-0.8.2.1.jar:/Users/seven/software/maven/repository/mysql/mysql-connector-java/5.1.31/mysql-connector-java-5.1.31.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/module/jackson-module-scala_2.11/2.6.5/jackson-module-scala_2.11-2.6.5.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/core/jackson-core/2.6.5/jackson-core-2.6.5.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/core/jackson-annotations/2.6.5/jackson-annotations-2.6.5.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/core/jackson-databind/2.6.5/jackson-databind-2.6.5.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/module/jackson-module-paranamer/2.6.5/jackson-module-paranamer-2.6.5.jar:/Users/seven/software/maven/repository/com/thoughtworks/paranamer/paranamer/2.6/paranamer-2.6.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-streaming-flume_2.11/2.2.0/spark-streaming-flume_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/flume/flume-ng-core/1.6.0/flume-ng-core-1.6.0.jar:/Users/seven/software/maven/repository/org/apache/flume/flume-ng-configuration/1.6.0/flume-ng-configuration-1.6.0.jar:/Users/seven/software/maven/repository/org/apache/avro/avro-ipc/1.7.4/avro-ipc-1.7.4.jar:/Users/seven/software/maven/repository/org/apache/velocity/velocity/1.7/velocity-1.7.jar:/Users/seven/software/maven/repository/joda-time/joda-time/2.1/joda-time-2.1.jar:/Users/seven/software/maven/repository/org/apache/mina/mina-core/2.0.4/mina-core-2.0.4.jar:/Users/seven/software/maven/repository/org/apache/flume/flume-ng-sdk/1.6.0/flume-ng-sdk-1.6.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-streaming-flume-sink_2.11/2.2.0/spark-streaming-flume-sink_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/flume/flume-ng-clients/flume-ng-log4jappender/1.6.0/flume-ng-log4jappender-1.6.0.jar:/Users/seven/software/maven/repository/net/jpountz/lz4/lz4/1.3.0/lz4-1.3.0.jar:/Applications/IntelliJ IDEA.app/Contents/lib/idea_rt.jar
[INFO][2018-05-24 20:23:25,926][org.apache.zookeeper.ZooKeeper]Client environment:java.library.path=/Users/seven/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:.
[INFO][2018-05-24 20:23:25,926][org.apache.zookeeper.ZooKeeper]Client environment:java.io.tmpdir=/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/
[INFO][2018-05-24 20:23:25,926][org.apache.zookeeper.ZooKeeper]Client environment:java.compiler=<NA>
[INFO][2018-05-24 20:23:25,926][org.apache.zookeeper.ZooKeeper]Client environment:os.name=Mac OS X
[INFO][2018-05-24 20:23:25,926][org.apache.zookeeper.ZooKeeper]Client environment:os.arch=x86_64
[INFO][2018-05-24 20:23:25,927][org.apache.zookeeper.ZooKeeper]Client environment:os.version=10.13.4
[INFO][2018-05-24 20:23:25,927][org.apache.zookeeper.ZooKeeper]Client environment:user.name=seven
[INFO][2018-05-24 20:23:25,927][org.apache.zookeeper.ZooKeeper]Client environment:user.home=/Users/seven
[INFO][2018-05-24 20:23:25,927][org.apache.zookeeper.ZooKeeper]Client environment:user.dir=/Users/seven/project/scala/dataMining
[INFO][2018-05-24 20:23:25,928][org.apache.zookeeper.ZooKeeper]Initiating client connection, connectString=vm-xaj-bigdata-da-d01:2181,vm-xaj-bigdata-da-d02:2181,vm-xaj-bigdata-da-d03:2181 sessionTimeout=60000 watcher=hconnection-0x2020b190x0, quorum=vm-xaj-bigdata-da-d01:2181,vm-xaj-bigdata-da-d02:2181,vm-xaj-bigdata-da-d03:2181, baseZNode=/hbase
[INFO][2018-05-24 20:23:30,066][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527164610000 ms
[WARN][2018-05-24 20:23:31,179][org.apache.hadoop.util.NativeCodeLoader]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO][2018-05-24 20:23:31,333][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 1
[INFO][2018-05-24 20:23:31,433][org.apache.spark.SparkContext]Starting job: collect at SimulationKafkaSendOutData.scala:25
[INFO][2018-05-24 20:23:31,443][org.apache.spark.scheduler.DAGScheduler]Got job 0 (collect at SimulationKafkaSendOutData.scala:25) with 2 output partitions
[INFO][2018-05-24 20:23:31,444][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 0 (collect at SimulationKafkaSendOutData.scala:25)
[INFO][2018-05-24 20:23:31,444][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:23:31,445][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:23:31,452][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 0 (hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/* MapPartitionsRDD[1] at textFile at SimulationKafkaSendOutData.scala:25), which has no missing parents
[INFO][2018-05-24 20:23:31,471][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1 stored as values in memory (estimated size 3.3 KB, free 912.1 MB)
[INFO][2018-05-24 20:23:31,473][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1_piece0 stored as bytes in memory (estimated size 1969.0 B, free 912.1 MB)
[INFO][2018-05-24 20:23:31,474][org.apache.spark.storage.BlockManagerInfo]Added broadcast_1_piece0 in memory on 192.168.0.102:49686 (size: 1969.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:23:31,474][org.apache.spark.SparkContext]Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:23:31,487][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ResultStage 0 (hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/* MapPartitionsRDD[1] at textFile at SimulationKafkaSendOutData.scala:25) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2018-05-24 20:23:31,488][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 0.0 with 2 tasks
[INFO][2018-05-24 20:23:31,525][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 4883 bytes)
[INFO][2018-05-24 20:23:31,528][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 4883 bytes)
[INFO][2018-05-24 20:23:31,535][org.apache.spark.executor.Executor]Running task 0.0 in stage 0.0 (TID 0)
[INFO][2018-05-24 20:23:31,536][org.apache.spark.executor.Executor]Running task 1.0 in stage 0.0 (TID 1)
[INFO][2018-05-24 20:23:31,593][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/part-00000:10717435+10717436
[INFO][2018-05-24 20:23:31,593][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/part-00000:0+10717435
[INFO][2018-05-24 20:23:35,952][org.apache.zookeeper.ClientCnxn]Opening socket connection to server master/10.213.4.25:2181. Will not attempt to authenticate using SASL (unknown error)
[INFO][2018-05-24 20:23:35,969][org.apache.zookeeper.ClientCnxn]Socket connection established, initiating session, client: /10.0.8.39:49696, server: master/10.213.4.25:2181
[INFO][2018-05-24 20:23:35,998][org.apache.zookeeper.ClientCnxn]Session establishment complete on server master/10.213.4.25:2181, sessionid = 0x162b4dc56095e9c, negotiated timeout = 60000
[WARN][2018-05-24 20:23:36,538][org.apache.hadoop.util.NativeCodeLoader]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO][2018-05-24 20:23:36,632][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:23:36,647][org.apache.spark.executor.Executor]Finished task 0.0 in stage 0.0 (TID 0). 751 bytes result sent to driver
[INFO][2018-05-24 20:23:36,654][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 0.0 (TID 0) in 11067 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:23:36,656][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:23:36,659][org.apache.spark.scheduler.DAGScheduler]ResultStage 0 (foreachPartition at ReceiveKafkaData.scala:65) finished in 11.082 s
[INFO][2018-05-24 20:23:36,666][org.apache.spark.scheduler.DAGScheduler]Job 0 finished: foreachPartition at ReceiveKafkaData.scala:65, took 11.474881 s
[INFO][2018-05-24 20:23:36,670][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527164600000 ms.0 from job set of time 1527164600000 ms
[INFO][2018-05-24 20:23:36,672][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 16.670 s for time 1527164600000 ms (execution: 11.515 s)
[INFO][2018-05-24 20:23:36,672][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527164610000 ms.0 from job set of time 1527164610000 ms
[INFO][2018-05-24 20:23:36,679][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:23:36,681][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:65
[INFO][2018-05-24 20:23:36,682][org.apache.spark.scheduler.DAGScheduler]Got job 1 (foreachPartition at ReceiveKafkaData.scala:65) with 1 output partitions
[INFO][2018-05-24 20:23:36,682][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 1 (foreachPartition at ReceiveKafkaData.scala:65)
[INFO][2018-05-24 20:23:36,683][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:23:36,683][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:23:36,683][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 1 (MapPartitionsRDD[3] at map at ReceiveKafkaData.scala:63), which has no missing parents
[INFO][2018-05-24 20:23:36,684][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 
[INFO][2018-05-24 20:23:36,686][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 20:23:36,690][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1_piece0 stored as bytes in memory (estimated size 1878.0 B, free 912.3 MB)
[INFO][2018-05-24 20:23:36,691][org.apache.spark.storage.BlockManagerInfo]Added broadcast_1_piece0 in memory on 192.168.0.102:49677 (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:23:36,692][org.apache.spark.SparkContext]Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:23:36,693][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at map at ReceiveKafkaData.scala:63) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:23:36,693][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 1.0 with 1 tasks
[INFO][2018-05-24 20:23:36,694][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:23:36,694][org.apache.spark.executor.Executor]Running task 0.0 in stage 1.0 (TID 1)
[INFO][2018-05-24 20:23:36,735][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12124 is the same as ending offset skipping seven 0
[INFO][2018-05-24 20:23:36,735][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:23:36,737][org.apache.spark.executor.Executor]Finished task 0.0 in stage 1.0 (TID 1). 708 bytes result sent to driver
[INFO][2018-05-24 20:23:36,738][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 1.0 (TID 1) in 44 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:23:36,739][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:23:36,739][org.apache.spark.scheduler.DAGScheduler]ResultStage 1 (foreachPartition at ReceiveKafkaData.scala:65) finished in 0.046 s
[INFO][2018-05-24 20:23:36,740][org.apache.spark.scheduler.DAGScheduler]Job 1 finished: foreachPartition at ReceiveKafkaData.scala:65, took 0.058596 s
[INFO][2018-05-24 20:23:36,741][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527164610000 ms.0 from job set of time 1527164610000 ms
[INFO][2018-05-24 20:23:36,741][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 6.741 s for time 1527164610000 ms (execution: 0.069 s)
[INFO][2018-05-24 20:23:36,742][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 1 from persistence list
[INFO][2018-05-24 20:23:36,749][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 0 from persistence list
[INFO][2018-05-24 20:23:36,750][org.apache.spark.storage.BlockManager]Removing RDD 1
[INFO][2018-05-24 20:23:36,750][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:23:36,750][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 
[INFO][2018-05-24 20:23:36,750][org.apache.spark.storage.BlockManager]Removing RDD 0
[INFO][2018-05-24 20:23:36,788][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_1_piece0 on 192.168.0.102:49677 in memory (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:23:36,791][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_0_piece0 on 192.168.0.102:49677 in memory (size: 1877.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:23:40,305][org.apache.spark.storage.memory.MemoryStore]Block taskresult_0 stored as bytes in memory (estimated size 10.4 MB, free 901.7 MB)
[INFO][2018-05-24 20:23:40,306][org.apache.spark.storage.BlockManagerInfo]Added taskresult_0 in memory on 192.168.0.102:49686 (size: 10.4 MB, free: 901.9 MB)
[INFO][2018-05-24 20:23:40,307][org.apache.spark.executor.Executor]Finished task 0.0 in stage 0.0 (TID 0). 10856349 bytes result sent via BlockManager)
[INFO][2018-05-24 20:23:40,345][org.apache.spark.network.client.TransportClientFactory]Successfully created connection to /192.168.0.102:49686 after 21 ms (0 ms spent in bootstraps)
[INFO][2018-05-24 20:23:40,658][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 0.0 (TID 0) in 9145 ms on localhost (executor driver) (1/2)
[INFO][2018-05-24 20:23:40,661][org.apache.spark.storage.BlockManagerInfo]Removed taskresult_0 on 192.168.0.102:49686 in memory (size: 10.4 MB, free: 912.3 MB)
[INFO][2018-05-24 20:23:41,853][org.apache.spark.storage.memory.MemoryStore]Block taskresult_1 stored as bytes in memory (estimated size 10.4 MB, free 901.7 MB)
[INFO][2018-05-24 20:23:41,854][org.apache.spark.storage.BlockManagerInfo]Added taskresult_1 in memory on 192.168.0.102:49686 (size: 10.4 MB, free: 901.9 MB)
[INFO][2018-05-24 20:23:41,855][org.apache.spark.executor.Executor]Finished task 1.0 in stage 0.0 (TID 1). 10855899 bytes result sent via BlockManager)
[INFO][2018-05-24 20:23:41,927][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 0.0 (TID 1) in 10400 ms on localhost (executor driver) (2/2)
[INFO][2018-05-24 20:23:41,928][org.apache.spark.storage.BlockManagerInfo]Removed taskresult_1 on 192.168.0.102:49686 in memory (size: 10.4 MB, free: 912.3 MB)
[INFO][2018-05-24 20:23:41,929][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:23:41,930][org.apache.spark.scheduler.DAGScheduler]ResultStage 0 (collect at SimulationKafkaSendOutData.scala:25) finished in 10.427 s
[INFO][2018-05-24 20:23:41,935][org.apache.spark.scheduler.DAGScheduler]Job 0 finished: collect at SimulationKafkaSendOutData.scala:25, took 10.501356 s
[INFO][2018-05-24 20:23:42,207][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_0_piece0 on 192.168.0.102:49686 in memory (size: 22.1 KB, free: 912.3 MB)
[INFO][2018-05-24 20:23:42,213][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_1_piece0 on 192.168.0.102:49686 in memory (size: 1969.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:23:42,217][org.spark_project.jetty.server.AbstractConnector]Stopped Spark@60ca0db6{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}
[INFO][2018-05-24 20:23:42,219][org.apache.spark.ui.SparkUI]Stopped Spark web UI at http://192.168.0.102:4041
[INFO][2018-05-24 20:23:42,227][org.apache.spark.MapOutputTrackerMasterEndpoint]MapOutputTrackerMasterEndpoint stopped!
[INFO][2018-05-24 20:23:42,240][org.apache.spark.storage.memory.MemoryStore]MemoryStore cleared
[INFO][2018-05-24 20:23:42,240][org.apache.spark.storage.BlockManager]BlockManager stopped
[INFO][2018-05-24 20:23:42,241][org.apache.spark.storage.BlockManagerMaster]BlockManagerMaster stopped
[INFO][2018-05-24 20:23:42,243][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint]OutputCommitCoordinator stopped!
[INFO][2018-05-24 20:23:42,244][org.apache.spark.SparkContext]Successfully stopped SparkContext
[INFO][2018-05-24 20:23:42,254][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	compression.type = none
	metric.reporters = []
	metadata.max.age.ms = 300000
	metadata.fetch.timeout.ms = 60000
	acks = all
	batch.size = 16384
	reconnect.backoff.ms = 10
	bootstrap.servers = [vm-xaj-bigdata-da-d01:9092, vm-xaj-bigdata-da-d02:9092, vm-xaj-bigdata-da-d03:9092, vm-xaj-bigdata-da-d04:9092, vm-xaj-bigdata-da-d05:9092, vm-xaj-bigdata-da-d06:9092, vm-xaj-bigdata-da-d07:9092]
	receive.buffer.bytes = 32768
	retry.backoff.ms = 100
	buffer.memory = 33554432
	timeout.ms = 30000
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	retries = 0
	max.request.size = 1048576
	block.on.buffer.full = true
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
	metrics.sample.window.ms = 30000
	send.buffer.bytes = 131072
	max.in.flight.requests.per.connection = 5
	metrics.num.samples = 2
	linger.ms = 1
	client.id = 

[INFO][2018-05-24 20:23:50,082][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527164620000 ms
[INFO][2018-05-24 20:23:50,084][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527164620000 ms.0 from job set of time 1527164620000 ms
[INFO][2018-05-24 20:23:50,109][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:65
[INFO][2018-05-24 20:23:50,111][org.apache.spark.scheduler.DAGScheduler]Got job 2 (foreachPartition at ReceiveKafkaData.scala:65) with 1 output partitions
[INFO][2018-05-24 20:23:50,111][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 2 (foreachPartition at ReceiveKafkaData.scala:65)
[INFO][2018-05-24 20:23:50,111][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:23:50,111][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:23:50,112][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 2 (MapPartitionsRDD[5] at map at ReceiveKafkaData.scala:63), which has no missing parents
[INFO][2018-05-24 20:23:50,115][org.apache.spark.storage.memory.MemoryStore]Block broadcast_2 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 20:23:50,119][org.apache.spark.storage.memory.MemoryStore]Block broadcast_2_piece0 stored as bytes in memory (estimated size 1878.0 B, free 912.3 MB)
[INFO][2018-05-24 20:23:50,120][org.apache.spark.storage.BlockManagerInfo]Added broadcast_2_piece0 in memory on 192.168.0.102:49677 (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:23:50,121][org.apache.spark.SparkContext]Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:23:50,122][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[5] at map at ReceiveKafkaData.scala:63) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:23:50,122][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 2.0 with 1 tasks
[INFO][2018-05-24 20:23:50,125][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:23:50,127][org.apache.spark.executor.Executor]Running task 0.0 in stage 2.0 (TID 2)
[INFO][2018-05-24 20:23:50,134][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12124 is the same as ending offset skipping seven 0
[INFO][2018-05-24 20:23:50,134][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:23:50,135][org.apache.spark.executor.Executor]Finished task 0.0 in stage 2.0 (TID 2). 751 bytes result sent to driver
[INFO][2018-05-24 20:23:50,137][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 2.0 (TID 2) in 14 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:23:50,138][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:23:50,143][org.apache.spark.scheduler.DAGScheduler]ResultStage 2 (foreachPartition at ReceiveKafkaData.scala:65) finished in 0.019 s
[INFO][2018-05-24 20:23:50,145][org.apache.spark.scheduler.DAGScheduler]Job 2 finished: foreachPartition at ReceiveKafkaData.scala:65, took 0.034766 s
[INFO][2018-05-24 20:23:50,146][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527164620000 ms.0 from job set of time 1527164620000 ms
[INFO][2018-05-24 20:23:50,147][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 10.145 s for time 1527164620000 ms (execution: 0.061 s)
[INFO][2018-05-24 20:23:50,149][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527164630000 ms
[INFO][2018-05-24 20:23:50,149][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 3 from persistence list
[INFO][2018-05-24 20:23:50,149][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527164630000 ms.0 from job set of time 1527164630000 ms
[INFO][2018-05-24 20:23:50,150][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 2 from persistence list
[INFO][2018-05-24 20:23:50,150][org.apache.spark.storage.BlockManager]Removing RDD 3
[INFO][2018-05-24 20:23:50,151][org.apache.spark.storage.BlockManager]Removing RDD 2
[INFO][2018-05-24 20:23:50,151][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:23:50,151][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527164600000 ms
[INFO][2018-05-24 20:23:50,166][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:65
[INFO][2018-05-24 20:23:50,167][org.apache.spark.scheduler.DAGScheduler]Got job 3 (foreachPartition at ReceiveKafkaData.scala:65) with 1 output partitions
[INFO][2018-05-24 20:23:50,167][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 3 (foreachPartition at ReceiveKafkaData.scala:65)
[INFO][2018-05-24 20:23:50,167][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:23:50,167][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:23:50,168][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 3 (MapPartitionsRDD[7] at map at ReceiveKafkaData.scala:63), which has no missing parents
[INFO][2018-05-24 20:23:50,172][org.apache.spark.storage.memory.MemoryStore]Block broadcast_3 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 20:23:50,181][org.apache.spark.storage.memory.MemoryStore]Block broadcast_3_piece0 stored as bytes in memory (estimated size 1878.0 B, free 912.3 MB)
[INFO][2018-05-24 20:23:50,183][org.apache.spark.storage.BlockManagerInfo]Added broadcast_3_piece0 in memory on 192.168.0.102:49677 (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:23:50,184][org.apache.spark.SparkContext]Created broadcast 3 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:23:50,185][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[7] at map at ReceiveKafkaData.scala:63) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:23:50,185][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 3.0 with 1 tasks
[INFO][2018-05-24 20:23:50,186][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 3.0 (TID 3, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:23:50,187][org.apache.spark.executor.Executor]Running task 0.0 in stage 3.0 (TID 3)
[INFO][2018-05-24 20:23:50,193][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12124 is the same as ending offset skipping seven 0
[INFO][2018-05-24 20:23:50,195][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:23:50,198][org.apache.spark.executor.Executor]Finished task 0.0 in stage 3.0 (TID 3). 708 bytes result sent to driver
[INFO][2018-05-24 20:23:50,200][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 3.0 (TID 3) in 14 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:23:50,201][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 3.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:23:50,203][org.apache.spark.scheduler.DAGScheduler]ResultStage 3 (foreachPartition at ReceiveKafkaData.scala:65) finished in 0.017 s
[INFO][2018-05-24 20:23:50,203][org.apache.spark.scheduler.DAGScheduler]Job 3 finished: foreachPartition at ReceiveKafkaData.scala:65, took 0.037327 s
[INFO][2018-05-24 20:23:50,204][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527164630000 ms.0 from job set of time 1527164630000 ms
[INFO][2018-05-24 20:23:50,204][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.204 s for time 1527164630000 ms (execution: 0.055 s)
[INFO][2018-05-24 20:23:50,205][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 5 from persistence list
[INFO][2018-05-24 20:23:50,205][org.apache.spark.storage.BlockManager]Removing RDD 5
[INFO][2018-05-24 20:23:50,206][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 4 from persistence list
[INFO][2018-05-24 20:23:50,206][org.apache.spark.storage.BlockManager]Removing RDD 4
[INFO][2018-05-24 20:23:50,207][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:23:50,208][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527164610000 ms
[INFO][2018-05-24 20:24:05,062][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527164640000 ms
[INFO][2018-05-24 20:24:05,062][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527164640000 ms.0 from job set of time 1527164640000 ms
[INFO][2018-05-24 20:24:05,069][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:65
[INFO][2018-05-24 20:24:05,071][org.apache.spark.scheduler.DAGScheduler]Got job 4 (foreachPartition at ReceiveKafkaData.scala:65) with 1 output partitions
[INFO][2018-05-24 20:24:05,071][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 4 (foreachPartition at ReceiveKafkaData.scala:65)
[INFO][2018-05-24 20:24:05,071][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:24:05,071][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:24:05,072][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 4 (MapPartitionsRDD[9] at map at ReceiveKafkaData.scala:63), which has no missing parents
[INFO][2018-05-24 20:24:05,074][org.apache.spark.storage.memory.MemoryStore]Block broadcast_4 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 20:24:05,075][org.apache.spark.storage.memory.MemoryStore]Block broadcast_4_piece0 stored as bytes in memory (estimated size 1878.0 B, free 912.3 MB)
[INFO][2018-05-24 20:24:05,076][org.apache.spark.storage.BlockManagerInfo]Added broadcast_4_piece0 in memory on 192.168.0.102:49677 (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:24:05,077][org.apache.spark.SparkContext]Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:24:05,078][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[9] at map at ReceiveKafkaData.scala:63) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:24:05,078][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 4.0 with 1 tasks
[INFO][2018-05-24 20:24:05,080][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 4.0 (TID 4, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:24:05,081][org.apache.spark.executor.Executor]Running task 0.0 in stage 4.0 (TID 4)
[INFO][2018-05-24 20:24:05,085][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12124 is the same as ending offset skipping seven 0
[INFO][2018-05-24 20:24:05,085][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:24:05,086][org.apache.spark.executor.Executor]Finished task 0.0 in stage 4.0 (TID 4). 708 bytes result sent to driver
[INFO][2018-05-24 20:24:05,088][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 4.0 (TID 4) in 8 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:24:05,088][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 4.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:24:05,089][org.apache.spark.scheduler.DAGScheduler]ResultStage 4 (foreachPartition at ReceiveKafkaData.scala:65) finished in 0.009 s
[INFO][2018-05-24 20:24:05,089][org.apache.spark.scheduler.DAGScheduler]Job 4 finished: foreachPartition at ReceiveKafkaData.scala:65, took 0.019718 s
[INFO][2018-05-24 20:24:05,090][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527164640000 ms.0 from job set of time 1527164640000 ms
[INFO][2018-05-24 20:24:05,090][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 5.090 s for time 1527164640000 ms (execution: 0.028 s)
[INFO][2018-05-24 20:24:05,090][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 7 from persistence list
[INFO][2018-05-24 20:24:05,090][org.apache.spark.storage.BlockManager]Removing RDD 7
[INFO][2018-05-24 20:24:05,091][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 6 from persistence list
[INFO][2018-05-24 20:24:05,092][org.apache.spark.storage.BlockManager]Removing RDD 6
[INFO][2018-05-24 20:24:05,092][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:24:05,092][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527164620000 ms
[INFO][2018-05-24 20:24:10,058][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527164650000 ms
[INFO][2018-05-24 20:24:10,059][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527164650000 ms.0 from job set of time 1527164650000 ms
[INFO][2018-05-24 20:24:10,065][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:65
[INFO][2018-05-24 20:24:10,066][org.apache.spark.scheduler.DAGScheduler]Got job 5 (foreachPartition at ReceiveKafkaData.scala:65) with 1 output partitions
[INFO][2018-05-24 20:24:10,066][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 5 (foreachPartition at ReceiveKafkaData.scala:65)
[INFO][2018-05-24 20:24:10,066][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:24:10,066][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:24:10,067][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 5 (MapPartitionsRDD[11] at map at ReceiveKafkaData.scala:63), which has no missing parents
[INFO][2018-05-24 20:24:10,068][org.apache.spark.storage.memory.MemoryStore]Block broadcast_5 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 20:24:10,070][org.apache.spark.storage.memory.MemoryStore]Block broadcast_5_piece0 stored as bytes in memory (estimated size 1878.0 B, free 912.3 MB)
[INFO][2018-05-24 20:24:10,072][org.apache.spark.storage.BlockManagerInfo]Added broadcast_5_piece0 in memory on 192.168.0.102:49677 (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:24:10,073][org.apache.spark.SparkContext]Created broadcast 5 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:24:10,074][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[11] at map at ReceiveKafkaData.scala:63) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:24:10,074][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 5.0 with 1 tasks
[INFO][2018-05-24 20:24:10,075][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 5.0 (TID 5, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:24:10,075][org.apache.spark.executor.Executor]Running task 0.0 in stage 5.0 (TID 5)
[INFO][2018-05-24 20:24:10,078][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12124 is the same as ending offset skipping seven 0
[INFO][2018-05-24 20:24:10,078][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:24:10,079][org.apache.spark.executor.Executor]Finished task 0.0 in stage 5.0 (TID 5). 665 bytes result sent to driver
[INFO][2018-05-24 20:24:10,080][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 5.0 (TID 5) in 6 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:24:10,080][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 5.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:24:10,080][org.apache.spark.scheduler.DAGScheduler]ResultStage 5 (foreachPartition at ReceiveKafkaData.scala:65) finished in 0.006 s
[INFO][2018-05-24 20:24:10,081][org.apache.spark.scheduler.DAGScheduler]Job 5 finished: foreachPartition at ReceiveKafkaData.scala:65, took 0.015288 s
[INFO][2018-05-24 20:24:10,081][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527164650000 ms.0 from job set of time 1527164650000 ms
[INFO][2018-05-24 20:24:10,081][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 9 from persistence list
[INFO][2018-05-24 20:24:10,082][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.081 s for time 1527164650000 ms (execution: 0.022 s)
[INFO][2018-05-24 20:24:10,082][org.apache.spark.storage.BlockManager]Removing RDD 9
[INFO][2018-05-24 20:24:10,082][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 8 from persistence list
[INFO][2018-05-24 20:24:10,083][org.apache.spark.storage.BlockManager]Removing RDD 8
[INFO][2018-05-24 20:24:10,083][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:24:10,083][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527164630000 ms
[INFO][2018-05-24 20:24:30,082][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527164660000 ms
[INFO][2018-05-24 20:24:30,084][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527164660000 ms.0 from job set of time 1527164660000 ms
[INFO][2018-05-24 20:24:30,090][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:65
[INFO][2018-05-24 20:24:30,091][org.apache.spark.scheduler.DAGScheduler]Got job 6 (foreachPartition at ReceiveKafkaData.scala:65) with 1 output partitions
[INFO][2018-05-24 20:24:30,091][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 6 (foreachPartition at ReceiveKafkaData.scala:65)
[INFO][2018-05-24 20:24:30,091][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:24:30,091][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:24:30,092][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 6 (MapPartitionsRDD[13] at map at ReceiveKafkaData.scala:63), which has no missing parents
[INFO][2018-05-24 20:24:30,094][org.apache.spark.storage.memory.MemoryStore]Block broadcast_6 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 20:24:30,096][org.apache.spark.storage.memory.MemoryStore]Block broadcast_6_piece0 stored as bytes in memory (estimated size 1878.0 B, free 912.3 MB)
[INFO][2018-05-24 20:24:30,098][org.apache.spark.storage.BlockManagerInfo]Added broadcast_6_piece0 in memory on 192.168.0.102:49677 (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:24:30,100][org.apache.spark.SparkContext]Created broadcast 6 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:24:30,101][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[13] at map at ReceiveKafkaData.scala:63) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:24:30,101][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 6.0 with 1 tasks
[INFO][2018-05-24 20:24:30,102][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 6.0 (TID 6, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:24:30,102][org.apache.spark.executor.Executor]Running task 0.0 in stage 6.0 (TID 6)
[INFO][2018-05-24 20:24:30,109][org.apache.spark.streaming.kafka.KafkaRDD]Computing topic seven, partition 0 offsets 12124 -> 12131
[INFO][2018-05-24 20:24:30,109][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 20:24:30,110][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 20:24:30,110][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 20:24:30,154][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527164670000 ms
[INFO][2018-05-24 20:24:30,536][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:24:30,537][org.apache.spark.executor.Executor]Finished task 0.0 in stage 6.0 (TID 6). 708 bytes result sent to driver
[INFO][2018-05-24 20:24:30,538][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 6.0 (TID 6) in 437 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:24:30,538][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 6.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:24:30,539][org.apache.spark.scheduler.DAGScheduler]ResultStage 6 (foreachPartition at ReceiveKafkaData.scala:65) finished in 0.438 s
[INFO][2018-05-24 20:24:30,539][org.apache.spark.scheduler.DAGScheduler]Job 6 finished: foreachPartition at ReceiveKafkaData.scala:65, took 0.448867 s
[INFO][2018-05-24 20:24:30,540][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527164660000 ms.0 from job set of time 1527164660000 ms
[INFO][2018-05-24 20:24:30,540][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 10.540 s for time 1527164660000 ms (execution: 0.456 s)
[INFO][2018-05-24 20:24:30,541][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527164670000 ms.0 from job set of time 1527164670000 ms
[INFO][2018-05-24 20:24:30,541][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 11 from persistence list
[INFO][2018-05-24 20:24:30,542][org.apache.spark.storage.BlockManager]Removing RDD 11
[INFO][2018-05-24 20:24:30,542][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 10 from persistence list
[INFO][2018-05-24 20:24:30,543][org.apache.spark.storage.BlockManager]Removing RDD 10
[INFO][2018-05-24 20:24:30,544][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:24:30,544][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527164640000 ms
[INFO][2018-05-24 20:24:30,549][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:65
[INFO][2018-05-24 20:24:30,550][org.apache.spark.scheduler.DAGScheduler]Got job 7 (foreachPartition at ReceiveKafkaData.scala:65) with 1 output partitions
[INFO][2018-05-24 20:24:30,550][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 7 (foreachPartition at ReceiveKafkaData.scala:65)
[INFO][2018-05-24 20:24:30,550][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:24:30,550][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:24:30,551][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 7 (MapPartitionsRDD[15] at map at ReceiveKafkaData.scala:63), which has no missing parents
[INFO][2018-05-24 20:24:30,554][org.apache.spark.storage.memory.MemoryStore]Block broadcast_7 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 20:24:30,556][org.apache.spark.storage.memory.MemoryStore]Block broadcast_7_piece0 stored as bytes in memory (estimated size 1878.0 B, free 912.3 MB)
[INFO][2018-05-24 20:24:30,557][org.apache.spark.storage.BlockManagerInfo]Added broadcast_7_piece0 in memory on 192.168.0.102:49677 (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:24:30,557][org.apache.spark.SparkContext]Created broadcast 7 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:24:30,558][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[15] at map at ReceiveKafkaData.scala:63) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:24:30,558][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 7.0 with 1 tasks
[INFO][2018-05-24 20:24:30,559][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 7.0 (TID 7, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:24:30,559][org.apache.spark.executor.Executor]Running task 0.0 in stage 7.0 (TID 7)
[INFO][2018-05-24 20:24:30,564][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12131 is the same as ending offset skipping seven 0
[INFO][2018-05-24 20:24:30,565][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:24:30,566][org.apache.spark.executor.Executor]Finished task 0.0 in stage 7.0 (TID 7). 708 bytes result sent to driver
[INFO][2018-05-24 20:24:30,567][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 7.0 (TID 7) in 8 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:24:30,567][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 7.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:24:30,567][org.apache.spark.scheduler.DAGScheduler]ResultStage 7 (foreachPartition at ReceiveKafkaData.scala:65) finished in 0.008 s
[INFO][2018-05-24 20:24:30,568][org.apache.spark.scheduler.DAGScheduler]Job 7 finished: foreachPartition at ReceiveKafkaData.scala:65, took 0.018430 s
[INFO][2018-05-24 20:24:30,568][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527164670000 ms.0 from job set of time 1527164670000 ms
[INFO][2018-05-24 20:24:30,569][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.568 s for time 1527164670000 ms (execution: 0.027 s)
[INFO][2018-05-24 20:24:30,569][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 13 from persistence list
[INFO][2018-05-24 20:24:30,569][org.apache.spark.storage.BlockManager]Removing RDD 13
[INFO][2018-05-24 20:24:30,569][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 12 from persistence list
[INFO][2018-05-24 20:24:30,570][org.apache.spark.storage.BlockManager]Removing RDD 12
[INFO][2018-05-24 20:24:30,570][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:24:30,570][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527164650000 ms
[INFO][2018-05-24 20:24:45,073][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527164680000 ms
[INFO][2018-05-24 20:24:45,073][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527164680000 ms.0 from job set of time 1527164680000 ms
[INFO][2018-05-24 20:24:45,078][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:65
[INFO][2018-05-24 20:24:45,079][org.apache.spark.scheduler.DAGScheduler]Got job 8 (foreachPartition at ReceiveKafkaData.scala:65) with 1 output partitions
[INFO][2018-05-24 20:24:45,079][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 8 (foreachPartition at ReceiveKafkaData.scala:65)
[INFO][2018-05-24 20:24:45,079][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:24:45,079][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:24:45,080][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 8 (MapPartitionsRDD[17] at map at ReceiveKafkaData.scala:63), which has no missing parents
[INFO][2018-05-24 20:24:45,082][org.apache.spark.storage.memory.MemoryStore]Block broadcast_8 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 20:24:45,084][org.apache.spark.storage.memory.MemoryStore]Block broadcast_8_piece0 stored as bytes in memory (estimated size 1878.0 B, free 912.3 MB)
[INFO][2018-05-24 20:24:45,084][org.apache.spark.storage.BlockManagerInfo]Added broadcast_8_piece0 in memory on 192.168.0.102:49677 (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:24:45,085][org.apache.spark.SparkContext]Created broadcast 8 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:24:45,086][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[17] at map at ReceiveKafkaData.scala:63) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:24:45,086][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 8.0 with 1 tasks
[INFO][2018-05-24 20:24:45,087][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 8.0 (TID 8, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:24:45,089][org.apache.spark.executor.Executor]Running task 0.0 in stage 8.0 (TID 8)
[INFO][2018-05-24 20:24:45,091][org.apache.spark.streaming.kafka.KafkaRDD]Computing topic seven, partition 0 offsets 12131 -> 12137
[INFO][2018-05-24 20:24:45,091][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 20:24:45,092][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 20:24:45,092][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 20:24:45,206][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:24:45,208][org.apache.spark.executor.Executor]Finished task 0.0 in stage 8.0 (TID 8). 665 bytes result sent to driver
[INFO][2018-05-24 20:24:45,209][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 8.0 (TID 8) in 122 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:24:45,209][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 8.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:24:45,209][org.apache.spark.scheduler.DAGScheduler]ResultStage 8 (foreachPartition at ReceiveKafkaData.scala:65) finished in 0.123 s
[INFO][2018-05-24 20:24:45,210][org.apache.spark.scheduler.DAGScheduler]Job 8 finished: foreachPartition at ReceiveKafkaData.scala:65, took 0.131346 s
[INFO][2018-05-24 20:24:45,211][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527164680000 ms.0 from job set of time 1527164680000 ms
[INFO][2018-05-24 20:24:45,211][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 5.211 s for time 1527164680000 ms (execution: 0.138 s)
[INFO][2018-05-24 20:24:45,211][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 15 from persistence list
[INFO][2018-05-24 20:24:45,212][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 14 from persistence list
[INFO][2018-05-24 20:24:45,213][org.apache.spark.storage.BlockManager]Removing RDD 15
[INFO][2018-05-24 20:24:45,213][org.apache.spark.storage.BlockManager]Removing RDD 14
[INFO][2018-05-24 20:24:45,214][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:24:45,214][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527164660000 ms
[INFO][2018-05-24 20:24:55,058][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527164690000 ms
[INFO][2018-05-24 20:24:55,058][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527164690000 ms.0 from job set of time 1527164690000 ms
[INFO][2018-05-24 20:24:55,064][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:65
[INFO][2018-05-24 20:24:55,066][org.apache.spark.scheduler.DAGScheduler]Got job 9 (foreachPartition at ReceiveKafkaData.scala:65) with 1 output partitions
[INFO][2018-05-24 20:24:55,066][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 9 (foreachPartition at ReceiveKafkaData.scala:65)
[INFO][2018-05-24 20:24:55,066][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:24:55,066][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:24:55,067][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 9 (MapPartitionsRDD[19] at map at ReceiveKafkaData.scala:63), which has no missing parents
[INFO][2018-05-24 20:24:55,068][org.apache.spark.storage.memory.MemoryStore]Block broadcast_9 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 20:24:55,070][org.apache.spark.storage.memory.MemoryStore]Block broadcast_9_piece0 stored as bytes in memory (estimated size 1878.0 B, free 912.3 MB)
[INFO][2018-05-24 20:24:55,071][org.apache.spark.storage.BlockManagerInfo]Added broadcast_9_piece0 in memory on 192.168.0.102:49677 (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:24:55,071][org.apache.spark.SparkContext]Created broadcast 9 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:24:55,072][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[19] at map at ReceiveKafkaData.scala:63) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:24:55,072][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 9.0 with 1 tasks
[INFO][2018-05-24 20:24:55,073][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 9.0 (TID 9, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:24:55,073][org.apache.spark.executor.Executor]Running task 0.0 in stage 9.0 (TID 9)
[INFO][2018-05-24 20:24:55,076][org.apache.spark.streaming.kafka.KafkaRDD]Computing topic seven, partition 0 offsets 12137 -> 12141
[INFO][2018-05-24 20:24:55,076][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 20:24:55,076][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 20:24:55,076][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 20:25:00,164][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:25:00,165][org.apache.spark.executor.Executor]Finished task 0.0 in stage 9.0 (TID 9). 708 bytes result sent to driver
[INFO][2018-05-24 20:25:00,166][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 9.0 (TID 9) in 5093 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:25:00,166][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 9.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:25:00,167][org.apache.spark.scheduler.DAGScheduler]ResultStage 9 (foreachPartition at ReceiveKafkaData.scala:65) finished in 5.095 s
[INFO][2018-05-24 20:25:00,168][org.apache.spark.scheduler.DAGScheduler]Job 9 finished: foreachPartition at ReceiveKafkaData.scala:65, took 5.103660 s
[INFO][2018-05-24 20:25:00,168][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527164690000 ms.0 from job set of time 1527164690000 ms
[INFO][2018-05-24 20:25:00,169][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 10.168 s for time 1527164690000 ms (execution: 5.110 s)
[INFO][2018-05-24 20:25:10,076][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527164700000 ms
[INFO][2018-05-24 20:25:10,076][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 17 from persistence list
[INFO][2018-05-24 20:25:10,077][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527164700000 ms.0 from job set of time 1527164700000 ms
[INFO][2018-05-24 20:25:10,077][org.apache.spark.storage.BlockManager]Removing RDD 17
[INFO][2018-05-24 20:25:10,077][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 16 from persistence list
[INFO][2018-05-24 20:25:10,078][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:25:10,079][org.apache.spark.storage.BlockManager]Removing RDD 16
[INFO][2018-05-24 20:25:10,079][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527164670000 ms
[INFO][2018-05-24 20:25:10,086][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:65
[INFO][2018-05-24 20:25:10,087][org.apache.spark.scheduler.DAGScheduler]Got job 10 (foreachPartition at ReceiveKafkaData.scala:65) with 1 output partitions
[INFO][2018-05-24 20:25:10,087][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 10 (foreachPartition at ReceiveKafkaData.scala:65)
[INFO][2018-05-24 20:25:10,087][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:25:10,087][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:25:10,088][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 10 (MapPartitionsRDD[21] at map at ReceiveKafkaData.scala:63), which has no missing parents
[INFO][2018-05-24 20:25:10,090][org.apache.spark.storage.memory.MemoryStore]Block broadcast_10 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 20:25:10,097][org.apache.spark.storage.memory.MemoryStore]Block broadcast_10_piece0 stored as bytes in memory (estimated size 1878.0 B, free 912.3 MB)
[INFO][2018-05-24 20:25:10,097][org.apache.spark.storage.BlockManagerInfo]Added broadcast_10_piece0 in memory on 192.168.0.102:49677 (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:25:10,099][org.apache.spark.SparkContext]Created broadcast 10 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:25:10,099][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[21] at map at ReceiveKafkaData.scala:63) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:25:10,100][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 10.0 with 1 tasks
[INFO][2018-05-24 20:25:10,100][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 10.0 (TID 10, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:25:10,101][org.apache.spark.executor.Executor]Running task 0.0 in stage 10.0 (TID 10)
[INFO][2018-05-24 20:25:10,102][org.apache.spark.streaming.kafka.KafkaRDD]Computing topic seven, partition 0 offsets 12141 -> 12147
[INFO][2018-05-24 20:25:10,103][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 20:25:10,103][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 20:25:10,104][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 20:25:10,133][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527164710000 ms
[INFO][2018-05-24 20:25:10,168][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:25:10,169][org.apache.spark.executor.Executor]Finished task 0.0 in stage 10.0 (TID 10). 665 bytes result sent to driver
[INFO][2018-05-24 20:25:10,169][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 10.0 (TID 10) in 69 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:25:10,170][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 10.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:25:10,170][org.apache.spark.scheduler.DAGScheduler]ResultStage 10 (foreachPartition at ReceiveKafkaData.scala:65) finished in 0.070 s
[INFO][2018-05-24 20:25:10,170][org.apache.spark.scheduler.DAGScheduler]Job 10 finished: foreachPartition at ReceiveKafkaData.scala:65, took 0.084291 s
[INFO][2018-05-24 20:25:10,171][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527164700000 ms.0 from job set of time 1527164700000 ms
[INFO][2018-05-24 20:25:10,171][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 10.171 s for time 1527164700000 ms (execution: 0.095 s)
[INFO][2018-05-24 20:25:10,171][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527164710000 ms.0 from job set of time 1527164710000 ms
[INFO][2018-05-24 20:25:10,172][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 19 from persistence list
[INFO][2018-05-24 20:25:10,172][org.apache.spark.storage.BlockManager]Removing RDD 19
[INFO][2018-05-24 20:25:10,173][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 18 from persistence list
[INFO][2018-05-24 20:25:10,173][org.apache.spark.storage.BlockManager]Removing RDD 18
[INFO][2018-05-24 20:25:10,174][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:25:10,174][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527164680000 ms
[INFO][2018-05-24 20:25:10,180][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:65
[INFO][2018-05-24 20:25:10,181][org.apache.spark.scheduler.DAGScheduler]Got job 11 (foreachPartition at ReceiveKafkaData.scala:65) with 1 output partitions
[INFO][2018-05-24 20:25:10,181][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 11 (foreachPartition at ReceiveKafkaData.scala:65)
[INFO][2018-05-24 20:25:10,182][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:25:10,182][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:25:10,183][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 11 (MapPartitionsRDD[23] at map at ReceiveKafkaData.scala:63), which has no missing parents
[INFO][2018-05-24 20:25:10,185][org.apache.spark.storage.memory.MemoryStore]Block broadcast_11 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 20:25:10,193][org.apache.spark.storage.memory.MemoryStore]Block broadcast_11_piece0 stored as bytes in memory (estimated size 1878.0 B, free 912.3 MB)
[INFO][2018-05-24 20:25:10,194][org.apache.spark.storage.BlockManagerInfo]Added broadcast_11_piece0 in memory on 192.168.0.102:49677 (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:25:10,195][org.apache.spark.SparkContext]Created broadcast 11 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:25:10,196][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[23] at map at ReceiveKafkaData.scala:63) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:25:10,196][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 11.0 with 1 tasks
[INFO][2018-05-24 20:25:10,197][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 11.0 (TID 11, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:25:10,197][org.apache.spark.executor.Executor]Running task 0.0 in stage 11.0 (TID 11)
[INFO][2018-05-24 20:25:10,199][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12147 is the same as ending offset skipping seven 0
[INFO][2018-05-24 20:25:10,199][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:25:10,200][org.apache.spark.executor.Executor]Finished task 0.0 in stage 11.0 (TID 11). 665 bytes result sent to driver
[INFO][2018-05-24 20:25:10,200][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 11.0 (TID 11) in 3 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:25:10,200][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 11.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:25:10,201][org.apache.spark.scheduler.DAGScheduler]ResultStage 11 (foreachPartition at ReceiveKafkaData.scala:65) finished in 0.005 s
[INFO][2018-05-24 20:25:10,201][org.apache.spark.scheduler.DAGScheduler]Job 11 finished: foreachPartition at ReceiveKafkaData.scala:65, took 0.020663 s
[INFO][2018-05-24 20:25:10,202][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527164710000 ms.0 from job set of time 1527164710000 ms
[INFO][2018-05-24 20:25:10,202][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 21 from persistence list
[INFO][2018-05-24 20:25:10,202][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.202 s for time 1527164710000 ms (execution: 0.031 s)
[INFO][2018-05-24 20:25:10,203][org.apache.spark.storage.BlockManager]Removing RDD 21
[INFO][2018-05-24 20:25:10,203][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 20 from persistence list
[INFO][2018-05-24 20:25:10,203][org.apache.spark.storage.BlockManager]Removing RDD 20
[INFO][2018-05-24 20:25:10,203][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:25:10,203][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527164690000 ms
[INFO][2018-05-24 20:25:25,065][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527164720000 ms
[INFO][2018-05-24 20:25:25,066][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527164720000 ms.0 from job set of time 1527164720000 ms
[INFO][2018-05-24 20:25:25,074][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:65
[INFO][2018-05-24 20:25:25,075][org.apache.spark.scheduler.DAGScheduler]Got job 12 (foreachPartition at ReceiveKafkaData.scala:65) with 1 output partitions
[INFO][2018-05-24 20:25:25,075][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 12 (foreachPartition at ReceiveKafkaData.scala:65)
[INFO][2018-05-24 20:25:25,075][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:25:25,075][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:25:25,076][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 12 (MapPartitionsRDD[25] at map at ReceiveKafkaData.scala:63), which has no missing parents
[INFO][2018-05-24 20:25:25,078][org.apache.spark.storage.memory.MemoryStore]Block broadcast_12 stored as values in memory (estimated size 3.3 KB, free 912.2 MB)
[INFO][2018-05-24 20:25:25,084][org.apache.spark.storage.memory.MemoryStore]Block broadcast_12_piece0 stored as bytes in memory (estimated size 1878.0 B, free 912.2 MB)
[INFO][2018-05-24 20:25:25,084][org.apache.spark.storage.BlockManagerInfo]Added broadcast_12_piece0 in memory on 192.168.0.102:49677 (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:25:25,085][org.apache.spark.SparkContext]Created broadcast 12 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:25:25,085][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[25] at map at ReceiveKafkaData.scala:63) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:25:25,085][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 12.0 with 1 tasks
[INFO][2018-05-24 20:25:25,086][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 12.0 (TID 12, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:25:25,087][org.apache.spark.executor.Executor]Running task 0.0 in stage 12.0 (TID 12)
[INFO][2018-05-24 20:25:25,088][org.apache.spark.streaming.kafka.KafkaRDD]Computing topic seven, partition 0 offsets 12147 -> 12153
[INFO][2018-05-24 20:25:25,088][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 20:25:25,089][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 20:25:25,089][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 20:25:25,153][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:25:25,153][org.apache.spark.executor.Executor]Finished task 0.0 in stage 12.0 (TID 12). 708 bytes result sent to driver
[INFO][2018-05-24 20:25:25,154][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 12.0 (TID 12) in 68 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:25:25,154][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 12.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:25:25,155][org.apache.spark.scheduler.DAGScheduler]ResultStage 12 (foreachPartition at ReceiveKafkaData.scala:65) finished in 0.069 s
[INFO][2018-05-24 20:25:25,156][org.apache.spark.scheduler.DAGScheduler]Job 12 finished: foreachPartition at ReceiveKafkaData.scala:65, took 0.081653 s
[INFO][2018-05-24 20:25:25,156][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527164720000 ms.0 from job set of time 1527164720000 ms
[INFO][2018-05-24 20:25:25,157][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 23 from persistence list
[INFO][2018-05-24 20:25:25,157][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 5.156 s for time 1527164720000 ms (execution: 0.090 s)
[INFO][2018-05-24 20:25:25,157][org.apache.spark.storage.BlockManager]Removing RDD 23
[INFO][2018-05-24 20:25:25,157][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 22 from persistence list
[INFO][2018-05-24 20:25:25,158][org.apache.spark.storage.BlockManager]Removing RDD 22
[INFO][2018-05-24 20:25:25,158][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:25:25,158][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527164700000 ms
[INFO][2018-05-24 20:25:35,062][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527164730000 ms
[INFO][2018-05-24 20:25:35,062][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527164730000 ms.0 from job set of time 1527164730000 ms
[INFO][2018-05-24 20:25:35,070][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:65
[INFO][2018-05-24 20:25:35,071][org.apache.spark.scheduler.DAGScheduler]Got job 13 (foreachPartition at ReceiveKafkaData.scala:65) with 1 output partitions
[INFO][2018-05-24 20:25:35,071][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 13 (foreachPartition at ReceiveKafkaData.scala:65)
[INFO][2018-05-24 20:25:35,071][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:25:35,071][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:25:35,072][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 13 (MapPartitionsRDD[27] at map at ReceiveKafkaData.scala:63), which has no missing parents
[INFO][2018-05-24 20:25:35,074][org.apache.spark.storage.memory.MemoryStore]Block broadcast_13 stored as values in memory (estimated size 3.3 KB, free 912.2 MB)
[INFO][2018-05-24 20:25:35,077][org.apache.spark.storage.memory.MemoryStore]Block broadcast_13_piece0 stored as bytes in memory (estimated size 1876.0 B, free 912.2 MB)
[INFO][2018-05-24 20:25:35,077][org.apache.spark.storage.BlockManagerInfo]Added broadcast_13_piece0 in memory on 192.168.0.102:49677 (size: 1876.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:25:35,078][org.apache.spark.SparkContext]Created broadcast 13 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:25:35,078][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[27] at map at ReceiveKafkaData.scala:63) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:25:35,078][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 13.0 with 1 tasks
[INFO][2018-05-24 20:25:35,079][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 13.0 (TID 13, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:25:35,079][org.apache.spark.executor.Executor]Running task 0.0 in stage 13.0 (TID 13)
[INFO][2018-05-24 20:25:35,080][org.apache.spark.streaming.kafka.KafkaRDD]Computing topic seven, partition 0 offsets 12153 -> 12157
[INFO][2018-05-24 20:25:35,081][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 20:25:35,081][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 20:25:35,081][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 20:25:40,157][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:25:40,159][org.apache.spark.executor.Executor]Finished task 0.0 in stage 13.0 (TID 13). 708 bytes result sent to driver
[INFO][2018-05-24 20:25:40,160][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 13.0 (TID 13) in 5081 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:25:40,160][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 13.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:25:40,161][org.apache.spark.scheduler.DAGScheduler]ResultStage 13 (foreachPartition at ReceiveKafkaData.scala:65) finished in 5.082 s
[INFO][2018-05-24 20:25:40,161][org.apache.spark.scheduler.DAGScheduler]Job 13 finished: foreachPartition at ReceiveKafkaData.scala:65, took 5.090816 s
[INFO][2018-05-24 20:25:40,162][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527164730000 ms.0 from job set of time 1527164730000 ms
[INFO][2018-05-24 20:25:40,162][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 10.162 s for time 1527164730000 ms (execution: 5.100 s)
[INFO][2018-05-24 20:25:45,100][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527164740000 ms
[INFO][2018-05-24 20:25:45,100][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 25 from persistence list
[INFO][2018-05-24 20:25:45,100][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527164740000 ms.0 from job set of time 1527164740000 ms
[INFO][2018-05-24 20:25:45,101][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 24 from persistence list
[INFO][2018-05-24 20:25:45,101][org.apache.spark.storage.BlockManager]Removing RDD 25
[INFO][2018-05-24 20:25:45,101][org.apache.spark.storage.BlockManager]Removing RDD 24
[INFO][2018-05-24 20:25:45,101][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:25:45,102][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527164710000 ms
[INFO][2018-05-24 20:25:45,105][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:65
[INFO][2018-05-24 20:25:45,106][org.apache.spark.scheduler.DAGScheduler]Got job 14 (foreachPartition at ReceiveKafkaData.scala:65) with 1 output partitions
[INFO][2018-05-24 20:25:45,106][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 14 (foreachPartition at ReceiveKafkaData.scala:65)
[INFO][2018-05-24 20:25:45,106][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:25:45,106][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:25:45,106][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 14 (MapPartitionsRDD[29] at map at ReceiveKafkaData.scala:63), which has no missing parents
[INFO][2018-05-24 20:25:45,108][org.apache.spark.storage.memory.MemoryStore]Block broadcast_14 stored as values in memory (estimated size 3.3 KB, free 912.2 MB)
[INFO][2018-05-24 20:25:45,111][org.apache.spark.storage.memory.MemoryStore]Block broadcast_14_piece0 stored as bytes in memory (estimated size 1878.0 B, free 912.2 MB)
[INFO][2018-05-24 20:25:45,112][org.apache.spark.storage.BlockManagerInfo]Added broadcast_14_piece0 in memory on 192.168.0.102:49677 (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:25:45,112][org.apache.spark.SparkContext]Created broadcast 14 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:25:45,113][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[29] at map at ReceiveKafkaData.scala:63) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:25:45,113][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 14.0 with 1 tasks
[INFO][2018-05-24 20:25:45,113][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 14.0 (TID 14, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:25:45,113][org.apache.spark.executor.Executor]Running task 0.0 in stage 14.0 (TID 14)
[INFO][2018-05-24 20:25:45,114][org.apache.spark.streaming.kafka.KafkaRDD]Computing topic seven, partition 0 offsets 12157 -> 12161
[INFO][2018-05-24 20:25:45,114][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 20:25:45,115][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 20:25:45,115][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 20:25:45,181][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:25:45,183][org.apache.spark.executor.Executor]Finished task 0.0 in stage 14.0 (TID 14). 708 bytes result sent to driver
[INFO][2018-05-24 20:25:45,184][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 14.0 (TID 14) in 71 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:25:45,184][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 14.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:25:45,184][org.apache.spark.scheduler.DAGScheduler]ResultStage 14 (foreachPartition at ReceiveKafkaData.scala:65) finished in 0.071 s
[INFO][2018-05-24 20:25:45,185][org.apache.spark.scheduler.DAGScheduler]Job 14 finished: foreachPartition at ReceiveKafkaData.scala:65, took 0.079871 s
[INFO][2018-05-24 20:25:45,185][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527164740000 ms.0 from job set of time 1527164740000 ms
[INFO][2018-05-24 20:25:45,186][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 5.185 s for time 1527164740000 ms (execution: 0.085 s)
[INFO][2018-05-24 20:25:45,186][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 27 from persistence list
[INFO][2018-05-24 20:25:45,186][org.apache.spark.storage.BlockManager]Removing RDD 27
[INFO][2018-05-24 20:25:45,187][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 26 from persistence list
[INFO][2018-05-24 20:25:45,188][org.apache.spark.storage.BlockManager]Removing RDD 26
[INFO][2018-05-24 20:25:45,188][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:25:45,188][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527164720000 ms
[INFO][2018-05-24 20:25:50,069][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527164750000 ms
[INFO][2018-05-24 20:25:50,070][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527164750000 ms.0 from job set of time 1527164750000 ms
[INFO][2018-05-24 20:25:50,082][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:65
[INFO][2018-05-24 20:25:50,084][org.apache.spark.scheduler.DAGScheduler]Got job 15 (foreachPartition at ReceiveKafkaData.scala:65) with 1 output partitions
[INFO][2018-05-24 20:25:50,084][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 15 (foreachPartition at ReceiveKafkaData.scala:65)
[INFO][2018-05-24 20:25:50,084][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:25:50,084][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:25:50,085][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 15 (MapPartitionsRDD[31] at map at ReceiveKafkaData.scala:63), which has no missing parents
[INFO][2018-05-24 20:25:50,086][org.apache.spark.storage.memory.MemoryStore]Block broadcast_15 stored as values in memory (estimated size 3.3 KB, free 912.2 MB)
[INFO][2018-05-24 20:25:50,093][org.apache.spark.storage.memory.MemoryStore]Block broadcast_15_piece0 stored as bytes in memory (estimated size 1878.0 B, free 912.2 MB)
[INFO][2018-05-24 20:25:50,093][org.apache.spark.storage.BlockManagerInfo]Added broadcast_15_piece0 in memory on 192.168.0.102:49677 (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:25:50,093][org.apache.spark.SparkContext]Created broadcast 15 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:25:50,094][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 15 (MapPartitionsRDD[31] at map at ReceiveKafkaData.scala:63) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:25:50,095][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 15.0 with 1 tasks
[INFO][2018-05-24 20:25:50,095][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 15.0 (TID 15, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:25:50,096][org.apache.spark.executor.Executor]Running task 0.0 in stage 15.0 (TID 15)
[INFO][2018-05-24 20:25:50,098][org.apache.spark.streaming.kafka.KafkaRDD]Computing topic seven, partition 0 offsets 12161 -> 12163
[INFO][2018-05-24 20:25:50,098][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 20:25:50,098][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 20:25:50,099][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 20:25:50,161][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:25:50,162][org.apache.spark.executor.Executor]Finished task 0.0 in stage 15.0 (TID 15). 708 bytes result sent to driver
[INFO][2018-05-24 20:25:50,164][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 15.0 (TID 15) in 69 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:25:50,164][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 15.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:25:50,165][org.apache.spark.scheduler.DAGScheduler]ResultStage 15 (foreachPartition at ReceiveKafkaData.scala:65) finished in 0.069 s
[INFO][2018-05-24 20:25:50,166][org.apache.spark.scheduler.DAGScheduler]Job 15 finished: foreachPartition at ReceiveKafkaData.scala:65, took 0.082892 s
[INFO][2018-05-24 20:25:50,166][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527164750000 ms.0 from job set of time 1527164750000 ms
[INFO][2018-05-24 20:25:50,167][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 29 from persistence list
[INFO][2018-05-24 20:25:50,167][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.166 s for time 1527164750000 ms (execution: 0.096 s)
[INFO][2018-05-24 20:25:50,167][org.apache.spark.storage.BlockManager]Removing RDD 29
[INFO][2018-05-24 20:25:50,167][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 28 from persistence list
[INFO][2018-05-24 20:25:50,168][org.apache.spark.storage.BlockManager]Removing RDD 28
[INFO][2018-05-24 20:25:50,168][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:25:50,169][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527164730000 ms
[INFO][2018-05-24 20:26:00,339][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527164760000 ms
[INFO][2018-05-24 20:26:00,339][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527164760000 ms.0 from job set of time 1527164760000 ms
[INFO][2018-05-24 20:26:00,346][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:65
[INFO][2018-05-24 20:26:00,347][org.apache.spark.scheduler.DAGScheduler]Got job 16 (foreachPartition at ReceiveKafkaData.scala:65) with 1 output partitions
[INFO][2018-05-24 20:26:00,347][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 16 (foreachPartition at ReceiveKafkaData.scala:65)
[INFO][2018-05-24 20:26:00,347][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:26:00,348][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:26:00,348][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 16 (MapPartitionsRDD[33] at map at ReceiveKafkaData.scala:63), which has no missing parents
[INFO][2018-05-24 20:26:00,350][org.apache.spark.storage.memory.MemoryStore]Block broadcast_16 stored as values in memory (estimated size 3.3 KB, free 912.2 MB)
[INFO][2018-05-24 20:26:00,357][org.apache.spark.storage.memory.MemoryStore]Block broadcast_16_piece0 stored as bytes in memory (estimated size 1878.0 B, free 912.2 MB)
[INFO][2018-05-24 20:26:00,358][org.apache.spark.storage.BlockManagerInfo]Added broadcast_16_piece0 in memory on 192.168.0.102:49677 (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:26:00,358][org.apache.spark.SparkContext]Created broadcast 16 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:26:00,359][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 16 (MapPartitionsRDD[33] at map at ReceiveKafkaData.scala:63) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:26:00,359][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 16.0 with 1 tasks
[INFO][2018-05-24 20:26:00,360][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 16.0 (TID 16, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:26:00,360][org.apache.spark.executor.Executor]Running task 0.0 in stage 16.0 (TID 16)
[INFO][2018-05-24 20:26:00,362][org.apache.spark.streaming.kafka.KafkaRDD]Computing topic seven, partition 0 offsets 12163 -> 12168
[INFO][2018-05-24 20:26:00,362][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 20:26:00,362][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 20:26:00,363][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 20:26:00,433][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:26:00,433][org.apache.spark.executor.Executor]Finished task 0.0 in stage 16.0 (TID 16). 708 bytes result sent to driver
[INFO][2018-05-24 20:26:00,434][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 16.0 (TID 16) in 74 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:26:00,434][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 16.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:26:00,434][org.apache.spark.scheduler.DAGScheduler]ResultStage 16 (foreachPartition at ReceiveKafkaData.scala:65) finished in 0.075 s
[INFO][2018-05-24 20:26:00,435][org.apache.spark.scheduler.DAGScheduler]Job 16 finished: foreachPartition at ReceiveKafkaData.scala:65, took 0.088039 s
[INFO][2018-05-24 20:26:00,435][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527164760000 ms.0 from job set of time 1527164760000 ms
[INFO][2018-05-24 20:26:00,435][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 31 from persistence list
[INFO][2018-05-24 20:26:00,436][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.435 s for time 1527164760000 ms (execution: 0.096 s)
[INFO][2018-05-24 20:26:00,436][org.apache.spark.storage.BlockManager]Removing RDD 31
[INFO][2018-05-24 20:26:00,436][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 30 from persistence list
[INFO][2018-05-24 20:26:00,437][org.apache.spark.storage.BlockManager]Removing RDD 30
[INFO][2018-05-24 20:26:00,437][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:26:00,437][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527164740000 ms
[INFO][2018-05-24 20:26:10,065][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527164770000 ms
[INFO][2018-05-24 20:26:10,067][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527164770000 ms.0 from job set of time 1527164770000 ms
[INFO][2018-05-24 20:26:10,079][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:65
[INFO][2018-05-24 20:26:10,079][org.apache.spark.scheduler.DAGScheduler]Got job 17 (foreachPartition at ReceiveKafkaData.scala:65) with 1 output partitions
[INFO][2018-05-24 20:26:10,079][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 17 (foreachPartition at ReceiveKafkaData.scala:65)
[INFO][2018-05-24 20:26:10,079][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:26:10,080][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:26:10,080][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 17 (MapPartitionsRDD[35] at map at ReceiveKafkaData.scala:63), which has no missing parents
[INFO][2018-05-24 20:26:10,081][org.apache.spark.storage.memory.MemoryStore]Block broadcast_17 stored as values in memory (estimated size 3.3 KB, free 912.2 MB)
[INFO][2018-05-24 20:26:10,090][org.apache.spark.storage.memory.MemoryStore]Block broadcast_17_piece0 stored as bytes in memory (estimated size 1878.0 B, free 912.2 MB)
[INFO][2018-05-24 20:26:10,091][org.apache.spark.storage.BlockManagerInfo]Added broadcast_17_piece0 in memory on 192.168.0.102:49677 (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:26:10,092][org.apache.spark.SparkContext]Created broadcast 17 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:26:10,093][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[35] at map at ReceiveKafkaData.scala:63) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:26:10,093][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 17.0 with 1 tasks
[INFO][2018-05-24 20:26:10,093][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 17.0 (TID 17, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:26:10,094][org.apache.spark.executor.Executor]Running task 0.0 in stage 17.0 (TID 17)
[INFO][2018-05-24 20:26:10,095][org.apache.spark.streaming.kafka.KafkaRDD]Computing topic seven, partition 0 offsets 12168 -> 12171
[INFO][2018-05-24 20:26:10,095][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 20:26:10,096][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 20:26:10,096][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 20:26:15,171][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:26:15,172][org.apache.spark.executor.Executor]Finished task 0.0 in stage 17.0 (TID 17). 665 bytes result sent to driver
[INFO][2018-05-24 20:26:15,172][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 17.0 (TID 17) in 5079 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:26:15,172][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 17.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:26:15,173][org.apache.spark.scheduler.DAGScheduler]ResultStage 17 (foreachPartition at ReceiveKafkaData.scala:65) finished in 5.080 s
[INFO][2018-05-24 20:26:15,173][org.apache.spark.scheduler.DAGScheduler]Job 17 finished: foreachPartition at ReceiveKafkaData.scala:65, took 5.094222 s
[INFO][2018-05-24 20:26:15,174][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527164770000 ms.0 from job set of time 1527164770000 ms
[INFO][2018-05-24 20:26:15,174][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 33 from persistence list
[INFO][2018-05-24 20:26:15,174][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 5.174 s for time 1527164770000 ms (execution: 5.108 s)
[INFO][2018-05-24 20:26:15,174][org.apache.spark.storage.BlockManager]Removing RDD 33
[INFO][2018-05-24 20:26:15,175][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 32 from persistence list
[INFO][2018-05-24 20:26:15,175][org.apache.spark.storage.BlockManager]Removing RDD 32
[INFO][2018-05-24 20:26:15,175][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:26:15,175][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527164750000 ms
[INFO][2018-05-24 20:26:17,815][org.apache.spark.util.ShutdownHookManager]Shutdown hook called
[INFO][2018-05-24 20:26:17,816][org.apache.spark.util.ShutdownHookManager]Deleting directory /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/spark-56730831-b55e-48e3-b817-1960173255d3
[INFO][2018-05-24 20:26:25,081][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527164780000 ms
[INFO][2018-05-24 20:26:25,082][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527164780000 ms.0 from job set of time 1527164780000 ms
[INFO][2018-05-24 20:26:25,088][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:65
[INFO][2018-05-24 20:26:25,089][org.apache.spark.scheduler.DAGScheduler]Got job 18 (foreachPartition at ReceiveKafkaData.scala:65) with 1 output partitions
[INFO][2018-05-24 20:26:25,089][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 18 (foreachPartition at ReceiveKafkaData.scala:65)
[INFO][2018-05-24 20:26:25,089][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:26:25,089][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:26:25,089][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 18 (MapPartitionsRDD[37] at map at ReceiveKafkaData.scala:63), which has no missing parents
[INFO][2018-05-24 20:26:25,091][org.apache.spark.storage.memory.MemoryStore]Block broadcast_18 stored as values in memory (estimated size 3.3 KB, free 912.2 MB)
[INFO][2018-05-24 20:26:25,097][org.apache.spark.storage.memory.MemoryStore]Block broadcast_18_piece0 stored as bytes in memory (estimated size 1878.0 B, free 912.2 MB)
[INFO][2018-05-24 20:26:25,098][org.apache.spark.storage.BlockManagerInfo]Added broadcast_18_piece0 in memory on 192.168.0.102:49677 (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:26:25,099][org.apache.spark.SparkContext]Created broadcast 18 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:26:25,099][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 18 (MapPartitionsRDD[37] at map at ReceiveKafkaData.scala:63) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:26:25,099][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 18.0 with 1 tasks
[INFO][2018-05-24 20:26:25,100][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 18.0 (TID 18, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:26:25,102][org.apache.spark.executor.Executor]Running task 0.0 in stage 18.0 (TID 18)
[INFO][2018-05-24 20:26:25,105][org.apache.spark.streaming.kafka.KafkaRDD]Computing topic seven, partition 0 offsets 12171 -> 12174
[INFO][2018-05-24 20:26:25,106][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 20:26:25,106][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 20:26:25,107][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 20:26:25,178][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:26:25,180][org.apache.spark.executor.Executor]Finished task 0.0 in stage 18.0 (TID 18). 708 bytes result sent to driver
[INFO][2018-05-24 20:26:25,182][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 18.0 (TID 18) in 82 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:26:25,182][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 18.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:26:25,184][org.apache.spark.scheduler.DAGScheduler]ResultStage 18 (foreachPartition at ReceiveKafkaData.scala:65) finished in 0.083 s
[INFO][2018-05-24 20:26:25,185][org.apache.spark.scheduler.DAGScheduler]Job 18 finished: foreachPartition at ReceiveKafkaData.scala:65, took 0.096651 s
[INFO][2018-05-24 20:26:25,187][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527164780000 ms.0 from job set of time 1527164780000 ms
[INFO][2018-05-24 20:26:25,188][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 5.187 s for time 1527164780000 ms (execution: 0.106 s)
[INFO][2018-05-24 20:26:25,188][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 35 from persistence list
[INFO][2018-05-24 20:26:25,189][org.apache.spark.storage.BlockManager]Removing RDD 35
[INFO][2018-05-24 20:26:25,189][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 34 from persistence list
[INFO][2018-05-24 20:26:25,189][org.apache.spark.storage.BlockManager]Removing RDD 34
[INFO][2018-05-24 20:26:25,190][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:26:25,191][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527164760000 ms
[INFO][2018-05-24 20:26:35,061][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527164790000 ms
[INFO][2018-05-24 20:26:35,062][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527164790000 ms.0 from job set of time 1527164790000 ms
[INFO][2018-05-24 20:26:35,068][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:65
[INFO][2018-05-24 20:26:35,069][org.apache.spark.scheduler.DAGScheduler]Got job 19 (foreachPartition at ReceiveKafkaData.scala:65) with 1 output partitions
[INFO][2018-05-24 20:26:35,069][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 19 (foreachPartition at ReceiveKafkaData.scala:65)
[INFO][2018-05-24 20:26:35,069][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:26:35,069][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:26:35,070][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 19 (MapPartitionsRDD[39] at map at ReceiveKafkaData.scala:63), which has no missing parents
[INFO][2018-05-24 20:26:35,071][org.apache.spark.storage.memory.MemoryStore]Block broadcast_19 stored as values in memory (estimated size 3.3 KB, free 912.2 MB)
[INFO][2018-05-24 20:26:35,078][org.apache.spark.storage.memory.MemoryStore]Block broadcast_19_piece0 stored as bytes in memory (estimated size 1878.0 B, free 912.2 MB)
[INFO][2018-05-24 20:26:35,078][org.apache.spark.storage.BlockManagerInfo]Added broadcast_19_piece0 in memory on 192.168.0.102:49677 (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:26:35,079][org.apache.spark.SparkContext]Created broadcast 19 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:26:35,080][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 19 (MapPartitionsRDD[39] at map at ReceiveKafkaData.scala:63) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:26:35,080][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 19.0 with 1 tasks
[INFO][2018-05-24 20:26:35,080][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 19.0 (TID 19, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:26:35,080][org.apache.spark.executor.Executor]Running task 0.0 in stage 19.0 (TID 19)
[INFO][2018-05-24 20:26:35,082][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12174 is the same as ending offset skipping seven 0
[INFO][2018-05-24 20:26:35,082][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:26:35,082][org.apache.spark.executor.Executor]Finished task 0.0 in stage 19.0 (TID 19). 665 bytes result sent to driver
[INFO][2018-05-24 20:26:35,083][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 19.0 (TID 19) in 3 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:26:35,083][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 19.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:26:35,084][org.apache.spark.scheduler.DAGScheduler]ResultStage 19 (foreachPartition at ReceiveKafkaData.scala:65) finished in 0.003 s
[INFO][2018-05-24 20:26:35,084][org.apache.spark.scheduler.DAGScheduler]Job 19 finished: foreachPartition at ReceiveKafkaData.scala:65, took 0.015479 s
[INFO][2018-05-24 20:26:35,084][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527164790000 ms.0 from job set of time 1527164790000 ms
[INFO][2018-05-24 20:26:35,084][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 5.084 s for time 1527164790000 ms (execution: 0.022 s)
[INFO][2018-05-24 20:26:35,084][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 37 from persistence list
[INFO][2018-05-24 20:26:35,085][org.apache.spark.storage.BlockManager]Removing RDD 37
[INFO][2018-05-24 20:26:35,085][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 36 from persistence list
[INFO][2018-05-24 20:26:35,085][org.apache.spark.storage.BlockManager]Removing RDD 36
[INFO][2018-05-24 20:26:35,085][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:26:35,085][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527164770000 ms
[INFO][2018-05-24 20:26:45,062][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527164800000 ms
[INFO][2018-05-24 20:26:45,063][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527164800000 ms.0 from job set of time 1527164800000 ms
[INFO][2018-05-24 20:26:45,070][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:65
[INFO][2018-05-24 20:26:45,070][org.apache.spark.scheduler.DAGScheduler]Got job 20 (foreachPartition at ReceiveKafkaData.scala:65) with 1 output partitions
[INFO][2018-05-24 20:26:45,071][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 20 (foreachPartition at ReceiveKafkaData.scala:65)
[INFO][2018-05-24 20:26:45,071][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:26:45,071][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:26:45,071][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 20 (MapPartitionsRDD[41] at map at ReceiveKafkaData.scala:63), which has no missing parents
[INFO][2018-05-24 20:26:45,073][org.apache.spark.storage.memory.MemoryStore]Block broadcast_20 stored as values in memory (estimated size 3.3 KB, free 912.2 MB)
[INFO][2018-05-24 20:26:45,077][org.apache.spark.storage.memory.MemoryStore]Block broadcast_20_piece0 stored as bytes in memory (estimated size 1878.0 B, free 912.2 MB)
[INFO][2018-05-24 20:26:45,078][org.apache.spark.storage.BlockManagerInfo]Added broadcast_20_piece0 in memory on 192.168.0.102:49677 (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:26:45,078][org.apache.spark.SparkContext]Created broadcast 20 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:26:45,078][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 20 (MapPartitionsRDD[41] at map at ReceiveKafkaData.scala:63) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:26:45,078][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 20.0 with 1 tasks
[INFO][2018-05-24 20:26:45,079][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 20.0 (TID 20, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:26:45,079][org.apache.spark.executor.Executor]Running task 0.0 in stage 20.0 (TID 20)
[INFO][2018-05-24 20:26:45,080][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12174 is the same as ending offset skipping seven 0
[INFO][2018-05-24 20:26:45,081][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:26:45,081][org.apache.spark.executor.Executor]Finished task 0.0 in stage 20.0 (TID 20). 708 bytes result sent to driver
[INFO][2018-05-24 20:26:45,081][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 20.0 (TID 20) in 2 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:26:45,081][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 20.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:26:45,082][org.apache.spark.scheduler.DAGScheduler]ResultStage 20 (foreachPartition at ReceiveKafkaData.scala:65) finished in 0.003 s
[INFO][2018-05-24 20:26:45,082][org.apache.spark.scheduler.DAGScheduler]Job 20 finished: foreachPartition at ReceiveKafkaData.scala:65, took 0.012678 s
[INFO][2018-05-24 20:26:45,083][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527164800000 ms.0 from job set of time 1527164800000 ms
[INFO][2018-05-24 20:26:45,083][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 5.083 s for time 1527164800000 ms (execution: 0.020 s)
[INFO][2018-05-24 20:26:45,083][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 39 from persistence list
[INFO][2018-05-24 20:26:45,083][org.apache.spark.storage.BlockManager]Removing RDD 39
[INFO][2018-05-24 20:26:45,084][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 38 from persistence list
[INFO][2018-05-24 20:26:45,084][org.apache.spark.storage.BlockManager]Removing RDD 38
[INFO][2018-05-24 20:26:45,084][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:26:45,084][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527164780000 ms
[INFO][2018-05-24 20:27:00,111][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527164810000 ms
[INFO][2018-05-24 20:27:00,112][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527164810000 ms.0 from job set of time 1527164810000 ms
[INFO][2018-05-24 20:27:00,129][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_12_piece0 on 192.168.0.102:49677 in memory (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:27:00,129][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:65
[INFO][2018-05-24 20:27:00,130][org.apache.spark.scheduler.DAGScheduler]Got job 21 (foreachPartition at ReceiveKafkaData.scala:65) with 1 output partitions
[INFO][2018-05-24 20:27:00,130][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_17_piece0 on 192.168.0.102:49677 in memory (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:27:00,130][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 21 (foreachPartition at ReceiveKafkaData.scala:65)
[INFO][2018-05-24 20:27:00,130][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:27:00,130][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:27:00,130][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 21 (MapPartitionsRDD[43] at map at ReceiveKafkaData.scala:63), which has no missing parents
[INFO][2018-05-24 20:27:00,131][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_20_piece0 on 192.168.0.102:49677 in memory (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:27:00,131][org.apache.spark.storage.memory.MemoryStore]Block broadcast_21 stored as values in memory (estimated size 3.3 KB, free 912.2 MB)
[INFO][2018-05-24 20:27:00,132][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_6_piece0 on 192.168.0.102:49677 in memory (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:27:00,132][org.apache.spark.storage.memory.MemoryStore]Block broadcast_21_piece0 stored as bytes in memory (estimated size 1878.0 B, free 912.2 MB)
[INFO][2018-05-24 20:27:00,133][org.apache.spark.storage.BlockManagerInfo]Added broadcast_21_piece0 in memory on 192.168.0.102:49677 (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:27:00,133][org.apache.spark.SparkContext]Created broadcast 21 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:27:00,134][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_2_piece0 on 192.168.0.102:49677 in memory (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:27:00,134][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 21 (MapPartitionsRDD[43] at map at ReceiveKafkaData.scala:63) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:27:00,134][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 21.0 with 1 tasks
[INFO][2018-05-24 20:27:00,134][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 21.0 (TID 21, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:27:00,135][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_8_piece0 on 192.168.0.102:49677 in memory (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:27:00,135][org.apache.spark.executor.Executor]Running task 0.0 in stage 21.0 (TID 21)
[INFO][2018-05-24 20:27:00,136][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12174 is the same as ending offset skipping seven 0
[INFO][2018-05-24 20:27:00,136][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:27:00,137][org.apache.spark.executor.Executor]Finished task 0.0 in stage 21.0 (TID 21). 665 bytes result sent to driver
[INFO][2018-05-24 20:27:00,137][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_9_piece0 on 192.168.0.102:49677 in memory (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:27:00,138][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 21.0 (TID 21) in 4 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:27:00,138][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 21.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:27:00,139][org.apache.spark.scheduler.DAGScheduler]ResultStage 21 (foreachPartition at ReceiveKafkaData.scala:65) finished in 0.004 s
[INFO][2018-05-24 20:27:00,139][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_14_piece0 on 192.168.0.102:49677 in memory (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:27:00,139][org.apache.spark.scheduler.DAGScheduler]Job 21 finished: foreachPartition at ReceiveKafkaData.scala:65, took 0.009671 s
[INFO][2018-05-24 20:27:00,140][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527164810000 ms.0 from job set of time 1527164810000 ms
[INFO][2018-05-24 20:27:00,140][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 10.140 s for time 1527164810000 ms (execution: 0.029 s)
[INFO][2018-05-24 20:27:00,142][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_19_piece0 on 192.168.0.102:49677 in memory (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:27:00,144][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_4_piece0 on 192.168.0.102:49677 in memory (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:27:00,145][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_15_piece0 on 192.168.0.102:49677 in memory (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:27:00,149][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_3_piece0 on 192.168.0.102:49677 in memory (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:27:00,154][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_11_piece0 on 192.168.0.102:49677 in memory (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:27:00,157][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_7_piece0 on 192.168.0.102:49677 in memory (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:27:00,159][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_16_piece0 on 192.168.0.102:49677 in memory (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:27:00,162][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_13_piece0 on 192.168.0.102:49677 in memory (size: 1876.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:27:00,163][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_10_piece0 on 192.168.0.102:49677 in memory (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:27:00,164][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_5_piece0 on 192.168.0.102:49677 in memory (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:27:00,165][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_18_piece0 on 192.168.0.102:49677 in memory (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:27:05,182][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527164820000 ms
[INFO][2018-05-24 20:27:05,182][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 41 from persistence list
[INFO][2018-05-24 20:27:05,182][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527164820000 ms.0 from job set of time 1527164820000 ms
[INFO][2018-05-24 20:27:05,183][org.apache.spark.storage.BlockManager]Removing RDD 41
[INFO][2018-05-24 20:27:05,183][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 40 from persistence list
[INFO][2018-05-24 20:27:05,184][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:27:05,184][org.apache.spark.storage.BlockManager]Removing RDD 40
[INFO][2018-05-24 20:27:05,184][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527164790000 ms
[INFO][2018-05-24 20:27:05,189][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:65
[INFO][2018-05-24 20:27:05,191][org.apache.spark.scheduler.DAGScheduler]Got job 22 (foreachPartition at ReceiveKafkaData.scala:65) with 1 output partitions
[INFO][2018-05-24 20:27:05,191][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 22 (foreachPartition at ReceiveKafkaData.scala:65)
[INFO][2018-05-24 20:27:05,191][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:27:05,191][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:27:05,192][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 22 (MapPartitionsRDD[45] at map at ReceiveKafkaData.scala:63), which has no missing parents
[INFO][2018-05-24 20:27:05,198][org.apache.spark.storage.memory.MemoryStore]Block broadcast_22 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 20:27:05,202][org.apache.spark.storage.memory.MemoryStore]Block broadcast_22_piece0 stored as bytes in memory (estimated size 1878.0 B, free 912.3 MB)
[INFO][2018-05-24 20:27:05,203][org.apache.spark.storage.BlockManagerInfo]Added broadcast_22_piece0 in memory on 192.168.0.102:49677 (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:27:05,203][org.apache.spark.SparkContext]Created broadcast 22 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:27:05,204][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 22 (MapPartitionsRDD[45] at map at ReceiveKafkaData.scala:63) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:27:05,204][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 22.0 with 1 tasks
[INFO][2018-05-24 20:27:05,204][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 22.0 (TID 22, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:27:05,205][org.apache.spark.executor.Executor]Running task 0.0 in stage 22.0 (TID 22)
[INFO][2018-05-24 20:27:05,206][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12174 is the same as ending offset skipping seven 0
[INFO][2018-05-24 20:27:05,206][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:27:05,207][org.apache.spark.executor.Executor]Finished task 0.0 in stage 22.0 (TID 22). 665 bytes result sent to driver
[INFO][2018-05-24 20:27:05,208][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 22.0 (TID 22) in 4 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:27:05,208][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 22.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:27:05,208][org.apache.spark.scheduler.DAGScheduler]ResultStage 22 (foreachPartition at ReceiveKafkaData.scala:65) finished in 0.004 s
[INFO][2018-05-24 20:27:05,208][org.apache.spark.scheduler.DAGScheduler]Job 22 finished: foreachPartition at ReceiveKafkaData.scala:65, took 0.018614 s
[INFO][2018-05-24 20:27:05,209][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527164820000 ms.0 from job set of time 1527164820000 ms
[INFO][2018-05-24 20:27:05,209][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 5.209 s for time 1527164820000 ms (execution: 0.027 s)
[INFO][2018-05-24 20:27:05,209][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 43 from persistence list
[INFO][2018-05-24 20:27:05,209][org.apache.spark.storage.BlockManager]Removing RDD 43
[INFO][2018-05-24 20:27:05,209][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 42 from persistence list
[INFO][2018-05-24 20:27:05,210][org.apache.spark.storage.BlockManager]Removing RDD 42
[INFO][2018-05-24 20:27:05,210][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:27:05,210][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527164800000 ms
[INFO][2018-05-24 20:27:10,062][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527164830000 ms
[INFO][2018-05-24 20:27:10,063][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527164830000 ms.0 from job set of time 1527164830000 ms
[INFO][2018-05-24 20:27:10,069][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:65
[INFO][2018-05-24 20:27:10,071][org.apache.spark.scheduler.DAGScheduler]Got job 23 (foreachPartition at ReceiveKafkaData.scala:65) with 1 output partitions
[INFO][2018-05-24 20:27:10,071][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 23 (foreachPartition at ReceiveKafkaData.scala:65)
[INFO][2018-05-24 20:27:10,071][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:27:10,071][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:27:10,071][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 23 (MapPartitionsRDD[47] at map at ReceiveKafkaData.scala:63), which has no missing parents
[INFO][2018-05-24 20:27:10,073][org.apache.spark.storage.memory.MemoryStore]Block broadcast_23 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 20:27:10,074][org.apache.spark.storage.memory.MemoryStore]Block broadcast_23_piece0 stored as bytes in memory (estimated size 1877.0 B, free 912.3 MB)
[INFO][2018-05-24 20:27:10,077][org.apache.spark.storage.BlockManagerInfo]Added broadcast_23_piece0 in memory on 192.168.0.102:49677 (size: 1877.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:27:10,077][org.apache.spark.SparkContext]Created broadcast 23 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:27:10,078][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 23 (MapPartitionsRDD[47] at map at ReceiveKafkaData.scala:63) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:27:10,078][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 23.0 with 1 tasks
[INFO][2018-05-24 20:27:10,078][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 23.0 (TID 23, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:27:10,079][org.apache.spark.executor.Executor]Running task 0.0 in stage 23.0 (TID 23)
[INFO][2018-05-24 20:27:10,080][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12174 is the same as ending offset skipping seven 0
[INFO][2018-05-24 20:27:10,081][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:27:10,081][org.apache.spark.executor.Executor]Finished task 0.0 in stage 23.0 (TID 23). 665 bytes result sent to driver
[INFO][2018-05-24 20:27:10,082][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 23.0 (TID 23) in 4 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:27:10,082][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 23.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:27:10,083][org.apache.spark.scheduler.DAGScheduler]ResultStage 23 (foreachPartition at ReceiveKafkaData.scala:65) finished in 0.005 s
[INFO][2018-05-24 20:27:10,084][org.apache.spark.scheduler.DAGScheduler]Job 23 finished: foreachPartition at ReceiveKafkaData.scala:65, took 0.014735 s
[INFO][2018-05-24 20:27:10,085][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527164830000 ms.0 from job set of time 1527164830000 ms
[INFO][2018-05-24 20:27:10,086][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.085 s for time 1527164830000 ms (execution: 0.023 s)
[INFO][2018-05-24 20:27:10,086][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 45 from persistence list
[INFO][2018-05-24 20:27:10,087][org.apache.spark.storage.BlockManager]Removing RDD 45
[INFO][2018-05-24 20:27:10,087][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 44 from persistence list
[INFO][2018-05-24 20:27:10,088][org.apache.spark.storage.BlockManager]Removing RDD 44
[INFO][2018-05-24 20:27:10,088][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:27:10,088][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527164810000 ms
[INFO][2018-05-24 20:27:25,076][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527164840000 ms
[INFO][2018-05-24 20:27:25,077][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527164840000 ms.0 from job set of time 1527164840000 ms
[INFO][2018-05-24 20:27:25,081][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:65
[INFO][2018-05-24 20:27:25,081][org.apache.spark.scheduler.DAGScheduler]Got job 24 (foreachPartition at ReceiveKafkaData.scala:65) with 1 output partitions
[INFO][2018-05-24 20:27:25,081][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 24 (foreachPartition at ReceiveKafkaData.scala:65)
[INFO][2018-05-24 20:27:25,081][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:27:25,081][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:27:25,081][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 24 (MapPartitionsRDD[49] at map at ReceiveKafkaData.scala:63), which has no missing parents
[INFO][2018-05-24 20:27:25,083][org.apache.spark.storage.memory.MemoryStore]Block broadcast_24 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 20:27:25,084][org.apache.spark.storage.memory.MemoryStore]Block broadcast_24_piece0 stored as bytes in memory (estimated size 1878.0 B, free 912.3 MB)
[INFO][2018-05-24 20:27:25,084][org.apache.spark.storage.BlockManagerInfo]Added broadcast_24_piece0 in memory on 192.168.0.102:49677 (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:27:25,085][org.apache.spark.SparkContext]Created broadcast 24 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:27:25,085][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 24 (MapPartitionsRDD[49] at map at ReceiveKafkaData.scala:63) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:27:25,085][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 24.0 with 1 tasks
[INFO][2018-05-24 20:27:25,085][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 24.0 (TID 24, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:27:25,086][org.apache.spark.executor.Executor]Running task 0.0 in stage 24.0 (TID 24)
[INFO][2018-05-24 20:27:25,087][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12174 is the same as ending offset skipping seven 0
[INFO][2018-05-24 20:27:25,087][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:27:25,088][org.apache.spark.executor.Executor]Finished task 0.0 in stage 24.0 (TID 24). 665 bytes result sent to driver
[INFO][2018-05-24 20:27:25,088][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 24.0 (TID 24) in 3 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:27:25,088][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 24.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:27:25,088][org.apache.spark.scheduler.DAGScheduler]ResultStage 24 (foreachPartition at ReceiveKafkaData.scala:65) finished in 0.003 s
[INFO][2018-05-24 20:27:25,089][org.apache.spark.scheduler.DAGScheduler]Job 24 finished: foreachPartition at ReceiveKafkaData.scala:65, took 0.008001 s
[INFO][2018-05-24 20:27:25,089][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527164840000 ms.0 from job set of time 1527164840000 ms
[INFO][2018-05-24 20:27:25,089][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 5.089 s for time 1527164840000 ms (execution: 0.012 s)
[INFO][2018-05-24 20:27:25,089][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 47 from persistence list
[INFO][2018-05-24 20:27:25,089][org.apache.spark.storage.BlockManager]Removing RDD 47
[INFO][2018-05-24 20:27:25,090][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 46 from persistence list
[INFO][2018-05-24 20:27:25,090][org.apache.spark.storage.BlockManager]Removing RDD 46
[INFO][2018-05-24 20:27:25,090][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:27:25,090][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527164820000 ms
[INFO][2018-05-24 20:27:40,082][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527164850000 ms
[INFO][2018-05-24 20:27:40,082][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527164850000 ms.0 from job set of time 1527164850000 ms
[INFO][2018-05-24 20:27:40,087][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:65
[INFO][2018-05-24 20:27:40,088][org.apache.spark.scheduler.DAGScheduler]Got job 25 (foreachPartition at ReceiveKafkaData.scala:65) with 1 output partitions
[INFO][2018-05-24 20:27:40,088][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 25 (foreachPartition at ReceiveKafkaData.scala:65)
[INFO][2018-05-24 20:27:40,088][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:27:40,088][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:27:40,088][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 25 (MapPartitionsRDD[51] at map at ReceiveKafkaData.scala:63), which has no missing parents
[INFO][2018-05-24 20:27:40,090][org.apache.spark.storage.memory.MemoryStore]Block broadcast_25 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 20:27:40,091][org.apache.spark.storage.memory.MemoryStore]Block broadcast_25_piece0 stored as bytes in memory (estimated size 1878.0 B, free 912.3 MB)
[INFO][2018-05-24 20:27:40,092][org.apache.spark.storage.BlockManagerInfo]Added broadcast_25_piece0 in memory on 192.168.0.102:49677 (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:27:40,092][org.apache.spark.SparkContext]Created broadcast 25 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:27:40,093][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 25 (MapPartitionsRDD[51] at map at ReceiveKafkaData.scala:63) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:27:40,093][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 25.0 with 1 tasks
[INFO][2018-05-24 20:27:40,094][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 25.0 (TID 25, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:27:40,094][org.apache.spark.executor.Executor]Running task 0.0 in stage 25.0 (TID 25)
[INFO][2018-05-24 20:27:40,095][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12174 is the same as ending offset skipping seven 0
[INFO][2018-05-24 20:27:40,096][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:27:40,096][org.apache.spark.executor.Executor]Finished task 0.0 in stage 25.0 (TID 25). 665 bytes result sent to driver
[INFO][2018-05-24 20:27:40,096][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 25.0 (TID 25) in 3 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:27:40,096][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 25.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:27:40,097][org.apache.spark.scheduler.DAGScheduler]ResultStage 25 (foreachPartition at ReceiveKafkaData.scala:65) finished in 0.004 s
[INFO][2018-05-24 20:27:40,097][org.apache.spark.scheduler.DAGScheduler]Job 25 finished: foreachPartition at ReceiveKafkaData.scala:65, took 0.009705 s
[INFO][2018-05-24 20:27:40,097][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527164850000 ms.0 from job set of time 1527164850000 ms
[INFO][2018-05-24 20:27:40,097][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 10.097 s for time 1527164850000 ms (execution: 0.015 s)
[INFO][2018-05-24 20:27:45,169][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527164860000 ms
[INFO][2018-05-24 20:27:45,169][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 49 from persistence list
[INFO][2018-05-24 20:27:45,169][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527164860000 ms.0 from job set of time 1527164860000 ms
[INFO][2018-05-24 20:27:45,169][org.apache.spark.storage.BlockManager]Removing RDD 49
[INFO][2018-05-24 20:27:45,170][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 48 from persistence list
[INFO][2018-05-24 20:27:45,170][org.apache.spark.storage.BlockManager]Removing RDD 48
[INFO][2018-05-24 20:27:45,171][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:27:45,171][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527164830000 ms
[INFO][2018-05-24 20:27:45,175][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:65
[INFO][2018-05-24 20:27:45,175][org.apache.spark.scheduler.DAGScheduler]Got job 26 (foreachPartition at ReceiveKafkaData.scala:65) with 1 output partitions
[INFO][2018-05-24 20:27:45,175][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 26 (foreachPartition at ReceiveKafkaData.scala:65)
[INFO][2018-05-24 20:27:45,175][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:27:45,175][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:27:45,175][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 26 (MapPartitionsRDD[53] at map at ReceiveKafkaData.scala:63), which has no missing parents
[INFO][2018-05-24 20:27:45,177][org.apache.spark.storage.memory.MemoryStore]Block broadcast_26 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 20:27:45,178][org.apache.spark.storage.memory.MemoryStore]Block broadcast_26_piece0 stored as bytes in memory (estimated size 1878.0 B, free 912.3 MB)
[INFO][2018-05-24 20:27:45,179][org.apache.spark.storage.BlockManagerInfo]Added broadcast_26_piece0 in memory on 192.168.0.102:49677 (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:27:45,180][org.apache.spark.SparkContext]Created broadcast 26 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:27:45,180][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 26 (MapPartitionsRDD[53] at map at ReceiveKafkaData.scala:63) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:27:45,180][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 26.0 with 1 tasks
[INFO][2018-05-24 20:27:45,181][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 26.0 (TID 26, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:27:45,181][org.apache.spark.executor.Executor]Running task 0.0 in stage 26.0 (TID 26)
[INFO][2018-05-24 20:27:45,183][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12174 is the same as ending offset skipping seven 0
[INFO][2018-05-24 20:27:45,183][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:27:45,184][org.apache.spark.executor.Executor]Finished task 0.0 in stage 26.0 (TID 26). 708 bytes result sent to driver
[INFO][2018-05-24 20:27:45,186][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 26.0 (TID 26) in 5 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:27:45,186][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 26.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:27:45,186][org.apache.spark.scheduler.DAGScheduler]ResultStage 26 (foreachPartition at ReceiveKafkaData.scala:65) finished in 0.005 s
[INFO][2018-05-24 20:27:45,187][org.apache.spark.scheduler.DAGScheduler]Job 26 finished: foreachPartition at ReceiveKafkaData.scala:65, took 0.012194 s
[INFO][2018-05-24 20:27:45,188][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527164860000 ms.0 from job set of time 1527164860000 ms
[INFO][2018-05-24 20:27:45,188][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 5.187 s for time 1527164860000 ms (execution: 0.018 s)
[INFO][2018-05-24 20:27:45,188][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 51 from persistence list
[INFO][2018-05-24 20:27:45,188][org.apache.spark.storage.BlockManager]Removing RDD 51
[INFO][2018-05-24 20:27:45,189][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 50 from persistence list
[INFO][2018-05-24 20:27:45,189][org.apache.spark.storage.BlockManager]Removing RDD 50
[INFO][2018-05-24 20:27:45,189][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:27:45,189][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527164840000 ms
[INFO][2018-05-24 20:27:55,065][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527164870000 ms
[INFO][2018-05-24 20:27:55,066][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527164870000 ms.0 from job set of time 1527164870000 ms
[INFO][2018-05-24 20:27:55,071][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:65
[INFO][2018-05-24 20:27:55,071][org.apache.spark.scheduler.DAGScheduler]Got job 27 (foreachPartition at ReceiveKafkaData.scala:65) with 1 output partitions
[INFO][2018-05-24 20:27:55,071][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 27 (foreachPartition at ReceiveKafkaData.scala:65)
[INFO][2018-05-24 20:27:55,071][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:27:55,071][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:27:55,071][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 27 (MapPartitionsRDD[55] at map at ReceiveKafkaData.scala:63), which has no missing parents
[INFO][2018-05-24 20:27:55,073][org.apache.spark.storage.memory.MemoryStore]Block broadcast_27 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 20:27:55,075][org.apache.spark.storage.memory.MemoryStore]Block broadcast_27_piece0 stored as bytes in memory (estimated size 1878.0 B, free 912.3 MB)
[INFO][2018-05-24 20:27:55,075][org.apache.spark.storage.BlockManagerInfo]Added broadcast_27_piece0 in memory on 192.168.0.102:49677 (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:27:55,075][org.apache.spark.SparkContext]Created broadcast 27 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:27:55,076][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 27 (MapPartitionsRDD[55] at map at ReceiveKafkaData.scala:63) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:27:55,076][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 27.0 with 1 tasks
[INFO][2018-05-24 20:27:55,077][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 27.0 (TID 27, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:27:55,077][org.apache.spark.executor.Executor]Running task 0.0 in stage 27.0 (TID 27)
[INFO][2018-05-24 20:27:55,078][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12174 is the same as ending offset skipping seven 0
[INFO][2018-05-24 20:27:55,078][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:27:55,078][org.apache.spark.executor.Executor]Finished task 0.0 in stage 27.0 (TID 27). 665 bytes result sent to driver
[INFO][2018-05-24 20:27:55,079][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 27.0 (TID 27) in 3 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:27:55,079][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 27.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:27:55,079][org.apache.spark.scheduler.DAGScheduler]ResultStage 27 (foreachPartition at ReceiveKafkaData.scala:65) finished in 0.003 s
[INFO][2018-05-24 20:27:55,083][org.apache.spark.scheduler.DAGScheduler]Job 27 finished: foreachPartition at ReceiveKafkaData.scala:65, took 0.010014 s
[INFO][2018-05-24 20:27:55,083][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527164870000 ms.0 from job set of time 1527164870000 ms
[INFO][2018-05-24 20:27:55,083][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 53 from persistence list
[INFO][2018-05-24 20:27:55,083][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 5.083 s for time 1527164870000 ms (execution: 0.017 s)
[INFO][2018-05-24 20:27:55,084][org.apache.spark.storage.BlockManager]Removing RDD 53
[INFO][2018-05-24 20:27:55,084][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 52 from persistence list
[INFO][2018-05-24 20:27:55,084][org.apache.spark.storage.BlockManager]Removing RDD 52
[INFO][2018-05-24 20:27:55,084][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:27:55,084][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527164850000 ms
[INFO][2018-05-24 20:27:55,574][org.apache.spark.streaming.StreamingContext]Invoking stop(stopGracefully=false) from shutdown hook
[INFO][2018-05-24 20:27:55,578][org.apache.spark.streaming.scheduler.ReceiverTracker]ReceiverTracker stopped
[INFO][2018-05-24 20:27:55,578][org.apache.spark.streaming.scheduler.JobGenerator]Stopping JobGenerator immediately
[INFO][2018-05-24 20:27:55,579][org.apache.spark.streaming.util.RecurringTimer]Stopped timer for JobGenerator after time 1527164870000
[INFO][2018-05-24 20:27:55,580][org.apache.spark.streaming.scheduler.JobGenerator]Stopped JobGenerator
[INFO][2018-05-24 20:27:55,581][org.apache.spark.streaming.scheduler.JobScheduler]Stopped JobScheduler
[INFO][2018-05-24 20:27:55,586][org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@57a48985{/streaming,null,UNAVAILABLE,@Spark}
[INFO][2018-05-24 20:27:55,586][org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@3f672204{/streaming/batch,null,UNAVAILABLE,@Spark}
[INFO][2018-05-24 20:27:55,587][org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@2d000e80{/static/streaming,null,UNAVAILABLE,@Spark}
[INFO][2018-05-24 20:27:55,588][org.apache.spark.streaming.StreamingContext]StreamingContext stopped successfully
[INFO][2018-05-24 20:27:55,588][org.apache.spark.SparkContext]Invoking stop() from shutdown hook
[INFO][2018-05-24 20:27:55,597][org.spark_project.jetty.server.AbstractConnector]Stopped Spark@6efa953f{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-05-24 20:27:55,598][org.apache.spark.ui.SparkUI]Stopped Spark web UI at http://192.168.0.102:4040
[INFO][2018-05-24 20:27:55,606][org.apache.spark.MapOutputTrackerMasterEndpoint]MapOutputTrackerMasterEndpoint stopped!
[INFO][2018-05-24 20:27:55,624][org.apache.spark.storage.memory.MemoryStore]MemoryStore cleared
[INFO][2018-05-24 20:27:55,625][org.apache.spark.storage.BlockManager]BlockManager stopped
[INFO][2018-05-24 20:27:55,625][org.apache.spark.storage.BlockManagerMaster]BlockManagerMaster stopped
[INFO][2018-05-24 20:27:55,630][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint]OutputCommitCoordinator stopped!
[INFO][2018-05-24 20:27:55,631][org.apache.spark.SparkContext]Successfully stopped SparkContext
[INFO][2018-05-24 20:27:55,631][org.apache.spark.util.ShutdownHookManager]Shutdown hook called
[INFO][2018-05-24 20:27:55,634][org.apache.spark.util.ShutdownHookManager]Deleting directory /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/spark-5f32ab1b-0bfe-40e6-b1ba-8e583b6261ee
[INFO][2018-05-24 20:28:46,928][org.apache.spark.SparkContext]Running Spark version 2.2.0
[INFO][2018-05-24 20:28:47,695][org.apache.spark.SparkContext]Submitted application: ReceiveKafkaData$
[INFO][2018-05-24 20:28:47,743][org.apache.spark.SecurityManager]Changing view acls to: seven
[INFO][2018-05-24 20:28:47,744][org.apache.spark.SecurityManager]Changing modify acls to: seven
[INFO][2018-05-24 20:28:47,749][org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO][2018-05-24 20:28:47,751][org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO][2018-05-24 20:28:47,752][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(seven); groups with view permissions: Set(); users  with modify permissions: Set(seven); groups with modify permissions: Set()
[INFO][2018-05-24 20:28:48,044][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 49838.
[INFO][2018-05-24 20:28:48,077][org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO][2018-05-24 20:28:48,094][org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO][2018-05-24 20:28:48,099][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2018-05-24 20:28:48,099][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
[INFO][2018-05-24 20:28:48,109][org.apache.spark.storage.DiskBlockManager]Created local directory at /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/blockmgr-529e64c9-ba60-4cb3-904e-31a647fe1d1b
[INFO][2018-05-24 20:28:48,124][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 912.3 MB
[INFO][2018-05-24 20:28:48,194][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO][2018-05-24 20:28:48,284][org.spark_project.jetty.util.log]Logging initialized @2311ms
[INFO][2018-05-24 20:28:48,355][org.spark_project.jetty.server.Server]jetty-9.3.z-SNAPSHOT
[INFO][2018-05-24 20:28:48,367][org.spark_project.jetty.server.Server]Started @2399ms
[INFO][2018-05-24 20:28:48,391][org.spark_project.jetty.server.AbstractConnector]Started ServerConnector@6d366c9b{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-05-24 20:28:48,391][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4040.
[INFO][2018-05-24 20:28:48,424][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2c1b9e4b{/jobs,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:28:48,425][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3d08f3f5{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:28:48,426][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1a1da881{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:28:48,430][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@165b8a71{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:28:48,431][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2f058b8a{/stages,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:28:48,433][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3f2ef586{/stages/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:28:48,434][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@76c7beb3{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:28:48,436][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7b139eab{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:28:48,440][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@611df6e3{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:28:48,441][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6273c5a4{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:28:48,442][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@53e211ee{/storage,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:28:48,447][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3d8bbcdc{/storage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:28:48,448][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@117e0fe5{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:28:48,449][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@78aea4b9{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:28:48,450][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4b85880b{/environment,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:28:48,452][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4215838f{/environment/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:28:48,453][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2289aca5{/executors,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:28:48,454][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@184497d1{/executors/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:28:48,455][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6ffab045{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:28:48,456][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3e2943ab{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:28:48,468][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4a9f80d3{/static,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:28:48,471][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4204541c{/,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:28:48,474][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4602c2a9{/api,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:28:48,477][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@69c79f09{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:28:48,478][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5fcacc0{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:28:48,481][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://192.168.0.102:4040
[INFO][2018-05-24 20:28:48,580][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
[INFO][2018-05-24 20:28:48,598][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 49839.
[INFO][2018-05-24 20:28:48,602][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 192.168.0.102:49839
[INFO][2018-05-24 20:28:48,610][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2018-05-24 20:28:48,612][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 192.168.0.102, 49839, None)
[INFO][2018-05-24 20:28:48,623][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 192.168.0.102:49839 with 912.3 MB RAM, BlockManagerId(driver, 192.168.0.102, 49839, None)
[INFO][2018-05-24 20:28:48,631][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 192.168.0.102, 49839, None)
[INFO][2018-05-24 20:28:48,634][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 192.168.0.102, 49839, None)
[INFO][2018-05-24 20:28:48,837][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@64a1923a{/metrics/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:28:48,995][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 20:28:48,998][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 20:28:48,998][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 20:29:04,466][org.apache.spark.streaming.kafka.DirectKafkaInputDStream]Slide time = 10000 ms
[INFO][2018-05-24 20:29:04,466][org.apache.spark.streaming.kafka.DirectKafkaInputDStream]Storage level = Serialized 1x Replicated
[INFO][2018-05-24 20:29:04,467][org.apache.spark.streaming.kafka.DirectKafkaInputDStream]Checkpoint interval = null
[INFO][2018-05-24 20:29:04,468][org.apache.spark.streaming.kafka.DirectKafkaInputDStream]Remember interval = 10000 ms
[INFO][2018-05-24 20:29:04,468][org.apache.spark.streaming.kafka.DirectKafkaInputDStream]Initialized and validated org.apache.spark.streaming.kafka.DirectKafkaInputDStream@d9b4795
[INFO][2018-05-24 20:29:04,468][org.apache.spark.streaming.dstream.MappedDStream]Slide time = 10000 ms
[INFO][2018-05-24 20:29:04,469][org.apache.spark.streaming.dstream.MappedDStream]Storage level = Serialized 1x Replicated
[INFO][2018-05-24 20:29:04,469][org.apache.spark.streaming.dstream.MappedDStream]Checkpoint interval = null
[INFO][2018-05-24 20:29:04,469][org.apache.spark.streaming.dstream.MappedDStream]Remember interval = 10000 ms
[INFO][2018-05-24 20:29:04,469][org.apache.spark.streaming.dstream.MappedDStream]Initialized and validated org.apache.spark.streaming.dstream.MappedDStream@4ed04674
[INFO][2018-05-24 20:29:04,469][org.apache.spark.streaming.dstream.ForEachDStream]Slide time = 10000 ms
[INFO][2018-05-24 20:29:04,469][org.apache.spark.streaming.dstream.ForEachDStream]Storage level = Serialized 1x Replicated
[INFO][2018-05-24 20:29:04,469][org.apache.spark.streaming.dstream.ForEachDStream]Checkpoint interval = null
[INFO][2018-05-24 20:29:04,470][org.apache.spark.streaming.dstream.ForEachDStream]Remember interval = 10000 ms
[INFO][2018-05-24 20:29:04,470][org.apache.spark.streaming.dstream.ForEachDStream]Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@2ddf9d3b
[INFO][2018-05-24 20:29:04,512][org.apache.spark.streaming.util.RecurringTimer]Started timer for JobGenerator at time 1527164950000
[INFO][2018-05-24 20:29:04,513][org.apache.spark.streaming.scheduler.JobGenerator]Started JobGenerator at 1527164950000 ms
[INFO][2018-05-24 20:29:04,514][org.apache.spark.streaming.scheduler.JobScheduler]Started JobScheduler
[INFO][2018-05-24 20:29:04,520][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6972c30a{/streaming,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:29:04,520][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1ab6718{/streaming/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:29:04,521][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5109e8cf{/streaming/batch,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:29:04,522][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@78b41097{/streaming/batch/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:29:04,523][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6c65860d{/static/streaming,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:29:04,524][org.apache.spark.streaming.StreamingContext]StreamingContext started
[INFO][2018-05-24 20:29:10,050][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 20:29:10,050][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 20:29:10,050][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 20:29:15,157][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527164950000 ms
[INFO][2018-05-24 20:29:15,160][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527164950000 ms.0 from job set of time 1527164950000 ms
[INFO][2018-05-24 20:29:15,201][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:65
[INFO][2018-05-24 20:29:15,219][org.apache.spark.scheduler.DAGScheduler]Got job 0 (foreachPartition at ReceiveKafkaData.scala:65) with 1 output partitions
[INFO][2018-05-24 20:29:15,220][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 0 (foreachPartition at ReceiveKafkaData.scala:65)
[INFO][2018-05-24 20:29:15,221][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:29:15,223][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:29:15,245][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 0 (MapPartitionsRDD[1] at map at ReceiveKafkaData.scala:63), which has no missing parents
[INFO][2018-05-24 20:29:15,362][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 20:29:15,397][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0_piece0 stored as bytes in memory (estimated size 1877.0 B, free 912.3 MB)
[INFO][2018-05-24 20:29:15,400][org.apache.spark.storage.BlockManagerInfo]Added broadcast_0_piece0 in memory on 192.168.0.102:49839 (size: 1877.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:29:15,404][org.apache.spark.SparkContext]Created broadcast 0 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:29:15,420][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at map at ReceiveKafkaData.scala:63) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:29:15,421][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 0.0 with 1 tasks
[INFO][2018-05-24 20:29:15,496][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:29:15,506][org.apache.spark.executor.Executor]Running task 0.0 in stage 0.0 (TID 0)
[INFO][2018-05-24 20:29:15,545][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12174 is the same as ending offset skipping seven 0
[INFO][2018-05-24 20:29:15,840][org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper]Process identifier=hconnection-0x361504d6 connecting to ZooKeeper ensemble=vm-xaj-bigdata-da-d01:2181,vm-xaj-bigdata-da-d02:2181,vm-xaj-bigdata-da-d03:2181
[INFO][2018-05-24 20:29:15,847][org.apache.zookeeper.ZooKeeper]Client environment:zookeeper.version=3.4.5-cdh5.7.0--1, built on 04/06/2016 14:24 GMT
[INFO][2018-05-24 20:29:15,847][org.apache.zookeeper.ZooKeeper]Client environment:host.name=192.168.0.102
[INFO][2018-05-24 20:29:15,847][org.apache.zookeeper.ZooKeeper]Client environment:java.version=1.8.0_161
[INFO][2018-05-24 20:29:15,847][org.apache.zookeeper.ZooKeeper]Client environment:java.vendor=Oracle Corporation
[INFO][2018-05-24 20:29:15,848][org.apache.zookeeper.ZooKeeper]Client environment:java.home=/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre
[INFO][2018-05-24 20:29:15,848][org.apache.zookeeper.ZooKeeper]Client environment:java.class.path=/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/charsets.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/deploy.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/cldrdata.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/dnsns.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/jaccess.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/jfxrt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/localedata.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/nashorn.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/sunec.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/sunjce_provider.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/sunpkcs11.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/zipfs.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/javaws.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/jce.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/jfr.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/jfxswt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/jsse.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/management-agent.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/plugin.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/resources.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/rt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/ant-javafx.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/dt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/javafx-mx.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/jconsole.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/packager.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/sa-jdi.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/tools.jar:/Users/seven/project/scala/dataMining/target/classes:/Users/seven/software/maven/repository/org/scala-lang/scala-library/2.11.12/scala-library-2.11.12.jar:/Users/seven/software/maven/repository/org/scala-lang/scala-compiler/2.11.12/scala-compiler-2.11.12.jar:/Users/seven/software/maven/repository/org/scala-lang/modules/scala-xml_2.11/1.0.5/scala-xml_2.11-1.0.5.jar:/Users/seven/software/maven/repository/org/scala-lang/modules/scala-parser-combinators_2.11/1.0.4/scala-parser-combinators_2.11-1.0.4.jar:/Users/seven/software/maven/repository/org/scala-lang/scala-reflect/2.11.12/scala-reflect-2.11.12.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-client/2.6.0-cdh5.7.0/hadoop-client-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-common/2.6.0-cdh5.7.0/hadoop-common-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/xmlenc/xmlenc/0.52/xmlenc-0.52.jar:/Users/seven/software/maven/repository/commons-net/commons-net/3.1/commons-net-3.1.jar:/Users/seven/software/maven/repository/commons-configuration/commons-configuration/1.6/commons-configuration-1.6.jar:/Users/seven/software/maven/repository/commons-digester/commons-digester/1.8/commons-digester-1.8.jar:/Users/seven/software/maven/repository/commons-beanutils/commons-beanutils/1.7.0/commons-beanutils-1.7.0.jar:/Users/seven/software/maven/repository/org/slf4j/slf4j-api/1.7.5/slf4j-api-1.7.5.jar:/Users/seven/software/maven/repository/org/slf4j/slf4j-log4j12/1.7.5/slf4j-log4j12-1.7.5.jar:/Users/seven/software/maven/repository/org/apache/avro/avro/1.7.6-cdh5.7.0/avro-1.7.6-cdh5.7.0.jar:/Users/seven/software/maven/repository/com/google/code/gson/gson/2.2.4/gson-2.2.4.jar:/Users/seven/software/maven/repository/org/apache/curator/curator-client/2.7.1/curator-client-2.7.1.jar:/Users/seven/software/maven/repository/org/apache/curator/curator-recipes/2.7.1/curator-recipes-2.7.1.jar:/Users/seven/software/maven/repository/com/google/code/findbugs/jsr305/3.0.0/jsr305-3.0.0.jar:/Users/seven/software/maven/repository/org/apache/htrace/htrace-core4/4.0.1-incubating/htrace-core4-4.0.1-incubating.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-compress/1.4.1/commons-compress-1.4.1.jar:/Users/seven/software/maven/repository/org/tukaani/xz/1.0/xz-1.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-hdfs/2.6.0-cdh5.7.0/hadoop-hdfs-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/io/netty/netty/3.6.2.Final/netty-3.6.2.Final.jar:/Users/seven/software/maven/repository/xerces/xercesImpl/2.9.1/xercesImpl-2.9.1.jar:/Users/seven/software/maven/repository/xml-apis/xml-apis/1.3.04/xml-apis-1.3.04.jar:/Users/seven/software/maven/repository/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-app/2.6.0-cdh5.7.0/hadoop-mapreduce-client-app-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-common/2.6.0-cdh5.7.0/hadoop-mapreduce-client-common-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-yarn-client/2.6.0-cdh5.7.0/hadoop-yarn-client-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-yarn-server-common/2.6.0-cdh5.7.0/hadoop-yarn-server-common-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-shuffle/2.6.0-cdh5.7.0/hadoop-mapreduce-client-shuffle-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-yarn-api/2.6.0-cdh5.7.0/hadoop-yarn-api-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-core/2.6.0-cdh5.7.0/hadoop-mapreduce-client-core-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-yarn-common/2.6.0-cdh5.7.0/hadoop-yarn-common-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/javax/xml/bind/jaxb-api/2.2.2/jaxb-api-2.2.2.jar:/Users/seven/software/maven/repository/javax/xml/stream/stax-api/1.0-2/stax-api-1.0-2.jar:/Users/seven/software/maven/repository/javax/activation/activation/1.1/activation-1.1.jar:/Users/seven/software/maven/repository/com/sun/jersey/jersey-client/1.9/jersey-client-1.9.jar:/Users/seven/software/maven/repository/org/codehaus/jackson/jackson-xc/1.8.8/jackson-xc-1.8.8.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.6.0-cdh5.7.0/hadoop-mapreduce-client-jobclient-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-aws/2.6.0-cdh5.7.0/hadoop-aws-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/com/amazonaws/aws-java-sdk-s3/1.10.6/aws-java-sdk-s3-1.10.6.jar:/Users/seven/software/maven/repository/com/amazonaws/aws-java-sdk-kms/1.10.6/aws-java-sdk-kms-1.10.6.jar:/Users/seven/software/maven/repository/com/amazonaws/aws-java-sdk-core/1.10.6/aws-java-sdk-core-1.10.6.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-annotations/2.6.0-cdh5.7.0/hadoop-annotations-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-client/1.2.0-cdh5.7.0/hbase-client-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-annotations/1.2.0-cdh5.7.0/hbase-annotations-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-common/1.2.0-cdh5.7.0/hbase-common-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-protocol/1.2.0-cdh5.7.0/hbase-protocol-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/commons-codec/commons-codec/1.9/commons-codec-1.9.jar:/Users/seven/software/maven/repository/commons-io/commons-io/2.4/commons-io-2.4.jar:/Users/seven/software/maven/repository/commons-lang/commons-lang/2.6/commons-lang-2.6.jar:/Users/seven/software/maven/repository/commons-logging/commons-logging/1.2/commons-logging-1.2.jar:/Users/seven/software/maven/repository/com/google/guava/guava/12.0.1/guava-12.0.1.jar:/Users/seven/software/maven/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/Users/seven/software/maven/repository/io/netty/netty-all/4.0.23.Final/netty-all-4.0.23.Final.jar:/Users/seven/software/maven/repository/org/apache/zookeeper/zookeeper/3.4.5-cdh5.7.0/zookeeper-3.4.5-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/htrace/htrace-core/3.2.0-incubating/htrace-core-3.2.0-incubating.jar:/Users/seven/software/maven/repository/org/codehaus/jackson/jackson-mapper-asl/1.8.8/jackson-mapper-asl-1.8.8.jar:/Users/seven/software/maven/repository/org/jruby/jcodings/jcodings/1.0.8/jcodings-1.0.8.jar:/Users/seven/software/maven/repository/org/jruby/joni/joni/2.1.2/joni-2.1.2.jar:/Users/seven/software/maven/repository/com/yammer/metrics/metrics-core/2.2.0/metrics-core-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-auth/2.6.0-cdh5.7.0/hadoop-auth-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/httpcomponents/httpclient/4.2.5/httpclient-4.2.5.jar:/Users/seven/software/maven/repository/org/apache/httpcomponents/httpcore/4.2.4/httpcore-4.2.4.jar:/Users/seven/software/maven/repository/org/apache/directory/server/apacheds-kerberos-codec/2.0.0-M15/apacheds-kerberos-codec-2.0.0-M15.jar:/Users/seven/software/maven/repository/org/apache/directory/server/apacheds-i18n/2.0.0-M15/apacheds-i18n-2.0.0-M15.jar:/Users/seven/software/maven/repository/org/apache/directory/api/api-asn1-api/1.0.0-M20/api-asn1-api-1.0.0-M20.jar:/Users/seven/software/maven/repository/org/apache/directory/api/api-util/1.0.0-M20/api-util-1.0.0-M20.jar:/Users/seven/software/maven/repository/org/apache/curator/curator-framework/2.7.1/curator-framework-2.7.1.jar:/Users/seven/software/maven/repository/com/github/stephenc/findbugs/findbugs-annotations/1.3.9-1/findbugs-annotations-1.3.9-1.jar:/Users/seven/software/maven/repository/junit/junit/4.12/junit-4.12.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-server/1.2.0-cdh5.7.0/hbase-server-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-procedure/1.2.0-cdh5.7.0/hbase-procedure-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-common/1.2.0-cdh5.7.0/hbase-common-1.2.0-cdh5.7.0-tests.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-prefix-tree/1.2.0-cdh5.7.0/hbase-prefix-tree-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar:/Users/seven/software/maven/repository/commons-collections/commons-collections/3.2.2/commons-collections-3.2.2.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-hadoop-compat/1.2.0-cdh5.7.0/hbase-hadoop-compat-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-hadoop2-compat/1.2.0-cdh5.7.0/hbase-hadoop2-compat-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/com/sun/jersey/jersey-core/1.9/jersey-core-1.9.jar:/Users/seven/software/maven/repository/com/sun/jersey/jersey-server/1.9/jersey-server-1.9.jar:/Users/seven/software/maven/repository/asm/asm/3.1/asm-3.1.jar:/Users/seven/software/maven/repository/commons-cli/commons-cli/1.2/commons-cli-1.2.jar:/Users/seven/software/maven/repository/com/github/stephenc/high-scale-lib/high-scale-lib/1.1.1/high-scale-lib-1.1.1.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-math/2.1/commons-math-2.1.jar:/Users/seven/software/maven/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jetty/6.1.26.cloudera.4/jetty-6.1.26.cloudera.4.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jetty-util/6.1.26.cloudera.4/jetty-util-6.1.26.cloudera.4.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jetty-sslengine/6.1.26.cloudera.4/jetty-sslengine-6.1.26.cloudera.4.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jsp-2.1/6.1.14/jsp-2.1-6.1.14.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jsp-api-2.1/6.1.14/jsp-api-2.1-6.1.14.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/servlet-api-2.5/6.1.14/servlet-api-2.5-6.1.14.jar:/Users/seven/software/maven/repository/org/codehaus/jackson/jackson-core-asl/1.8.8/jackson-core-asl-1.8.8.jar:/Users/seven/software/maven/repository/org/codehaus/jackson/jackson-jaxrs/1.8.8/jackson-jaxrs-1.8.8.jar:/Users/seven/software/maven/repository/tomcat/jasper-compiler/5.5.23/jasper-compiler-5.5.23.jar:/Users/seven/software/maven/repository/tomcat/jasper-runtime/5.5.23/jasper-runtime-5.5.23.jar:/Users/seven/software/maven/repository/commons-el/commons-el/1.0/commons-el-1.0.jar:/Users/seven/software/maven/repository/org/jamon/jamon-runtime/2.4.1/jamon-runtime-2.4.1.jar:/Users/seven/software/maven/repository/com/lmax/disruptor/3.3.0/disruptor-3.3.0.jar:/Users/seven/software/maven/repository/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar:/Users/seven/software/maven/repository/org/owasp/esapi/esapi/2.1.0/esapi-2.1.0.jar:/Users/seven/software/maven/repository/commons-beanutils/commons-beanutils-core/1.7.0/commons-beanutils-core-1.7.0.jar:/Users/seven/software/maven/repository/commons-fileupload/commons-fileupload/1.2/commons-fileupload-1.2.jar:/Users/seven/software/maven/repository/xom/xom/1.2.5/xom-1.2.5.jar:/Users/seven/software/maven/repository/xalan/xalan/2.7.0/xalan-2.7.0.jar:/Users/seven/software/maven/repository/org/beanshell/bsh-core/2.0b4/bsh-core-2.0b4.jar:/Users/seven/software/maven/repository/org/owasp/antisamy/antisamy/1.4.3/antisamy-1.4.3.jar:/Users/seven/software/maven/repository/org/apache/xmlgraphics/batik-css/1.7/batik-css-1.7.jar:/Users/seven/software/maven/repository/org/apache/xmlgraphics/batik-ext/1.7/batik-ext-1.7.jar:/Users/seven/software/maven/repository/org/apache/xmlgraphics/batik-util/1.7/batik-util-1.7.jar:/Users/seven/software/maven/repository/xml-apis/xml-apis-ext/1.3.04/xml-apis-ext-1.3.04.jar:/Users/seven/software/maven/repository/net/sourceforge/nekohtml/nekohtml/1.9.12/nekohtml-1.9.12.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-core/2.6.0-mr1-cdh5.7.0/hadoop-core-2.6.0-mr1-cdh5.7.0.jar:/Users/seven/software/maven/repository/javax/servlet/jsp/jsp-api/2.1/jsp-api-2.1.jar:/Users/seven/software/maven/repository/net/java/dev/jets3t/jets3t/0.6.1/jets3t-0.6.1.jar:/Users/seven/software/maven/repository/javax/servlet/servlet-api/2.5/servlet-api-2.5.jar:/Users/seven/software/maven/repository/hsqldb/hsqldb/1.8.0.10/hsqldb-1.8.0.10.jar:/Users/seven/software/maven/repository/org/eclipse/jdt/core/3.1.1/core-3.1.1.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-hdfs/2.6.0-cdh5.7.0/hadoop-hdfs-2.6.0-cdh5.7.0-tests.jar:/Users/seven/software/maven/repository/commons-daemon/commons-daemon/1.0.13/commons-daemon-1.0.13.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-lang3/3.5/commons-lang3-3.5.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-pool2/2.4.2/commons-pool2-2.4.2.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-mllib_2.11/2.2.0/spark-mllib_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-core_2.11/2.2.0/spark-core_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/avro/avro-mapred/1.7.7/avro-mapred-1.7.7-hadoop2.jar:/Users/seven/software/maven/repository/org/apache/avro/avro-ipc/1.7.7/avro-ipc-1.7.7-tests.jar:/Users/seven/software/maven/repository/com/twitter/chill_2.11/0.8.0/chill_2.11-0.8.0.jar:/Users/seven/software/maven/repository/com/esotericsoftware/kryo-shaded/3.0.3/kryo-shaded-3.0.3.jar:/Users/seven/software/maven/repository/com/esotericsoftware/minlog/1.3.0/minlog-1.3.0.jar:/Users/seven/software/maven/repository/org/objenesis/objenesis/2.1/objenesis-2.1.jar:/Users/seven/software/maven/repository/com/twitter/chill-java/0.8.0/chill-java-0.8.0.jar:/Users/seven/software/maven/repository/org/apache/xbean/xbean-asm5-shaded/4.4/xbean-asm5-shaded-4.4.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-launcher_2.11/2.2.0/spark-launcher_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-network-common_2.11/2.2.0/spark-network-common_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-network-shuffle_2.11/2.2.0/spark-network-shuffle_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-unsafe_2.11/2.2.0/spark-unsafe_2.11-2.2.0.jar:/Users/seven/software/maven/repository/javax/servlet/javax.servlet-api/3.1.0/javax.servlet-api-3.1.0.jar:/Users/seven/software/maven/repository/org/slf4j/jul-to-slf4j/1.7.16/jul-to-slf4j-1.7.16.jar:/Users/seven/software/maven/repository/org/slf4j/jcl-over-slf4j/1.7.16/jcl-over-slf4j-1.7.16.jar:/Users/seven/software/maven/repository/com/ning/compress-lzf/1.0.3/compress-lzf-1.0.3.jar:/Users/seven/software/maven/repository/org/xerial/snappy/snappy-java/1.1.2.6/snappy-java-1.1.2.6.jar:/Users/seven/software/maven/repository/org/roaringbitmap/RoaringBitmap/0.5.11/RoaringBitmap-0.5.11.jar:/Users/seven/software/maven/repository/org/json4s/json4s-jackson_2.11/3.2.11/json4s-jackson_2.11-3.2.11.jar:/Users/seven/software/maven/repository/org/json4s/json4s-core_2.11/3.2.11/json4s-core_2.11-3.2.11.jar:/Users/seven/software/maven/repository/org/json4s/json4s-ast_2.11/3.2.11/json4s-ast_2.11-3.2.11.jar:/Users/seven/software/maven/repository/org/scala-lang/scalap/2.11.0/scalap-2.11.0.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/core/jersey-client/2.22.2/jersey-client-2.22.2.jar:/Users/seven/software/maven/repository/javax/ws/rs/javax.ws.rs-api/2.0.1/javax.ws.rs-api-2.0.1.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/hk2-api/2.4.0-b34/hk2-api-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/hk2-utils/2.4.0-b34/hk2-utils-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/external/aopalliance-repackaged/2.4.0-b34/aopalliance-repackaged-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/external/javax.inject/2.4.0-b34/javax.inject-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/hk2-locator/2.4.0-b34/hk2-locator-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/javassist/javassist/3.18.1-GA/javassist-3.18.1-GA.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/core/jersey-common/2.22.2/jersey-common-2.22.2.jar:/Users/seven/software/maven/repository/javax/annotation/javax.annotation-api/1.2/javax.annotation-api-1.2.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/bundles/repackaged/jersey-guava/2.22.2/jersey-guava-2.22.2.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/osgi-resource-locator/1.0.1/osgi-resource-locator-1.0.1.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/core/jersey-server/2.22.2/jersey-server-2.22.2.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/media/jersey-media-jaxb/2.22.2/jersey-media-jaxb-2.22.2.jar:/Users/seven/software/maven/repository/javax/validation/validation-api/1.1.0.Final/validation-api-1.1.0.Final.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/containers/jersey-container-servlet/2.22.2/jersey-container-servlet-2.22.2.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/containers/jersey-container-servlet-core/2.22.2/jersey-container-servlet-core-2.22.2.jar:/Users/seven/software/maven/repository/com/clearspring/analytics/stream/2.7.0/stream-2.7.0.jar:/Users/seven/software/maven/repository/io/dropwizard/metrics/metrics-core/3.1.2/metrics-core-3.1.2.jar:/Users/seven/software/maven/repository/io/dropwizard/metrics/metrics-jvm/3.1.2/metrics-jvm-3.1.2.jar:/Users/seven/software/maven/repository/io/dropwizard/metrics/metrics-json/3.1.2/metrics-json-3.1.2.jar:/Users/seven/software/maven/repository/io/dropwizard/metrics/metrics-graphite/3.1.2/metrics-graphite-3.1.2.jar:/Users/seven/software/maven/repository/org/apache/ivy/ivy/2.4.0/ivy-2.4.0.jar:/Users/seven/software/maven/repository/oro/oro/2.0.8/oro-2.0.8.jar:/Users/seven/software/maven/repository/net/razorvine/pyrolite/4.13/pyrolite-4.13.jar:/Users/seven/software/maven/repository/net/sf/py4j/py4j/0.10.4/py4j-0.10.4.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-crypto/1.0.0/commons-crypto-1.0.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-streaming_2.11/2.2.0/spark-streaming_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-sql_2.11/2.2.0/spark-sql_2.11-2.2.0.jar:/Users/seven/software/maven/repository/com/univocity/univocity-parsers/2.2.1/univocity-parsers-2.2.1.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-sketch_2.11/2.2.0/spark-sketch_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-catalyst_2.11/2.2.0/spark-catalyst_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/codehaus/janino/janino/3.0.0/janino-3.0.0.jar:/Users/seven/software/maven/repository/org/codehaus/janino/commons-compiler/3.0.0/commons-compiler-3.0.0.jar:/Users/seven/software/maven/repository/org/antlr/antlr4-runtime/4.5.3/antlr4-runtime-4.5.3.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-column/1.8.2/parquet-column-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-common/1.8.2/parquet-common-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-encoding/1.8.2/parquet-encoding-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-hadoop/1.8.2/parquet-hadoop-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-format/2.3.1/parquet-format-2.3.1.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-jackson/1.8.2/parquet-jackson-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-graphx_2.11/2.2.0/spark-graphx_2.11-2.2.0.jar:/Users/seven/software/maven/repository/com/github/fommil/netlib/core/1.1.2/core-1.1.2.jar:/Users/seven/software/maven/repository/net/sourceforge/f2j/arpack_combined_all/0.1/arpack_combined_all-0.1.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-mllib-local_2.11/2.2.0/spark-mllib-local_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/scalanlp/breeze_2.11/0.13.1/breeze_2.11-0.13.1.jar:/Users/seven/software/maven/repository/org/scalanlp/breeze-macros_2.11/0.13.1/breeze-macros_2.11-0.13.1.jar:/Users/seven/software/maven/repository/net/sf/opencsv/opencsv/2.3/opencsv-2.3.jar:/Users/seven/software/maven/repository/com/github/rwl/jtransforms/2.4.0/jtransforms-2.4.0.jar:/Users/seven/software/maven/repository/org/spire-math/spire_2.11/0.13.0/spire_2.11-0.13.0.jar:/Users/seven/software/maven/repository/org/spire-math/spire-macros_2.11/0.13.0/spire-macros_2.11-0.13.0.jar:/Users/seven/software/maven/repository/org/typelevel/machinist_2.11/0.6.1/machinist_2.11-0.6.1.jar:/Users/seven/software/maven/repository/com/chuusai/shapeless_2.11/2.3.2/shapeless_2.11-2.3.2.jar:/Users/seven/software/maven/repository/org/typelevel/macro-compat_2.11/1.1.1/macro-compat_2.11-1.1.1.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-math3/3.4.1/commons-math3-3.4.1.jar:/Users/seven/software/maven/repository/org/jpmml/pmml-model/1.2.15/pmml-model-1.2.15.jar:/Users/seven/software/maven/repository/org/jpmml/pmml-schema/1.2.15/pmml-schema-1.2.15.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-tags_2.11/2.2.0/spark-tags_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-streaming-kafka-0-8_2.11/2.2.0/spark-streaming-kafka-0-8_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/kafka/kafka_2.11/0.8.2.1/kafka_2.11-0.8.2.1.jar:/Users/seven/software/maven/repository/com/101tec/zkclient/0.3/zkclient-0.3.jar:/Users/seven/software/maven/repository/org/apache/kafka/kafka-clients/0.8.2.1/kafka-clients-0.8.2.1.jar:/Users/seven/software/maven/repository/mysql/mysql-connector-java/5.1.31/mysql-connector-java-5.1.31.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/module/jackson-module-scala_2.11/2.6.5/jackson-module-scala_2.11-2.6.5.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/core/jackson-core/2.6.5/jackson-core-2.6.5.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/core/jackson-annotations/2.6.5/jackson-annotations-2.6.5.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/core/jackson-databind/2.6.5/jackson-databind-2.6.5.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/module/jackson-module-paranamer/2.6.5/jackson-module-paranamer-2.6.5.jar:/Users/seven/software/maven/repository/com/thoughtworks/paranamer/paranamer/2.6/paranamer-2.6.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-streaming-flume_2.11/2.2.0/spark-streaming-flume_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/flume/flume-ng-core/1.6.0/flume-ng-core-1.6.0.jar:/Users/seven/software/maven/repository/org/apache/flume/flume-ng-configuration/1.6.0/flume-ng-configuration-1.6.0.jar:/Users/seven/software/maven/repository/org/apache/avro/avro-ipc/1.7.4/avro-ipc-1.7.4.jar:/Users/seven/software/maven/repository/org/apache/velocity/velocity/1.7/velocity-1.7.jar:/Users/seven/software/maven/repository/joda-time/joda-time/2.1/joda-time-2.1.jar:/Users/seven/software/maven/repository/org/apache/mina/mina-core/2.0.4/mina-core-2.0.4.jar:/Users/seven/software/maven/repository/org/apache/flume/flume-ng-sdk/1.6.0/flume-ng-sdk-1.6.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-streaming-flume-sink_2.11/2.2.0/spark-streaming-flume-sink_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/flume/flume-ng-clients/flume-ng-log4jappender/1.6.0/flume-ng-log4jappender-1.6.0.jar:/Users/seven/software/maven/repository/net/jpountz/lz4/lz4/1.3.0/lz4-1.3.0.jar:/Applications/IntelliJ IDEA.app/Contents/lib/idea_rt.jar
[INFO][2018-05-24 20:29:15,849][org.apache.zookeeper.ZooKeeper]Client environment:java.library.path=/Users/seven/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:.
[INFO][2018-05-24 20:29:15,849][org.apache.zookeeper.ZooKeeper]Client environment:java.io.tmpdir=/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/
[INFO][2018-05-24 20:29:15,849][org.apache.zookeeper.ZooKeeper]Client environment:java.compiler=<NA>
[INFO][2018-05-24 20:29:15,849][org.apache.zookeeper.ZooKeeper]Client environment:os.name=Mac OS X
[INFO][2018-05-24 20:29:15,849][org.apache.zookeeper.ZooKeeper]Client environment:os.arch=x86_64
[INFO][2018-05-24 20:29:15,849][org.apache.zookeeper.ZooKeeper]Client environment:os.version=10.13.4
[INFO][2018-05-24 20:29:15,849][org.apache.zookeeper.ZooKeeper]Client environment:user.name=seven
[INFO][2018-05-24 20:29:15,850][org.apache.zookeeper.ZooKeeper]Client environment:user.home=/Users/seven
[INFO][2018-05-24 20:29:15,850][org.apache.zookeeper.ZooKeeper]Client environment:user.dir=/Users/seven/project/scala/dataMining
[INFO][2018-05-24 20:29:15,851][org.apache.zookeeper.ZooKeeper]Initiating client connection, connectString=vm-xaj-bigdata-da-d01:2181,vm-xaj-bigdata-da-d02:2181,vm-xaj-bigdata-da-d03:2181 sessionTimeout=60000 watcher=hconnection-0x361504d60x0, quorum=vm-xaj-bigdata-da-d01:2181,vm-xaj-bigdata-da-d02:2181,vm-xaj-bigdata-da-d03:2181, baseZNode=/hbase
[INFO][2018-05-24 20:29:20,061][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527164960000 ms
[INFO][2018-05-24 20:29:21,051][org.apache.spark.SparkContext]Running Spark version 2.2.0
[INFO][2018-05-24 20:29:21,839][org.apache.spark.SparkContext]Submitted application: SimulationKafkaSendOutData$
[INFO][2018-05-24 20:29:21,865][org.apache.spark.SecurityManager]Changing view acls to: seven
[INFO][2018-05-24 20:29:21,866][org.apache.spark.SecurityManager]Changing modify acls to: seven
[INFO][2018-05-24 20:29:21,867][org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO][2018-05-24 20:29:21,868][org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO][2018-05-24 20:29:21,868][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(seven); groups with view permissions: Set(); users  with modify permissions: Set(seven); groups with modify permissions: Set()
[INFO][2018-05-24 20:29:22,165][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 49904.
[INFO][2018-05-24 20:29:22,188][org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO][2018-05-24 20:29:22,203][org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO][2018-05-24 20:29:22,206][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2018-05-24 20:29:22,206][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
[INFO][2018-05-24 20:29:22,215][org.apache.spark.storage.DiskBlockManager]Created local directory at /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/blockmgr-97cf478f-6d94-4ba9-a362-be7ae7101bc2
[INFO][2018-05-24 20:29:22,233][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 912.3 MB
[INFO][2018-05-24 20:29:22,310][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO][2018-05-24 20:29:22,383][org.spark_project.jetty.util.log]Logging initialized @2585ms
[INFO][2018-05-24 20:29:22,464][org.spark_project.jetty.server.Server]jetty-9.3.z-SNAPSHOT
[INFO][2018-05-24 20:29:22,485][org.spark_project.jetty.server.Server]Started @2688ms
[WARN][2018-05-24 20:29:22,502][org.apache.spark.util.Utils]Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[INFO][2018-05-24 20:29:22,507][org.spark_project.jetty.server.AbstractConnector]Started ServerConnector@60ca0db6{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}
[INFO][2018-05-24 20:29:22,507][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4041.
[INFO][2018-05-24 20:29:22,529][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@180e6ac4{/jobs,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:29:22,530][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@c7a975a{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:29:22,530][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@757d6814{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:29:22,531][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4c168660{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:29:22,532][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@fd0e5b6{/stages,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:29:22,532][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@36b0fcd5{/stages/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:29:22,533][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@475835b1{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:29:22,534][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@77192705{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:29:22,534][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7e809b79{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:29:22,535][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@625e134e{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:29:22,535][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@89c10b7{/storage,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:29:22,536][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4fe89c24{/storage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:29:22,537][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3d08f3f5{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:29:22,537][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1a1da881{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:29:22,538][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7fd4acee{/environment,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:29:22,539][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6175619b{/environment/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:29:22,539][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@756cf158{/executors,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:29:22,540][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@751d3241{/executors/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:29:22,541][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@64337702{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:29:22,541][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@30ea8c23{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:29:22,546][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4e76dac{/static,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:29:22,547][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@f9d87b{/,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:29:22,548][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@26fb628{/api,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:29:22,549][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5ac86ba5{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:29:22,550][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2c9399a4{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:29:22,552][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://192.168.0.102:4041
[INFO][2018-05-24 20:29:22,640][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
[INFO][2018-05-24 20:29:22,658][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 49905.
[INFO][2018-05-24 20:29:22,658][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 192.168.0.102:49905
[INFO][2018-05-24 20:29:22,660][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2018-05-24 20:29:22,661][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 192.168.0.102, 49905, None)
[INFO][2018-05-24 20:29:22,664][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 192.168.0.102:49905 with 912.3 MB RAM, BlockManagerId(driver, 192.168.0.102, 49905, None)
[INFO][2018-05-24 20:29:22,667][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 192.168.0.102, 49905, None)
[INFO][2018-05-24 20:29:22,668][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 192.168.0.102, 49905, None)
[INFO][2018-05-24 20:29:22,824][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@405325cf{/metrics/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 20:29:23,429][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0 stored as values in memory (estimated size 228.1 KB, free 912.1 MB)
[INFO][2018-05-24 20:29:23,512][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0_piece0 stored as bytes in memory (estimated size 22.1 KB, free 912.1 MB)
[INFO][2018-05-24 20:29:23,515][org.apache.spark.storage.BlockManagerInfo]Added broadcast_0_piece0 in memory on 192.168.0.102:49905 (size: 22.1 KB, free: 912.3 MB)
[INFO][2018-05-24 20:29:23,519][org.apache.spark.SparkContext]Created broadcast 0 from textFile at SimulationKafkaSendOutData.scala:25
[INFO][2018-05-24 20:29:25,929][org.apache.zookeeper.ClientCnxn]Opening socket connection to server master/10.213.4.25:2181. Will not attempt to authenticate using SASL (unknown error)
[INFO][2018-05-24 20:29:25,941][org.apache.zookeeper.ClientCnxn]Socket connection established, initiating session, client: /10.0.8.39:49906, server: master/10.213.4.25:2181
[INFO][2018-05-24 20:29:25,971][org.apache.zookeeper.ClientCnxn]Session establishment complete on server master/10.213.4.25:2181, sessionid = 0x162b4dc56095ea7, negotiated timeout = 60000
[WARN][2018-05-24 20:29:26,422][org.apache.hadoop.util.NativeCodeLoader]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO][2018-05-24 20:29:26,498][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:29:26,507][org.apache.spark.executor.Executor]Finished task 0.0 in stage 0.0 (TID 0). 794 bytes result sent to driver
[INFO][2018-05-24 20:29:26,514][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 0.0 (TID 0) in 11041 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:29:26,515][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:29:26,518][org.apache.spark.scheduler.DAGScheduler]ResultStage 0 (foreachPartition at ReceiveKafkaData.scala:65) finished in 11.064 s
[INFO][2018-05-24 20:29:26,523][org.apache.spark.scheduler.DAGScheduler]Job 0 finished: foreachPartition at ReceiveKafkaData.scala:65, took 11.321840 s
[INFO][2018-05-24 20:29:26,527][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527164950000 ms.0 from job set of time 1527164950000 ms
[INFO][2018-05-24 20:29:26,528][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 16.526 s for time 1527164950000 ms (execution: 11.367 s)
[INFO][2018-05-24 20:29:26,528][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527164960000 ms.0 from job set of time 1527164960000 ms
[INFO][2018-05-24 20:29:26,534][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:29:26,534][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:65
[INFO][2018-05-24 20:29:26,536][org.apache.spark.scheduler.DAGScheduler]Got job 1 (foreachPartition at ReceiveKafkaData.scala:65) with 1 output partitions
[INFO][2018-05-24 20:29:26,536][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 1 (foreachPartition at ReceiveKafkaData.scala:65)
[INFO][2018-05-24 20:29:26,536][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:29:26,536][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:29:26,536][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 1 (MapPartitionsRDD[3] at map at ReceiveKafkaData.scala:63), which has no missing parents
[INFO][2018-05-24 20:29:26,538][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 
[INFO][2018-05-24 20:29:26,539][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 20:29:26,540][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1_piece0 stored as bytes in memory (estimated size 1878.0 B, free 912.3 MB)
[INFO][2018-05-24 20:29:26,541][org.apache.spark.storage.BlockManagerInfo]Added broadcast_1_piece0 in memory on 192.168.0.102:49839 (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:29:26,541][org.apache.spark.SparkContext]Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:29:26,542][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at map at ReceiveKafkaData.scala:63) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:29:26,542][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 1.0 with 1 tasks
[INFO][2018-05-24 20:29:26,543][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:29:26,543][org.apache.spark.executor.Executor]Running task 0.0 in stage 1.0 (TID 1)
[INFO][2018-05-24 20:29:26,561][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12174 is the same as ending offset skipping seven 0
[INFO][2018-05-24 20:29:26,562][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:29:26,565][org.apache.spark.executor.Executor]Finished task 0.0 in stage 1.0 (TID 1). 751 bytes result sent to driver
[INFO][2018-05-24 20:29:26,567][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 1.0 (TID 1) in 24 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:29:26,567][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:29:26,568][org.apache.spark.scheduler.DAGScheduler]ResultStage 1 (foreachPartition at ReceiveKafkaData.scala:65) finished in 0.026 s
[INFO][2018-05-24 20:29:26,568][org.apache.spark.scheduler.DAGScheduler]Job 1 finished: foreachPartition at ReceiveKafkaData.scala:65, took 0.033455 s
[INFO][2018-05-24 20:29:26,569][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527164960000 ms.0 from job set of time 1527164960000 ms
[INFO][2018-05-24 20:29:26,569][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 6.569 s for time 1527164960000 ms (execution: 0.041 s)
[INFO][2018-05-24 20:29:26,570][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 1 from persistence list
[INFO][2018-05-24 20:29:26,574][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 0 from persistence list
[INFO][2018-05-24 20:29:26,574][org.apache.spark.storage.BlockManager]Removing RDD 1
[INFO][2018-05-24 20:29:26,575][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:29:26,575][org.apache.spark.storage.BlockManager]Removing RDD 0
[INFO][2018-05-24 20:29:26,575][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 
[WARN][2018-05-24 20:29:28,935][org.apache.hadoop.util.NativeCodeLoader]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO][2018-05-24 20:29:29,087][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 1
[INFO][2018-05-24 20:29:29,172][org.apache.spark.SparkContext]Starting job: collect at SimulationKafkaSendOutData.scala:25
[INFO][2018-05-24 20:29:29,184][org.apache.spark.scheduler.DAGScheduler]Got job 0 (collect at SimulationKafkaSendOutData.scala:25) with 2 output partitions
[INFO][2018-05-24 20:29:29,184][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 0 (collect at SimulationKafkaSendOutData.scala:25)
[INFO][2018-05-24 20:29:29,185][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:29:29,186][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:29:29,193][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 0 (hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/* MapPartitionsRDD[1] at textFile at SimulationKafkaSendOutData.scala:25), which has no missing parents
[INFO][2018-05-24 20:29:29,210][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1 stored as values in memory (estimated size 3.3 KB, free 912.1 MB)
[INFO][2018-05-24 20:29:29,212][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1_piece0 stored as bytes in memory (estimated size 1969.0 B, free 912.1 MB)
[INFO][2018-05-24 20:29:29,213][org.apache.spark.storage.BlockManagerInfo]Added broadcast_1_piece0 in memory on 192.168.0.102:49905 (size: 1969.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:29:29,213][org.apache.spark.SparkContext]Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:29:29,232][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ResultStage 0 (hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/* MapPartitionsRDD[1] at textFile at SimulationKafkaSendOutData.scala:25) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2018-05-24 20:29:29,233][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 0.0 with 2 tasks
[INFO][2018-05-24 20:29:29,272][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 4883 bytes)
[INFO][2018-05-24 20:29:29,274][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 4883 bytes)
[INFO][2018-05-24 20:29:29,286][org.apache.spark.executor.Executor]Running task 0.0 in stage 0.0 (TID 0)
[INFO][2018-05-24 20:29:29,286][org.apache.spark.executor.Executor]Running task 1.0 in stage 0.0 (TID 1)
[INFO][2018-05-24 20:29:29,344][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/part-00000:10717435+10717436
[INFO][2018-05-24 20:29:29,346][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/part-00000:0+10717435
[INFO][2018-05-24 20:29:38,759][org.apache.spark.storage.memory.MemoryStore]Block taskresult_0 stored as bytes in memory (estimated size 10.4 MB, free 901.7 MB)
[INFO][2018-05-24 20:29:38,762][org.apache.spark.storage.BlockManagerInfo]Added taskresult_0 in memory on 192.168.0.102:49905 (size: 10.4 MB, free: 901.9 MB)
[INFO][2018-05-24 20:29:38,763][org.apache.spark.executor.Executor]Finished task 0.0 in stage 0.0 (TID 0). 10856349 bytes result sent via BlockManager)
[INFO][2018-05-24 20:29:38,806][org.apache.spark.network.client.TransportClientFactory]Successfully created connection to /192.168.0.102:49905 after 25 ms (0 ms spent in bootstraps)
[INFO][2018-05-24 20:29:39,204][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 0.0 (TID 0) in 9943 ms on localhost (executor driver) (1/2)
[INFO][2018-05-24 20:29:39,209][org.apache.spark.storage.BlockManagerInfo]Removed taskresult_0 on 192.168.0.102:49905 in memory (size: 10.4 MB, free: 912.3 MB)
[INFO][2018-05-24 20:29:39,394][org.apache.spark.storage.memory.MemoryStore]Block taskresult_1 stored as bytes in memory (estimated size 10.4 MB, free 901.7 MB)
[INFO][2018-05-24 20:29:39,395][org.apache.spark.storage.BlockManagerInfo]Added taskresult_1 in memory on 192.168.0.102:49905 (size: 10.4 MB, free: 901.9 MB)
[INFO][2018-05-24 20:29:39,395][org.apache.spark.executor.Executor]Finished task 1.0 in stage 0.0 (TID 1). 10855899 bytes result sent via BlockManager)
[INFO][2018-05-24 20:29:39,464][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 0.0 (TID 1) in 10190 ms on localhost (executor driver) (2/2)
[INFO][2018-05-24 20:29:39,464][org.apache.spark.storage.BlockManagerInfo]Removed taskresult_1 on 192.168.0.102:49905 in memory (size: 10.4 MB, free: 912.3 MB)
[INFO][2018-05-24 20:29:39,465][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:29:39,466][org.apache.spark.scheduler.DAGScheduler]ResultStage 0 (collect at SimulationKafkaSendOutData.scala:25) finished in 10.219 s
[INFO][2018-05-24 20:29:39,470][org.apache.spark.scheduler.DAGScheduler]Job 0 finished: collect at SimulationKafkaSendOutData.scala:25, took 10.297329 s
[INFO][2018-05-24 20:29:39,655][org.spark_project.jetty.server.AbstractConnector]Stopped Spark@60ca0db6{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}
[INFO][2018-05-24 20:29:39,658][org.apache.spark.ui.SparkUI]Stopped Spark web UI at http://192.168.0.102:4041
[INFO][2018-05-24 20:29:39,665][org.apache.spark.MapOutputTrackerMasterEndpoint]MapOutputTrackerMasterEndpoint stopped!
[INFO][2018-05-24 20:29:39,677][org.apache.spark.storage.memory.MemoryStore]MemoryStore cleared
[INFO][2018-05-24 20:29:39,677][org.apache.spark.storage.BlockManager]BlockManager stopped
[INFO][2018-05-24 20:29:39,678][org.apache.spark.storage.BlockManagerMaster]BlockManagerMaster stopped
[INFO][2018-05-24 20:29:39,680][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint]OutputCommitCoordinator stopped!
[INFO][2018-05-24 20:29:39,681][org.apache.spark.SparkContext]Successfully stopped SparkContext
[INFO][2018-05-24 20:29:39,690][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	compression.type = none
	metric.reporters = []
	metadata.max.age.ms = 300000
	metadata.fetch.timeout.ms = 60000
	acks = all
	batch.size = 16384
	reconnect.backoff.ms = 10
	bootstrap.servers = [vm-xaj-bigdata-da-d01:9092, vm-xaj-bigdata-da-d02:9092, vm-xaj-bigdata-da-d03:9092, vm-xaj-bigdata-da-d04:9092, vm-xaj-bigdata-da-d05:9092, vm-xaj-bigdata-da-d06:9092, vm-xaj-bigdata-da-d07:9092]
	receive.buffer.bytes = 32768
	retry.backoff.ms = 100
	buffer.memory = 33554432
	timeout.ms = 30000
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	retries = 0
	max.request.size = 1048576
	block.on.buffer.full = true
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
	metrics.sample.window.ms = 30000
	send.buffer.bytes = 131072
	max.in.flight.requests.per.connection = 5
	metrics.num.samples = 2
	linger.ms = 1
	client.id = 

[INFO][2018-05-24 20:29:40,098][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527164970000 ms
[INFO][2018-05-24 20:29:40,100][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527164970000 ms.0 from job set of time 1527164970000 ms
[INFO][2018-05-24 20:29:40,139][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:65
[INFO][2018-05-24 20:29:40,150][org.apache.spark.scheduler.DAGScheduler]Got job 2 (foreachPartition at ReceiveKafkaData.scala:65) with 1 output partitions
[INFO][2018-05-24 20:29:40,150][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 2 (foreachPartition at ReceiveKafkaData.scala:65)
[INFO][2018-05-24 20:29:40,150][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:29:40,151][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:29:40,153][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 2 (MapPartitionsRDD[5] at map at ReceiveKafkaData.scala:63), which has no missing parents
[INFO][2018-05-24 20:29:40,162][org.apache.spark.storage.memory.MemoryStore]Block broadcast_2 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 20:29:40,177][org.apache.spark.storage.memory.MemoryStore]Block broadcast_2_piece0 stored as bytes in memory (estimated size 1878.0 B, free 912.3 MB)
[INFO][2018-05-24 20:29:40,180][org.apache.spark.storage.BlockManagerInfo]Added broadcast_2_piece0 in memory on 192.168.0.102:49839 (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:29:40,181][org.apache.spark.SparkContext]Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:29:40,182][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[5] at map at ReceiveKafkaData.scala:63) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:29:40,183][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 2.0 with 1 tasks
[INFO][2018-05-24 20:29:40,185][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:29:40,186][org.apache.spark.executor.Executor]Running task 0.0 in stage 2.0 (TID 2)
[INFO][2018-05-24 20:29:40,194][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12174 is the same as ending offset skipping seven 0
[INFO][2018-05-24 20:29:40,195][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:29:40,198][org.apache.spark.executor.Executor]Finished task 0.0 in stage 2.0 (TID 2). 708 bytes result sent to driver
[INFO][2018-05-24 20:29:40,200][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 2.0 (TID 2) in 16 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:29:40,200][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:29:40,201][org.apache.spark.scheduler.DAGScheduler]ResultStage 2 (foreachPartition at ReceiveKafkaData.scala:65) finished in 0.017 s
[INFO][2018-05-24 20:29:40,202][org.apache.spark.scheduler.DAGScheduler]Job 2 finished: foreachPartition at ReceiveKafkaData.scala:65, took 0.062815 s
[INFO][2018-05-24 20:29:40,203][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527164970000 ms.0 from job set of time 1527164970000 ms
[INFO][2018-05-24 20:29:40,203][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 10.203 s for time 1527164970000 ms (execution: 0.103 s)
[INFO][2018-05-24 20:29:45,173][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527164980000 ms
[INFO][2018-05-24 20:29:45,175][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 3 from persistence list
[INFO][2018-05-24 20:29:45,180][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527164980000 ms.0 from job set of time 1527164980000 ms
[INFO][2018-05-24 20:29:45,180][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 2 from persistence list
[INFO][2018-05-24 20:29:45,180][org.apache.spark.storage.BlockManager]Removing RDD 3
[INFO][2018-05-24 20:29:45,182][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:29:45,182][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527164950000 ms
[INFO][2018-05-24 20:29:45,182][org.apache.spark.storage.BlockManager]Removing RDD 2
[INFO][2018-05-24 20:29:45,187][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:65
[INFO][2018-05-24 20:29:45,187][org.apache.spark.scheduler.DAGScheduler]Got job 3 (foreachPartition at ReceiveKafkaData.scala:65) with 1 output partitions
[INFO][2018-05-24 20:29:45,187][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 3 (foreachPartition at ReceiveKafkaData.scala:65)
[INFO][2018-05-24 20:29:45,187][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:29:45,188][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:29:45,188][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 3 (MapPartitionsRDD[7] at map at ReceiveKafkaData.scala:63), which has no missing parents
[INFO][2018-05-24 20:29:45,190][org.apache.spark.storage.memory.MemoryStore]Block broadcast_3 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 20:29:45,199][org.apache.spark.storage.memory.MemoryStore]Block broadcast_3_piece0 stored as bytes in memory (estimated size 1878.0 B, free 912.3 MB)
[INFO][2018-05-24 20:29:45,202][org.apache.spark.storage.BlockManagerInfo]Added broadcast_3_piece0 in memory on 192.168.0.102:49839 (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:29:45,203][org.apache.spark.SparkContext]Created broadcast 3 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:29:45,204][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[7] at map at ReceiveKafkaData.scala:63) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:29:45,204][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 3.0 with 1 tasks
[INFO][2018-05-24 20:29:45,205][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 3.0 (TID 3, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:29:45,205][org.apache.spark.executor.Executor]Running task 0.0 in stage 3.0 (TID 3)
[INFO][2018-05-24 20:29:45,209][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12174 is the same as ending offset skipping seven 0
[INFO][2018-05-24 20:29:45,209][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:29:45,210][org.apache.spark.executor.Executor]Finished task 0.0 in stage 3.0 (TID 3). 708 bytes result sent to driver
[INFO][2018-05-24 20:29:45,211][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 3.0 (TID 3) in 6 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:29:45,211][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 3.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:29:45,212][org.apache.spark.scheduler.DAGScheduler]ResultStage 3 (foreachPartition at ReceiveKafkaData.scala:65) finished in 0.007 s
[INFO][2018-05-24 20:29:45,212][org.apache.spark.scheduler.DAGScheduler]Job 3 finished: foreachPartition at ReceiveKafkaData.scala:65, took 0.025134 s
[INFO][2018-05-24 20:29:45,212][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527164980000 ms.0 from job set of time 1527164980000 ms
[INFO][2018-05-24 20:29:45,213][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 5 from persistence list
[INFO][2018-05-24 20:29:45,213][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 5.212 s for time 1527164980000 ms (execution: 0.032 s)
[INFO][2018-05-24 20:29:45,213][org.apache.spark.storage.BlockManager]Removing RDD 5
[INFO][2018-05-24 20:29:45,213][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 4 from persistence list
[INFO][2018-05-24 20:29:45,214][org.apache.spark.storage.BlockManager]Removing RDD 4
[INFO][2018-05-24 20:29:45,214][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:29:45,214][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527164960000 ms
[INFO][2018-05-24 20:29:55,074][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527164990000 ms
[INFO][2018-05-24 20:29:55,074][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527164990000 ms.0 from job set of time 1527164990000 ms
[INFO][2018-05-24 20:29:55,081][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:65
[INFO][2018-05-24 20:29:55,082][org.apache.spark.scheduler.DAGScheduler]Got job 4 (foreachPartition at ReceiveKafkaData.scala:65) with 1 output partitions
[INFO][2018-05-24 20:29:55,082][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 4 (foreachPartition at ReceiveKafkaData.scala:65)
[INFO][2018-05-24 20:29:55,082][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:29:55,082][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:29:55,083][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 4 (MapPartitionsRDD[9] at map at ReceiveKafkaData.scala:63), which has no missing parents
[INFO][2018-05-24 20:29:55,085][org.apache.spark.storage.memory.MemoryStore]Block broadcast_4 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 20:29:55,087][org.apache.spark.storage.memory.MemoryStore]Block broadcast_4_piece0 stored as bytes in memory (estimated size 1878.0 B, free 912.3 MB)
[INFO][2018-05-24 20:29:55,088][org.apache.spark.storage.BlockManagerInfo]Added broadcast_4_piece0 in memory on 192.168.0.102:49839 (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:29:55,089][org.apache.spark.SparkContext]Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:29:55,090][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[9] at map at ReceiveKafkaData.scala:63) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:29:55,090][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 4.0 with 1 tasks
[INFO][2018-05-24 20:29:55,091][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 4.0 (TID 4, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:29:55,092][org.apache.spark.executor.Executor]Running task 0.0 in stage 4.0 (TID 4)
[INFO][2018-05-24 20:29:55,095][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12174 is the same as ending offset skipping seven 0
[INFO][2018-05-24 20:29:55,095][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:29:55,096][org.apache.spark.executor.Executor]Finished task 0.0 in stage 4.0 (TID 4). 708 bytes result sent to driver
[INFO][2018-05-24 20:29:55,097][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 4.0 (TID 4) in 5 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:29:55,097][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 4.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:29:55,097][org.apache.spark.scheduler.DAGScheduler]ResultStage 4 (foreachPartition at ReceiveKafkaData.scala:65) finished in 0.006 s
[INFO][2018-05-24 20:29:55,097][org.apache.spark.scheduler.DAGScheduler]Job 4 finished: foreachPartition at ReceiveKafkaData.scala:65, took 0.016353 s
[INFO][2018-05-24 20:29:55,098][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527164990000 ms.0 from job set of time 1527164990000 ms
[INFO][2018-05-24 20:29:55,098][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 7 from persistence list
[INFO][2018-05-24 20:29:55,098][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 5.098 s for time 1527164990000 ms (execution: 0.024 s)
[INFO][2018-05-24 20:29:55,098][org.apache.spark.storage.BlockManager]Removing RDD 7
[INFO][2018-05-24 20:29:55,098][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 6 from persistence list
[INFO][2018-05-24 20:29:55,099][org.apache.spark.storage.BlockManager]Removing RDD 6
[INFO][2018-05-24 20:29:55,099][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:29:55,100][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527164970000 ms
[INFO][2018-05-24 20:30:05,098][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527165000000 ms
[INFO][2018-05-24 20:30:05,106][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:65
[INFO][2018-05-24 20:30:05,099][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527165000000 ms.0 from job set of time 1527165000000 ms
[INFO][2018-05-24 20:30:05,108][org.apache.spark.scheduler.DAGScheduler]Got job 5 (foreachPartition at ReceiveKafkaData.scala:65) with 1 output partitions
[INFO][2018-05-24 20:30:05,108][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 5 (foreachPartition at ReceiveKafkaData.scala:65)
[INFO][2018-05-24 20:30:05,108][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:30:05,108][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:30:05,108][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 5 (MapPartitionsRDD[11] at map at ReceiveKafkaData.scala:63), which has no missing parents
[INFO][2018-05-24 20:30:05,110][org.apache.spark.storage.memory.MemoryStore]Block broadcast_5 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 20:30:05,112][org.apache.spark.storage.memory.MemoryStore]Block broadcast_5_piece0 stored as bytes in memory (estimated size 1878.0 B, free 912.3 MB)
[INFO][2018-05-24 20:30:05,112][org.apache.spark.storage.BlockManagerInfo]Added broadcast_5_piece0 in memory on 192.168.0.102:49839 (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:30:05,113][org.apache.spark.SparkContext]Created broadcast 5 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:30:05,114][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[11] at map at ReceiveKafkaData.scala:63) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:30:05,114][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 5.0 with 1 tasks
[INFO][2018-05-24 20:30:05,114][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 5.0 (TID 5, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:30:05,115][org.apache.spark.executor.Executor]Running task 0.0 in stage 5.0 (TID 5)
[INFO][2018-05-24 20:30:05,117][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12174 is the same as ending offset skipping seven 0
[INFO][2018-05-24 20:30:05,118][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:30:05,119][org.apache.spark.executor.Executor]Finished task 0.0 in stage 5.0 (TID 5). 708 bytes result sent to driver
[INFO][2018-05-24 20:30:05,120][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 5.0 (TID 5) in 6 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:30:05,120][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 5.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:30:05,120][org.apache.spark.scheduler.DAGScheduler]ResultStage 5 (foreachPartition at ReceiveKafkaData.scala:65) finished in 0.006 s
[INFO][2018-05-24 20:30:05,121][org.apache.spark.scheduler.DAGScheduler]Job 5 finished: foreachPartition at ReceiveKafkaData.scala:65, took 0.013930 s
[INFO][2018-05-24 20:30:05,122][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527165000000 ms.0 from job set of time 1527165000000 ms
[INFO][2018-05-24 20:30:05,122][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 9 from persistence list
[INFO][2018-05-24 20:30:05,122][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 5.122 s for time 1527165000000 ms (execution: 0.023 s)
[INFO][2018-05-24 20:30:05,122][org.apache.spark.storage.BlockManager]Removing RDD 9
[INFO][2018-05-24 20:30:05,122][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 8 from persistence list
[INFO][2018-05-24 20:30:05,123][org.apache.spark.storage.BlockManager]Removing RDD 8
[INFO][2018-05-24 20:30:05,123][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:30:05,123][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527164980000 ms
[INFO][2018-05-24 20:30:10,055][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527165010000 ms
[INFO][2018-05-24 20:30:10,056][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527165010000 ms.0 from job set of time 1527165010000 ms
[INFO][2018-05-24 20:30:10,062][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:65
[INFO][2018-05-24 20:30:10,063][org.apache.spark.scheduler.DAGScheduler]Got job 6 (foreachPartition at ReceiveKafkaData.scala:65) with 1 output partitions
[INFO][2018-05-24 20:30:10,063][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 6 (foreachPartition at ReceiveKafkaData.scala:65)
[INFO][2018-05-24 20:30:10,063][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:30:10,063][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:30:10,064][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 6 (MapPartitionsRDD[13] at map at ReceiveKafkaData.scala:63), which has no missing parents
[INFO][2018-05-24 20:30:10,066][org.apache.spark.storage.memory.MemoryStore]Block broadcast_6 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 20:30:10,067][org.apache.spark.storage.memory.MemoryStore]Block broadcast_6_piece0 stored as bytes in memory (estimated size 1878.0 B, free 912.3 MB)
[INFO][2018-05-24 20:30:10,068][org.apache.spark.storage.BlockManagerInfo]Added broadcast_6_piece0 in memory on 192.168.0.102:49839 (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:30:10,068][org.apache.spark.SparkContext]Created broadcast 6 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:30:10,069][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[13] at map at ReceiveKafkaData.scala:63) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:30:10,069][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 6.0 with 1 tasks
[INFO][2018-05-24 20:30:10,070][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 6.0 (TID 6, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:30:10,070][org.apache.spark.executor.Executor]Running task 0.0 in stage 6.0 (TID 6)
[INFO][2018-05-24 20:30:10,078][org.apache.spark.streaming.kafka.KafkaRDD]Computing topic seven, partition 0 offsets 12174 -> 12175
[INFO][2018-05-24 20:30:10,078][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 20:30:10,078][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 20:30:10,078][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 20:30:15,437][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_1_piece0 on 192.168.0.102:49839 in memory (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:30:15,441][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_4_piece0 on 192.168.0.102:49839 in memory (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:30:15,443][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_5_piece0 on 192.168.0.102:49839 in memory (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:30:15,444][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_2_piece0 on 192.168.0.102:49839 in memory (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:30:15,445][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_3_piece0 on 192.168.0.102:49839 in memory (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:30:15,619][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:30:15,620][org.apache.spark.executor.Executor]Finished task 0.0 in stage 6.0 (TID 6). 751 bytes result sent to driver
[INFO][2018-05-24 20:30:15,621][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 6.0 (TID 6) in 5552 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:30:15,621][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 6.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:30:15,622][org.apache.spark.scheduler.DAGScheduler]ResultStage 6 (foreachPartition at ReceiveKafkaData.scala:65) finished in 5.553 s
[INFO][2018-05-24 20:30:15,622][org.apache.spark.scheduler.DAGScheduler]Job 6 finished: foreachPartition at ReceiveKafkaData.scala:65, took 5.559862 s
[INFO][2018-05-24 20:30:15,623][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527165010000 ms.0 from job set of time 1527165010000 ms
[INFO][2018-05-24 20:30:15,624][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 5.623 s for time 1527165010000 ms (execution: 5.567 s)
[INFO][2018-05-24 20:30:15,624][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 11 from persistence list
[INFO][2018-05-24 20:30:15,624][org.apache.spark.storage.BlockManager]Removing RDD 11
[INFO][2018-05-24 20:30:15,624][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 10 from persistence list
[INFO][2018-05-24 20:30:15,625][org.apache.spark.storage.BlockManager]Removing RDD 10
[INFO][2018-05-24 20:30:15,625][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:30:15,626][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527164990000 ms
[INFO][2018-05-24 20:30:20,063][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527165020000 ms
[INFO][2018-05-24 20:30:20,064][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527165020000 ms.0 from job set of time 1527165020000 ms
[INFO][2018-05-24 20:30:20,074][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:65
[INFO][2018-05-24 20:30:20,075][org.apache.spark.scheduler.DAGScheduler]Got job 7 (foreachPartition at ReceiveKafkaData.scala:65) with 1 output partitions
[INFO][2018-05-24 20:30:20,076][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 7 (foreachPartition at ReceiveKafkaData.scala:65)
[INFO][2018-05-24 20:30:20,076][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:30:20,076][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:30:20,076][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 7 (MapPartitionsRDD[15] at map at ReceiveKafkaData.scala:63), which has no missing parents
[INFO][2018-05-24 20:30:20,079][org.apache.spark.storage.memory.MemoryStore]Block broadcast_7 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 20:30:20,083][org.apache.spark.storage.memory.MemoryStore]Block broadcast_7_piece0 stored as bytes in memory (estimated size 1878.0 B, free 912.3 MB)
[INFO][2018-05-24 20:30:20,084][org.apache.spark.storage.BlockManagerInfo]Added broadcast_7_piece0 in memory on 192.168.0.102:49839 (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:30:20,085][org.apache.spark.SparkContext]Created broadcast 7 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:30:20,085][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[15] at map at ReceiveKafkaData.scala:63) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:30:20,086][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 7.0 with 1 tasks
[INFO][2018-05-24 20:30:20,087][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 7.0 (TID 7, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:30:20,087][org.apache.spark.executor.Executor]Running task 0.0 in stage 7.0 (TID 7)
[INFO][2018-05-24 20:30:20,091][org.apache.spark.streaming.kafka.KafkaRDD]Computing topic seven, partition 0 offsets 12175 -> 12179
[INFO][2018-05-24 20:30:20,091][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 20:30:20,092][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 20:30:20,092][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 20:30:20,225][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:30:20,227][org.apache.spark.executor.Executor]Finished task 0.0 in stage 7.0 (TID 7). 708 bytes result sent to driver
[INFO][2018-05-24 20:30:20,228][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 7.0 (TID 7) in 142 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:30:20,228][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 7.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:30:20,229][org.apache.spark.scheduler.DAGScheduler]ResultStage 7 (foreachPartition at ReceiveKafkaData.scala:65) finished in 0.143 s
[INFO][2018-05-24 20:30:20,229][org.apache.spark.scheduler.DAGScheduler]Job 7 finished: foreachPartition at ReceiveKafkaData.scala:65, took 0.154707 s
[INFO][2018-05-24 20:30:20,230][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527165020000 ms.0 from job set of time 1527165020000 ms
[INFO][2018-05-24 20:30:20,230][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 13 from persistence list
[INFO][2018-05-24 20:30:20,230][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.230 s for time 1527165020000 ms (execution: 0.166 s)
[INFO][2018-05-24 20:30:20,231][org.apache.spark.storage.BlockManager]Removing RDD 13
[INFO][2018-05-24 20:30:20,231][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 12 from persistence list
[INFO][2018-05-24 20:30:20,231][org.apache.spark.storage.BlockManager]Removing RDD 12
[INFO][2018-05-24 20:30:20,231][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:30:20,231][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527165000000 ms
[INFO][2018-05-24 20:30:35,026][org.apache.spark.util.ShutdownHookManager]Shutdown hook called
[INFO][2018-05-24 20:30:35,029][org.apache.spark.util.ShutdownHookManager]Deleting directory /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/spark-c88eeae2-3280-4283-aef7-9436a78dfcb8
[INFO][2018-05-24 20:30:35,067][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527165030000 ms
[INFO][2018-05-24 20:30:35,067][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527165030000 ms.0 from job set of time 1527165030000 ms
[INFO][2018-05-24 20:30:35,075][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:65
[INFO][2018-05-24 20:30:35,075][org.apache.spark.scheduler.DAGScheduler]Got job 8 (foreachPartition at ReceiveKafkaData.scala:65) with 1 output partitions
[INFO][2018-05-24 20:30:35,076][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 8 (foreachPartition at ReceiveKafkaData.scala:65)
[INFO][2018-05-24 20:30:35,076][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:30:35,076][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:30:35,076][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 8 (MapPartitionsRDD[17] at map at ReceiveKafkaData.scala:63), which has no missing parents
[INFO][2018-05-24 20:30:35,078][org.apache.spark.storage.memory.MemoryStore]Block broadcast_8 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 20:30:35,083][org.apache.spark.storage.memory.MemoryStore]Block broadcast_8_piece0 stored as bytes in memory (estimated size 1878.0 B, free 912.3 MB)
[INFO][2018-05-24 20:30:35,084][org.apache.spark.storage.BlockManagerInfo]Added broadcast_8_piece0 in memory on 192.168.0.102:49839 (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:30:35,084][org.apache.spark.SparkContext]Created broadcast 8 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:30:35,085][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[17] at map at ReceiveKafkaData.scala:63) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:30:35,085][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 8.0 with 1 tasks
[INFO][2018-05-24 20:30:35,086][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 8.0 (TID 8, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:30:35,087][org.apache.spark.executor.Executor]Running task 0.0 in stage 8.0 (TID 8)
[INFO][2018-05-24 20:30:35,089][org.apache.spark.streaming.kafka.KafkaRDD]Computing topic seven, partition 0 offsets 12179 -> 12184
[INFO][2018-05-24 20:30:35,089][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 20:30:35,090][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 20:30:35,090][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 20:30:40,192][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:30:40,194][org.apache.spark.executor.Executor]Finished task 0.0 in stage 8.0 (TID 8). 708 bytes result sent to driver
[INFO][2018-05-24 20:30:40,195][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 8.0 (TID 8) in 5108 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:30:40,195][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 8.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:30:40,197][org.apache.spark.scheduler.DAGScheduler]ResultStage 8 (foreachPartition at ReceiveKafkaData.scala:65) finished in 5.111 s
[INFO][2018-05-24 20:30:40,199][org.apache.spark.scheduler.DAGScheduler]Job 8 finished: foreachPartition at ReceiveKafkaData.scala:65, took 5.123408 s
[INFO][2018-05-24 20:30:40,200][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527165030000 ms.0 from job set of time 1527165030000 ms
[INFO][2018-05-24 20:30:40,201][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 10.200 s for time 1527165030000 ms (execution: 5.133 s)
[INFO][2018-05-24 20:30:45,060][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527165040000 ms
[INFO][2018-05-24 20:30:45,061][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 15 from persistence list
[INFO][2018-05-24 20:30:45,061][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527165040000 ms.0 from job set of time 1527165040000 ms
[INFO][2018-05-24 20:30:45,062][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 14 from persistence list
[INFO][2018-05-24 20:30:45,062][org.apache.spark.storage.BlockManager]Removing RDD 15
[INFO][2018-05-24 20:30:45,063][org.apache.spark.storage.BlockManager]Removing RDD 14
[INFO][2018-05-24 20:30:45,063][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:30:45,064][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527165010000 ms
[INFO][2018-05-24 20:30:45,070][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:65
[INFO][2018-05-24 20:30:45,071][org.apache.spark.scheduler.DAGScheduler]Got job 9 (foreachPartition at ReceiveKafkaData.scala:65) with 1 output partitions
[INFO][2018-05-24 20:30:45,071][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 9 (foreachPartition at ReceiveKafkaData.scala:65)
[INFO][2018-05-24 20:30:45,071][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:30:45,071][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:30:45,072][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 9 (MapPartitionsRDD[19] at map at ReceiveKafkaData.scala:63), which has no missing parents
[INFO][2018-05-24 20:30:45,074][org.apache.spark.storage.memory.MemoryStore]Block broadcast_9 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 20:30:45,081][org.apache.spark.storage.memory.MemoryStore]Block broadcast_9_piece0 stored as bytes in memory (estimated size 1878.0 B, free 912.3 MB)
[INFO][2018-05-24 20:30:45,082][org.apache.spark.storage.BlockManagerInfo]Added broadcast_9_piece0 in memory on 192.168.0.102:49839 (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:30:45,083][org.apache.spark.SparkContext]Created broadcast 9 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:30:45,084][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[19] at map at ReceiveKafkaData.scala:63) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:30:45,084][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 9.0 with 1 tasks
[INFO][2018-05-24 20:30:45,087][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 9.0 (TID 9, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:30:45,088][org.apache.spark.executor.Executor]Running task 0.0 in stage 9.0 (TID 9)
[INFO][2018-05-24 20:30:45,090][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12184 is the same as ending offset skipping seven 0
[INFO][2018-05-24 20:30:45,091][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:30:45,092][org.apache.spark.executor.Executor]Finished task 0.0 in stage 9.0 (TID 9). 708 bytes result sent to driver
[INFO][2018-05-24 20:30:45,092][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 9.0 (TID 9) in 5 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:30:45,093][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 9.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:30:45,093][org.apache.spark.scheduler.DAGScheduler]ResultStage 9 (foreachPartition at ReceiveKafkaData.scala:65) finished in 0.008 s
[INFO][2018-05-24 20:30:45,094][org.apache.spark.scheduler.DAGScheduler]Job 9 finished: foreachPartition at ReceiveKafkaData.scala:65, took 0.023504 s
[INFO][2018-05-24 20:30:45,094][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527165040000 ms.0 from job set of time 1527165040000 ms
[INFO][2018-05-24 20:30:45,095][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 5.094 s for time 1527165040000 ms (execution: 0.033 s)
[INFO][2018-05-24 20:30:45,095][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 17 from persistence list
[INFO][2018-05-24 20:30:45,095][org.apache.spark.storage.BlockManager]Removing RDD 17
[INFO][2018-05-24 20:30:45,095][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 16 from persistence list
[INFO][2018-05-24 20:30:45,096][org.apache.spark.storage.BlockManager]Removing RDD 16
[INFO][2018-05-24 20:30:45,096][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:30:45,096][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527165020000 ms
[INFO][2018-05-24 20:30:55,067][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527165050000 ms
[INFO][2018-05-24 20:30:55,068][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527165050000 ms.0 from job set of time 1527165050000 ms
[INFO][2018-05-24 20:30:55,074][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:65
[INFO][2018-05-24 20:30:55,075][org.apache.spark.scheduler.DAGScheduler]Got job 10 (foreachPartition at ReceiveKafkaData.scala:65) with 1 output partitions
[INFO][2018-05-24 20:30:55,075][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 10 (foreachPartition at ReceiveKafkaData.scala:65)
[INFO][2018-05-24 20:30:55,075][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:30:55,075][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:30:55,076][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 10 (MapPartitionsRDD[21] at map at ReceiveKafkaData.scala:63), which has no missing parents
[INFO][2018-05-24 20:30:55,078][org.apache.spark.storage.memory.MemoryStore]Block broadcast_10 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 20:30:55,083][org.apache.spark.storage.memory.MemoryStore]Block broadcast_10_piece0 stored as bytes in memory (estimated size 1878.0 B, free 912.3 MB)
[INFO][2018-05-24 20:30:55,084][org.apache.spark.storage.BlockManagerInfo]Added broadcast_10_piece0 in memory on 192.168.0.102:49839 (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:30:55,084][org.apache.spark.SparkContext]Created broadcast 10 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:30:55,085][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[21] at map at ReceiveKafkaData.scala:63) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:30:55,085][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 10.0 with 1 tasks
[INFO][2018-05-24 20:30:55,086][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 10.0 (TID 10, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:30:55,086][org.apache.spark.executor.Executor]Running task 0.0 in stage 10.0 (TID 10)
[INFO][2018-05-24 20:30:55,088][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12184 is the same as ending offset skipping seven 0
[INFO][2018-05-24 20:30:55,088][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:30:55,089][org.apache.spark.executor.Executor]Finished task 0.0 in stage 10.0 (TID 10). 708 bytes result sent to driver
[INFO][2018-05-24 20:30:55,090][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 10.0 (TID 10) in 5 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:30:55,090][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 10.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:30:55,091][org.apache.spark.scheduler.DAGScheduler]ResultStage 10 (foreachPartition at ReceiveKafkaData.scala:65) finished in 0.006 s
[INFO][2018-05-24 20:30:55,091][org.apache.spark.scheduler.DAGScheduler]Job 10 finished: foreachPartition at ReceiveKafkaData.scala:65, took 0.016832 s
[INFO][2018-05-24 20:30:55,092][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527165050000 ms.0 from job set of time 1527165050000 ms
[INFO][2018-05-24 20:30:55,093][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 19 from persistence list
[INFO][2018-05-24 20:30:55,093][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 5.092 s for time 1527165050000 ms (execution: 0.024 s)
[INFO][2018-05-24 20:30:55,093][org.apache.spark.storage.BlockManager]Removing RDD 19
[INFO][2018-05-24 20:30:55,093][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 18 from persistence list
[INFO][2018-05-24 20:30:55,094][org.apache.spark.storage.BlockManager]Removing RDD 18
[INFO][2018-05-24 20:30:55,095][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:30:55,095][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527165030000 ms
[INFO][2018-05-24 20:31:00,137][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527165060000 ms
[INFO][2018-05-24 20:31:00,138][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527165060000 ms.0 from job set of time 1527165060000 ms
[INFO][2018-05-24 20:31:00,144][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:65
[INFO][2018-05-24 20:31:00,145][org.apache.spark.scheduler.DAGScheduler]Got job 11 (foreachPartition at ReceiveKafkaData.scala:65) with 1 output partitions
[INFO][2018-05-24 20:31:00,145][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 11 (foreachPartition at ReceiveKafkaData.scala:65)
[INFO][2018-05-24 20:31:00,145][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:31:00,145][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:31:00,146][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 11 (MapPartitionsRDD[23] at map at ReceiveKafkaData.scala:63), which has no missing parents
[INFO][2018-05-24 20:31:00,147][org.apache.spark.storage.memory.MemoryStore]Block broadcast_11 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 20:31:00,151][org.apache.spark.storage.memory.MemoryStore]Block broadcast_11_piece0 stored as bytes in memory (estimated size 1878.0 B, free 912.3 MB)
[INFO][2018-05-24 20:31:00,152][org.apache.spark.storage.BlockManagerInfo]Added broadcast_11_piece0 in memory on 192.168.0.102:49839 (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:31:00,152][org.apache.spark.SparkContext]Created broadcast 11 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:31:00,153][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[23] at map at ReceiveKafkaData.scala:63) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:31:00,153][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 11.0 with 1 tasks
[INFO][2018-05-24 20:31:00,154][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 11.0 (TID 11, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:31:00,154][org.apache.spark.executor.Executor]Running task 0.0 in stage 11.0 (TID 11)
[INFO][2018-05-24 20:31:00,156][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12184 is the same as ending offset skipping seven 0
[INFO][2018-05-24 20:31:00,156][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:31:00,157][org.apache.spark.executor.Executor]Finished task 0.0 in stage 11.0 (TID 11). 665 bytes result sent to driver
[INFO][2018-05-24 20:31:00,157][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 11.0 (TID 11) in 3 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:31:00,158][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 11.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:31:00,158][org.apache.spark.scheduler.DAGScheduler]ResultStage 11 (foreachPartition at ReceiveKafkaData.scala:65) finished in 0.004 s
[INFO][2018-05-24 20:31:00,158][org.apache.spark.scheduler.DAGScheduler]Job 11 finished: foreachPartition at ReceiveKafkaData.scala:65, took 0.013833 s
[INFO][2018-05-24 20:31:00,159][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527165060000 ms.0 from job set of time 1527165060000 ms
[INFO][2018-05-24 20:31:00,159][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 21 from persistence list
[INFO][2018-05-24 20:31:00,159][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.159 s for time 1527165060000 ms (execution: 0.021 s)
[INFO][2018-05-24 20:31:00,160][org.apache.spark.storage.BlockManager]Removing RDD 21
[INFO][2018-05-24 20:31:00,160][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 20 from persistence list
[INFO][2018-05-24 20:31:00,160][org.apache.spark.storage.BlockManager]Removing RDD 20
[INFO][2018-05-24 20:31:00,161][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:31:00,161][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527165040000 ms
[INFO][2018-05-24 20:31:15,068][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527165070000 ms
[INFO][2018-05-24 20:31:15,069][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527165070000 ms.0 from job set of time 1527165070000 ms
[INFO][2018-05-24 20:31:15,076][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:65
[INFO][2018-05-24 20:31:15,077][org.apache.spark.scheduler.DAGScheduler]Got job 12 (foreachPartition at ReceiveKafkaData.scala:65) with 1 output partitions
[INFO][2018-05-24 20:31:15,077][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 12 (foreachPartition at ReceiveKafkaData.scala:65)
[INFO][2018-05-24 20:31:15,077][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:31:15,077][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:31:15,077][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 12 (MapPartitionsRDD[25] at map at ReceiveKafkaData.scala:63), which has no missing parents
[INFO][2018-05-24 20:31:15,079][org.apache.spark.storage.memory.MemoryStore]Block broadcast_12 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 20:31:15,084][org.apache.spark.storage.memory.MemoryStore]Block broadcast_12_piece0 stored as bytes in memory (estimated size 1878.0 B, free 912.3 MB)
[INFO][2018-05-24 20:31:15,085][org.apache.spark.storage.BlockManagerInfo]Added broadcast_12_piece0 in memory on 192.168.0.102:49839 (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:31:15,086][org.apache.spark.SparkContext]Created broadcast 12 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:31:15,086][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[25] at map at ReceiveKafkaData.scala:63) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:31:15,086][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 12.0 with 1 tasks
[INFO][2018-05-24 20:31:15,087][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 12.0 (TID 12, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:31:15,088][org.apache.spark.executor.Executor]Running task 0.0 in stage 12.0 (TID 12)
[INFO][2018-05-24 20:31:15,090][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12184 is the same as ending offset skipping seven 0
[INFO][2018-05-24 20:31:15,090][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:31:15,091][org.apache.spark.executor.Executor]Finished task 0.0 in stage 12.0 (TID 12). 708 bytes result sent to driver
[INFO][2018-05-24 20:31:15,091][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 12.0 (TID 12) in 4 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:31:15,091][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 12.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:31:15,092][org.apache.spark.scheduler.DAGScheduler]ResultStage 12 (foreachPartition at ReceiveKafkaData.scala:65) finished in 0.005 s
[INFO][2018-05-24 20:31:15,093][org.apache.spark.scheduler.DAGScheduler]Job 12 finished: foreachPartition at ReceiveKafkaData.scala:65, took 0.016272 s
[INFO][2018-05-24 20:31:15,093][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527165070000 ms.0 from job set of time 1527165070000 ms
[INFO][2018-05-24 20:31:15,094][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 23 from persistence list
[INFO][2018-05-24 20:31:15,094][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 5.093 s for time 1527165070000 ms (execution: 0.024 s)
[INFO][2018-05-24 20:31:15,095][org.apache.spark.storage.BlockManager]Removing RDD 23
[INFO][2018-05-24 20:31:15,095][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 22 from persistence list
[INFO][2018-05-24 20:31:15,096][org.apache.spark.storage.BlockManager]Removing RDD 22
[INFO][2018-05-24 20:31:15,096][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:31:15,097][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527165050000 ms
[INFO][2018-05-24 20:31:20,054][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527165080000 ms
[INFO][2018-05-24 20:31:20,055][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527165080000 ms.0 from job set of time 1527165080000 ms
[INFO][2018-05-24 20:31:20,067][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:65
[INFO][2018-05-24 20:31:20,068][org.apache.spark.scheduler.DAGScheduler]Got job 13 (foreachPartition at ReceiveKafkaData.scala:65) with 1 output partitions
[INFO][2018-05-24 20:31:20,068][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 13 (foreachPartition at ReceiveKafkaData.scala:65)
[INFO][2018-05-24 20:31:20,068][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:31:20,068][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:31:20,068][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 13 (MapPartitionsRDD[27] at map at ReceiveKafkaData.scala:63), which has no missing parents
[INFO][2018-05-24 20:31:20,070][org.apache.spark.storage.memory.MemoryStore]Block broadcast_13 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 20:31:20,073][org.apache.spark.storage.memory.MemoryStore]Block broadcast_13_piece0 stored as bytes in memory (estimated size 1878.0 B, free 912.3 MB)
[INFO][2018-05-24 20:31:20,074][org.apache.spark.storage.BlockManagerInfo]Added broadcast_13_piece0 in memory on 192.168.0.102:49839 (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:31:20,074][org.apache.spark.SparkContext]Created broadcast 13 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:31:20,075][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[27] at map at ReceiveKafkaData.scala:63) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:31:20,075][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 13.0 with 1 tasks
[INFO][2018-05-24 20:31:20,075][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 13.0 (TID 13, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:31:20,076][org.apache.spark.executor.Executor]Running task 0.0 in stage 13.0 (TID 13)
[INFO][2018-05-24 20:31:20,077][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12184 is the same as ending offset skipping seven 0
[INFO][2018-05-24 20:31:20,077][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:31:20,078][org.apache.spark.executor.Executor]Finished task 0.0 in stage 13.0 (TID 13). 665 bytes result sent to driver
[INFO][2018-05-24 20:31:20,079][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 13.0 (TID 13) in 4 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:31:20,079][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 13.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:31:20,080][org.apache.spark.scheduler.DAGScheduler]ResultStage 13 (foreachPartition at ReceiveKafkaData.scala:65) finished in 0.005 s
[INFO][2018-05-24 20:31:20,080][org.apache.spark.scheduler.DAGScheduler]Job 13 finished: foreachPartition at ReceiveKafkaData.scala:65, took 0.012553 s
[INFO][2018-05-24 20:31:20,081][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527165080000 ms.0 from job set of time 1527165080000 ms
[INFO][2018-05-24 20:31:20,081][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.080 s for time 1527165080000 ms (execution: 0.025 s)
[INFO][2018-05-24 20:31:20,081][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 25 from persistence list
[INFO][2018-05-24 20:31:20,082][org.apache.spark.storage.BlockManager]Removing RDD 25
[INFO][2018-05-24 20:31:20,082][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 24 from persistence list
[INFO][2018-05-24 20:31:20,083][org.apache.spark.storage.BlockManager]Removing RDD 24
[INFO][2018-05-24 20:31:20,083][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 20:31:20,083][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527165060000 ms
[INFO][2018-05-24 20:31:34,658][org.apache.spark.streaming.StreamingContext]Invoking stop(stopGracefully=false) from shutdown hook
[INFO][2018-05-24 20:31:34,663][org.apache.spark.streaming.scheduler.ReceiverTracker]ReceiverTracker stopped
[INFO][2018-05-24 20:31:34,664][org.apache.spark.streaming.scheduler.JobGenerator]Stopping JobGenerator immediately
[INFO][2018-05-24 20:31:34,664][org.apache.spark.streaming.util.RecurringTimer]Stopped timer for JobGenerator after time 1527165090000
[INFO][2018-05-24 20:31:35,066][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527165090000 ms
[INFO][2018-05-24 20:31:35,066][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527165090000 ms.0 from job set of time 1527165090000 ms
[INFO][2018-05-24 20:31:35,069][org.apache.spark.streaming.scheduler.JobGenerator]Stopped JobGenerator
[INFO][2018-05-24 20:31:35,071][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:65
[INFO][2018-05-24 20:31:35,072][org.apache.spark.scheduler.DAGScheduler]Got job 14 (foreachPartition at ReceiveKafkaData.scala:65) with 1 output partitions
[INFO][2018-05-24 20:31:35,072][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 14 (foreachPartition at ReceiveKafkaData.scala:65)
[INFO][2018-05-24 20:31:35,072][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 20:31:35,072][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 20:31:35,072][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 14 (MapPartitionsRDD[29] at map at ReceiveKafkaData.scala:63), which has no missing parents
[INFO][2018-05-24 20:31:35,074][org.apache.spark.storage.memory.MemoryStore]Block broadcast_14 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 20:31:35,075][org.apache.spark.storage.memory.MemoryStore]Block broadcast_14_piece0 stored as bytes in memory (estimated size 1878.0 B, free 912.3 MB)
[INFO][2018-05-24 20:31:35,075][org.apache.spark.storage.BlockManagerInfo]Added broadcast_14_piece0 in memory on 192.168.0.102:49839 (size: 1878.0 B, free: 912.3 MB)
[INFO][2018-05-24 20:31:35,076][org.apache.spark.SparkContext]Created broadcast 14 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 20:31:35,076][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[29] at map at ReceiveKafkaData.scala:63) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 20:31:35,076][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 14.0 with 1 tasks
[INFO][2018-05-24 20:31:35,077][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 14.0 (TID 14, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 20:31:35,077][org.apache.spark.executor.Executor]Running task 0.0 in stage 14.0 (TID 14)
[INFO][2018-05-24 20:31:35,079][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12184 is the same as ending offset skipping seven 0
[INFO][2018-05-24 20:31:35,079][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 20:31:35,079][org.apache.spark.executor.Executor]Finished task 0.0 in stage 14.0 (TID 14). 665 bytes result sent to driver
[INFO][2018-05-24 20:31:35,080][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 14.0 (TID 14) in 3 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 20:31:35,080][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 14.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 20:31:35,080][org.apache.spark.scheduler.DAGScheduler]ResultStage 14 (foreachPartition at ReceiveKafkaData.scala:65) finished in 0.003 s
[INFO][2018-05-24 20:31:35,081][org.apache.spark.scheduler.DAGScheduler]Job 14 finished: foreachPartition at ReceiveKafkaData.scala:65, took 0.009407 s
[INFO][2018-05-24 20:31:35,081][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527165090000 ms.0 from job set of time 1527165090000 ms
[INFO][2018-05-24 20:31:35,081][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 5.081 s for time 1527165090000 ms (execution: 0.015 s)
[INFO][2018-05-24 20:31:35,082][org.apache.spark.streaming.scheduler.JobScheduler]Stopped JobScheduler
[INFO][2018-05-24 20:31:35,088][org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@6972c30a{/streaming,null,UNAVAILABLE,@Spark}
[INFO][2018-05-24 20:31:35,089][org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@5109e8cf{/streaming/batch,null,UNAVAILABLE,@Spark}
[INFO][2018-05-24 20:31:35,090][org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@6c65860d{/static/streaming,null,UNAVAILABLE,@Spark}
[INFO][2018-05-24 20:31:35,091][org.apache.spark.streaming.StreamingContext]StreamingContext stopped successfully
[INFO][2018-05-24 20:31:35,091][org.apache.spark.SparkContext]Invoking stop() from shutdown hook
[INFO][2018-05-24 20:31:35,097][org.spark_project.jetty.server.AbstractConnector]Stopped Spark@6d366c9b{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-05-24 20:31:35,100][org.apache.spark.ui.SparkUI]Stopped Spark web UI at http://192.168.0.102:4040
[INFO][2018-05-24 20:31:35,109][org.apache.spark.MapOutputTrackerMasterEndpoint]MapOutputTrackerMasterEndpoint stopped!
[INFO][2018-05-24 20:31:35,143][org.apache.spark.storage.memory.MemoryStore]MemoryStore cleared
[INFO][2018-05-24 20:31:35,143][org.apache.spark.storage.BlockManager]BlockManager stopped
[INFO][2018-05-24 20:31:35,144][org.apache.spark.storage.BlockManagerMaster]BlockManagerMaster stopped
[INFO][2018-05-24 20:31:35,146][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint]OutputCommitCoordinator stopped!
[INFO][2018-05-24 20:31:35,148][org.apache.spark.SparkContext]Successfully stopped SparkContext
[INFO][2018-05-24 20:31:35,148][org.apache.spark.util.ShutdownHookManager]Shutdown hook called
[INFO][2018-05-24 20:31:35,149][org.apache.spark.util.ShutdownHookManager]Deleting directory /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/spark-33cf4f2e-c784-426c-9bb3-23cc9b7b32a6
[INFO][2018-05-24 21:07:35,344][org.apache.spark.SparkContext]Running Spark version 2.2.0
[INFO][2018-05-24 21:07:36,300][org.apache.spark.SparkContext]Submitted application: ReceiveKafkaData$
[INFO][2018-05-24 21:07:36,341][org.apache.spark.SecurityManager]Changing view acls to: seven
[INFO][2018-05-24 21:07:36,342][org.apache.spark.SecurityManager]Changing modify acls to: seven
[INFO][2018-05-24 21:07:36,343][org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO][2018-05-24 21:07:36,343][org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO][2018-05-24 21:07:36,344][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(seven); groups with view permissions: Set(); users  with modify permissions: Set(seven); groups with modify permissions: Set()
[INFO][2018-05-24 21:07:36,627][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 50745.
[INFO][2018-05-24 21:07:36,656][org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO][2018-05-24 21:07:36,676][org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO][2018-05-24 21:07:36,679][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2018-05-24 21:07:36,680][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
[INFO][2018-05-24 21:07:36,689][org.apache.spark.storage.DiskBlockManager]Created local directory at /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/blockmgr-7fa97866-6035-4d19-9e1b-29b7ca7349b7
[INFO][2018-05-24 21:07:36,708][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 912.3 MB
[INFO][2018-05-24 21:07:36,802][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO][2018-05-24 21:07:36,893][org.spark_project.jetty.util.log]Logging initialized @2758ms
[INFO][2018-05-24 21:07:36,945][org.spark_project.jetty.server.Server]jetty-9.3.z-SNAPSHOT
[INFO][2018-05-24 21:07:36,959][org.spark_project.jetty.server.Server]Started @2825ms
[INFO][2018-05-24 21:07:36,977][org.spark_project.jetty.server.AbstractConnector]Started ServerConnector@6d366c9b{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-05-24 21:07:36,978][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4040.
[INFO][2018-05-24 21:07:36,998][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2c1b9e4b{/jobs,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:07:36,998][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3d08f3f5{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:07:36,999][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1a1da881{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:07:37,000][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@165b8a71{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:07:37,001][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2f058b8a{/stages,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:07:37,001][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3f2ef586{/stages/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:07:37,001][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@76c7beb3{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:07:37,003][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7b139eab{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:07:37,003][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@611df6e3{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:07:37,004][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6273c5a4{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:07:37,005][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@53e211ee{/storage,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:07:37,005][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3d8bbcdc{/storage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:07:37,006][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@117e0fe5{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:07:37,007][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@78aea4b9{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:07:37,007][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4b85880b{/environment,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:07:37,008][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4215838f{/environment/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:07:37,009][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2289aca5{/executors,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:07:37,010][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@184497d1{/executors/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:07:37,011][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6ffab045{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:07:37,011][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3e2943ab{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:07:37,019][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4a9f80d3{/static,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:07:37,020][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4204541c{/,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:07:37,021][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4602c2a9{/api,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:07:37,022][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@69c79f09{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:07:37,023][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5fcacc0{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:07:37,025][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://192.168.0.102:4040
[INFO][2018-05-24 21:07:37,109][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
[INFO][2018-05-24 21:07:37,148][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 50746.
[INFO][2018-05-24 21:07:37,161][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 192.168.0.102:50746
[INFO][2018-05-24 21:07:37,164][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2018-05-24 21:07:37,166][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 192.168.0.102, 50746, None)
[INFO][2018-05-24 21:07:37,170][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 192.168.0.102:50746 with 912.3 MB RAM, BlockManagerId(driver, 192.168.0.102, 50746, None)
[INFO][2018-05-24 21:07:37,175][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 192.168.0.102, 50746, None)
[INFO][2018-05-24 21:07:37,176][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 192.168.0.102, 50746, None)
[INFO][2018-05-24 21:07:37,465][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@64a1923a{/metrics/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:07:37,600][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 21:07:37,603][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 21:07:37,603][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 21:07:48,354][org.apache.spark.streaming.kafka.DirectKafkaInputDStream]Slide time = 10000 ms
[INFO][2018-05-24 21:07:48,355][org.apache.spark.streaming.kafka.DirectKafkaInputDStream]Storage level = Serialized 1x Replicated
[INFO][2018-05-24 21:07:48,356][org.apache.spark.streaming.kafka.DirectKafkaInputDStream]Checkpoint interval = null
[INFO][2018-05-24 21:07:48,356][org.apache.spark.streaming.kafka.DirectKafkaInputDStream]Remember interval = 10000 ms
[INFO][2018-05-24 21:07:48,357][org.apache.spark.streaming.kafka.DirectKafkaInputDStream]Initialized and validated org.apache.spark.streaming.kafka.DirectKafkaInputDStream@e38788f
[INFO][2018-05-24 21:07:48,357][org.apache.spark.streaming.dstream.MappedDStream]Slide time = 10000 ms
[INFO][2018-05-24 21:07:48,357][org.apache.spark.streaming.dstream.MappedDStream]Storage level = Serialized 1x Replicated
[INFO][2018-05-24 21:07:48,357][org.apache.spark.streaming.dstream.MappedDStream]Checkpoint interval = null
[INFO][2018-05-24 21:07:48,358][org.apache.spark.streaming.dstream.MappedDStream]Remember interval = 10000 ms
[INFO][2018-05-24 21:07:48,358][org.apache.spark.streaming.dstream.MappedDStream]Initialized and validated org.apache.spark.streaming.dstream.MappedDStream@13adac79
[INFO][2018-05-24 21:07:48,358][org.apache.spark.streaming.dstream.ForEachDStream]Slide time = 10000 ms
[INFO][2018-05-24 21:07:48,358][org.apache.spark.streaming.dstream.ForEachDStream]Storage level = Serialized 1x Replicated
[INFO][2018-05-24 21:07:48,358][org.apache.spark.streaming.dstream.ForEachDStream]Checkpoint interval = null
[INFO][2018-05-24 21:07:48,358][org.apache.spark.streaming.dstream.ForEachDStream]Remember interval = 10000 ms
[INFO][2018-05-24 21:07:48,358][org.apache.spark.streaming.dstream.ForEachDStream]Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@6b574b9b
[INFO][2018-05-24 21:07:48,404][org.apache.spark.streaming.util.RecurringTimer]Started timer for JobGenerator at time 1527167270000
[INFO][2018-05-24 21:07:48,404][org.apache.spark.streaming.scheduler.JobGenerator]Started JobGenerator at 1527167270000 ms
[INFO][2018-05-24 21:07:48,405][org.apache.spark.streaming.scheduler.JobScheduler]Started JobScheduler
[INFO][2018-05-24 21:07:48,411][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1ab6718{/streaming,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:07:48,412][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@460510aa{/streaming/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:07:48,413][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@78b41097{/streaming/batch,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:07:48,414][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@327c7bea{/streaming/batch/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:07:48,415][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7cf283e1{/static/streaming,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:07:48,415][org.apache.spark.streaming.StreamingContext]StreamingContext started
[INFO][2018-05-24 21:07:50,047][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 21:07:50,047][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 21:07:50,047][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 21:07:51,671][org.apache.spark.SparkContext]Running Spark version 2.2.0
[INFO][2018-05-24 21:07:52,499][org.apache.spark.SparkContext]Submitted application: SimulationKafkaSendOutData$
[INFO][2018-05-24 21:07:52,534][org.apache.spark.SecurityManager]Changing view acls to: seven
[INFO][2018-05-24 21:07:52,535][org.apache.spark.SecurityManager]Changing modify acls to: seven
[INFO][2018-05-24 21:07:52,536][org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO][2018-05-24 21:07:52,537][org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO][2018-05-24 21:07:52,538][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(seven); groups with view permissions: Set(); users  with modify permissions: Set(seven); groups with modify permissions: Set()
[INFO][2018-05-24 21:07:53,461][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 50762.
[INFO][2018-05-24 21:07:53,510][org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO][2018-05-24 21:07:53,528][org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO][2018-05-24 21:07:53,530][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2018-05-24 21:07:53,531][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
[INFO][2018-05-24 21:07:53,542][org.apache.spark.storage.DiskBlockManager]Created local directory at /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/blockmgr-1eeefccd-f09e-45c9-8f33-7d3cd383921e
[INFO][2018-05-24 21:07:53,561][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 912.3 MB
[INFO][2018-05-24 21:07:53,639][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO][2018-05-24 21:07:53,730][org.spark_project.jetty.util.log]Logging initialized @3155ms
[INFO][2018-05-24 21:07:53,797][org.spark_project.jetty.server.Server]jetty-9.3.z-SNAPSHOT
[INFO][2018-05-24 21:07:53,810][org.spark_project.jetty.server.Server]Started @3236ms
[WARN][2018-05-24 21:07:53,823][org.apache.spark.util.Utils]Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[INFO][2018-05-24 21:07:53,829][org.spark_project.jetty.server.AbstractConnector]Started ServerConnector@16751330{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}
[INFO][2018-05-24 21:07:53,829][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4041.
[INFO][2018-05-24 21:07:53,850][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7e985ce9{/jobs,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:07:53,851][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@757d6814{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:07:53,851][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3c0fae6c{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:07:53,852][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@fd0e5b6{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:07:53,853][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@36b0fcd5{/stages,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:07:53,854][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@475835b1{/stages/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:07:53,854][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5241cf67{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:07:53,855][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7e809b79{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:07:53,856][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@625e134e{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:07:53,857][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@89c10b7{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:07:53,858][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4fe89c24{/storage,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:07:53,859][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3d08f3f5{/storage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:07:53,859][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1a1da881{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:07:53,860][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7fd4acee{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:07:53,861][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6175619b{/environment,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:07:53,863][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@756cf158{/environment/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:07:53,864][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@751d3241{/executors,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:07:53,864][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@64337702{/executors/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:07:53,865][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@30ea8c23{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:07:53,866][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4e76dac{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:07:53,871][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5f2f577{/static,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:07:53,874][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@26fb628{/,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:07:53,878][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@70dd7e15{/api,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:07:53,879][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2c9399a4{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:07:53,880][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@9635fa{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:07:53,886][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://192.168.0.102:4041
[INFO][2018-05-24 21:07:53,980][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
[INFO][2018-05-24 21:07:54,013][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 50763.
[INFO][2018-05-24 21:07:54,013][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 192.168.0.102:50763
[INFO][2018-05-24 21:07:54,016][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2018-05-24 21:07:54,018][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 192.168.0.102, 50763, None)
[INFO][2018-05-24 21:07:54,024][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 192.168.0.102:50763 with 912.3 MB RAM, BlockManagerId(driver, 192.168.0.102, 50763, None)
[INFO][2018-05-24 21:07:54,030][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 192.168.0.102, 50763, None)
[INFO][2018-05-24 21:07:54,032][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 192.168.0.102, 50763, None)
[INFO][2018-05-24 21:07:54,311][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@79c3f01f{/metrics/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:07:54,860][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0 stored as values in memory (estimated size 228.1 KB, free 912.1 MB)
[INFO][2018-05-24 21:07:54,929][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0_piece0 stored as bytes in memory (estimated size 22.1 KB, free 912.1 MB)
[INFO][2018-05-24 21:07:54,931][org.apache.spark.storage.BlockManagerInfo]Added broadcast_0_piece0 in memory on 192.168.0.102:50763 (size: 22.1 KB, free: 912.3 MB)
[INFO][2018-05-24 21:07:54,937][org.apache.spark.SparkContext]Created broadcast 0 from textFile at SimulationKafkaSendOutData.scala:25
[INFO][2018-05-24 21:07:55,160][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527167270000 ms
[INFO][2018-05-24 21:07:55,163][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527167270000 ms.0 from job set of time 1527167270000 ms
[INFO][2018-05-24 21:07:55,196][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:07:55,211][org.apache.spark.scheduler.DAGScheduler]Got job 0 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:07:55,212][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 0 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:07:55,212][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:07:55,213][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:07:55,219][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 0 (MapPartitionsRDD[1] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:07:55,333][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 21:07:55,355][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0_piece0 stored as bytes in memory (estimated size 1867.0 B, free 912.3 MB)
[INFO][2018-05-24 21:07:55,357][org.apache.spark.storage.BlockManagerInfo]Added broadcast_0_piece0 in memory on 192.168.0.102:50746 (size: 1867.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:07:55,359][org.apache.spark.SparkContext]Created broadcast 0 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:07:55,373][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:07:55,374][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 0.0 with 1 tasks
[INFO][2018-05-24 21:07:55,407][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:07:55,418][org.apache.spark.executor.Executor]Running task 0.0 in stage 0.0 (TID 0)
[INFO][2018-05-24 21:07:55,449][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12184 is the same as ending offset skipping seven 0
[INFO][2018-05-24 21:07:55,589][org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper]Process identifier=hconnection-0x54bcd26 connecting to ZooKeeper ensemble=vm-xaj-bigdata-da-d01:2181,vm-xaj-bigdata-da-d02:2181,vm-xaj-bigdata-da-d03:2181
[INFO][2018-05-24 21:07:55,595][org.apache.zookeeper.ZooKeeper]Client environment:zookeeper.version=3.4.5-cdh5.7.0--1, built on 04/06/2016 14:24 GMT
[INFO][2018-05-24 21:07:55,596][org.apache.zookeeper.ZooKeeper]Client environment:host.name=192.168.0.102
[INFO][2018-05-24 21:07:55,596][org.apache.zookeeper.ZooKeeper]Client environment:java.version=1.8.0_161
[INFO][2018-05-24 21:07:55,596][org.apache.zookeeper.ZooKeeper]Client environment:java.vendor=Oracle Corporation
[INFO][2018-05-24 21:07:55,596][org.apache.zookeeper.ZooKeeper]Client environment:java.home=/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre
[INFO][2018-05-24 21:07:55,596][org.apache.zookeeper.ZooKeeper]Client environment:java.class.path=/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/charsets.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/deploy.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/cldrdata.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/dnsns.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/jaccess.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/jfxrt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/localedata.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/nashorn.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/sunec.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/sunjce_provider.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/sunpkcs11.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/zipfs.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/javaws.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/jce.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/jfr.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/jfxswt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/jsse.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/management-agent.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/plugin.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/resources.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/rt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/ant-javafx.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/dt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/javafx-mx.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/jconsole.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/packager.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/sa-jdi.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/tools.jar:/Users/seven/project/scala/dataMining/target/classes:/Users/seven/software/maven/repository/org/scala-lang/scala-library/2.11.12/scala-library-2.11.12.jar:/Users/seven/software/maven/repository/org/scala-lang/scala-compiler/2.11.12/scala-compiler-2.11.12.jar:/Users/seven/software/maven/repository/org/scala-lang/modules/scala-xml_2.11/1.0.5/scala-xml_2.11-1.0.5.jar:/Users/seven/software/maven/repository/org/scala-lang/modules/scala-parser-combinators_2.11/1.0.4/scala-parser-combinators_2.11-1.0.4.jar:/Users/seven/software/maven/repository/org/scala-lang/scala-reflect/2.11.12/scala-reflect-2.11.12.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-client/2.6.0-cdh5.7.0/hadoop-client-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-common/2.6.0-cdh5.7.0/hadoop-common-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/xmlenc/xmlenc/0.52/xmlenc-0.52.jar:/Users/seven/software/maven/repository/commons-net/commons-net/3.1/commons-net-3.1.jar:/Users/seven/software/maven/repository/commons-configuration/commons-configuration/1.6/commons-configuration-1.6.jar:/Users/seven/software/maven/repository/commons-digester/commons-digester/1.8/commons-digester-1.8.jar:/Users/seven/software/maven/repository/commons-beanutils/commons-beanutils/1.7.0/commons-beanutils-1.7.0.jar:/Users/seven/software/maven/repository/org/slf4j/slf4j-api/1.7.5/slf4j-api-1.7.5.jar:/Users/seven/software/maven/repository/org/slf4j/slf4j-log4j12/1.7.5/slf4j-log4j12-1.7.5.jar:/Users/seven/software/maven/repository/org/apache/avro/avro/1.7.6-cdh5.7.0/avro-1.7.6-cdh5.7.0.jar:/Users/seven/software/maven/repository/com/google/code/gson/gson/2.2.4/gson-2.2.4.jar:/Users/seven/software/maven/repository/org/apache/curator/curator-client/2.7.1/curator-client-2.7.1.jar:/Users/seven/software/maven/repository/org/apache/curator/curator-recipes/2.7.1/curator-recipes-2.7.1.jar:/Users/seven/software/maven/repository/com/google/code/findbugs/jsr305/3.0.0/jsr305-3.0.0.jar:/Users/seven/software/maven/repository/org/apache/htrace/htrace-core4/4.0.1-incubating/htrace-core4-4.0.1-incubating.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-compress/1.4.1/commons-compress-1.4.1.jar:/Users/seven/software/maven/repository/org/tukaani/xz/1.0/xz-1.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-hdfs/2.6.0-cdh5.7.0/hadoop-hdfs-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/io/netty/netty/3.6.2.Final/netty-3.6.2.Final.jar:/Users/seven/software/maven/repository/xerces/xercesImpl/2.9.1/xercesImpl-2.9.1.jar:/Users/seven/software/maven/repository/xml-apis/xml-apis/1.3.04/xml-apis-1.3.04.jar:/Users/seven/software/maven/repository/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-app/2.6.0-cdh5.7.0/hadoop-mapreduce-client-app-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-common/2.6.0-cdh5.7.0/hadoop-mapreduce-client-common-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-yarn-client/2.6.0-cdh5.7.0/hadoop-yarn-client-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-yarn-server-common/2.6.0-cdh5.7.0/hadoop-yarn-server-common-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-shuffle/2.6.0-cdh5.7.0/hadoop-mapreduce-client-shuffle-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-yarn-api/2.6.0-cdh5.7.0/hadoop-yarn-api-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-core/2.6.0-cdh5.7.0/hadoop-mapreduce-client-core-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-yarn-common/2.6.0-cdh5.7.0/hadoop-yarn-common-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/javax/xml/bind/jaxb-api/2.2.2/jaxb-api-2.2.2.jar:/Users/seven/software/maven/repository/javax/xml/stream/stax-api/1.0-2/stax-api-1.0-2.jar:/Users/seven/software/maven/repository/javax/activation/activation/1.1/activation-1.1.jar:/Users/seven/software/maven/repository/com/sun/jersey/jersey-client/1.9/jersey-client-1.9.jar:/Users/seven/software/maven/repository/org/codehaus/jackson/jackson-xc/1.8.8/jackson-xc-1.8.8.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.6.0-cdh5.7.0/hadoop-mapreduce-client-jobclient-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-aws/2.6.0-cdh5.7.0/hadoop-aws-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/com/amazonaws/aws-java-sdk-s3/1.10.6/aws-java-sdk-s3-1.10.6.jar:/Users/seven/software/maven/repository/com/amazonaws/aws-java-sdk-kms/1.10.6/aws-java-sdk-kms-1.10.6.jar:/Users/seven/software/maven/repository/com/amazonaws/aws-java-sdk-core/1.10.6/aws-java-sdk-core-1.10.6.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-annotations/2.6.0-cdh5.7.0/hadoop-annotations-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-client/1.2.0-cdh5.7.0/hbase-client-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-annotations/1.2.0-cdh5.7.0/hbase-annotations-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-common/1.2.0-cdh5.7.0/hbase-common-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-protocol/1.2.0-cdh5.7.0/hbase-protocol-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/commons-codec/commons-codec/1.9/commons-codec-1.9.jar:/Users/seven/software/maven/repository/commons-io/commons-io/2.4/commons-io-2.4.jar:/Users/seven/software/maven/repository/commons-lang/commons-lang/2.6/commons-lang-2.6.jar:/Users/seven/software/maven/repository/commons-logging/commons-logging/1.2/commons-logging-1.2.jar:/Users/seven/software/maven/repository/com/google/guava/guava/12.0.1/guava-12.0.1.jar:/Users/seven/software/maven/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/Users/seven/software/maven/repository/io/netty/netty-all/4.0.23.Final/netty-all-4.0.23.Final.jar:/Users/seven/software/maven/repository/org/apache/zookeeper/zookeeper/3.4.5-cdh5.7.0/zookeeper-3.4.5-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/htrace/htrace-core/3.2.0-incubating/htrace-core-3.2.0-incubating.jar:/Users/seven/software/maven/repository/org/codehaus/jackson/jackson-mapper-asl/1.8.8/jackson-mapper-asl-1.8.8.jar:/Users/seven/software/maven/repository/org/jruby/jcodings/jcodings/1.0.8/jcodings-1.0.8.jar:/Users/seven/software/maven/repository/org/jruby/joni/joni/2.1.2/joni-2.1.2.jar:/Users/seven/software/maven/repository/com/yammer/metrics/metrics-core/2.2.0/metrics-core-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-auth/2.6.0-cdh5.7.0/hadoop-auth-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/httpcomponents/httpclient/4.2.5/httpclient-4.2.5.jar:/Users/seven/software/maven/repository/org/apache/httpcomponents/httpcore/4.2.4/httpcore-4.2.4.jar:/Users/seven/software/maven/repository/org/apache/directory/server/apacheds-kerberos-codec/2.0.0-M15/apacheds-kerberos-codec-2.0.0-M15.jar:/Users/seven/software/maven/repository/org/apache/directory/server/apacheds-i18n/2.0.0-M15/apacheds-i18n-2.0.0-M15.jar:/Users/seven/software/maven/repository/org/apache/directory/api/api-asn1-api/1.0.0-M20/api-asn1-api-1.0.0-M20.jar:/Users/seven/software/maven/repository/org/apache/directory/api/api-util/1.0.0-M20/api-util-1.0.0-M20.jar:/Users/seven/software/maven/repository/org/apache/curator/curator-framework/2.7.1/curator-framework-2.7.1.jar:/Users/seven/software/maven/repository/com/github/stephenc/findbugs/findbugs-annotations/1.3.9-1/findbugs-annotations-1.3.9-1.jar:/Users/seven/software/maven/repository/junit/junit/4.12/junit-4.12.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-server/1.2.0-cdh5.7.0/hbase-server-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-procedure/1.2.0-cdh5.7.0/hbase-procedure-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-common/1.2.0-cdh5.7.0/hbase-common-1.2.0-cdh5.7.0-tests.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-prefix-tree/1.2.0-cdh5.7.0/hbase-prefix-tree-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar:/Users/seven/software/maven/repository/commons-collections/commons-collections/3.2.2/commons-collections-3.2.2.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-hadoop-compat/1.2.0-cdh5.7.0/hbase-hadoop-compat-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-hadoop2-compat/1.2.0-cdh5.7.0/hbase-hadoop2-compat-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/com/sun/jersey/jersey-core/1.9/jersey-core-1.9.jar:/Users/seven/software/maven/repository/com/sun/jersey/jersey-server/1.9/jersey-server-1.9.jar:/Users/seven/software/maven/repository/asm/asm/3.1/asm-3.1.jar:/Users/seven/software/maven/repository/commons-cli/commons-cli/1.2/commons-cli-1.2.jar:/Users/seven/software/maven/repository/com/github/stephenc/high-scale-lib/high-scale-lib/1.1.1/high-scale-lib-1.1.1.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-math/2.1/commons-math-2.1.jar:/Users/seven/software/maven/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jetty/6.1.26.cloudera.4/jetty-6.1.26.cloudera.4.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jetty-util/6.1.26.cloudera.4/jetty-util-6.1.26.cloudera.4.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jetty-sslengine/6.1.26.cloudera.4/jetty-sslengine-6.1.26.cloudera.4.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jsp-2.1/6.1.14/jsp-2.1-6.1.14.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jsp-api-2.1/6.1.14/jsp-api-2.1-6.1.14.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/servlet-api-2.5/6.1.14/servlet-api-2.5-6.1.14.jar:/Users/seven/software/maven/repository/org/codehaus/jackson/jackson-core-asl/1.8.8/jackson-core-asl-1.8.8.jar:/Users/seven/software/maven/repository/org/codehaus/jackson/jackson-jaxrs/1.8.8/jackson-jaxrs-1.8.8.jar:/Users/seven/software/maven/repository/tomcat/jasper-compiler/5.5.23/jasper-compiler-5.5.23.jar:/Users/seven/software/maven/repository/tomcat/jasper-runtime/5.5.23/jasper-runtime-5.5.23.jar:/Users/seven/software/maven/repository/commons-el/commons-el/1.0/commons-el-1.0.jar:/Users/seven/software/maven/repository/org/jamon/jamon-runtime/2.4.1/jamon-runtime-2.4.1.jar:/Users/seven/software/maven/repository/com/lmax/disruptor/3.3.0/disruptor-3.3.0.jar:/Users/seven/software/maven/repository/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar:/Users/seven/software/maven/repository/org/owasp/esapi/esapi/2.1.0/esapi-2.1.0.jar:/Users/seven/software/maven/repository/commons-beanutils/commons-beanutils-core/1.7.0/commons-beanutils-core-1.7.0.jar:/Users/seven/software/maven/repository/commons-fileupload/commons-fileupload/1.2/commons-fileupload-1.2.jar:/Users/seven/software/maven/repository/xom/xom/1.2.5/xom-1.2.5.jar:/Users/seven/software/maven/repository/xalan/xalan/2.7.0/xalan-2.7.0.jar:/Users/seven/software/maven/repository/org/beanshell/bsh-core/2.0b4/bsh-core-2.0b4.jar:/Users/seven/software/maven/repository/org/owasp/antisamy/antisamy/1.4.3/antisamy-1.4.3.jar:/Users/seven/software/maven/repository/org/apache/xmlgraphics/batik-css/1.7/batik-css-1.7.jar:/Users/seven/software/maven/repository/org/apache/xmlgraphics/batik-ext/1.7/batik-ext-1.7.jar:/Users/seven/software/maven/repository/org/apache/xmlgraphics/batik-util/1.7/batik-util-1.7.jar:/Users/seven/software/maven/repository/xml-apis/xml-apis-ext/1.3.04/xml-apis-ext-1.3.04.jar:/Users/seven/software/maven/repository/net/sourceforge/nekohtml/nekohtml/1.9.12/nekohtml-1.9.12.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-core/2.6.0-mr1-cdh5.7.0/hadoop-core-2.6.0-mr1-cdh5.7.0.jar:/Users/seven/software/maven/repository/javax/servlet/jsp/jsp-api/2.1/jsp-api-2.1.jar:/Users/seven/software/maven/repository/net/java/dev/jets3t/jets3t/0.6.1/jets3t-0.6.1.jar:/Users/seven/software/maven/repository/javax/servlet/servlet-api/2.5/servlet-api-2.5.jar:/Users/seven/software/maven/repository/hsqldb/hsqldb/1.8.0.10/hsqldb-1.8.0.10.jar:/Users/seven/software/maven/repository/org/eclipse/jdt/core/3.1.1/core-3.1.1.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-hdfs/2.6.0-cdh5.7.0/hadoop-hdfs-2.6.0-cdh5.7.0-tests.jar:/Users/seven/software/maven/repository/commons-daemon/commons-daemon/1.0.13/commons-daemon-1.0.13.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-lang3/3.5/commons-lang3-3.5.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-pool2/2.4.2/commons-pool2-2.4.2.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-mllib_2.11/2.2.0/spark-mllib_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-core_2.11/2.2.0/spark-core_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/avro/avro-mapred/1.7.7/avro-mapred-1.7.7-hadoop2.jar:/Users/seven/software/maven/repository/org/apache/avro/avro-ipc/1.7.7/avro-ipc-1.7.7-tests.jar:/Users/seven/software/maven/repository/com/twitter/chill_2.11/0.8.0/chill_2.11-0.8.0.jar:/Users/seven/software/maven/repository/com/esotericsoftware/kryo-shaded/3.0.3/kryo-shaded-3.0.3.jar:/Users/seven/software/maven/repository/com/esotericsoftware/minlog/1.3.0/minlog-1.3.0.jar:/Users/seven/software/maven/repository/org/objenesis/objenesis/2.1/objenesis-2.1.jar:/Users/seven/software/maven/repository/com/twitter/chill-java/0.8.0/chill-java-0.8.0.jar:/Users/seven/software/maven/repository/org/apache/xbean/xbean-asm5-shaded/4.4/xbean-asm5-shaded-4.4.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-launcher_2.11/2.2.0/spark-launcher_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-network-common_2.11/2.2.0/spark-network-common_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-network-shuffle_2.11/2.2.0/spark-network-shuffle_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-unsafe_2.11/2.2.0/spark-unsafe_2.11-2.2.0.jar:/Users/seven/software/maven/repository/javax/servlet/javax.servlet-api/3.1.0/javax.servlet-api-3.1.0.jar:/Users/seven/software/maven/repository/org/slf4j/jul-to-slf4j/1.7.16/jul-to-slf4j-1.7.16.jar:/Users/seven/software/maven/repository/org/slf4j/jcl-over-slf4j/1.7.16/jcl-over-slf4j-1.7.16.jar:/Users/seven/software/maven/repository/com/ning/compress-lzf/1.0.3/compress-lzf-1.0.3.jar:/Users/seven/software/maven/repository/org/xerial/snappy/snappy-java/1.1.2.6/snappy-java-1.1.2.6.jar:/Users/seven/software/maven/repository/org/roaringbitmap/RoaringBitmap/0.5.11/RoaringBitmap-0.5.11.jar:/Users/seven/software/maven/repository/org/json4s/json4s-jackson_2.11/3.2.11/json4s-jackson_2.11-3.2.11.jar:/Users/seven/software/maven/repository/org/json4s/json4s-core_2.11/3.2.11/json4s-core_2.11-3.2.11.jar:/Users/seven/software/maven/repository/org/json4s/json4s-ast_2.11/3.2.11/json4s-ast_2.11-3.2.11.jar:/Users/seven/software/maven/repository/org/scala-lang/scalap/2.11.0/scalap-2.11.0.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/core/jersey-client/2.22.2/jersey-client-2.22.2.jar:/Users/seven/software/maven/repository/javax/ws/rs/javax.ws.rs-api/2.0.1/javax.ws.rs-api-2.0.1.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/hk2-api/2.4.0-b34/hk2-api-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/hk2-utils/2.4.0-b34/hk2-utils-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/external/aopalliance-repackaged/2.4.0-b34/aopalliance-repackaged-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/external/javax.inject/2.4.0-b34/javax.inject-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/hk2-locator/2.4.0-b34/hk2-locator-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/javassist/javassist/3.18.1-GA/javassist-3.18.1-GA.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/core/jersey-common/2.22.2/jersey-common-2.22.2.jar:/Users/seven/software/maven/repository/javax/annotation/javax.annotation-api/1.2/javax.annotation-api-1.2.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/bundles/repackaged/jersey-guava/2.22.2/jersey-guava-2.22.2.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/osgi-resource-locator/1.0.1/osgi-resource-locator-1.0.1.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/core/jersey-server/2.22.2/jersey-server-2.22.2.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/media/jersey-media-jaxb/2.22.2/jersey-media-jaxb-2.22.2.jar:/Users/seven/software/maven/repository/javax/validation/validation-api/1.1.0.Final/validation-api-1.1.0.Final.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/containers/jersey-container-servlet/2.22.2/jersey-container-servlet-2.22.2.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/containers/jersey-container-servlet-core/2.22.2/jersey-container-servlet-core-2.22.2.jar:/Users/seven/software/maven/repository/com/clearspring/analytics/stream/2.7.0/stream-2.7.0.jar:/Users/seven/software/maven/repository/io/dropwizard/metrics/metrics-core/3.1.2/metrics-core-3.1.2.jar:/Users/seven/software/maven/repository/io/dropwizard/metrics/metrics-jvm/3.1.2/metrics-jvm-3.1.2.jar:/Users/seven/software/maven/repository/io/dropwizard/metrics/metrics-json/3.1.2/metrics-json-3.1.2.jar:/Users/seven/software/maven/repository/io/dropwizard/metrics/metrics-graphite/3.1.2/metrics-graphite-3.1.2.jar:/Users/seven/software/maven/repository/org/apache/ivy/ivy/2.4.0/ivy-2.4.0.jar:/Users/seven/software/maven/repository/oro/oro/2.0.8/oro-2.0.8.jar:/Users/seven/software/maven/repository/net/razorvine/pyrolite/4.13/pyrolite-4.13.jar:/Users/seven/software/maven/repository/net/sf/py4j/py4j/0.10.4/py4j-0.10.4.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-crypto/1.0.0/commons-crypto-1.0.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-streaming_2.11/2.2.0/spark-streaming_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-sql_2.11/2.2.0/spark-sql_2.11-2.2.0.jar:/Users/seven/software/maven/repository/com/univocity/univocity-parsers/2.2.1/univocity-parsers-2.2.1.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-sketch_2.11/2.2.0/spark-sketch_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-catalyst_2.11/2.2.0/spark-catalyst_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/codehaus/janino/janino/3.0.0/janino-3.0.0.jar:/Users/seven/software/maven/repository/org/codehaus/janino/commons-compiler/3.0.0/commons-compiler-3.0.0.jar:/Users/seven/software/maven/repository/org/antlr/antlr4-runtime/4.5.3/antlr4-runtime-4.5.3.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-column/1.8.2/parquet-column-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-common/1.8.2/parquet-common-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-encoding/1.8.2/parquet-encoding-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-hadoop/1.8.2/parquet-hadoop-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-format/2.3.1/parquet-format-2.3.1.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-jackson/1.8.2/parquet-jackson-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-graphx_2.11/2.2.0/spark-graphx_2.11-2.2.0.jar:/Users/seven/software/maven/repository/com/github/fommil/netlib/core/1.1.2/core-1.1.2.jar:/Users/seven/software/maven/repository/net/sourceforge/f2j/arpack_combined_all/0.1/arpack_combined_all-0.1.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-mllib-local_2.11/2.2.0/spark-mllib-local_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/scalanlp/breeze_2.11/0.13.1/breeze_2.11-0.13.1.jar:/Users/seven/software/maven/repository/org/scalanlp/breeze-macros_2.11/0.13.1/breeze-macros_2.11-0.13.1.jar:/Users/seven/software/maven/repository/net/sf/opencsv/opencsv/2.3/opencsv-2.3.jar:/Users/seven/software/maven/repository/com/github/rwl/jtransforms/2.4.0/jtransforms-2.4.0.jar:/Users/seven/software/maven/repository/org/spire-math/spire_2.11/0.13.0/spire_2.11-0.13.0.jar:/Users/seven/software/maven/repository/org/spire-math/spire-macros_2.11/0.13.0/spire-macros_2.11-0.13.0.jar:/Users/seven/software/maven/repository/org/typelevel/machinist_2.11/0.6.1/machinist_2.11-0.6.1.jar:/Users/seven/software/maven/repository/com/chuusai/shapeless_2.11/2.3.2/shapeless_2.11-2.3.2.jar:/Users/seven/software/maven/repository/org/typelevel/macro-compat_2.11/1.1.1/macro-compat_2.11-1.1.1.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-math3/3.4.1/commons-math3-3.4.1.jar:/Users/seven/software/maven/repository/org/jpmml/pmml-model/1.2.15/pmml-model-1.2.15.jar:/Users/seven/software/maven/repository/org/jpmml/pmml-schema/1.2.15/pmml-schema-1.2.15.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-tags_2.11/2.2.0/spark-tags_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-streaming-kafka-0-8_2.11/2.2.0/spark-streaming-kafka-0-8_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/kafka/kafka_2.11/0.8.2.1/kafka_2.11-0.8.2.1.jar:/Users/seven/software/maven/repository/com/101tec/zkclient/0.3/zkclient-0.3.jar:/Users/seven/software/maven/repository/org/apache/kafka/kafka-clients/0.8.2.1/kafka-clients-0.8.2.1.jar:/Users/seven/software/maven/repository/mysql/mysql-connector-java/5.1.31/mysql-connector-java-5.1.31.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/module/jackson-module-scala_2.11/2.6.5/jackson-module-scala_2.11-2.6.5.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/core/jackson-core/2.6.5/jackson-core-2.6.5.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/core/jackson-annotations/2.6.5/jackson-annotations-2.6.5.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/core/jackson-databind/2.6.5/jackson-databind-2.6.5.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/module/jackson-module-paranamer/2.6.5/jackson-module-paranamer-2.6.5.jar:/Users/seven/software/maven/repository/com/thoughtworks/paranamer/paranamer/2.6/paranamer-2.6.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-streaming-flume_2.11/2.2.0/spark-streaming-flume_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/flume/flume-ng-core/1.6.0/flume-ng-core-1.6.0.jar:/Users/seven/software/maven/repository/org/apache/flume/flume-ng-configuration/1.6.0/flume-ng-configuration-1.6.0.jar:/Users/seven/software/maven/repository/org/apache/avro/avro-ipc/1.7.4/avro-ipc-1.7.4.jar:/Users/seven/software/maven/repository/org/apache/velocity/velocity/1.7/velocity-1.7.jar:/Users/seven/software/maven/repository/joda-time/joda-time/2.1/joda-time-2.1.jar:/Users/seven/software/maven/repository/org/apache/mina/mina-core/2.0.4/mina-core-2.0.4.jar:/Users/seven/software/maven/repository/org/apache/flume/flume-ng-sdk/1.6.0/flume-ng-sdk-1.6.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-streaming-flume-sink_2.11/2.2.0/spark-streaming-flume-sink_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/flume/flume-ng-clients/flume-ng-log4jappender/1.6.0/flume-ng-log4jappender-1.6.0.jar:/Users/seven/software/maven/repository/net/jpountz/lz4/lz4/1.3.0/lz4-1.3.0.jar:/Applications/IntelliJ IDEA.app/Contents/lib/idea_rt.jar
[INFO][2018-05-24 21:07:55,597][org.apache.zookeeper.ZooKeeper]Client environment:java.library.path=/Users/seven/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:.
[INFO][2018-05-24 21:07:55,597][org.apache.zookeeper.ZooKeeper]Client environment:java.io.tmpdir=/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/
[INFO][2018-05-24 21:07:55,597][org.apache.zookeeper.ZooKeeper]Client environment:java.compiler=<NA>
[INFO][2018-05-24 21:07:55,597][org.apache.zookeeper.ZooKeeper]Client environment:os.name=Mac OS X
[INFO][2018-05-24 21:07:55,597][org.apache.zookeeper.ZooKeeper]Client environment:os.arch=x86_64
[INFO][2018-05-24 21:07:55,597][org.apache.zookeeper.ZooKeeper]Client environment:os.version=10.13.4
[INFO][2018-05-24 21:07:55,597][org.apache.zookeeper.ZooKeeper]Client environment:user.name=seven
[INFO][2018-05-24 21:07:55,597][org.apache.zookeeper.ZooKeeper]Client environment:user.home=/Users/seven
[INFO][2018-05-24 21:07:55,597][org.apache.zookeeper.ZooKeeper]Client environment:user.dir=/Users/seven/project/scala/dataMining
[INFO][2018-05-24 21:07:55,598][org.apache.zookeeper.ZooKeeper]Initiating client connection, connectString=vm-xaj-bigdata-da-d01:2181,vm-xaj-bigdata-da-d02:2181,vm-xaj-bigdata-da-d03:2181 sessionTimeout=60000 watcher=hconnection-0x54bcd260x0, quorum=vm-xaj-bigdata-da-d01:2181,vm-xaj-bigdata-da-d02:2181,vm-xaj-bigdata-da-d03:2181, baseZNode=/hbase
[INFO][2018-05-24 21:08:00,323][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527167280000 ms
[WARN][2018-05-24 21:08:00,380][org.apache.hadoop.util.NativeCodeLoader]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO][2018-05-24 21:08:00,532][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 1
[INFO][2018-05-24 21:08:00,633][org.apache.zookeeper.ClientCnxn]Opening socket connection to server vm-xaj-bigdata-da-d03/10.213.4.27:2181. Will not attempt to authenticate using SASL (unknown error)
[INFO][2018-05-24 21:08:00,649][org.apache.zookeeper.ClientCnxn]Socket connection established, initiating session, client: /10.0.8.39:50777, server: vm-xaj-bigdata-da-d03/10.213.4.27:2181
[INFO][2018-05-24 21:08:00,667][org.apache.spark.SparkContext]Starting job: collect at SimulationKafkaSendOutData.scala:25
[INFO][2018-05-24 21:08:00,676][org.apache.zookeeper.ClientCnxn]Session establishment complete on server vm-xaj-bigdata-da-d03/10.213.4.27:2181, sessionid = 0x362b50d75ec5f7f, negotiated timeout = 60000
[INFO][2018-05-24 21:08:00,687][org.apache.spark.scheduler.DAGScheduler]Got job 0 (collect at SimulationKafkaSendOutData.scala:25) with 2 output partitions
[INFO][2018-05-24 21:08:00,689][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 0 (collect at SimulationKafkaSendOutData.scala:25)
[INFO][2018-05-24 21:08:00,690][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:08:00,691][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:08:00,703][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 0 (hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/* MapPartitionsRDD[1] at textFile at SimulationKafkaSendOutData.scala:25), which has no missing parents
[INFO][2018-05-24 21:08:00,745][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1 stored as values in memory (estimated size 3.3 KB, free 912.1 MB)
[INFO][2018-05-24 21:08:00,748][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1_piece0 stored as bytes in memory (estimated size 1969.0 B, free 912.1 MB)
[INFO][2018-05-24 21:08:00,748][org.apache.spark.storage.BlockManagerInfo]Added broadcast_1_piece0 in memory on 192.168.0.102:50763 (size: 1969.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:08:00,752][org.apache.spark.SparkContext]Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:08:00,794][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ResultStage 0 (hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/* MapPartitionsRDD[1] at textFile at SimulationKafkaSendOutData.scala:25) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2018-05-24 21:08:00,795][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 0.0 with 2 tasks
[INFO][2018-05-24 21:08:00,847][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 4883 bytes)
[INFO][2018-05-24 21:08:00,849][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 4883 bytes)
[INFO][2018-05-24 21:08:00,858][org.apache.spark.executor.Executor]Running task 0.0 in stage 0.0 (TID 0)
[INFO][2018-05-24 21:08:00,858][org.apache.spark.executor.Executor]Running task 1.0 in stage 0.0 (TID 1)
[INFO][2018-05-24 21:08:00,931][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/part-00000:0+10717435
[INFO][2018-05-24 21:08:00,932][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/part-00000:10717435+10717436
[WARN][2018-05-24 21:08:01,331][org.apache.hadoop.util.NativeCodeLoader]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO][2018-05-24 21:08:01,432][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:08:01,451][org.apache.spark.executor.Executor]Finished task 0.0 in stage 0.0 (TID 0). 751 bytes result sent to driver
[INFO][2018-05-24 21:08:01,458][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 0.0 (TID 0) in 6058 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:08:01,461][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:08:01,465][org.apache.spark.scheduler.DAGScheduler]ResultStage 0 (foreachPartition at ReceiveKafkaData.scala:66) finished in 6.073 s
[INFO][2018-05-24 21:08:01,472][org.apache.spark.scheduler.DAGScheduler]Job 0 finished: foreachPartition at ReceiveKafkaData.scala:66, took 6.275625 s
[INFO][2018-05-24 21:08:01,477][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527167270000 ms.0 from job set of time 1527167270000 ms
[INFO][2018-05-24 21:08:01,478][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 11.476 s for time 1527167270000 ms (execution: 6.314 s)
[INFO][2018-05-24 21:08:01,478][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527167280000 ms.0 from job set of time 1527167280000 ms
[INFO][2018-05-24 21:08:01,487][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 21:08:01,487][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:08:01,488][org.apache.spark.scheduler.DAGScheduler]Got job 1 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:08:01,488][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 1 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:08:01,488][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:08:01,488][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:08:01,489][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 1 (MapPartitionsRDD[3] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:08:01,491][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 
[INFO][2018-05-24 21:08:01,494][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 21:08:01,499][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1_piece0 stored as bytes in memory (estimated size 1868.0 B, free 912.3 MB)
[INFO][2018-05-24 21:08:01,500][org.apache.spark.storage.BlockManagerInfo]Added broadcast_1_piece0 in memory on 192.168.0.102:50746 (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:08:01,501][org.apache.spark.SparkContext]Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:08:01,502][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:08:01,502][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 1.0 with 1 tasks
[INFO][2018-05-24 21:08:01,503][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:08:01,504][org.apache.spark.executor.Executor]Running task 0.0 in stage 1.0 (TID 1)
[INFO][2018-05-24 21:08:01,544][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12184 is the same as ending offset skipping seven 0
[INFO][2018-05-24 21:08:01,544][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:08:01,546][org.apache.spark.executor.Executor]Finished task 0.0 in stage 1.0 (TID 1). 708 bytes result sent to driver
[INFO][2018-05-24 21:08:01,547][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 1.0 (TID 1) in 44 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:08:01,547][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:08:01,548][org.apache.spark.scheduler.DAGScheduler]ResultStage 1 (foreachPartition at ReceiveKafkaData.scala:66) finished in 0.045 s
[INFO][2018-05-24 21:08:01,548][org.apache.spark.scheduler.DAGScheduler]Job 1 finished: foreachPartition at ReceiveKafkaData.scala:66, took 0.061364 s
[INFO][2018-05-24 21:08:01,550][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527167280000 ms.0 from job set of time 1527167280000 ms
[INFO][2018-05-24 21:08:01,550][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 1.550 s for time 1527167280000 ms (execution: 0.072 s)
[INFO][2018-05-24 21:08:01,551][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 1 from persistence list
[INFO][2018-05-24 21:08:01,556][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 0 from persistence list
[INFO][2018-05-24 21:08:01,557][org.apache.spark.storage.BlockManager]Removing RDD 1
[INFO][2018-05-24 21:08:01,558][org.apache.spark.storage.BlockManager]Removing RDD 0
[INFO][2018-05-24 21:08:01,559][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 21:08:01,560][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 
[INFO][2018-05-24 21:08:03,515][org.apache.spark.storage.memory.MemoryStore]Block taskresult_0 stored as bytes in memory (estimated size 10.4 MB, free 901.7 MB)
[INFO][2018-05-24 21:08:03,518][org.apache.spark.storage.BlockManagerInfo]Added taskresult_0 in memory on 192.168.0.102:50763 (size: 10.4 MB, free: 901.9 MB)
[INFO][2018-05-24 21:08:03,519][org.apache.spark.executor.Executor]Finished task 0.0 in stage 0.0 (TID 0). 10856306 bytes result sent via BlockManager)
[INFO][2018-05-24 21:08:03,552][org.apache.spark.network.client.TransportClientFactory]Successfully created connection to /192.168.0.102:50763 after 19 ms (0 ms spent in bootstraps)
[INFO][2018-05-24 21:08:03,732][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 0.0 (TID 0) in 2899 ms on localhost (executor driver) (1/2)
[INFO][2018-05-24 21:08:03,735][org.apache.spark.storage.BlockManagerInfo]Removed taskresult_0 on 192.168.0.102:50763 in memory (size: 10.4 MB, free: 912.3 MB)
[INFO][2018-05-24 21:08:10,092][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527167290000 ms
[INFO][2018-05-24 21:08:10,094][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527167290000 ms.0 from job set of time 1527167290000 ms
[INFO][2018-05-24 21:08:10,105][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:08:10,107][org.apache.spark.scheduler.DAGScheduler]Got job 2 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:08:10,107][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 2 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:08:10,107][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:08:10,107][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:08:10,109][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 2 (MapPartitionsRDD[5] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:08:10,114][org.apache.spark.storage.memory.MemoryStore]Block broadcast_2 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 21:08:10,118][org.apache.spark.storage.memory.MemoryStore]Block broadcast_2_piece0 stored as bytes in memory (estimated size 1868.0 B, free 912.3 MB)
[INFO][2018-05-24 21:08:10,119][org.apache.spark.storage.BlockManagerInfo]Added broadcast_2_piece0 in memory on 192.168.0.102:50746 (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:08:10,123][org.apache.spark.SparkContext]Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:08:10,125][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[5] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:08:10,125][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 2.0 with 1 tasks
[INFO][2018-05-24 21:08:10,128][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:08:10,129][org.apache.spark.executor.Executor]Running task 0.0 in stage 2.0 (TID 2)
[INFO][2018-05-24 21:08:10,134][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12184 is the same as ending offset skipping seven 0
[INFO][2018-05-24 21:08:10,136][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:08:10,138][org.apache.spark.executor.Executor]Finished task 0.0 in stage 2.0 (TID 2). 708 bytes result sent to driver
[INFO][2018-05-24 21:08:10,139][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 2.0 (TID 2) in 12 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:08:10,139][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:08:10,140][org.apache.spark.scheduler.DAGScheduler]ResultStage 2 (foreachPartition at ReceiveKafkaData.scala:66) finished in 0.014 s
[INFO][2018-05-24 21:08:10,141][org.apache.spark.scheduler.DAGScheduler]Job 2 finished: foreachPartition at ReceiveKafkaData.scala:66, took 0.035053 s
[INFO][2018-05-24 21:08:10,141][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527167290000 ms.0 from job set of time 1527167290000 ms
[INFO][2018-05-24 21:08:10,141][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 3 from persistence list
[INFO][2018-05-24 21:08:10,141][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.141 s for time 1527167290000 ms (execution: 0.047 s)
[INFO][2018-05-24 21:08:10,142][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 2 from persistence list
[INFO][2018-05-24 21:08:10,142][org.apache.spark.storage.BlockManager]Removing RDD 3
[INFO][2018-05-24 21:08:10,143][org.apache.spark.storage.BlockManager]Removing RDD 2
[INFO][2018-05-24 21:08:10,144][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 21:08:10,144][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527167270000 ms
[INFO][2018-05-24 21:08:30,107][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527167300000 ms
[INFO][2018-05-24 21:08:30,108][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527167300000 ms.0 from job set of time 1527167300000 ms
[INFO][2018-05-24 21:08:30,116][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:08:30,117][org.apache.spark.scheduler.DAGScheduler]Got job 3 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:08:30,117][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 3 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:08:30,117][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:08:30,117][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:08:30,118][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 3 (MapPartitionsRDD[7] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:08:30,120][org.apache.spark.storage.memory.MemoryStore]Block broadcast_3 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 21:08:30,125][org.apache.spark.storage.memory.MemoryStore]Block broadcast_3_piece0 stored as bytes in memory (estimated size 1868.0 B, free 912.3 MB)
[INFO][2018-05-24 21:08:30,126][org.apache.spark.storage.BlockManagerInfo]Added broadcast_3_piece0 in memory on 192.168.0.102:50746 (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:08:30,127][org.apache.spark.SparkContext]Created broadcast 3 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:08:30,128][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[7] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:08:30,128][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 3.0 with 1 tasks
[INFO][2018-05-24 21:08:30,129][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 3.0 (TID 3, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:08:30,130][org.apache.spark.executor.Executor]Running task 0.0 in stage 3.0 (TID 3)
[INFO][2018-05-24 21:08:30,133][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12184 is the same as ending offset skipping seven 0
[INFO][2018-05-24 21:08:30,133][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:08:30,134][org.apache.spark.executor.Executor]Finished task 0.0 in stage 3.0 (TID 3). 708 bytes result sent to driver
[INFO][2018-05-24 21:08:30,135][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 3.0 (TID 3) in 6 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:08:30,135][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 3.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:08:30,136][org.apache.spark.scheduler.DAGScheduler]ResultStage 3 (foreachPartition at ReceiveKafkaData.scala:66) finished in 0.007 s
[INFO][2018-05-24 21:08:30,136][org.apache.spark.scheduler.DAGScheduler]Job 3 finished: foreachPartition at ReceiveKafkaData.scala:66, took 0.020197 s
[INFO][2018-05-24 21:08:30,137][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527167300000 ms.0 from job set of time 1527167300000 ms
[INFO][2018-05-24 21:08:30,137][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 10.137 s for time 1527167300000 ms (execution: 0.029 s)
[INFO][2018-05-24 21:08:35,189][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527167310000 ms
[INFO][2018-05-24 21:08:35,189][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 5 from persistence list
[INFO][2018-05-24 21:08:35,191][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527167310000 ms.0 from job set of time 1527167310000 ms
[INFO][2018-05-24 21:08:35,191][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 4 from persistence list
[INFO][2018-05-24 21:08:35,192][org.apache.spark.storage.BlockManager]Removing RDD 5
[INFO][2018-05-24 21:08:35,195][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 21:08:35,196][org.apache.spark.storage.BlockManager]Removing RDD 4
[INFO][2018-05-24 21:08:35,196][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527167280000 ms
[INFO][2018-05-24 21:08:35,199][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:08:35,200][org.apache.spark.scheduler.DAGScheduler]Got job 4 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:08:35,200][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 4 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:08:35,200][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:08:35,200][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:08:35,201][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 4 (MapPartitionsRDD[9] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:08:35,203][org.apache.spark.storage.memory.MemoryStore]Block broadcast_4 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 21:08:35,207][org.apache.spark.storage.memory.MemoryStore]Block broadcast_4_piece0 stored as bytes in memory (estimated size 1868.0 B, free 912.3 MB)
[INFO][2018-05-24 21:08:35,208][org.apache.spark.storage.BlockManagerInfo]Added broadcast_4_piece0 in memory on 192.168.0.102:50746 (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:08:35,209][org.apache.spark.SparkContext]Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:08:35,211][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[9] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:08:35,211][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 4.0 with 1 tasks
[INFO][2018-05-24 21:08:35,212][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 4.0 (TID 4, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:08:35,212][org.apache.spark.executor.Executor]Running task 0.0 in stage 4.0 (TID 4)
[INFO][2018-05-24 21:08:35,216][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12184 is the same as ending offset skipping seven 0
[INFO][2018-05-24 21:08:35,216][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:08:35,217][org.apache.spark.executor.Executor]Finished task 0.0 in stage 4.0 (TID 4). 708 bytes result sent to driver
[INFO][2018-05-24 21:08:35,218][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 4.0 (TID 4) in 6 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:08:35,218][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 4.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:08:35,219][org.apache.spark.scheduler.DAGScheduler]ResultStage 4 (foreachPartition at ReceiveKafkaData.scala:66) finished in 0.008 s
[INFO][2018-05-24 21:08:35,219][org.apache.spark.scheduler.DAGScheduler]Job 4 finished: foreachPartition at ReceiveKafkaData.scala:66, took 0.020352 s
[INFO][2018-05-24 21:08:35,220][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527167310000 ms.0 from job set of time 1527167310000 ms
[INFO][2018-05-24 21:08:35,220][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 7 from persistence list
[INFO][2018-05-24 21:08:35,220][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 5.219 s for time 1527167310000 ms (execution: 0.028 s)
[INFO][2018-05-24 21:08:35,220][org.apache.spark.storage.BlockManager]Removing RDD 7
[INFO][2018-05-24 21:08:35,220][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 6 from persistence list
[INFO][2018-05-24 21:08:35,220][org.apache.spark.storage.BlockManager]Removing RDD 6
[INFO][2018-05-24 21:08:35,221][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 21:08:35,221][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527167290000 ms
[INFO][2018-05-24 21:08:46,093][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527167320000 ms
[INFO][2018-05-24 21:08:46,094][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527167320000 ms.0 from job set of time 1527167320000 ms
[INFO][2018-05-24 21:08:46,101][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:08:46,102][org.apache.spark.scheduler.DAGScheduler]Got job 5 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:08:46,102][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 5 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:08:46,102][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:08:46,102][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:08:46,102][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 5 (MapPartitionsRDD[11] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:08:46,105][org.apache.spark.storage.memory.MemoryStore]Block broadcast_5 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 21:08:46,109][org.apache.spark.storage.memory.MemoryStore]Block broadcast_5_piece0 stored as bytes in memory (estimated size 1868.0 B, free 912.3 MB)
[INFO][2018-05-24 21:08:46,110][org.apache.spark.storage.BlockManagerInfo]Added broadcast_5_piece0 in memory on 192.168.0.102:50746 (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:08:46,110][org.apache.spark.SparkContext]Created broadcast 5 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:08:46,111][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[11] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:08:46,112][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 5.0 with 1 tasks
[INFO][2018-05-24 21:08:46,113][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 5.0 (TID 5, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:08:46,113][org.apache.spark.executor.Executor]Running task 0.0 in stage 5.0 (TID 5)
[INFO][2018-05-24 21:08:46,116][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12184 is the same as ending offset skipping seven 0
[INFO][2018-05-24 21:08:46,116][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:08:46,117][org.apache.spark.executor.Executor]Finished task 0.0 in stage 5.0 (TID 5). 708 bytes result sent to driver
[INFO][2018-05-24 21:08:46,119][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 5.0 (TID 5) in 7 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:08:46,119][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 5.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:08:46,120][org.apache.spark.scheduler.DAGScheduler]ResultStage 5 (foreachPartition at ReceiveKafkaData.scala:66) finished in 0.007 s
[INFO][2018-05-24 21:08:46,120][org.apache.spark.scheduler.DAGScheduler]Job 5 finished: foreachPartition at ReceiveKafkaData.scala:66, took 0.019206 s
[INFO][2018-05-24 21:08:46,121][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527167320000 ms.0 from job set of time 1527167320000 ms
[INFO][2018-05-24 21:08:46,121][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 9 from persistence list
[INFO][2018-05-24 21:08:46,121][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 6.121 s for time 1527167320000 ms (execution: 0.027 s)
[INFO][2018-05-24 21:08:46,122][org.apache.spark.storage.BlockManager]Removing RDD 9
[INFO][2018-05-24 21:08:46,122][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 8 from persistence list
[INFO][2018-05-24 21:08:46,124][org.apache.spark.storage.BlockManager]Removing RDD 8
[INFO][2018-05-24 21:08:46,124][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 21:08:46,125][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527167300000 ms
[INFO][2018-05-24 21:08:55,079][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527167330000 ms
[INFO][2018-05-24 21:08:55,081][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527167330000 ms.0 from job set of time 1527167330000 ms
[INFO][2018-05-24 21:08:55,089][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:08:55,089][org.apache.spark.scheduler.DAGScheduler]Got job 6 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:08:55,089][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 6 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:08:55,089][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:08:55,090][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:08:55,090][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 6 (MapPartitionsRDD[13] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:08:55,092][org.apache.spark.storage.memory.MemoryStore]Block broadcast_6 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 21:08:55,136][org.apache.spark.storage.memory.MemoryStore]Block broadcast_6_piece0 stored as bytes in memory (estimated size 1868.0 B, free 912.3 MB)
[INFO][2018-05-24 21:08:55,139][org.apache.spark.storage.BlockManagerInfo]Added broadcast_6_piece0 in memory on 192.168.0.102:50746 (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:08:55,140][org.apache.spark.SparkContext]Created broadcast 6 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:08:55,142][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[13] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:08:55,142][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 6.0 with 1 tasks
[INFO][2018-05-24 21:08:55,144][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 6.0 (TID 6, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:08:55,146][org.apache.spark.executor.Executor]Running task 0.0 in stage 6.0 (TID 6)
[INFO][2018-05-24 21:08:55,149][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12184 is the same as ending offset skipping seven 0
[INFO][2018-05-24 21:08:55,149][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:08:55,150][org.apache.spark.executor.Executor]Finished task 0.0 in stage 6.0 (TID 6). 708 bytes result sent to driver
[INFO][2018-05-24 21:08:55,150][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 6.0 (TID 6) in 6 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:08:55,151][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 6.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:08:55,151][org.apache.spark.scheduler.DAGScheduler]ResultStage 6 (foreachPartition at ReceiveKafkaData.scala:66) finished in 0.008 s
[INFO][2018-05-24 21:08:55,152][org.apache.spark.scheduler.DAGScheduler]Job 6 finished: foreachPartition at ReceiveKafkaData.scala:66, took 0.062843 s
[INFO][2018-05-24 21:08:55,152][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527167330000 ms.0 from job set of time 1527167330000 ms
[INFO][2018-05-24 21:08:55,152][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 11 from persistence list
[INFO][2018-05-24 21:08:55,152][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 5.152 s for time 1527167330000 ms (execution: 0.071 s)
[INFO][2018-05-24 21:08:55,153][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_5_piece0 on 192.168.0.102:50746 in memory (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:08:55,153][org.apache.spark.storage.BlockManager]Removing RDD 11
[INFO][2018-05-24 21:08:55,154][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 10 from persistence list
[INFO][2018-05-24 21:08:55,154][org.apache.spark.storage.BlockManager]Removing RDD 10
[INFO][2018-05-24 21:08:55,154][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 21:08:55,154][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527167310000 ms
[INFO][2018-05-24 21:08:55,155][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_0_piece0 on 192.168.0.102:50746 in memory (size: 1867.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:08:55,157][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_3_piece0 on 192.168.0.102:50746 in memory (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:08:55,158][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_1_piece0 on 192.168.0.102:50746 in memory (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:08:55,159][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_2_piece0 on 192.168.0.102:50746 in memory (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:08:55,161][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_4_piece0 on 192.168.0.102:50746 in memory (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:09:00,103][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527167340000 ms
[INFO][2018-05-24 21:09:00,103][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527167340000 ms.0 from job set of time 1527167340000 ms
[INFO][2018-05-24 21:09:00,109][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:09:00,110][org.apache.spark.scheduler.DAGScheduler]Got job 7 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:09:00,110][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 7 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:09:00,110][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:09:00,110][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:09:00,111][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 7 (MapPartitionsRDD[15] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:09:00,113][org.apache.spark.storage.memory.MemoryStore]Block broadcast_7 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 21:09:00,115][org.apache.spark.storage.memory.MemoryStore]Block broadcast_7_piece0 stored as bytes in memory (estimated size 1868.0 B, free 912.3 MB)
[INFO][2018-05-24 21:09:00,115][org.apache.spark.storage.BlockManagerInfo]Added broadcast_7_piece0 in memory on 192.168.0.102:50746 (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:09:00,116][org.apache.spark.SparkContext]Created broadcast 7 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:09:00,117][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[15] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:09:00,117][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 7.0 with 1 tasks
[INFO][2018-05-24 21:09:00,118][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 7.0 (TID 7, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:09:00,118][org.apache.spark.executor.Executor]Running task 0.0 in stage 7.0 (TID 7)
[INFO][2018-05-24 21:09:00,121][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12184 is the same as ending offset skipping seven 0
[INFO][2018-05-24 21:09:00,121][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:09:00,122][org.apache.spark.executor.Executor]Finished task 0.0 in stage 7.0 (TID 7). 708 bytes result sent to driver
[INFO][2018-05-24 21:09:00,123][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 7.0 (TID 7) in 5 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:09:00,123][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 7.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:09:00,124][org.apache.spark.scheduler.DAGScheduler]ResultStage 7 (foreachPartition at ReceiveKafkaData.scala:66) finished in 0.007 s
[INFO][2018-05-24 21:09:00,124][org.apache.spark.scheduler.DAGScheduler]Job 7 finished: foreachPartition at ReceiveKafkaData.scala:66, took 0.014799 s
[INFO][2018-05-24 21:09:00,126][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527167340000 ms.0 from job set of time 1527167340000 ms
[INFO][2018-05-24 21:09:00,126][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 13 from persistence list
[INFO][2018-05-24 21:09:00,126][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.126 s for time 1527167340000 ms (execution: 0.023 s)
[INFO][2018-05-24 21:09:00,127][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 12 from persistence list
[INFO][2018-05-24 21:09:00,127][org.apache.spark.storage.BlockManager]Removing RDD 13
[INFO][2018-05-24 21:09:00,128][org.apache.spark.storage.BlockManager]Removing RDD 12
[INFO][2018-05-24 21:09:00,128][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 21:09:00,128][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527167320000 ms
[INFO][2018-05-24 21:09:20,099][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527167350000 ms
[INFO][2018-05-24 21:09:20,099][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527167350000 ms.0 from job set of time 1527167350000 ms
[INFO][2018-05-24 21:09:20,105][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:09:20,106][org.apache.spark.scheduler.DAGScheduler]Got job 8 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:09:20,106][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 8 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:09:20,106][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:09:20,107][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:09:20,107][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 8 (MapPartitionsRDD[17] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:09:20,109][org.apache.spark.storage.memory.MemoryStore]Block broadcast_8 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 21:09:20,110][org.apache.spark.storage.memory.MemoryStore]Block broadcast_8_piece0 stored as bytes in memory (estimated size 1868.0 B, free 912.3 MB)
[INFO][2018-05-24 21:09:20,111][org.apache.spark.storage.BlockManagerInfo]Added broadcast_8_piece0 in memory on 192.168.0.102:50746 (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:09:20,112][org.apache.spark.SparkContext]Created broadcast 8 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:09:20,113][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[17] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:09:20,113][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 8.0 with 1 tasks
[INFO][2018-05-24 21:09:20,114][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 8.0 (TID 8, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:09:20,115][org.apache.spark.executor.Executor]Running task 0.0 in stage 8.0 (TID 8)
[INFO][2018-05-24 21:09:20,117][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12184 is the same as ending offset skipping seven 0
[INFO][2018-05-24 21:09:20,117][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:09:20,118][org.apache.spark.executor.Executor]Finished task 0.0 in stage 8.0 (TID 8). 708 bytes result sent to driver
[INFO][2018-05-24 21:09:20,119][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 8.0 (TID 8) in 5 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:09:20,119][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 8.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:09:20,120][org.apache.spark.scheduler.DAGScheduler]ResultStage 8 (foreachPartition at ReceiveKafkaData.scala:66) finished in 0.006 s
[INFO][2018-05-24 21:09:20,122][org.apache.spark.scheduler.DAGScheduler]Job 8 finished: foreachPartition at ReceiveKafkaData.scala:66, took 0.016624 s
[INFO][2018-05-24 21:09:20,123][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527167350000 ms.0 from job set of time 1527167350000 ms
[INFO][2018-05-24 21:09:20,123][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 10.123 s for time 1527167350000 ms (execution: 0.024 s)
[INFO][2018-05-24 21:09:20,176][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527167360000 ms
[INFO][2018-05-24 21:09:20,176][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 15 from persistence list
[INFO][2018-05-24 21:09:20,176][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527167360000 ms.0 from job set of time 1527167360000 ms
[INFO][2018-05-24 21:09:20,177][org.apache.spark.storage.BlockManager]Removing RDD 15
[INFO][2018-05-24 21:09:20,177][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 14 from persistence list
[INFO][2018-05-24 21:09:20,179][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 21:09:20,179][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527167330000 ms
[INFO][2018-05-24 21:09:20,179][org.apache.spark.storage.BlockManager]Removing RDD 14
[INFO][2018-05-24 21:09:20,183][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:09:20,184][org.apache.spark.scheduler.DAGScheduler]Got job 9 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:09:20,184][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 9 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:09:20,184][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:09:20,184][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:09:20,185][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 9 (MapPartitionsRDD[19] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:09:20,187][org.apache.spark.storage.memory.MemoryStore]Block broadcast_9 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 21:09:20,188][org.apache.spark.storage.memory.MemoryStore]Block broadcast_9_piece0 stored as bytes in memory (estimated size 1868.0 B, free 912.3 MB)
[INFO][2018-05-24 21:09:20,189][org.apache.spark.storage.BlockManagerInfo]Added broadcast_9_piece0 in memory on 192.168.0.102:50746 (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:09:20,190][org.apache.spark.SparkContext]Created broadcast 9 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:09:20,190][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[19] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:09:20,190][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 9.0 with 1 tasks
[INFO][2018-05-24 21:09:20,191][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 9.0 (TID 9, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:09:20,192][org.apache.spark.executor.Executor]Running task 0.0 in stage 9.0 (TID 9)
[INFO][2018-05-24 21:09:20,193][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12184 is the same as ending offset skipping seven 0
[INFO][2018-05-24 21:09:20,194][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:09:20,195][org.apache.spark.executor.Executor]Finished task 0.0 in stage 9.0 (TID 9). 708 bytes result sent to driver
[INFO][2018-05-24 21:09:20,195][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 9.0 (TID 9) in 4 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:09:20,195][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 9.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:09:20,196][org.apache.spark.scheduler.DAGScheduler]ResultStage 9 (foreachPartition at ReceiveKafkaData.scala:66) finished in 0.005 s
[INFO][2018-05-24 21:09:20,196][org.apache.spark.scheduler.DAGScheduler]Job 9 finished: foreachPartition at ReceiveKafkaData.scala:66, took 0.012819 s
[INFO][2018-05-24 21:09:20,197][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527167360000 ms.0 from job set of time 1527167360000 ms
[INFO][2018-05-24 21:09:20,197][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 17 from persistence list
[INFO][2018-05-24 21:09:20,197][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.197 s for time 1527167360000 ms (execution: 0.021 s)
[INFO][2018-05-24 21:09:20,197][org.apache.spark.storage.BlockManager]Removing RDD 17
[INFO][2018-05-24 21:09:20,197][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 16 from persistence list
[INFO][2018-05-24 21:09:20,198][org.apache.spark.storage.BlockManager]Removing RDD 16
[INFO][2018-05-24 21:09:20,198][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 21:09:20,198][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527167340000 ms
[INFO][2018-05-24 21:09:35,068][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527167370000 ms
[INFO][2018-05-24 21:09:35,069][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527167370000 ms.0 from job set of time 1527167370000 ms
[INFO][2018-05-24 21:09:35,075][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:09:35,075][org.apache.spark.scheduler.DAGScheduler]Got job 10 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:09:35,076][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 10 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:09:35,076][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:09:35,076][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:09:35,076][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 10 (MapPartitionsRDD[21] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:09:35,078][org.apache.spark.storage.memory.MemoryStore]Block broadcast_10 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 21:09:35,079][org.apache.spark.storage.memory.MemoryStore]Block broadcast_10_piece0 stored as bytes in memory (estimated size 1868.0 B, free 912.3 MB)
[INFO][2018-05-24 21:09:35,080][org.apache.spark.storage.BlockManagerInfo]Added broadcast_10_piece0 in memory on 192.168.0.102:50746 (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:09:35,080][org.apache.spark.SparkContext]Created broadcast 10 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:09:35,081][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[21] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:09:35,081][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 10.0 with 1 tasks
[INFO][2018-05-24 21:09:35,081][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 10.0 (TID 10, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:09:35,082][org.apache.spark.executor.Executor]Running task 0.0 in stage 10.0 (TID 10)
[INFO][2018-05-24 21:09:35,083][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12184 is the same as ending offset skipping seven 0
[INFO][2018-05-24 21:09:35,083][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:09:35,084][org.apache.spark.executor.Executor]Finished task 0.0 in stage 10.0 (TID 10). 708 bytes result sent to driver
[INFO][2018-05-24 21:09:35,085][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 10.0 (TID 10) in 4 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:09:35,085][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 10.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:09:35,085][org.apache.spark.scheduler.DAGScheduler]ResultStage 10 (foreachPartition at ReceiveKafkaData.scala:66) finished in 0.004 s
[INFO][2018-05-24 21:09:35,085][org.apache.spark.scheduler.DAGScheduler]Job 10 finished: foreachPartition at ReceiveKafkaData.scala:66, took 0.010696 s
[INFO][2018-05-24 21:09:35,086][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527167370000 ms.0 from job set of time 1527167370000 ms
[INFO][2018-05-24 21:09:35,086][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 5.086 s for time 1527167370000 ms (execution: 0.017 s)
[INFO][2018-05-24 21:09:35,086][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 19 from persistence list
[INFO][2018-05-24 21:09:35,087][org.apache.spark.storage.BlockManager]Removing RDD 19
[INFO][2018-05-24 21:09:35,087][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 18 from persistence list
[INFO][2018-05-24 21:09:35,087][org.apache.spark.storage.BlockManager]Removing RDD 18
[INFO][2018-05-24 21:09:35,087][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 21:09:35,087][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527167350000 ms
[INFO][2018-05-24 21:09:37,216][org.apache.spark.storage.memory.MemoryStore]Block taskresult_1 stored as bytes in memory (estimated size 10.4 MB, free 901.7 MB)
[INFO][2018-05-24 21:09:37,217][org.apache.spark.storage.BlockManagerInfo]Added taskresult_1 in memory on 192.168.0.102:50763 (size: 10.4 MB, free: 901.9 MB)
[INFO][2018-05-24 21:09:37,217][org.apache.spark.executor.Executor]Finished task 1.0 in stage 0.0 (TID 1). 10855899 bytes result sent via BlockManager)
[INFO][2018-05-24 21:09:37,288][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 0.0 (TID 1) in 96438 ms on localhost (executor driver) (2/2)
[INFO][2018-05-24 21:09:37,288][org.apache.spark.storage.BlockManagerInfo]Removed taskresult_1 on 192.168.0.102:50763 in memory (size: 10.4 MB, free: 912.3 MB)
[INFO][2018-05-24 21:09:37,289][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:09:37,290][org.apache.spark.scheduler.DAGScheduler]ResultStage 0 (collect at SimulationKafkaSendOutData.scala:25) finished in 96.469 s
[INFO][2018-05-24 21:09:37,295][org.apache.spark.scheduler.DAGScheduler]Job 0 finished: collect at SimulationKafkaSendOutData.scala:25, took 96.626874 s
[INFO][2018-05-24 21:09:37,438][org.spark_project.jetty.server.AbstractConnector]Stopped Spark@16751330{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}
[INFO][2018-05-24 21:09:37,440][org.apache.spark.ui.SparkUI]Stopped Spark web UI at http://192.168.0.102:4041
[INFO][2018-05-24 21:09:37,448][org.apache.spark.MapOutputTrackerMasterEndpoint]MapOutputTrackerMasterEndpoint stopped!
[INFO][2018-05-24 21:09:37,481][org.apache.spark.storage.memory.MemoryStore]MemoryStore cleared
[INFO][2018-05-24 21:09:37,481][org.apache.spark.storage.BlockManager]BlockManager stopped
[INFO][2018-05-24 21:09:37,482][org.apache.spark.storage.BlockManagerMaster]BlockManagerMaster stopped
[INFO][2018-05-24 21:09:37,485][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint]OutputCommitCoordinator stopped!
[INFO][2018-05-24 21:09:37,486][org.apache.spark.SparkContext]Successfully stopped SparkContext
[INFO][2018-05-24 21:09:37,496][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	compression.type = none
	metric.reporters = []
	metadata.max.age.ms = 300000
	metadata.fetch.timeout.ms = 60000
	acks = all
	batch.size = 16384
	reconnect.backoff.ms = 10
	bootstrap.servers = [vm-xaj-bigdata-da-d01:9092, vm-xaj-bigdata-da-d02:9092, vm-xaj-bigdata-da-d03:9092, vm-xaj-bigdata-da-d04:9092, vm-xaj-bigdata-da-d05:9092, vm-xaj-bigdata-da-d06:9092, vm-xaj-bigdata-da-d07:9092]
	receive.buffer.bytes = 32768
	retry.backoff.ms = 100
	buffer.memory = 33554432
	timeout.ms = 30000
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	retries = 0
	max.request.size = 1048576
	block.on.buffer.full = true
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
	metrics.sample.window.ms = 30000
	send.buffer.bytes = 131072
	max.in.flight.requests.per.connection = 5
	metrics.num.samples = 2
	linger.ms = 1
	client.id = 

[INFO][2018-05-24 21:09:40,053][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527167380000 ms
[INFO][2018-05-24 21:09:40,054][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527167380000 ms.0 from job set of time 1527167380000 ms
[INFO][2018-05-24 21:09:40,065][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:09:40,066][org.apache.spark.scheduler.DAGScheduler]Got job 11 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:09:40,066][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 11 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:09:40,066][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:09:40,066][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:09:40,067][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 11 (MapPartitionsRDD[23] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:09:40,069][org.apache.spark.storage.memory.MemoryStore]Block broadcast_11 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 21:09:40,070][org.apache.spark.storage.memory.MemoryStore]Block broadcast_11_piece0 stored as bytes in memory (estimated size 1868.0 B, free 912.3 MB)
[INFO][2018-05-24 21:09:40,071][org.apache.spark.storage.BlockManagerInfo]Added broadcast_11_piece0 in memory on 192.168.0.102:50746 (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:09:40,071][org.apache.spark.SparkContext]Created broadcast 11 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:09:40,072][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[23] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:09:40,072][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 11.0 with 1 tasks
[INFO][2018-05-24 21:09:40,073][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 11.0 (TID 11, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:09:40,073][org.apache.spark.executor.Executor]Running task 0.0 in stage 11.0 (TID 11)
[INFO][2018-05-24 21:09:40,075][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12184 is the same as ending offset skipping seven 0
[INFO][2018-05-24 21:09:40,075][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:09:40,076][org.apache.spark.executor.Executor]Finished task 0.0 in stage 11.0 (TID 11). 708 bytes result sent to driver
[INFO][2018-05-24 21:09:40,077][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 11.0 (TID 11) in 5 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:09:40,077][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 11.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:09:40,077][org.apache.spark.scheduler.DAGScheduler]ResultStage 11 (foreachPartition at ReceiveKafkaData.scala:66) finished in 0.005 s
[INFO][2018-05-24 21:09:40,078][org.apache.spark.scheduler.DAGScheduler]Job 11 finished: foreachPartition at ReceiveKafkaData.scala:66, took 0.012115 s
[INFO][2018-05-24 21:09:40,078][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527167380000 ms.0 from job set of time 1527167380000 ms
[INFO][2018-05-24 21:09:40,078][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 21 from persistence list
[INFO][2018-05-24 21:09:40,078][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.078 s for time 1527167380000 ms (execution: 0.025 s)
[INFO][2018-05-24 21:09:40,079][org.apache.spark.storage.BlockManager]Removing RDD 21
[INFO][2018-05-24 21:09:40,079][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 20 from persistence list
[INFO][2018-05-24 21:09:40,080][org.apache.spark.storage.BlockManager]Removing RDD 20
[INFO][2018-05-24 21:09:40,080][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 21:09:40,080][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527167360000 ms
[INFO][2018-05-24 21:09:50,070][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527167390000 ms
[INFO][2018-05-24 21:09:50,070][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527167390000 ms.0 from job set of time 1527167390000 ms
[INFO][2018-05-24 21:09:50,077][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:09:50,078][org.apache.spark.scheduler.DAGScheduler]Got job 12 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:09:50,078][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 12 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:09:50,078][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:09:50,078][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:09:50,078][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 12 (MapPartitionsRDD[25] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:09:50,080][org.apache.spark.storage.memory.MemoryStore]Block broadcast_12 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 21:09:50,082][org.apache.spark.storage.memory.MemoryStore]Block broadcast_12_piece0 stored as bytes in memory (estimated size 1868.0 B, free 912.3 MB)
[INFO][2018-05-24 21:09:50,082][org.apache.spark.storage.BlockManagerInfo]Added broadcast_12_piece0 in memory on 192.168.0.102:50746 (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:09:50,083][org.apache.spark.SparkContext]Created broadcast 12 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:09:50,084][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[25] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:09:50,084][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 12.0 with 1 tasks
[INFO][2018-05-24 21:09:50,084][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 12.0 (TID 12, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:09:50,085][org.apache.spark.executor.Executor]Running task 0.0 in stage 12.0 (TID 12)
[INFO][2018-05-24 21:09:50,086][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12184 is the same as ending offset skipping seven 0
[INFO][2018-05-24 21:09:50,086][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:09:50,087][org.apache.spark.executor.Executor]Finished task 0.0 in stage 12.0 (TID 12). 665 bytes result sent to driver
[INFO][2018-05-24 21:09:50,087][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 12.0 (TID 12) in 3 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:09:50,088][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 12.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:09:50,088][org.apache.spark.scheduler.DAGScheduler]ResultStage 12 (foreachPartition at ReceiveKafkaData.scala:66) finished in 0.004 s
[INFO][2018-05-24 21:09:50,088][org.apache.spark.scheduler.DAGScheduler]Job 12 finished: foreachPartition at ReceiveKafkaData.scala:66, took 0.011433 s
[INFO][2018-05-24 21:09:50,089][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527167390000 ms.0 from job set of time 1527167390000 ms
[INFO][2018-05-24 21:09:50,089][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.089 s for time 1527167390000 ms (execution: 0.019 s)
[INFO][2018-05-24 21:09:50,089][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 23 from persistence list
[INFO][2018-05-24 21:09:50,089][org.apache.spark.storage.BlockManager]Removing RDD 23
[INFO][2018-05-24 21:09:50,090][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 22 from persistence list
[INFO][2018-05-24 21:09:50,090][org.apache.spark.storage.BlockManager]Removing RDD 22
[INFO][2018-05-24 21:09:50,090][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 21:09:50,090][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527167370000 ms
[INFO][2018-05-24 21:10:05,065][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527167400000 ms
[INFO][2018-05-24 21:10:05,066][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527167400000 ms.0 from job set of time 1527167400000 ms
[INFO][2018-05-24 21:10:05,071][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:10:05,072][org.apache.spark.scheduler.DAGScheduler]Got job 13 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:10:05,072][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 13 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:10:05,072][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:10:05,072][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:10:05,073][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 13 (MapPartitionsRDD[27] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:10:05,074][org.apache.spark.storage.memory.MemoryStore]Block broadcast_13 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 21:10:05,077][org.apache.spark.storage.memory.MemoryStore]Block broadcast_13_piece0 stored as bytes in memory (estimated size 1868.0 B, free 912.3 MB)
[INFO][2018-05-24 21:10:05,078][org.apache.spark.storage.BlockManagerInfo]Added broadcast_13_piece0 in memory on 192.168.0.102:50746 (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:10:05,078][org.apache.spark.SparkContext]Created broadcast 13 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:10:05,079][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[27] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:10:05,079][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 13.0 with 1 tasks
[INFO][2018-05-24 21:10:05,080][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 13.0 (TID 13, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:10:05,080][org.apache.spark.executor.Executor]Running task 0.0 in stage 13.0 (TID 13)
[INFO][2018-05-24 21:10:05,082][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12184 is the same as ending offset skipping seven 0
[INFO][2018-05-24 21:10:05,082][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:10:05,083][org.apache.spark.executor.Executor]Finished task 0.0 in stage 13.0 (TID 13). 708 bytes result sent to driver
[INFO][2018-05-24 21:10:05,083][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 13.0 (TID 13) in 3 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:10:05,083][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 13.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:10:05,083][org.apache.spark.scheduler.DAGScheduler]ResultStage 13 (foreachPartition at ReceiveKafkaData.scala:66) finished in 0.004 s
[INFO][2018-05-24 21:10:05,084][org.apache.spark.scheduler.DAGScheduler]Job 13 finished: foreachPartition at ReceiveKafkaData.scala:66, took 0.012404 s
[INFO][2018-05-24 21:10:05,084][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527167400000 ms.0 from job set of time 1527167400000 ms
[INFO][2018-05-24 21:10:05,084][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 5.084 s for time 1527167400000 ms (execution: 0.018 s)
[INFO][2018-05-24 21:10:05,084][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 25 from persistence list
[INFO][2018-05-24 21:10:05,084][org.apache.spark.storage.BlockManager]Removing RDD 25
[INFO][2018-05-24 21:10:05,085][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 24 from persistence list
[INFO][2018-05-24 21:10:05,085][org.apache.spark.storage.BlockManager]Removing RDD 24
[INFO][2018-05-24 21:10:05,085][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 21:10:05,085][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527167380000 ms
[INFO][2018-05-24 21:10:15,071][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527167410000 ms
[INFO][2018-05-24 21:10:15,071][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527167410000 ms.0 from job set of time 1527167410000 ms
[INFO][2018-05-24 21:10:15,077][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:10:15,077][org.apache.spark.scheduler.DAGScheduler]Got job 14 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:10:15,078][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 14 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:10:15,078][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:10:15,078][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:10:15,078][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 14 (MapPartitionsRDD[29] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:10:15,080][org.apache.spark.storage.memory.MemoryStore]Block broadcast_14 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 21:10:15,081][org.apache.spark.storage.memory.MemoryStore]Block broadcast_14_piece0 stored as bytes in memory (estimated size 1868.0 B, free 912.3 MB)
[INFO][2018-05-24 21:10:15,082][org.apache.spark.storage.BlockManagerInfo]Added broadcast_14_piece0 in memory on 192.168.0.102:50746 (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:10:15,082][org.apache.spark.SparkContext]Created broadcast 14 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:10:15,083][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[29] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:10:15,083][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 14.0 with 1 tasks
[INFO][2018-05-24 21:10:15,084][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 14.0 (TID 14, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:10:15,084][org.apache.spark.executor.Executor]Running task 0.0 in stage 14.0 (TID 14)
[INFO][2018-05-24 21:10:15,088][org.apache.spark.streaming.kafka.KafkaRDD]Computing topic seven, partition 0 offsets 12184 -> 12185
[INFO][2018-05-24 21:10:15,089][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 21:10:15,089][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 21:10:15,089][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 21:10:20,522][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527167420000 ms
[INFO][2018-05-24 21:10:20,550][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:10:20,551][org.apache.spark.executor.Executor]Finished task 0.0 in stage 14.0 (TID 14). 665 bytes result sent to driver
[INFO][2018-05-24 21:10:20,551][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 14.0 (TID 14) in 5468 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:10:20,551][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 14.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:10:20,552][org.apache.spark.scheduler.DAGScheduler]ResultStage 14 (foreachPartition at ReceiveKafkaData.scala:66) finished in 5.469 s
[INFO][2018-05-24 21:10:20,552][org.apache.spark.scheduler.DAGScheduler]Job 14 finished: foreachPartition at ReceiveKafkaData.scala:66, took 5.475123 s
[INFO][2018-05-24 21:10:20,552][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527167410000 ms.0 from job set of time 1527167410000 ms
[INFO][2018-05-24 21:10:20,553][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 10.552 s for time 1527167410000 ms (execution: 5.481 s)
[INFO][2018-05-24 21:10:20,553][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527167420000 ms.0 from job set of time 1527167420000 ms
[INFO][2018-05-24 21:10:20,553][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 27 from persistence list
[INFO][2018-05-24 21:10:20,556][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 26 from persistence list
[INFO][2018-05-24 21:10:20,557][org.apache.spark.storage.BlockManager]Removing RDD 27
[INFO][2018-05-24 21:10:20,558][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 21:10:20,558][org.apache.spark.storage.BlockManager]Removing RDD 26
[INFO][2018-05-24 21:10:20,558][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527167390000 ms
[INFO][2018-05-24 21:10:20,559][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:10:20,560][org.apache.spark.scheduler.DAGScheduler]Got job 15 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:10:20,560][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 15 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:10:20,560][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:10:20,560][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:10:20,560][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 15 (MapPartitionsRDD[31] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:10:20,561][org.apache.spark.storage.memory.MemoryStore]Block broadcast_15 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 21:10:20,562][org.apache.spark.storage.memory.MemoryStore]Block broadcast_15_piece0 stored as bytes in memory (estimated size 1868.0 B, free 912.3 MB)
[INFO][2018-05-24 21:10:20,563][org.apache.spark.storage.BlockManagerInfo]Added broadcast_15_piece0 in memory on 192.168.0.102:50746 (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:10:20,563][org.apache.spark.SparkContext]Created broadcast 15 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:10:20,564][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 15 (MapPartitionsRDD[31] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:10:20,564][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 15.0 with 1 tasks
[INFO][2018-05-24 21:10:20,565][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 15.0 (TID 15, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:10:20,565][org.apache.spark.executor.Executor]Running task 0.0 in stage 15.0 (TID 15)
[INFO][2018-05-24 21:10:20,567][org.apache.spark.streaming.kafka.KafkaRDD]Computing topic seven, partition 0 offsets 12185 -> 12188
[INFO][2018-05-24 21:10:20,567][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 21:10:20,567][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 21:10:20,567][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 21:10:20,632][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:10:20,635][org.apache.spark.executor.Executor]Finished task 0.0 in stage 15.0 (TID 15). 708 bytes result sent to driver
[INFO][2018-05-24 21:10:20,638][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 15.0 (TID 15) in 73 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:10:20,638][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 15.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:10:20,638][org.apache.spark.scheduler.DAGScheduler]ResultStage 15 (foreachPartition at ReceiveKafkaData.scala:66) finished in 0.074 s
[INFO][2018-05-24 21:10:20,639][org.apache.spark.scheduler.DAGScheduler]Job 15 finished: foreachPartition at ReceiveKafkaData.scala:66, took 0.079641 s
[INFO][2018-05-24 21:10:20,639][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527167420000 ms.0 from job set of time 1527167420000 ms
[INFO][2018-05-24 21:10:20,640][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.639 s for time 1527167420000 ms (execution: 0.086 s)
[INFO][2018-05-24 21:10:20,640][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 29 from persistence list
[INFO][2018-05-24 21:10:20,640][org.apache.spark.storage.BlockManager]Removing RDD 29
[INFO][2018-05-24 21:10:20,641][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 28 from persistence list
[INFO][2018-05-24 21:10:20,641][org.apache.spark.storage.BlockManager]Removing RDD 28
[INFO][2018-05-24 21:10:20,642][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 21:10:20,642][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527167400000 ms
[INFO][2018-05-24 21:10:35,060][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527167430000 ms
[INFO][2018-05-24 21:10:35,061][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527167430000 ms.0 from job set of time 1527167430000 ms
[INFO][2018-05-24 21:10:35,067][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:10:35,068][org.apache.spark.scheduler.DAGScheduler]Got job 16 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:10:35,068][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 16 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:10:35,068][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:10:35,068][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:10:35,068][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 16 (MapPartitionsRDD[33] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:10:35,070][org.apache.spark.storage.memory.MemoryStore]Block broadcast_16 stored as values in memory (estimated size 3.3 KB, free 912.2 MB)
[INFO][2018-05-24 21:10:35,076][org.apache.spark.storage.memory.MemoryStore]Block broadcast_16_piece0 stored as bytes in memory (estimated size 1868.0 B, free 912.2 MB)
[INFO][2018-05-24 21:10:35,076][org.apache.spark.storage.BlockManagerInfo]Added broadcast_16_piece0 in memory on 192.168.0.102:50746 (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:10:35,077][org.apache.spark.SparkContext]Created broadcast 16 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:10:35,078][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 16 (MapPartitionsRDD[33] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:10:35,078][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 16.0 with 1 tasks
[INFO][2018-05-24 21:10:35,078][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 16.0 (TID 16, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:10:35,079][org.apache.spark.executor.Executor]Running task 0.0 in stage 16.0 (TID 16)
[INFO][2018-05-24 21:10:35,080][org.apache.spark.streaming.kafka.KafkaRDD]Computing topic seven, partition 0 offsets 12188 -> 12193
[INFO][2018-05-24 21:10:35,080][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 21:10:35,080][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 21:10:35,080][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 21:10:37,859][org.apache.spark.util.ShutdownHookManager]Shutdown hook called
[INFO][2018-05-24 21:10:37,861][org.apache.spark.util.ShutdownHookManager]Deleting directory /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/spark-a2240a3a-0a8f-4655-87ee-cae952ae8136
[INFO][2018-05-24 21:10:39,302][org.apache.spark.streaming.StreamingContext]Invoking stop(stopGracefully=false) from shutdown hook
[INFO][2018-05-24 21:10:39,304][org.apache.spark.streaming.scheduler.ReceiverTracker]ReceiverTracker stopped
[INFO][2018-05-24 21:10:39,304][org.apache.spark.streaming.scheduler.JobGenerator]Stopping JobGenerator immediately
[INFO][2018-05-24 21:10:39,305][org.apache.spark.streaming.util.RecurringTimer]Stopped timer for JobGenerator after time 1527167430000
[INFO][2018-05-24 21:10:39,309][org.apache.spark.streaming.scheduler.JobGenerator]Stopped JobGenerator
[INFO][2018-05-24 21:10:40,153][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:10:40,154][org.apache.spark.executor.Executor]Finished task 0.0 in stage 16.0 (TID 16). 708 bytes result sent to driver
[INFO][2018-05-24 21:10:40,155][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 16.0 (TID 16) in 5077 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:10:40,156][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 16.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:10:40,156][org.apache.spark.scheduler.DAGScheduler]ResultStage 16 (foreachPartition at ReceiveKafkaData.scala:66) finished in 5.078 s
[INFO][2018-05-24 21:10:40,157][org.apache.spark.scheduler.DAGScheduler]Job 16 finished: foreachPartition at ReceiveKafkaData.scala:66, took 5.089574 s
[INFO][2018-05-24 21:10:40,157][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527167430000 ms.0 from job set of time 1527167430000 ms
[INFO][2018-05-24 21:10:40,158][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 10.157 s for time 1527167430000 ms (execution: 5.096 s)
[INFO][2018-05-24 21:10:40,160][org.apache.spark.streaming.scheduler.JobScheduler]Stopped JobScheduler
[INFO][2018-05-24 21:10:40,166][org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@1ab6718{/streaming,null,UNAVAILABLE,@Spark}
[INFO][2018-05-24 21:10:40,166][org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@78b41097{/streaming/batch,null,UNAVAILABLE,@Spark}
[INFO][2018-05-24 21:10:40,167][org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@7cf283e1{/static/streaming,null,UNAVAILABLE,@Spark}
[INFO][2018-05-24 21:10:40,168][org.apache.spark.streaming.StreamingContext]StreamingContext stopped successfully
[INFO][2018-05-24 21:10:40,168][org.apache.spark.SparkContext]Invoking stop() from shutdown hook
[INFO][2018-05-24 21:10:40,175][org.spark_project.jetty.server.AbstractConnector]Stopped Spark@6d366c9b{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-05-24 21:10:40,176][org.apache.spark.ui.SparkUI]Stopped Spark web UI at http://192.168.0.102:4040
[INFO][2018-05-24 21:10:40,185][org.apache.spark.MapOutputTrackerMasterEndpoint]MapOutputTrackerMasterEndpoint stopped!
[INFO][2018-05-24 21:10:40,203][org.apache.spark.storage.memory.MemoryStore]MemoryStore cleared
[INFO][2018-05-24 21:10:40,204][org.apache.spark.storage.BlockManager]BlockManager stopped
[INFO][2018-05-24 21:10:40,204][org.apache.spark.storage.BlockManagerMaster]BlockManagerMaster stopped
[INFO][2018-05-24 21:10:40,207][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint]OutputCommitCoordinator stopped!
[INFO][2018-05-24 21:10:40,208][org.apache.spark.SparkContext]Successfully stopped SparkContext
[INFO][2018-05-24 21:10:40,209][org.apache.spark.util.ShutdownHookManager]Shutdown hook called
[INFO][2018-05-24 21:10:40,210][org.apache.spark.util.ShutdownHookManager]Deleting directory /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/spark-232c486d-c3c1-442e-9d8d-bc5001c86041
[INFO][2018-05-24 21:11:04,061][org.apache.spark.SparkContext]Running Spark version 2.2.0
[INFO][2018-05-24 21:11:05,162][org.apache.spark.SparkContext]Submitted application: ReceiveKafkaData$
[INFO][2018-05-24 21:11:05,184][org.apache.spark.SecurityManager]Changing view acls to: seven
[INFO][2018-05-24 21:11:05,185][org.apache.spark.SecurityManager]Changing modify acls to: seven
[INFO][2018-05-24 21:11:05,185][org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO][2018-05-24 21:11:05,186][org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO][2018-05-24 21:11:05,186][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(seven); groups with view permissions: Set(); users  with modify permissions: Set(seven); groups with modify permissions: Set()
[INFO][2018-05-24 21:11:05,474][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 50859.
[INFO][2018-05-24 21:11:05,499][org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO][2018-05-24 21:11:05,534][org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO][2018-05-24 21:11:05,540][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2018-05-24 21:11:05,541][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
[INFO][2018-05-24 21:11:05,556][org.apache.spark.storage.DiskBlockManager]Created local directory at /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/blockmgr-e2b10f97-7286-4247-a936-d6282f209627
[INFO][2018-05-24 21:11:05,576][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 912.3 MB
[INFO][2018-05-24 21:11:05,676][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO][2018-05-24 21:11:05,797][org.spark_project.jetty.util.log]Logging initialized @2742ms
[INFO][2018-05-24 21:11:05,866][org.spark_project.jetty.server.Server]jetty-9.3.z-SNAPSHOT
[INFO][2018-05-24 21:11:05,880][org.spark_project.jetty.server.Server]Started @2827ms
[INFO][2018-05-24 21:11:05,909][org.spark_project.jetty.server.AbstractConnector]Started ServerConnector@1b85fa0b{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-05-24 21:11:05,910][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4040.
[INFO][2018-05-24 21:11:05,937][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@c7a975a{/jobs,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:11:05,939][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@55795845{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:11:05,940][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@119f1f2a{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:11:05,941][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7fd4acee{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:11:05,941][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6175619b{/stages,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:11:05,942][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@756cf158{/stages/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:11:05,943][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@751d3241{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:11:05,944][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@30ea8c23{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:11:05,945][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4e76dac{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:11:05,945][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5f2f577{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:11:05,946][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5d465e4b{/storage,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:11:05,947][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@41a90fa8{/storage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:11:05,947][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@52500920{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:11:05,948][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@18a3962d{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:11:05,949][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2a65bb85{/environment,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:11:05,953][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4f936da8{/environment/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:11:05,954][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@452ba1db{/executors,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:11:05,955][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@76a36b71{/executors/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:11:05,956][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@f9d87b{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:11:05,956][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@26fb628{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:11:05,968][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@70dd7e15{/static,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:11:05,973][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@60d8c0dc{/,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:11:05,978][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6a62689d{/api,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:11:05,979][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1849db1a{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:11:05,980][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1ca25c47{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:11:05,982][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://192.168.0.102:4040
[INFO][2018-05-24 21:11:06,084][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
[INFO][2018-05-24 21:11:06,103][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 50860.
[INFO][2018-05-24 21:11:06,104][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 192.168.0.102:50860
[INFO][2018-05-24 21:11:06,106][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2018-05-24 21:11:06,108][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 192.168.0.102, 50860, None)
[INFO][2018-05-24 21:11:06,111][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 192.168.0.102:50860 with 912.3 MB RAM, BlockManagerId(driver, 192.168.0.102, 50860, None)
[INFO][2018-05-24 21:11:06,114][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 192.168.0.102, 50860, None)
[INFO][2018-05-24 21:11:06,116][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 192.168.0.102, 50860, None)
[INFO][2018-05-24 21:11:06,332][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@17f460bb{/metrics/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:11:06,509][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 21:11:06,514][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 21:11:06,515][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 21:11:19,133][org.apache.spark.SparkContext]Running Spark version 2.2.0
[INFO][2018-05-24 21:11:19,944][org.apache.spark.SparkContext]Submitted application: SimulationKafkaSendOutData$
[INFO][2018-05-24 21:11:19,970][org.apache.spark.SecurityManager]Changing view acls to: seven
[INFO][2018-05-24 21:11:19,972][org.apache.spark.SecurityManager]Changing modify acls to: seven
[INFO][2018-05-24 21:11:19,973][org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO][2018-05-24 21:11:19,974][org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO][2018-05-24 21:11:19,975][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(seven); groups with view permissions: Set(); users  with modify permissions: Set(seven); groups with modify permissions: Set()
[INFO][2018-05-24 21:11:20,382][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 50868.
[INFO][2018-05-24 21:11:20,406][org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO][2018-05-24 21:11:20,423][org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO][2018-05-24 21:11:20,425][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2018-05-24 21:11:20,426][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
[INFO][2018-05-24 21:11:20,438][org.apache.spark.storage.DiskBlockManager]Created local directory at /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/blockmgr-f312f2c6-cef6-4af9-a3a3-7ad8c6ebfeaa
[INFO][2018-05-24 21:11:20,463][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 912.3 MB
[INFO][2018-05-24 21:11:20,561][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO][2018-05-24 21:11:20,715][org.spark_project.jetty.util.log]Logging initialized @2499ms
[INFO][2018-05-24 21:11:20,790][org.spark_project.jetty.server.Server]jetty-9.3.z-SNAPSHOT
[INFO][2018-05-24 21:11:20,802][org.spark_project.jetty.server.Server]Started @2588ms
[WARN][2018-05-24 21:11:20,816][org.apache.spark.util.Utils]Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[INFO][2018-05-24 21:11:20,821][org.spark_project.jetty.server.AbstractConnector]Started ServerConnector@5427d3ac{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}
[INFO][2018-05-24 21:11:20,821][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4041.
[INFO][2018-05-24 21:11:20,843][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@42b64ab8{/jobs,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:11:20,844][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2c1b9e4b{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:11:20,844][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@649725e3{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:11:20,845][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@52b56a3e{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:11:20,846][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4eed46ee{/stages,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:11:20,846][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4fad94a7{/stages/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:11:20,847][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6326d182{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:11:20,848][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@226642a5{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:11:20,849][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5cc126dc{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:11:20,849][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@72bd06ca{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:11:20,850][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5dbe30be{/storage,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:11:20,851][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@55795845{/storage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:11:20,851][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@119f1f2a{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:11:20,852][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5b970f7{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:11:20,853][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@165b8a71{/environment,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:11:20,855][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2f058b8a{/environment/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:11:20,855][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3f2ef586{/executors,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:11:20,856][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@76c7beb3{/executors/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:11:20,856][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2cf92cc7{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:11:20,857][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7b139eab{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:11:20,863][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@611df6e3{/static,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:11:20,864][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6ffab045{/,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:11:20,865][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3e2943ab{/api,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:11:20,866][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6c67e137{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:11:20,866][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@191ae03f{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:11:20,869][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://192.168.0.102:4041
[INFO][2018-05-24 21:11:21,014][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
[INFO][2018-05-24 21:11:21,047][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 50869.
[INFO][2018-05-24 21:11:21,048][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 192.168.0.102:50869
[INFO][2018-05-24 21:11:21,050][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2018-05-24 21:11:21,052][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 192.168.0.102, 50869, None)
[INFO][2018-05-24 21:11:21,056][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 192.168.0.102:50869 with 912.3 MB RAM, BlockManagerId(driver, 192.168.0.102, 50869, None)
[INFO][2018-05-24 21:11:21,061][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 192.168.0.102, 50869, None)
[INFO][2018-05-24 21:11:21,062][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 192.168.0.102, 50869, None)
[INFO][2018-05-24 21:11:21,255][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3e1162e7{/metrics/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:11:21,823][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0 stored as values in memory (estimated size 228.1 KB, free 912.1 MB)
[INFO][2018-05-24 21:11:21,906][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0_piece0 stored as bytes in memory (estimated size 22.1 KB, free 912.1 MB)
[INFO][2018-05-24 21:11:21,909][org.apache.spark.storage.BlockManagerInfo]Added broadcast_0_piece0 in memory on 192.168.0.102:50869 (size: 22.1 KB, free: 912.3 MB)
[INFO][2018-05-24 21:11:21,921][org.apache.spark.SparkContext]Created broadcast 0 from textFile at SimulationKafkaSendOutData.scala:25
[INFO][2018-05-24 21:11:22,143][org.apache.spark.streaming.kafka.DirectKafkaInputDStream]Slide time = 10000 ms
[INFO][2018-05-24 21:11:22,143][org.apache.spark.streaming.kafka.DirectKafkaInputDStream]Storage level = Serialized 1x Replicated
[INFO][2018-05-24 21:11:22,144][org.apache.spark.streaming.kafka.DirectKafkaInputDStream]Checkpoint interval = null
[INFO][2018-05-24 21:11:22,145][org.apache.spark.streaming.kafka.DirectKafkaInputDStream]Remember interval = 10000 ms
[INFO][2018-05-24 21:11:22,145][org.apache.spark.streaming.kafka.DirectKafkaInputDStream]Initialized and validated org.apache.spark.streaming.kafka.DirectKafkaInputDStream@459095da
[INFO][2018-05-24 21:11:22,146][org.apache.spark.streaming.dstream.MappedDStream]Slide time = 10000 ms
[INFO][2018-05-24 21:11:22,146][org.apache.spark.streaming.dstream.MappedDStream]Storage level = Serialized 1x Replicated
[INFO][2018-05-24 21:11:22,146][org.apache.spark.streaming.dstream.MappedDStream]Checkpoint interval = null
[INFO][2018-05-24 21:11:22,146][org.apache.spark.streaming.dstream.MappedDStream]Remember interval = 10000 ms
[INFO][2018-05-24 21:11:22,146][org.apache.spark.streaming.dstream.MappedDStream]Initialized and validated org.apache.spark.streaming.dstream.MappedDStream@41341f7f
[INFO][2018-05-24 21:11:22,146][org.apache.spark.streaming.dstream.ForEachDStream]Slide time = 10000 ms
[INFO][2018-05-24 21:11:22,146][org.apache.spark.streaming.dstream.ForEachDStream]Storage level = Serialized 1x Replicated
[INFO][2018-05-24 21:11:22,147][org.apache.spark.streaming.dstream.ForEachDStream]Checkpoint interval = null
[INFO][2018-05-24 21:11:22,147][org.apache.spark.streaming.dstream.ForEachDStream]Remember interval = 10000 ms
[INFO][2018-05-24 21:11:22,147][org.apache.spark.streaming.dstream.ForEachDStream]Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@3f1bae71
[INFO][2018-05-24 21:11:22,198][org.apache.spark.streaming.util.RecurringTimer]Started timer for JobGenerator at time 1527167490000
[INFO][2018-05-24 21:11:22,198][org.apache.spark.streaming.scheduler.JobGenerator]Started JobGenerator at 1527167490000 ms
[INFO][2018-05-24 21:11:22,199][org.apache.spark.streaming.scheduler.JobScheduler]Started JobScheduler
[INFO][2018-05-24 21:11:22,203][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@57a48985{/streaming,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:11:22,203][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3910fe11{/streaming/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:11:22,206][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3f672204{/streaming/batch,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:11:22,207][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2c2db130{/streaming/batch/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:11:22,208][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2d000e80{/static/streaming,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:11:22,208][org.apache.spark.streaming.StreamingContext]StreamingContext started
[WARN][2018-05-24 21:11:27,351][org.apache.hadoop.util.NativeCodeLoader]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO][2018-05-24 21:11:27,497][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 1
[INFO][2018-05-24 21:11:27,574][org.apache.spark.SparkContext]Starting job: collect at SimulationKafkaSendOutData.scala:25
[INFO][2018-05-24 21:11:27,596][org.apache.spark.scheduler.DAGScheduler]Got job 0 (collect at SimulationKafkaSendOutData.scala:25) with 2 output partitions
[INFO][2018-05-24 21:11:27,598][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 0 (collect at SimulationKafkaSendOutData.scala:25)
[INFO][2018-05-24 21:11:27,598][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:11:27,601][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:11:27,618][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 0 (hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/* MapPartitionsRDD[1] at textFile at SimulationKafkaSendOutData.scala:25), which has no missing parents
[INFO][2018-05-24 21:11:27,640][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1 stored as values in memory (estimated size 3.3 KB, free 912.1 MB)
[INFO][2018-05-24 21:11:27,642][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1_piece0 stored as bytes in memory (estimated size 1969.0 B, free 912.1 MB)
[INFO][2018-05-24 21:11:27,643][org.apache.spark.storage.BlockManagerInfo]Added broadcast_1_piece0 in memory on 192.168.0.102:50869 (size: 1969.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:11:27,643][org.apache.spark.SparkContext]Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:11:27,656][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ResultStage 0 (hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/* MapPartitionsRDD[1] at textFile at SimulationKafkaSendOutData.scala:25) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2018-05-24 21:11:27,657][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 0.0 with 2 tasks
[INFO][2018-05-24 21:11:27,691][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 4883 bytes)
[INFO][2018-05-24 21:11:27,693][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 4883 bytes)
[INFO][2018-05-24 21:11:27,700][org.apache.spark.executor.Executor]Running task 1.0 in stage 0.0 (TID 1)
[INFO][2018-05-24 21:11:27,700][org.apache.spark.executor.Executor]Running task 0.0 in stage 0.0 (TID 0)
[INFO][2018-05-24 21:11:27,750][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/part-00000:0+10717435
[INFO][2018-05-24 21:11:27,750][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/part-00000:10717435+10717436
[INFO][2018-05-24 21:11:30,053][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 21:11:30,053][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 21:11:30,053][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 21:11:35,162][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527167490000 ms
[INFO][2018-05-24 21:11:35,166][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527167490000 ms.0 from job set of time 1527167490000 ms
[INFO][2018-05-24 21:11:35,206][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:11:35,223][org.apache.spark.scheduler.DAGScheduler]Got job 0 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:11:35,224][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 0 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:11:35,225][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:11:35,226][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:11:35,236][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 0 (MapPartitionsRDD[1] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:11:35,372][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 21:11:35,397][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0_piece0 stored as bytes in memory (estimated size 1867.0 B, free 912.3 MB)
[INFO][2018-05-24 21:11:35,398][org.apache.spark.storage.BlockManagerInfo]Added broadcast_0_piece0 in memory on 192.168.0.102:50860 (size: 1867.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:11:35,400][org.apache.spark.SparkContext]Created broadcast 0 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:11:35,414][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:11:35,415][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 0.0 with 1 tasks
[INFO][2018-05-24 21:11:35,450][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:11:35,459][org.apache.spark.executor.Executor]Running task 0.0 in stage 0.0 (TID 0)
[INFO][2018-05-24 21:11:35,490][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12194 is the same as ending offset skipping seven 0
[INFO][2018-05-24 21:11:35,694][org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper]Process identifier=hconnection-0x53fa245a connecting to ZooKeeper ensemble=vm-xaj-bigdata-da-d01:2181,vm-xaj-bigdata-da-d02:2181,vm-xaj-bigdata-da-d03:2181
[INFO][2018-05-24 21:11:35,700][org.apache.zookeeper.ZooKeeper]Client environment:zookeeper.version=3.4.5-cdh5.7.0--1, built on 04/06/2016 14:24 GMT
[INFO][2018-05-24 21:11:35,700][org.apache.zookeeper.ZooKeeper]Client environment:host.name=192.168.0.102
[INFO][2018-05-24 21:11:35,700][org.apache.zookeeper.ZooKeeper]Client environment:java.version=1.8.0_161
[INFO][2018-05-24 21:11:35,700][org.apache.zookeeper.ZooKeeper]Client environment:java.vendor=Oracle Corporation
[INFO][2018-05-24 21:11:35,700][org.apache.zookeeper.ZooKeeper]Client environment:java.home=/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre
[INFO][2018-05-24 21:11:35,700][org.apache.zookeeper.ZooKeeper]Client environment:java.class.path=/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/charsets.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/deploy.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/cldrdata.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/dnsns.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/jaccess.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/jfxrt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/localedata.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/nashorn.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/sunec.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/sunjce_provider.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/sunpkcs11.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/zipfs.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/javaws.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/jce.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/jfr.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/jfxswt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/jsse.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/management-agent.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/plugin.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/resources.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/rt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/ant-javafx.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/dt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/javafx-mx.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/jconsole.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/packager.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/sa-jdi.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/tools.jar:/Users/seven/project/scala/dataMining/target/classes:/Users/seven/software/maven/repository/org/scala-lang/scala-library/2.11.12/scala-library-2.11.12.jar:/Users/seven/software/maven/repository/org/scala-lang/scala-compiler/2.11.12/scala-compiler-2.11.12.jar:/Users/seven/software/maven/repository/org/scala-lang/modules/scala-xml_2.11/1.0.5/scala-xml_2.11-1.0.5.jar:/Users/seven/software/maven/repository/org/scala-lang/modules/scala-parser-combinators_2.11/1.0.4/scala-parser-combinators_2.11-1.0.4.jar:/Users/seven/software/maven/repository/org/scala-lang/scala-reflect/2.11.12/scala-reflect-2.11.12.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-client/2.6.0-cdh5.7.0/hadoop-client-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-common/2.6.0-cdh5.7.0/hadoop-common-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/xmlenc/xmlenc/0.52/xmlenc-0.52.jar:/Users/seven/software/maven/repository/commons-net/commons-net/3.1/commons-net-3.1.jar:/Users/seven/software/maven/repository/commons-configuration/commons-configuration/1.6/commons-configuration-1.6.jar:/Users/seven/software/maven/repository/commons-digester/commons-digester/1.8/commons-digester-1.8.jar:/Users/seven/software/maven/repository/commons-beanutils/commons-beanutils/1.7.0/commons-beanutils-1.7.0.jar:/Users/seven/software/maven/repository/org/slf4j/slf4j-api/1.7.5/slf4j-api-1.7.5.jar:/Users/seven/software/maven/repository/org/slf4j/slf4j-log4j12/1.7.5/slf4j-log4j12-1.7.5.jar:/Users/seven/software/maven/repository/org/apache/avro/avro/1.7.6-cdh5.7.0/avro-1.7.6-cdh5.7.0.jar:/Users/seven/software/maven/repository/com/google/code/gson/gson/2.2.4/gson-2.2.4.jar:/Users/seven/software/maven/repository/org/apache/curator/curator-client/2.7.1/curator-client-2.7.1.jar:/Users/seven/software/maven/repository/org/apache/curator/curator-recipes/2.7.1/curator-recipes-2.7.1.jar:/Users/seven/software/maven/repository/com/google/code/findbugs/jsr305/3.0.0/jsr305-3.0.0.jar:/Users/seven/software/maven/repository/org/apache/htrace/htrace-core4/4.0.1-incubating/htrace-core4-4.0.1-incubating.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-compress/1.4.1/commons-compress-1.4.1.jar:/Users/seven/software/maven/repository/org/tukaani/xz/1.0/xz-1.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-hdfs/2.6.0-cdh5.7.0/hadoop-hdfs-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/io/netty/netty/3.6.2.Final/netty-3.6.2.Final.jar:/Users/seven/software/maven/repository/xerces/xercesImpl/2.9.1/xercesImpl-2.9.1.jar:/Users/seven/software/maven/repository/xml-apis/xml-apis/1.3.04/xml-apis-1.3.04.jar:/Users/seven/software/maven/repository/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-app/2.6.0-cdh5.7.0/hadoop-mapreduce-client-app-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-common/2.6.0-cdh5.7.0/hadoop-mapreduce-client-common-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-yarn-client/2.6.0-cdh5.7.0/hadoop-yarn-client-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-yarn-server-common/2.6.0-cdh5.7.0/hadoop-yarn-server-common-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-shuffle/2.6.0-cdh5.7.0/hadoop-mapreduce-client-shuffle-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-yarn-api/2.6.0-cdh5.7.0/hadoop-yarn-api-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-core/2.6.0-cdh5.7.0/hadoop-mapreduce-client-core-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-yarn-common/2.6.0-cdh5.7.0/hadoop-yarn-common-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/javax/xml/bind/jaxb-api/2.2.2/jaxb-api-2.2.2.jar:/Users/seven/software/maven/repository/javax/xml/stream/stax-api/1.0-2/stax-api-1.0-2.jar:/Users/seven/software/maven/repository/javax/activation/activation/1.1/activation-1.1.jar:/Users/seven/software/maven/repository/com/sun/jersey/jersey-client/1.9/jersey-client-1.9.jar:/Users/seven/software/maven/repository/org/codehaus/jackson/jackson-xc/1.8.8/jackson-xc-1.8.8.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.6.0-cdh5.7.0/hadoop-mapreduce-client-jobclient-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-aws/2.6.0-cdh5.7.0/hadoop-aws-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/com/amazonaws/aws-java-sdk-s3/1.10.6/aws-java-sdk-s3-1.10.6.jar:/Users/seven/software/maven/repository/com/amazonaws/aws-java-sdk-kms/1.10.6/aws-java-sdk-kms-1.10.6.jar:/Users/seven/software/maven/repository/com/amazonaws/aws-java-sdk-core/1.10.6/aws-java-sdk-core-1.10.6.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-annotations/2.6.0-cdh5.7.0/hadoop-annotations-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-client/1.2.0-cdh5.7.0/hbase-client-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-annotations/1.2.0-cdh5.7.0/hbase-annotations-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-common/1.2.0-cdh5.7.0/hbase-common-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-protocol/1.2.0-cdh5.7.0/hbase-protocol-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/commons-codec/commons-codec/1.9/commons-codec-1.9.jar:/Users/seven/software/maven/repository/commons-io/commons-io/2.4/commons-io-2.4.jar:/Users/seven/software/maven/repository/commons-lang/commons-lang/2.6/commons-lang-2.6.jar:/Users/seven/software/maven/repository/commons-logging/commons-logging/1.2/commons-logging-1.2.jar:/Users/seven/software/maven/repository/com/google/guava/guava/12.0.1/guava-12.0.1.jar:/Users/seven/software/maven/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/Users/seven/software/maven/repository/io/netty/netty-all/4.0.23.Final/netty-all-4.0.23.Final.jar:/Users/seven/software/maven/repository/org/apache/zookeeper/zookeeper/3.4.5-cdh5.7.0/zookeeper-3.4.5-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/htrace/htrace-core/3.2.0-incubating/htrace-core-3.2.0-incubating.jar:/Users/seven/software/maven/repository/org/codehaus/jackson/jackson-mapper-asl/1.8.8/jackson-mapper-asl-1.8.8.jar:/Users/seven/software/maven/repository/org/jruby/jcodings/jcodings/1.0.8/jcodings-1.0.8.jar:/Users/seven/software/maven/repository/org/jruby/joni/joni/2.1.2/joni-2.1.2.jar:/Users/seven/software/maven/repository/com/yammer/metrics/metrics-core/2.2.0/metrics-core-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-auth/2.6.0-cdh5.7.0/hadoop-auth-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/httpcomponents/httpclient/4.2.5/httpclient-4.2.5.jar:/Users/seven/software/maven/repository/org/apache/httpcomponents/httpcore/4.2.4/httpcore-4.2.4.jar:/Users/seven/software/maven/repository/org/apache/directory/server/apacheds-kerberos-codec/2.0.0-M15/apacheds-kerberos-codec-2.0.0-M15.jar:/Users/seven/software/maven/repository/org/apache/directory/server/apacheds-i18n/2.0.0-M15/apacheds-i18n-2.0.0-M15.jar:/Users/seven/software/maven/repository/org/apache/directory/api/api-asn1-api/1.0.0-M20/api-asn1-api-1.0.0-M20.jar:/Users/seven/software/maven/repository/org/apache/directory/api/api-util/1.0.0-M20/api-util-1.0.0-M20.jar:/Users/seven/software/maven/repository/org/apache/curator/curator-framework/2.7.1/curator-framework-2.7.1.jar:/Users/seven/software/maven/repository/com/github/stephenc/findbugs/findbugs-annotations/1.3.9-1/findbugs-annotations-1.3.9-1.jar:/Users/seven/software/maven/repository/junit/junit/4.12/junit-4.12.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-server/1.2.0-cdh5.7.0/hbase-server-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-procedure/1.2.0-cdh5.7.0/hbase-procedure-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-common/1.2.0-cdh5.7.0/hbase-common-1.2.0-cdh5.7.0-tests.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-prefix-tree/1.2.0-cdh5.7.0/hbase-prefix-tree-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar:/Users/seven/software/maven/repository/commons-collections/commons-collections/3.2.2/commons-collections-3.2.2.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-hadoop-compat/1.2.0-cdh5.7.0/hbase-hadoop-compat-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-hadoop2-compat/1.2.0-cdh5.7.0/hbase-hadoop2-compat-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/com/sun/jersey/jersey-core/1.9/jersey-core-1.9.jar:/Users/seven/software/maven/repository/com/sun/jersey/jersey-server/1.9/jersey-server-1.9.jar:/Users/seven/software/maven/repository/asm/asm/3.1/asm-3.1.jar:/Users/seven/software/maven/repository/commons-cli/commons-cli/1.2/commons-cli-1.2.jar:/Users/seven/software/maven/repository/com/github/stephenc/high-scale-lib/high-scale-lib/1.1.1/high-scale-lib-1.1.1.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-math/2.1/commons-math-2.1.jar:/Users/seven/software/maven/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jetty/6.1.26.cloudera.4/jetty-6.1.26.cloudera.4.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jetty-util/6.1.26.cloudera.4/jetty-util-6.1.26.cloudera.4.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jetty-sslengine/6.1.26.cloudera.4/jetty-sslengine-6.1.26.cloudera.4.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jsp-2.1/6.1.14/jsp-2.1-6.1.14.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jsp-api-2.1/6.1.14/jsp-api-2.1-6.1.14.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/servlet-api-2.5/6.1.14/servlet-api-2.5-6.1.14.jar:/Users/seven/software/maven/repository/org/codehaus/jackson/jackson-core-asl/1.8.8/jackson-core-asl-1.8.8.jar:/Users/seven/software/maven/repository/org/codehaus/jackson/jackson-jaxrs/1.8.8/jackson-jaxrs-1.8.8.jar:/Users/seven/software/maven/repository/tomcat/jasper-compiler/5.5.23/jasper-compiler-5.5.23.jar:/Users/seven/software/maven/repository/tomcat/jasper-runtime/5.5.23/jasper-runtime-5.5.23.jar:/Users/seven/software/maven/repository/commons-el/commons-el/1.0/commons-el-1.0.jar:/Users/seven/software/maven/repository/org/jamon/jamon-runtime/2.4.1/jamon-runtime-2.4.1.jar:/Users/seven/software/maven/repository/com/lmax/disruptor/3.3.0/disruptor-3.3.0.jar:/Users/seven/software/maven/repository/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar:/Users/seven/software/maven/repository/org/owasp/esapi/esapi/2.1.0/esapi-2.1.0.jar:/Users/seven/software/maven/repository/commons-beanutils/commons-beanutils-core/1.7.0/commons-beanutils-core-1.7.0.jar:/Users/seven/software/maven/repository/commons-fileupload/commons-fileupload/1.2/commons-fileupload-1.2.jar:/Users/seven/software/maven/repository/xom/xom/1.2.5/xom-1.2.5.jar:/Users/seven/software/maven/repository/xalan/xalan/2.7.0/xalan-2.7.0.jar:/Users/seven/software/maven/repository/org/beanshell/bsh-core/2.0b4/bsh-core-2.0b4.jar:/Users/seven/software/maven/repository/org/owasp/antisamy/antisamy/1.4.3/antisamy-1.4.3.jar:/Users/seven/software/maven/repository/org/apache/xmlgraphics/batik-css/1.7/batik-css-1.7.jar:/Users/seven/software/maven/repository/org/apache/xmlgraphics/batik-ext/1.7/batik-ext-1.7.jar:/Users/seven/software/maven/repository/org/apache/xmlgraphics/batik-util/1.7/batik-util-1.7.jar:/Users/seven/software/maven/repository/xml-apis/xml-apis-ext/1.3.04/xml-apis-ext-1.3.04.jar:/Users/seven/software/maven/repository/net/sourceforge/nekohtml/nekohtml/1.9.12/nekohtml-1.9.12.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-core/2.6.0-mr1-cdh5.7.0/hadoop-core-2.6.0-mr1-cdh5.7.0.jar:/Users/seven/software/maven/repository/javax/servlet/jsp/jsp-api/2.1/jsp-api-2.1.jar:/Users/seven/software/maven/repository/net/java/dev/jets3t/jets3t/0.6.1/jets3t-0.6.1.jar:/Users/seven/software/maven/repository/javax/servlet/servlet-api/2.5/servlet-api-2.5.jar:/Users/seven/software/maven/repository/hsqldb/hsqldb/1.8.0.10/hsqldb-1.8.0.10.jar:/Users/seven/software/maven/repository/org/eclipse/jdt/core/3.1.1/core-3.1.1.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-hdfs/2.6.0-cdh5.7.0/hadoop-hdfs-2.6.0-cdh5.7.0-tests.jar:/Users/seven/software/maven/repository/commons-daemon/commons-daemon/1.0.13/commons-daemon-1.0.13.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-lang3/3.5/commons-lang3-3.5.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-pool2/2.4.2/commons-pool2-2.4.2.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-mllib_2.11/2.2.0/spark-mllib_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-core_2.11/2.2.0/spark-core_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/avro/avro-mapred/1.7.7/avro-mapred-1.7.7-hadoop2.jar:/Users/seven/software/maven/repository/org/apache/avro/avro-ipc/1.7.7/avro-ipc-1.7.7-tests.jar:/Users/seven/software/maven/repository/com/twitter/chill_2.11/0.8.0/chill_2.11-0.8.0.jar:/Users/seven/software/maven/repository/com/esotericsoftware/kryo-shaded/3.0.3/kryo-shaded-3.0.3.jar:/Users/seven/software/maven/repository/com/esotericsoftware/minlog/1.3.0/minlog-1.3.0.jar:/Users/seven/software/maven/repository/org/objenesis/objenesis/2.1/objenesis-2.1.jar:/Users/seven/software/maven/repository/com/twitter/chill-java/0.8.0/chill-java-0.8.0.jar:/Users/seven/software/maven/repository/org/apache/xbean/xbean-asm5-shaded/4.4/xbean-asm5-shaded-4.4.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-launcher_2.11/2.2.0/spark-launcher_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-network-common_2.11/2.2.0/spark-network-common_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-network-shuffle_2.11/2.2.0/spark-network-shuffle_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-unsafe_2.11/2.2.0/spark-unsafe_2.11-2.2.0.jar:/Users/seven/software/maven/repository/javax/servlet/javax.servlet-api/3.1.0/javax.servlet-api-3.1.0.jar:/Users/seven/software/maven/repository/org/slf4j/jul-to-slf4j/1.7.16/jul-to-slf4j-1.7.16.jar:/Users/seven/software/maven/repository/org/slf4j/jcl-over-slf4j/1.7.16/jcl-over-slf4j-1.7.16.jar:/Users/seven/software/maven/repository/com/ning/compress-lzf/1.0.3/compress-lzf-1.0.3.jar:/Users/seven/software/maven/repository/org/xerial/snappy/snappy-java/1.1.2.6/snappy-java-1.1.2.6.jar:/Users/seven/software/maven/repository/org/roaringbitmap/RoaringBitmap/0.5.11/RoaringBitmap-0.5.11.jar:/Users/seven/software/maven/repository/org/json4s/json4s-jackson_2.11/3.2.11/json4s-jackson_2.11-3.2.11.jar:/Users/seven/software/maven/repository/org/json4s/json4s-core_2.11/3.2.11/json4s-core_2.11-3.2.11.jar:/Users/seven/software/maven/repository/org/json4s/json4s-ast_2.11/3.2.11/json4s-ast_2.11-3.2.11.jar:/Users/seven/software/maven/repository/org/scala-lang/scalap/2.11.0/scalap-2.11.0.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/core/jersey-client/2.22.2/jersey-client-2.22.2.jar:/Users/seven/software/maven/repository/javax/ws/rs/javax.ws.rs-api/2.0.1/javax.ws.rs-api-2.0.1.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/hk2-api/2.4.0-b34/hk2-api-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/hk2-utils/2.4.0-b34/hk2-utils-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/external/aopalliance-repackaged/2.4.0-b34/aopalliance-repackaged-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/external/javax.inject/2.4.0-b34/javax.inject-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/hk2-locator/2.4.0-b34/hk2-locator-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/javassist/javassist/3.18.1-GA/javassist-3.18.1-GA.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/core/jersey-common/2.22.2/jersey-common-2.22.2.jar:/Users/seven/software/maven/repository/javax/annotation/javax.annotation-api/1.2/javax.annotation-api-1.2.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/bundles/repackaged/jersey-guava/2.22.2/jersey-guava-2.22.2.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/osgi-resource-locator/1.0.1/osgi-resource-locator-1.0.1.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/core/jersey-server/2.22.2/jersey-server-2.22.2.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/media/jersey-media-jaxb/2.22.2/jersey-media-jaxb-2.22.2.jar:/Users/seven/software/maven/repository/javax/validation/validation-api/1.1.0.Final/validation-api-1.1.0.Final.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/containers/jersey-container-servlet/2.22.2/jersey-container-servlet-2.22.2.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/containers/jersey-container-servlet-core/2.22.2/jersey-container-servlet-core-2.22.2.jar:/Users/seven/software/maven/repository/com/clearspring/analytics/stream/2.7.0/stream-2.7.0.jar:/Users/seven/software/maven/repository/io/dropwizard/metrics/metrics-core/3.1.2/metrics-core-3.1.2.jar:/Users/seven/software/maven/repository/io/dropwizard/metrics/metrics-jvm/3.1.2/metrics-jvm-3.1.2.jar:/Users/seven/software/maven/repository/io/dropwizard/metrics/metrics-json/3.1.2/metrics-json-3.1.2.jar:/Users/seven/software/maven/repository/io/dropwizard/metrics/metrics-graphite/3.1.2/metrics-graphite-3.1.2.jar:/Users/seven/software/maven/repository/org/apache/ivy/ivy/2.4.0/ivy-2.4.0.jar:/Users/seven/software/maven/repository/oro/oro/2.0.8/oro-2.0.8.jar:/Users/seven/software/maven/repository/net/razorvine/pyrolite/4.13/pyrolite-4.13.jar:/Users/seven/software/maven/repository/net/sf/py4j/py4j/0.10.4/py4j-0.10.4.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-crypto/1.0.0/commons-crypto-1.0.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-streaming_2.11/2.2.0/spark-streaming_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-sql_2.11/2.2.0/spark-sql_2.11-2.2.0.jar:/Users/seven/software/maven/repository/com/univocity/univocity-parsers/2.2.1/univocity-parsers-2.2.1.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-sketch_2.11/2.2.0/spark-sketch_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-catalyst_2.11/2.2.0/spark-catalyst_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/codehaus/janino/janino/3.0.0/janino-3.0.0.jar:/Users/seven/software/maven/repository/org/codehaus/janino/commons-compiler/3.0.0/commons-compiler-3.0.0.jar:/Users/seven/software/maven/repository/org/antlr/antlr4-runtime/4.5.3/antlr4-runtime-4.5.3.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-column/1.8.2/parquet-column-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-common/1.8.2/parquet-common-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-encoding/1.8.2/parquet-encoding-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-hadoop/1.8.2/parquet-hadoop-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-format/2.3.1/parquet-format-2.3.1.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-jackson/1.8.2/parquet-jackson-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-graphx_2.11/2.2.0/spark-graphx_2.11-2.2.0.jar:/Users/seven/software/maven/repository/com/github/fommil/netlib/core/1.1.2/core-1.1.2.jar:/Users/seven/software/maven/repository/net/sourceforge/f2j/arpack_combined_all/0.1/arpack_combined_all-0.1.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-mllib-local_2.11/2.2.0/spark-mllib-local_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/scalanlp/breeze_2.11/0.13.1/breeze_2.11-0.13.1.jar:/Users/seven/software/maven/repository/org/scalanlp/breeze-macros_2.11/0.13.1/breeze-macros_2.11-0.13.1.jar:/Users/seven/software/maven/repository/net/sf/opencsv/opencsv/2.3/opencsv-2.3.jar:/Users/seven/software/maven/repository/com/github/rwl/jtransforms/2.4.0/jtransforms-2.4.0.jar:/Users/seven/software/maven/repository/org/spire-math/spire_2.11/0.13.0/spire_2.11-0.13.0.jar:/Users/seven/software/maven/repository/org/spire-math/spire-macros_2.11/0.13.0/spire-macros_2.11-0.13.0.jar:/Users/seven/software/maven/repository/org/typelevel/machinist_2.11/0.6.1/machinist_2.11-0.6.1.jar:/Users/seven/software/maven/repository/com/chuusai/shapeless_2.11/2.3.2/shapeless_2.11-2.3.2.jar:/Users/seven/software/maven/repository/org/typelevel/macro-compat_2.11/1.1.1/macro-compat_2.11-1.1.1.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-math3/3.4.1/commons-math3-3.4.1.jar:/Users/seven/software/maven/repository/org/jpmml/pmml-model/1.2.15/pmml-model-1.2.15.jar:/Users/seven/software/maven/repository/org/jpmml/pmml-schema/1.2.15/pmml-schema-1.2.15.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-tags_2.11/2.2.0/spark-tags_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-streaming-kafka-0-8_2.11/2.2.0/spark-streaming-kafka-0-8_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/kafka/kafka_2.11/0.8.2.1/kafka_2.11-0.8.2.1.jar:/Users/seven/software/maven/repository/com/101tec/zkclient/0.3/zkclient-0.3.jar:/Users/seven/software/maven/repository/org/apache/kafka/kafka-clients/0.8.2.1/kafka-clients-0.8.2.1.jar:/Users/seven/software/maven/repository/mysql/mysql-connector-java/5.1.31/mysql-connector-java-5.1.31.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/module/jackson-module-scala_2.11/2.6.5/jackson-module-scala_2.11-2.6.5.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/core/jackson-core/2.6.5/jackson-core-2.6.5.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/core/jackson-annotations/2.6.5/jackson-annotations-2.6.5.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/core/jackson-databind/2.6.5/jackson-databind-2.6.5.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/module/jackson-module-paranamer/2.6.5/jackson-module-paranamer-2.6.5.jar:/Users/seven/software/maven/repository/com/thoughtworks/paranamer/paranamer/2.6/paranamer-2.6.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-streaming-flume_2.11/2.2.0/spark-streaming-flume_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/flume/flume-ng-core/1.6.0/flume-ng-core-1.6.0.jar:/Users/seven/software/maven/repository/org/apache/flume/flume-ng-configuration/1.6.0/flume-ng-configuration-1.6.0.jar:/Users/seven/software/maven/repository/org/apache/avro/avro-ipc/1.7.4/avro-ipc-1.7.4.jar:/Users/seven/software/maven/repository/org/apache/velocity/velocity/1.7/velocity-1.7.jar:/Users/seven/software/maven/repository/joda-time/joda-time/2.1/joda-time-2.1.jar:/Users/seven/software/maven/repository/org/apache/mina/mina-core/2.0.4/mina-core-2.0.4.jar:/Users/seven/software/maven/repository/org/apache/flume/flume-ng-sdk/1.6.0/flume-ng-sdk-1.6.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-streaming-flume-sink_2.11/2.2.0/spark-streaming-flume-sink_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/flume/flume-ng-clients/flume-ng-log4jappender/1.6.0/flume-ng-log4jappender-1.6.0.jar:/Users/seven/software/maven/repository/net/jpountz/lz4/lz4/1.3.0/lz4-1.3.0.jar:/Applications/IntelliJ IDEA.app/Contents/lib/idea_rt.jar
[INFO][2018-05-24 21:11:35,701][org.apache.zookeeper.ZooKeeper]Client environment:java.library.path=/Users/seven/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:.
[INFO][2018-05-24 21:11:35,701][org.apache.zookeeper.ZooKeeper]Client environment:java.io.tmpdir=/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/
[INFO][2018-05-24 21:11:35,701][org.apache.zookeeper.ZooKeeper]Client environment:java.compiler=<NA>
[INFO][2018-05-24 21:11:35,701][org.apache.zookeeper.ZooKeeper]Client environment:os.name=Mac OS X
[INFO][2018-05-24 21:11:35,701][org.apache.zookeeper.ZooKeeper]Client environment:os.arch=x86_64
[INFO][2018-05-24 21:11:35,701][org.apache.zookeeper.ZooKeeper]Client environment:os.version=10.13.4
[INFO][2018-05-24 21:11:35,701][org.apache.zookeeper.ZooKeeper]Client environment:user.name=seven
[INFO][2018-05-24 21:11:35,701][org.apache.zookeeper.ZooKeeper]Client environment:user.home=/Users/seven
[INFO][2018-05-24 21:11:35,701][org.apache.zookeeper.ZooKeeper]Client environment:user.dir=/Users/seven/project/scala/dataMining
[INFO][2018-05-24 21:11:35,703][org.apache.zookeeper.ZooKeeper]Initiating client connection, connectString=vm-xaj-bigdata-da-d01:2181,vm-xaj-bigdata-da-d02:2181,vm-xaj-bigdata-da-d03:2181 sessionTimeout=60000 watcher=hconnection-0x53fa245a0x0, quorum=vm-xaj-bigdata-da-d01:2181,vm-xaj-bigdata-da-d02:2181,vm-xaj-bigdata-da-d03:2181, baseZNode=/hbase
[INFO][2018-05-24 21:11:40,000][org.apache.spark.storage.memory.MemoryStore]Block taskresult_0 stored as bytes in memory (estimated size 10.4 MB, free 901.7 MB)
[INFO][2018-05-24 21:11:40,002][org.apache.spark.storage.BlockManagerInfo]Added taskresult_0 in memory on 192.168.0.102:50869 (size: 10.4 MB, free: 901.9 MB)
[INFO][2018-05-24 21:11:40,003][org.apache.spark.executor.Executor]Finished task 0.0 in stage 0.0 (TID 0). 10856349 bytes result sent via BlockManager)
[INFO][2018-05-24 21:11:40,038][org.apache.spark.network.client.TransportClientFactory]Successfully created connection to /192.168.0.102:50869 after 19 ms (0 ms spent in bootstraps)
[INFO][2018-05-24 21:11:40,237][org.apache.spark.storage.memory.MemoryStore]Block taskresult_1 stored as bytes in memory (estimated size 10.4 MB, free 891.3 MB)
[INFO][2018-05-24 21:11:40,238][org.apache.spark.storage.BlockManagerInfo]Added taskresult_1 in memory on 192.168.0.102:50869 (size: 10.4 MB, free: 891.6 MB)
[INFO][2018-05-24 21:11:40,241][org.apache.spark.executor.Executor]Finished task 1.0 in stage 0.0 (TID 1). 10855899 bytes result sent via BlockManager)
[INFO][2018-05-24 21:11:40,428][org.apache.spark.storage.BlockManagerInfo]Removed taskresult_0 on 192.168.0.102:50869 in memory (size: 10.4 MB, free: 901.9 MB)
[INFO][2018-05-24 21:11:40,436][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 0.0 (TID 0) in 12747 ms on localhost (executor driver) (1/2)
[INFO][2018-05-24 21:11:40,451][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 0.0 (TID 1) in 12758 ms on localhost (executor driver) (2/2)
[INFO][2018-05-24 21:11:40,452][org.apache.spark.storage.BlockManagerInfo]Removed taskresult_1 on 192.168.0.102:50869 in memory (size: 10.4 MB, free: 912.3 MB)
[INFO][2018-05-24 21:11:40,454][org.apache.spark.scheduler.DAGScheduler]ResultStage 0 (collect at SimulationKafkaSendOutData.scala:25) finished in 12.783 s
[INFO][2018-05-24 21:11:40,457][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:11:40,461][org.apache.spark.scheduler.DAGScheduler]Job 0 finished: collect at SimulationKafkaSendOutData.scala:25, took 12.887064 s
[INFO][2018-05-24 21:11:40,613][org.spark_project.jetty.server.AbstractConnector]Stopped Spark@5427d3ac{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}
[INFO][2018-05-24 21:11:40,617][org.apache.spark.ui.SparkUI]Stopped Spark web UI at http://192.168.0.102:4041
[INFO][2018-05-24 21:11:40,631][org.apache.spark.MapOutputTrackerMasterEndpoint]MapOutputTrackerMasterEndpoint stopped!
[INFO][2018-05-24 21:11:40,647][org.apache.spark.storage.memory.MemoryStore]MemoryStore cleared
[INFO][2018-05-24 21:11:40,648][org.apache.spark.storage.BlockManager]BlockManager stopped
[INFO][2018-05-24 21:11:40,648][org.apache.spark.storage.BlockManagerMaster]BlockManagerMaster stopped
[INFO][2018-05-24 21:11:40,650][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint]OutputCommitCoordinator stopped!
[INFO][2018-05-24 21:11:40,651][org.apache.spark.SparkContext]Successfully stopped SparkContext
[INFO][2018-05-24 21:11:40,661][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	compression.type = none
	metric.reporters = []
	metadata.max.age.ms = 300000
	metadata.fetch.timeout.ms = 60000
	acks = all
	batch.size = 16384
	reconnect.backoff.ms = 10
	bootstrap.servers = [vm-xaj-bigdata-da-d01:9092, vm-xaj-bigdata-da-d02:9092, vm-xaj-bigdata-da-d03:9092, vm-xaj-bigdata-da-d04:9092, vm-xaj-bigdata-da-d05:9092, vm-xaj-bigdata-da-d06:9092, vm-xaj-bigdata-da-d07:9092]
	receive.buffer.bytes = 32768
	retry.backoff.ms = 100
	buffer.memory = 33554432
	timeout.ms = 30000
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	retries = 0
	max.request.size = 1048576
	block.on.buffer.full = true
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
	metrics.sample.window.ms = 30000
	send.buffer.bytes = 131072
	max.in.flight.requests.per.connection = 5
	metrics.num.samples = 2
	linger.ms = 1
	client.id = 

[INFO][2018-05-24 21:11:40,725][org.apache.zookeeper.ClientCnxn]Opening socket connection to server vm-xaj-bigdata-da-d03/10.213.4.27:2181. Will not attempt to authenticate using SASL (unknown error)
[INFO][2018-05-24 21:11:40,740][org.apache.zookeeper.ClientCnxn]Socket connection established, initiating session, client: /10.0.8.39:50882, server: vm-xaj-bigdata-da-d03/10.213.4.27:2181
[INFO][2018-05-24 21:11:40,773][org.apache.zookeeper.ClientCnxn]Session establishment complete on server vm-xaj-bigdata-da-d03/10.213.4.27:2181, sessionid = 0x362b50d75ec5f83, negotiated timeout = 60000
[INFO][2018-05-24 21:11:40,775][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527167500000 ms
[WARN][2018-05-24 21:11:41,279][org.apache.hadoop.util.NativeCodeLoader]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO][2018-05-24 21:11:41,363][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:11:41,377][org.apache.spark.executor.Executor]Finished task 0.0 in stage 0.0 (TID 0). 708 bytes result sent to driver
[INFO][2018-05-24 21:11:41,383][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 0.0 (TID 0) in 5942 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:11:41,385][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:11:41,389][org.apache.spark.scheduler.DAGScheduler]ResultStage 0 (foreachPartition at ReceiveKafkaData.scala:66) finished in 5.959 s
[INFO][2018-05-24 21:11:41,396][org.apache.spark.scheduler.DAGScheduler]Job 0 finished: foreachPartition at ReceiveKafkaData.scala:66, took 6.189089 s
[INFO][2018-05-24 21:11:41,400][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527167490000 ms.0 from job set of time 1527167490000 ms
[INFO][2018-05-24 21:11:41,402][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 11.400 s for time 1527167490000 ms (execution: 6.236 s)
[INFO][2018-05-24 21:11:41,402][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527167500000 ms.0 from job set of time 1527167500000 ms
[INFO][2018-05-24 21:11:41,411][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:11:41,414][org.apache.spark.scheduler.DAGScheduler]Got job 1 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:11:41,414][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 1 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:11:41,414][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:11:41,414][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 21:11:41,414][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:11:41,415][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 1 (MapPartitionsRDD[3] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:11:41,417][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 21:11:41,421][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 
[INFO][2018-05-24 21:11:41,423][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1_piece0 stored as bytes in memory (estimated size 1868.0 B, free 912.3 MB)
[INFO][2018-05-24 21:11:41,424][org.apache.spark.storage.BlockManagerInfo]Added broadcast_1_piece0 in memory on 192.168.0.102:50860 (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:11:41,424][org.apache.spark.SparkContext]Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:11:41,426][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:11:41,427][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 1.0 with 1 tasks
[INFO][2018-05-24 21:11:41,428][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:11:41,429][org.apache.spark.executor.Executor]Running task 0.0 in stage 1.0 (TID 1)
[INFO][2018-05-24 21:11:41,464][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12194 is the same as ending offset skipping seven 0
[INFO][2018-05-24 21:11:41,464][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:11:41,466][org.apache.spark.executor.Executor]Finished task 0.0 in stage 1.0 (TID 1). 708 bytes result sent to driver
[INFO][2018-05-24 21:11:41,467][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 1.0 (TID 1) in 39 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:11:41,467][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:11:41,468][org.apache.spark.scheduler.DAGScheduler]ResultStage 1 (foreachPartition at ReceiveKafkaData.scala:66) finished in 0.040 s
[INFO][2018-05-24 21:11:41,468][org.apache.spark.scheduler.DAGScheduler]Job 1 finished: foreachPartition at ReceiveKafkaData.scala:66, took 0.057110 s
[INFO][2018-05-24 21:11:41,469][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527167500000 ms.0 from job set of time 1527167500000 ms
[INFO][2018-05-24 21:11:41,470][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 1.469 s for time 1527167500000 ms (execution: 0.067 s)
[INFO][2018-05-24 21:11:41,470][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 1 from persistence list
[INFO][2018-05-24 21:11:41,476][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 0 from persistence list
[INFO][2018-05-24 21:11:41,477][org.apache.spark.storage.BlockManager]Removing RDD 1
[INFO][2018-05-24 21:11:41,477][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 21:11:41,478][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 
[INFO][2018-05-24 21:11:41,479][org.apache.spark.storage.BlockManager]Removing RDD 0
[INFO][2018-05-24 21:11:50,052][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527167510000 ms
[INFO][2018-05-24 21:11:50,053][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527167510000 ms.0 from job set of time 1527167510000 ms
[INFO][2018-05-24 21:11:50,063][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:11:50,064][org.apache.spark.scheduler.DAGScheduler]Got job 2 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:11:50,064][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 2 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:11:50,064][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:11:50,065][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:11:50,066][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 2 (MapPartitionsRDD[5] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:11:50,069][org.apache.spark.storage.memory.MemoryStore]Block broadcast_2 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 21:11:50,081][org.apache.spark.storage.memory.MemoryStore]Block broadcast_2_piece0 stored as bytes in memory (estimated size 1868.0 B, free 912.3 MB)
[INFO][2018-05-24 21:11:50,081][org.apache.spark.storage.BlockManagerInfo]Added broadcast_2_piece0 in memory on 192.168.0.102:50860 (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:11:50,086][org.apache.spark.SparkContext]Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:11:50,087][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[5] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:11:50,087][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 2.0 with 1 tasks
[INFO][2018-05-24 21:11:50,089][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:11:50,090][org.apache.spark.executor.Executor]Running task 0.0 in stage 2.0 (TID 2)
[INFO][2018-05-24 21:11:50,099][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12194 is the same as ending offset skipping seven 0
[INFO][2018-05-24 21:11:50,100][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:11:50,101][org.apache.spark.executor.Executor]Finished task 0.0 in stage 2.0 (TID 2). 708 bytes result sent to driver
[INFO][2018-05-24 21:11:50,103][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 2.0 (TID 2) in 15 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:11:50,103][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:11:50,104][org.apache.spark.scheduler.DAGScheduler]ResultStage 2 (foreachPartition at ReceiveKafkaData.scala:66) finished in 0.015 s
[INFO][2018-05-24 21:11:50,104][org.apache.spark.scheduler.DAGScheduler]Job 2 finished: foreachPartition at ReceiveKafkaData.scala:66, took 0.041326 s
[INFO][2018-05-24 21:11:50,105][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527167510000 ms.0 from job set of time 1527167510000 ms
[INFO][2018-05-24 21:11:50,106][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.105 s for time 1527167510000 ms (execution: 0.052 s)
[INFO][2018-05-24 21:11:50,106][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 3 from persistence list
[INFO][2018-05-24 21:11:50,106][org.apache.spark.storage.BlockManager]Removing RDD 3
[INFO][2018-05-24 21:11:50,107][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 2 from persistence list
[INFO][2018-05-24 21:11:50,109][org.apache.spark.storage.BlockManager]Removing RDD 2
[INFO][2018-05-24 21:11:50,110][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 21:11:50,110][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527167490000 ms
[INFO][2018-05-24 21:12:10,114][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527167520000 ms
[INFO][2018-05-24 21:12:10,116][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527167520000 ms.0 from job set of time 1527167520000 ms
[INFO][2018-05-24 21:12:10,130][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:12:10,131][org.apache.spark.scheduler.DAGScheduler]Got job 3 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:12:10,131][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 3 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:12:10,131][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:12:10,131][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:12:10,132][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 3 (MapPartitionsRDD[7] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:12:10,134][org.apache.spark.storage.memory.MemoryStore]Block broadcast_3 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 21:12:10,138][org.apache.spark.storage.memory.MemoryStore]Block broadcast_3_piece0 stored as bytes in memory (estimated size 1868.0 B, free 912.3 MB)
[INFO][2018-05-24 21:12:10,139][org.apache.spark.storage.BlockManagerInfo]Added broadcast_3_piece0 in memory on 192.168.0.102:50860 (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:12:10,139][org.apache.spark.SparkContext]Created broadcast 3 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:12:10,140][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[7] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:12:10,140][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 3.0 with 1 tasks
[INFO][2018-05-24 21:12:10,141][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 3.0 (TID 3, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:12:10,142][org.apache.spark.executor.Executor]Running task 0.0 in stage 3.0 (TID 3)
[INFO][2018-05-24 21:12:10,145][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12194 is the same as ending offset skipping seven 0
[INFO][2018-05-24 21:12:10,145][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:12:10,146][org.apache.spark.executor.Executor]Finished task 0.0 in stage 3.0 (TID 3). 708 bytes result sent to driver
[INFO][2018-05-24 21:12:10,148][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 3.0 (TID 3) in 7 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:12:10,148][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 3.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:12:10,148][org.apache.spark.scheduler.DAGScheduler]ResultStage 3 (foreachPartition at ReceiveKafkaData.scala:66) finished in 0.007 s
[INFO][2018-05-24 21:12:10,149][org.apache.spark.scheduler.DAGScheduler]Job 3 finished: foreachPartition at ReceiveKafkaData.scala:66, took 0.018131 s
[INFO][2018-05-24 21:12:10,149][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527167520000 ms.0 from job set of time 1527167520000 ms
[INFO][2018-05-24 21:12:10,150][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 10.149 s for time 1527167520000 ms (execution: 0.033 s)
[INFO][2018-05-24 21:12:10,245][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527167530000 ms
[INFO][2018-05-24 21:12:10,245][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 5 from persistence list
[INFO][2018-05-24 21:12:10,245][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527167530000 ms.0 from job set of time 1527167530000 ms
[INFO][2018-05-24 21:12:10,245][org.apache.spark.storage.BlockManager]Removing RDD 5
[INFO][2018-05-24 21:12:10,246][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 4 from persistence list
[INFO][2018-05-24 21:12:10,246][org.apache.spark.storage.BlockManager]Removing RDD 4
[INFO][2018-05-24 21:12:10,247][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 21:12:10,247][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527167500000 ms
[INFO][2018-05-24 21:12:10,253][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:12:10,254][org.apache.spark.scheduler.DAGScheduler]Got job 4 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:12:10,255][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 4 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:12:10,255][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:12:10,255][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:12:10,255][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 4 (MapPartitionsRDD[9] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:12:10,257][org.apache.spark.storage.memory.MemoryStore]Block broadcast_4 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 21:12:10,273][org.apache.spark.storage.memory.MemoryStore]Block broadcast_4_piece0 stored as bytes in memory (estimated size 1868.0 B, free 912.3 MB)
[INFO][2018-05-24 21:12:10,276][org.apache.spark.storage.BlockManagerInfo]Added broadcast_4_piece0 in memory on 192.168.0.102:50860 (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:12:10,278][org.apache.spark.SparkContext]Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:12:10,279][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[9] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:12:10,280][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 4.0 with 1 tasks
[INFO][2018-05-24 21:12:10,280][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 4.0 (TID 4, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:12:10,281][org.apache.spark.executor.Executor]Running task 0.0 in stage 4.0 (TID 4)
[INFO][2018-05-24 21:12:10,284][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12194 is the same as ending offset skipping seven 0
[INFO][2018-05-24 21:12:10,284][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:12:10,286][org.apache.spark.executor.Executor]Finished task 0.0 in stage 4.0 (TID 4). 708 bytes result sent to driver
[INFO][2018-05-24 21:12:10,287][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 4.0 (TID 4) in 6 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:12:10,287][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 4.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:12:10,287][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_1_piece0 on 192.168.0.102:50860 in memory (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:12:10,287][org.apache.spark.scheduler.DAGScheduler]ResultStage 4 (foreachPartition at ReceiveKafkaData.scala:66) finished in 0.007 s
[INFO][2018-05-24 21:12:10,288][org.apache.spark.scheduler.DAGScheduler]Job 4 finished: foreachPartition at ReceiveKafkaData.scala:66, took 0.034278 s
[INFO][2018-05-24 21:12:10,288][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527167530000 ms.0 from job set of time 1527167530000 ms
[INFO][2018-05-24 21:12:10,289][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 7 from persistence list
[INFO][2018-05-24 21:12:10,289][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.288 s for time 1527167530000 ms (execution: 0.043 s)
[INFO][2018-05-24 21:12:10,290][org.apache.spark.storage.BlockManager]Removing RDD 7
[INFO][2018-05-24 21:12:10,290][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 6 from persistence list
[INFO][2018-05-24 21:12:10,291][org.apache.spark.storage.BlockManager]Removing RDD 6
[INFO][2018-05-24 21:12:10,291][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 21:12:10,291][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_0_piece0 on 192.168.0.102:50860 in memory (size: 1867.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:12:10,291][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527167510000 ms
[INFO][2018-05-24 21:12:10,293][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_2_piece0 on 192.168.0.102:50860 in memory (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:12:10,294][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_3_piece0 on 192.168.0.102:50860 in memory (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:12:25,119][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527167540000 ms
[INFO][2018-05-24 21:12:25,120][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527167540000 ms.0 from job set of time 1527167540000 ms
[INFO][2018-05-24 21:12:25,131][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:12:25,132][org.apache.spark.scheduler.DAGScheduler]Got job 5 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:12:25,132][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 5 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:12:25,132][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:12:25,133][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:12:25,133][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 5 (MapPartitionsRDD[11] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:12:25,137][org.apache.spark.storage.memory.MemoryStore]Block broadcast_5 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 21:12:25,143][org.apache.spark.storage.memory.MemoryStore]Block broadcast_5_piece0 stored as bytes in memory (estimated size 1868.0 B, free 912.3 MB)
[INFO][2018-05-24 21:12:25,147][org.apache.spark.storage.BlockManagerInfo]Added broadcast_5_piece0 in memory on 192.168.0.102:50860 (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:12:25,147][org.apache.spark.SparkContext]Created broadcast 5 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:12:25,148][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[11] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:12:25,148][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 5.0 with 1 tasks
[INFO][2018-05-24 21:12:25,151][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 5.0 (TID 5, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:12:25,151][org.apache.spark.executor.Executor]Running task 0.0 in stage 5.0 (TID 5)
[INFO][2018-05-24 21:12:25,160][org.apache.spark.streaming.kafka.KafkaRDD]Computing topic seven, partition 0 offsets 12194 -> 12200
[INFO][2018-05-24 21:12:25,160][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 21:12:25,160][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 21:12:25,161][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 21:12:30,106][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527167550000 ms
[INFO][2018-05-24 21:12:30,752][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:12:30,753][org.apache.spark.executor.Executor]Finished task 0.0 in stage 5.0 (TID 5). 708 bytes result sent to driver
[INFO][2018-05-24 21:12:30,754][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 5.0 (TID 5) in 5604 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:12:30,754][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 5.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:12:30,754][org.apache.spark.scheduler.DAGScheduler]ResultStage 5 (foreachPartition at ReceiveKafkaData.scala:66) finished in 5.604 s
[INFO][2018-05-24 21:12:30,755][org.apache.spark.scheduler.DAGScheduler]Job 5 finished: foreachPartition at ReceiveKafkaData.scala:66, took 5.623008 s
[INFO][2018-05-24 21:12:30,755][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527167540000 ms.0 from job set of time 1527167540000 ms
[INFO][2018-05-24 21:12:30,755][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 9 from persistence list
[INFO][2018-05-24 21:12:30,755][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 10.755 s for time 1527167540000 ms (execution: 5.635 s)
[INFO][2018-05-24 21:12:30,756][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527167550000 ms.0 from job set of time 1527167550000 ms
[INFO][2018-05-24 21:12:30,756][org.apache.spark.storage.BlockManager]Removing RDD 9
[INFO][2018-05-24 21:12:30,756][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 8 from persistence list
[INFO][2018-05-24 21:12:30,757][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 21:12:30,757][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527167520000 ms
[INFO][2018-05-24 21:12:30,757][org.apache.spark.storage.BlockManager]Removing RDD 8
[INFO][2018-05-24 21:12:30,762][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:12:30,762][org.apache.spark.scheduler.DAGScheduler]Got job 6 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:12:30,762][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 6 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:12:30,763][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:12:30,763][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:12:30,763][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 6 (MapPartitionsRDD[13] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:12:30,765][org.apache.spark.storage.memory.MemoryStore]Block broadcast_6 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 21:12:30,767][org.apache.spark.storage.memory.MemoryStore]Block broadcast_6_piece0 stored as bytes in memory (estimated size 1868.0 B, free 912.3 MB)
[INFO][2018-05-24 21:12:30,768][org.apache.spark.storage.BlockManagerInfo]Added broadcast_6_piece0 in memory on 192.168.0.102:50860 (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:12:30,769][org.apache.spark.SparkContext]Created broadcast 6 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:12:30,769][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[13] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:12:30,769][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 6.0 with 1 tasks
[INFO][2018-05-24 21:12:30,770][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 6.0 (TID 6, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:12:30,770][org.apache.spark.executor.Executor]Running task 0.0 in stage 6.0 (TID 6)
[INFO][2018-05-24 21:12:30,773][org.apache.spark.streaming.kafka.KafkaRDD]Computing topic seven, partition 0 offsets 12200 -> 12202
[INFO][2018-05-24 21:12:30,773][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 21:12:30,773][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 21:12:30,773][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 21:12:30,905][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:12:30,906][org.apache.spark.executor.Executor]Finished task 0.0 in stage 6.0 (TID 6). 708 bytes result sent to driver
[INFO][2018-05-24 21:12:30,907][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 6.0 (TID 6) in 137 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:12:30,907][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 6.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:12:30,908][org.apache.spark.scheduler.DAGScheduler]ResultStage 6 (foreachPartition at ReceiveKafkaData.scala:66) finished in 0.138 s
[INFO][2018-05-24 21:12:30,908][org.apache.spark.scheduler.DAGScheduler]Job 6 finished: foreachPartition at ReceiveKafkaData.scala:66, took 0.146409 s
[INFO][2018-05-24 21:12:30,909][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527167550000 ms.0 from job set of time 1527167550000 ms
[INFO][2018-05-24 21:12:30,909][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 11 from persistence list
[INFO][2018-05-24 21:12:30,909][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.909 s for time 1527167550000 ms (execution: 0.153 s)
[INFO][2018-05-24 21:12:30,910][org.apache.spark.storage.BlockManager]Removing RDD 11
[INFO][2018-05-24 21:12:30,910][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 10 from persistence list
[INFO][2018-05-24 21:12:30,911][org.apache.spark.storage.BlockManager]Removing RDD 10
[INFO][2018-05-24 21:12:30,911][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 21:12:30,911][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527167530000 ms
[INFO][2018-05-24 21:12:36,134][org.apache.spark.util.ShutdownHookManager]Shutdown hook called
[INFO][2018-05-24 21:12:36,134][org.apache.spark.util.ShutdownHookManager]Deleting directory /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/spark-ad6ac75d-3ce7-4fde-97ce-25be816868bf
[INFO][2018-05-24 21:12:50,188][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527167560000 ms
[INFO][2018-05-24 21:12:50,189][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527167560000 ms.0 from job set of time 1527167560000 ms
[INFO][2018-05-24 21:12:50,195][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:12:50,197][org.apache.spark.scheduler.DAGScheduler]Got job 7 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:12:50,198][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 7 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:12:50,198][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:12:50,199][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:12:50,200][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 7 (MapPartitionsRDD[15] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:12:50,204][org.apache.spark.storage.memory.MemoryStore]Block broadcast_7 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 21:12:50,206][org.apache.spark.storage.memory.MemoryStore]Block broadcast_7_piece0 stored as bytes in memory (estimated size 1868.0 B, free 912.3 MB)
[INFO][2018-05-24 21:12:50,206][org.apache.spark.storage.BlockManagerInfo]Added broadcast_7_piece0 in memory on 192.168.0.102:50860 (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:12:50,207][org.apache.spark.SparkContext]Created broadcast 7 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:12:50,208][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[15] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:12:50,208][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 7.0 with 1 tasks
[INFO][2018-05-24 21:12:50,209][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 7.0 (TID 7, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:12:50,209][org.apache.spark.executor.Executor]Running task 0.0 in stage 7.0 (TID 7)
[INFO][2018-05-24 21:12:50,212][org.apache.spark.streaming.kafka.KafkaRDD]Computing topic seven, partition 0 offsets 12202 -> 12204
[INFO][2018-05-24 21:12:50,213][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 21:12:50,213][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 21:12:50,213][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 21:12:50,359][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:12:50,360][org.apache.spark.executor.Executor]Finished task 0.0 in stage 7.0 (TID 7). 708 bytes result sent to driver
[INFO][2018-05-24 21:12:50,361][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 7.0 (TID 7) in 152 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:12:50,361][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 7.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:12:50,363][org.apache.spark.scheduler.DAGScheduler]ResultStage 7 (foreachPartition at ReceiveKafkaData.scala:66) finished in 0.154 s
[INFO][2018-05-24 21:12:50,363][org.apache.spark.scheduler.DAGScheduler]Job 7 finished: foreachPartition at ReceiveKafkaData.scala:66, took 0.167807 s
[INFO][2018-05-24 21:12:50,364][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527167560000 ms.0 from job set of time 1527167560000 ms
[INFO][2018-05-24 21:12:50,365][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 10.364 s for time 1527167560000 ms (execution: 0.176 s)
[INFO][2018-05-24 21:12:55,365][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527167570000 ms
[INFO][2018-05-24 21:12:55,365][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 13 from persistence list
[INFO][2018-05-24 21:12:55,366][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527167570000 ms.0 from job set of time 1527167570000 ms
[INFO][2018-05-24 21:12:55,366][org.apache.spark.storage.BlockManager]Removing RDD 13
[INFO][2018-05-24 21:12:55,366][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 12 from persistence list
[INFO][2018-05-24 21:12:55,367][org.apache.spark.storage.BlockManager]Removing RDD 12
[INFO][2018-05-24 21:12:55,367][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 21:12:55,367][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527167540000 ms
[INFO][2018-05-24 21:12:55,371][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:12:55,372][org.apache.spark.scheduler.DAGScheduler]Got job 8 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:12:55,372][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 8 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:12:55,372][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:12:55,373][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:12:55,373][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 8 (MapPartitionsRDD[17] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:12:55,374][org.apache.spark.storage.memory.MemoryStore]Block broadcast_8 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 21:12:55,376][org.apache.spark.storage.memory.MemoryStore]Block broadcast_8_piece0 stored as bytes in memory (estimated size 1868.0 B, free 912.3 MB)
[INFO][2018-05-24 21:12:55,376][org.apache.spark.storage.BlockManagerInfo]Added broadcast_8_piece0 in memory on 192.168.0.102:50860 (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:12:55,377][org.apache.spark.SparkContext]Created broadcast 8 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:12:55,378][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[17] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:12:55,378][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 8.0 with 1 tasks
[INFO][2018-05-24 21:12:55,379][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 8.0 (TID 8, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:12:55,379][org.apache.spark.executor.Executor]Running task 0.0 in stage 8.0 (TID 8)
[INFO][2018-05-24 21:12:55,381][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12204 is the same as ending offset skipping seven 0
[INFO][2018-05-24 21:12:55,382][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:12:55,383][org.apache.spark.executor.Executor]Finished task 0.0 in stage 8.0 (TID 8). 751 bytes result sent to driver
[INFO][2018-05-24 21:12:55,384][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 8.0 (TID 8) in 6 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:12:55,384][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 8.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:12:55,385][org.apache.spark.scheduler.DAGScheduler]ResultStage 8 (foreachPartition at ReceiveKafkaData.scala:66) finished in 0.006 s
[INFO][2018-05-24 21:12:55,385][org.apache.spark.scheduler.DAGScheduler]Job 8 finished: foreachPartition at ReceiveKafkaData.scala:66, took 0.013658 s
[INFO][2018-05-24 21:12:55,386][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527167570000 ms.0 from job set of time 1527167570000 ms
[INFO][2018-05-24 21:12:55,387][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 5.386 s for time 1527167570000 ms (execution: 0.020 s)
[INFO][2018-05-24 21:12:55,387][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 15 from persistence list
[INFO][2018-05-24 21:12:55,387][org.apache.spark.storage.BlockManager]Removing RDD 15
[INFO][2018-05-24 21:12:55,388][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 14 from persistence list
[INFO][2018-05-24 21:12:55,389][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 21:12:55,389][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527167550000 ms
[INFO][2018-05-24 21:12:55,389][org.apache.spark.storage.BlockManager]Removing RDD 14
[INFO][2018-05-24 21:13:05,027][org.apache.spark.streaming.StreamingContext]Invoking stop(stopGracefully=false) from shutdown hook
[INFO][2018-05-24 21:13:05,028][org.apache.spark.streaming.scheduler.ReceiverTracker]ReceiverTracker stopped
[INFO][2018-05-24 21:13:05,031][org.apache.spark.streaming.scheduler.JobGenerator]Stopping JobGenerator immediately
[INFO][2018-05-24 21:13:05,032][org.apache.spark.streaming.util.RecurringTimer]Stopped timer for JobGenerator after time 1527167580000
[INFO][2018-05-24 21:13:05,061][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527167580000 ms
[INFO][2018-05-24 21:13:05,062][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527167580000 ms.0 from job set of time 1527167580000 ms
[INFO][2018-05-24 21:13:05,063][org.apache.spark.streaming.scheduler.JobGenerator]Stopped JobGenerator
[INFO][2018-05-24 21:13:05,067][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:13:05,068][org.apache.spark.scheduler.DAGScheduler]Got job 9 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:13:05,068][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 9 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:13:05,068][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:13:05,069][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:13:05,069][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 9 (MapPartitionsRDD[19] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:13:05,071][org.apache.spark.storage.memory.MemoryStore]Block broadcast_9 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 21:13:05,073][org.apache.spark.storage.memory.MemoryStore]Block broadcast_9_piece0 stored as bytes in memory (estimated size 1868.0 B, free 912.3 MB)
[INFO][2018-05-24 21:13:05,074][org.apache.spark.storage.BlockManagerInfo]Added broadcast_9_piece0 in memory on 192.168.0.102:50860 (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:13:05,075][org.apache.spark.SparkContext]Created broadcast 9 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:13:05,076][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[19] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:13:05,076][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 9.0 with 1 tasks
[INFO][2018-05-24 21:13:05,077][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 9.0 (TID 9, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:13:05,077][org.apache.spark.executor.Executor]Running task 0.0 in stage 9.0 (TID 9)
[INFO][2018-05-24 21:13:05,079][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12204 is the same as ending offset skipping seven 0
[INFO][2018-05-24 21:13:05,079][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:13:05,080][org.apache.spark.executor.Executor]Finished task 0.0 in stage 9.0 (TID 9). 708 bytes result sent to driver
[INFO][2018-05-24 21:13:05,081][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 9.0 (TID 9) in 5 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:13:05,082][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 9.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:13:05,082][org.apache.spark.scheduler.DAGScheduler]ResultStage 9 (foreachPartition at ReceiveKafkaData.scala:66) finished in 0.006 s
[INFO][2018-05-24 21:13:05,083][org.apache.spark.scheduler.DAGScheduler]Job 9 finished: foreachPartition at ReceiveKafkaData.scala:66, took 0.015242 s
[INFO][2018-05-24 21:13:05,083][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527167580000 ms.0 from job set of time 1527167580000 ms
[INFO][2018-05-24 21:13:05,084][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 5.083 s for time 1527167580000 ms (execution: 0.022 s)
[INFO][2018-05-24 21:13:05,085][org.apache.spark.streaming.scheduler.JobScheduler]Stopped JobScheduler
[INFO][2018-05-24 21:13:05,092][org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@57a48985{/streaming,null,UNAVAILABLE,@Spark}
[INFO][2018-05-24 21:13:05,093][org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@3f672204{/streaming/batch,null,UNAVAILABLE,@Spark}
[INFO][2018-05-24 21:13:05,094][org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@2d000e80{/static/streaming,null,UNAVAILABLE,@Spark}
[INFO][2018-05-24 21:13:05,095][org.apache.spark.streaming.StreamingContext]StreamingContext stopped successfully
[INFO][2018-05-24 21:13:05,095][org.apache.spark.SparkContext]Invoking stop() from shutdown hook
[INFO][2018-05-24 21:13:05,101][org.spark_project.jetty.server.AbstractConnector]Stopped Spark@1b85fa0b{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-05-24 21:13:05,104][org.apache.spark.ui.SparkUI]Stopped Spark web UI at http://192.168.0.102:4040
[ERROR][2018-05-24 21:13:05,108][org.apache.spark.scheduler.LiveListenerBus]SparkListenerBus has already stopped! Dropping event SparkListenerExecutorMetricsUpdate(driver,WrappedArray())
[INFO][2018-05-24 21:13:05,113][org.apache.spark.MapOutputTrackerMasterEndpoint]MapOutputTrackerMasterEndpoint stopped!
[INFO][2018-05-24 21:13:05,130][org.apache.spark.storage.memory.MemoryStore]MemoryStore cleared
[INFO][2018-05-24 21:13:05,130][org.apache.spark.storage.BlockManager]BlockManager stopped
[INFO][2018-05-24 21:13:05,131][org.apache.spark.storage.BlockManagerMaster]BlockManagerMaster stopped
[INFO][2018-05-24 21:13:05,133][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint]OutputCommitCoordinator stopped!
[INFO][2018-05-24 21:13:05,134][org.apache.spark.SparkContext]Successfully stopped SparkContext
[INFO][2018-05-24 21:13:05,134][org.apache.spark.util.ShutdownHookManager]Shutdown hook called
[INFO][2018-05-24 21:13:05,135][org.apache.spark.util.ShutdownHookManager]Deleting directory /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/spark-bd15aa67-b8af-4cac-a0a3-d589736f3d1d
[INFO][2018-05-24 21:18:46,633][org.apache.spark.SparkContext]Running Spark version 2.2.0
[INFO][2018-05-24 21:18:47,594][org.apache.spark.SparkContext]Submitted application: ReceiveKafkaData$
[INFO][2018-05-24 21:18:47,616][org.apache.spark.SecurityManager]Changing view acls to: seven
[INFO][2018-05-24 21:18:47,619][org.apache.spark.SecurityManager]Changing modify acls to: seven
[INFO][2018-05-24 21:18:47,621][org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO][2018-05-24 21:18:47,622][org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO][2018-05-24 21:18:47,623][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(seven); groups with view permissions: Set(); users  with modify permissions: Set(seven); groups with modify permissions: Set()
[INFO][2018-05-24 21:18:47,976][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 51152.
[INFO][2018-05-24 21:18:47,997][org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO][2018-05-24 21:18:48,015][org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO][2018-05-24 21:18:48,017][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2018-05-24 21:18:48,017][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
[INFO][2018-05-24 21:18:48,027][org.apache.spark.storage.DiskBlockManager]Created local directory at /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/blockmgr-9d1c075e-9db1-43d2-bdb8-bae54f78524c
[INFO][2018-05-24 21:18:48,042][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 912.3 MB
[INFO][2018-05-24 21:18:48,114][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO][2018-05-24 21:18:48,212][org.spark_project.jetty.util.log]Logging initialized @2595ms
[INFO][2018-05-24 21:18:48,287][org.spark_project.jetty.server.Server]jetty-9.3.z-SNAPSHOT
[INFO][2018-05-24 21:18:48,306][org.spark_project.jetty.server.Server]Started @2692ms
[INFO][2018-05-24 21:18:48,330][org.spark_project.jetty.server.AbstractConnector]Started ServerConnector@25177d29{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-05-24 21:18:48,330][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4040.
[INFO][2018-05-24 21:18:48,370][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@c7a975a{/jobs,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:18:48,371][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@55795845{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:18:48,372][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@119f1f2a{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:18:48,373][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7fd4acee{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:18:48,373][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6175619b{/stages,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:18:48,374][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@756cf158{/stages/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:18:48,375][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@751d3241{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:18:48,376][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@30ea8c23{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:18:48,378][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4e76dac{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:18:48,379][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5f2f577{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:18:48,380][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5d465e4b{/storage,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:18:48,381][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@41a90fa8{/storage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:18:48,382][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@52500920{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:18:48,383][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@18a3962d{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:18:48,384][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2a65bb85{/environment,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:18:48,386][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4f936da8{/environment/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:18:48,386][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@452ba1db{/executors,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:18:48,387][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@76a36b71{/executors/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:18:48,388][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@f9d87b{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:18:48,389][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@26fb628{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:18:48,403][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@70dd7e15{/static,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:18:48,404][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@60d8c0dc{/,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:18:48,406][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6a62689d{/api,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:18:48,407][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1849db1a{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:18:48,408][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1ca25c47{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:18:48,410][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://192.168.0.102:4040
[INFO][2018-05-24 21:18:48,492][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
[INFO][2018-05-24 21:18:48,516][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 51153.
[INFO][2018-05-24 21:18:48,517][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 192.168.0.102:51153
[INFO][2018-05-24 21:18:48,519][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2018-05-24 21:18:48,521][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 192.168.0.102, 51153, None)
[INFO][2018-05-24 21:18:48,526][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 192.168.0.102:51153 with 912.3 MB RAM, BlockManagerId(driver, 192.168.0.102, 51153, None)
[INFO][2018-05-24 21:18:48,549][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 192.168.0.102, 51153, None)
[INFO][2018-05-24 21:18:48,551][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 192.168.0.102, 51153, None)
[INFO][2018-05-24 21:18:48,824][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@17f460bb{/metrics/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:18:48,948][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 21:18:48,952][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 21:18:48,952][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 21:18:59,634][org.apache.spark.streaming.kafka.DirectKafkaInputDStream]Slide time = 5000 ms
[INFO][2018-05-24 21:18:59,635][org.apache.spark.streaming.kafka.DirectKafkaInputDStream]Storage level = Serialized 1x Replicated
[INFO][2018-05-24 21:18:59,636][org.apache.spark.streaming.kafka.DirectKafkaInputDStream]Checkpoint interval = null
[INFO][2018-05-24 21:18:59,636][org.apache.spark.streaming.kafka.DirectKafkaInputDStream]Remember interval = 5000 ms
[INFO][2018-05-24 21:18:59,639][org.apache.spark.streaming.kafka.DirectKafkaInputDStream]Initialized and validated org.apache.spark.streaming.kafka.DirectKafkaInputDStream@2c50c720
[INFO][2018-05-24 21:18:59,640][org.apache.spark.streaming.dstream.MappedDStream]Slide time = 5000 ms
[INFO][2018-05-24 21:18:59,640][org.apache.spark.streaming.dstream.MappedDStream]Storage level = Serialized 1x Replicated
[INFO][2018-05-24 21:18:59,641][org.apache.spark.streaming.dstream.MappedDStream]Checkpoint interval = null
[INFO][2018-05-24 21:18:59,641][org.apache.spark.streaming.dstream.MappedDStream]Remember interval = 5000 ms
[INFO][2018-05-24 21:18:59,641][org.apache.spark.streaming.dstream.MappedDStream]Initialized and validated org.apache.spark.streaming.dstream.MappedDStream@1ee99424
[INFO][2018-05-24 21:18:59,641][org.apache.spark.streaming.dstream.ForEachDStream]Slide time = 5000 ms
[INFO][2018-05-24 21:18:59,641][org.apache.spark.streaming.dstream.ForEachDStream]Storage level = Serialized 1x Replicated
[INFO][2018-05-24 21:18:59,641][org.apache.spark.streaming.dstream.ForEachDStream]Checkpoint interval = null
[INFO][2018-05-24 21:18:59,642][org.apache.spark.streaming.dstream.ForEachDStream]Remember interval = 5000 ms
[INFO][2018-05-24 21:18:59,642][org.apache.spark.streaming.dstream.ForEachDStream]Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@141f35a8
[INFO][2018-05-24 21:18:59,724][org.apache.spark.streaming.util.RecurringTimer]Started timer for JobGenerator at time 1527167940000
[INFO][2018-05-24 21:18:59,725][org.apache.spark.streaming.scheduler.JobGenerator]Started JobGenerator at 1527167940000 ms
[INFO][2018-05-24 21:18:59,726][org.apache.spark.streaming.scheduler.JobScheduler]Started JobScheduler
[INFO][2018-05-24 21:18:59,730][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@57a48985{/streaming,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:18:59,731][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3910fe11{/streaming/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:18:59,733][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3f672204{/streaming/batch,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:18:59,734][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2c2db130{/streaming/batch/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:18:59,735][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2d000e80{/static/streaming,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:18:59,736][org.apache.spark.streaming.StreamingContext]StreamingContext started
[INFO][2018-05-24 21:19:00,086][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 21:19:00,089][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 21:19:00,091][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 21:19:05,205][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527167940000 ms
[INFO][2018-05-24 21:19:05,210][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527167940000 ms.0 from job set of time 1527167940000 ms
[INFO][2018-05-24 21:19:05,261][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:19:05,282][org.apache.spark.scheduler.DAGScheduler]Got job 0 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:19:05,284][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 0 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:19:05,285][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:19:05,287][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:19:05,288][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527167945000 ms
[INFO][2018-05-24 21:19:05,300][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 0 (MapPartitionsRDD[1] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:19:05,510][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 21:19:05,575][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0_piece0 stored as bytes in memory (estimated size 1867.0 B, free 912.3 MB)
[INFO][2018-05-24 21:19:05,578][org.apache.spark.storage.BlockManagerInfo]Added broadcast_0_piece0 in memory on 192.168.0.102:51153 (size: 1867.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:19:05,585][org.apache.spark.SparkContext]Created broadcast 0 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:19:05,613][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:19:05,614][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 0.0 with 1 tasks
[INFO][2018-05-24 21:19:05,652][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:19:05,663][org.apache.spark.executor.Executor]Running task 0.0 in stage 0.0 (TID 0)
[INFO][2018-05-24 21:19:05,708][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12204 is the same as ending offset skipping seven 0
[INFO][2018-05-24 21:19:05,927][org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper]Process identifier=hconnection-0x4841d9cd connecting to ZooKeeper ensemble=vm-xaj-bigdata-da-d01:2181,vm-xaj-bigdata-da-d02:2181,vm-xaj-bigdata-da-d03:2181
[INFO][2018-05-24 21:19:06,055][org.apache.zookeeper.ZooKeeper]Client environment:zookeeper.version=3.4.5-cdh5.7.0--1, built on 04/06/2016 14:24 GMT
[INFO][2018-05-24 21:19:06,055][org.apache.zookeeper.ZooKeeper]Client environment:host.name=192.168.0.102
[INFO][2018-05-24 21:19:06,055][org.apache.zookeeper.ZooKeeper]Client environment:java.version=1.8.0_161
[INFO][2018-05-24 21:19:06,055][org.apache.zookeeper.ZooKeeper]Client environment:java.vendor=Oracle Corporation
[INFO][2018-05-24 21:19:06,055][org.apache.zookeeper.ZooKeeper]Client environment:java.home=/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre
[INFO][2018-05-24 21:19:06,055][org.apache.zookeeper.ZooKeeper]Client environment:java.class.path=/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/charsets.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/deploy.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/cldrdata.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/dnsns.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/jaccess.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/jfxrt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/localedata.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/nashorn.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/sunec.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/sunjce_provider.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/sunpkcs11.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/ext/zipfs.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/javaws.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/jce.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/jfr.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/jfxswt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/jsse.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/management-agent.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/plugin.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/resources.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre/lib/rt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/ant-javafx.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/dt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/javafx-mx.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/jconsole.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/packager.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/sa-jdi.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/lib/tools.jar:/Users/seven/project/scala/dataMining/target/classes:/Users/seven/software/maven/repository/org/scala-lang/scala-library/2.11.12/scala-library-2.11.12.jar:/Users/seven/software/maven/repository/org/scala-lang/scala-compiler/2.11.12/scala-compiler-2.11.12.jar:/Users/seven/software/maven/repository/org/scala-lang/modules/scala-xml_2.11/1.0.5/scala-xml_2.11-1.0.5.jar:/Users/seven/software/maven/repository/org/scala-lang/modules/scala-parser-combinators_2.11/1.0.4/scala-parser-combinators_2.11-1.0.4.jar:/Users/seven/software/maven/repository/org/scala-lang/scala-reflect/2.11.12/scala-reflect-2.11.12.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-client/2.6.0-cdh5.7.0/hadoop-client-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-common/2.6.0-cdh5.7.0/hadoop-common-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/xmlenc/xmlenc/0.52/xmlenc-0.52.jar:/Users/seven/software/maven/repository/commons-net/commons-net/3.1/commons-net-3.1.jar:/Users/seven/software/maven/repository/commons-configuration/commons-configuration/1.6/commons-configuration-1.6.jar:/Users/seven/software/maven/repository/commons-digester/commons-digester/1.8/commons-digester-1.8.jar:/Users/seven/software/maven/repository/commons-beanutils/commons-beanutils/1.7.0/commons-beanutils-1.7.0.jar:/Users/seven/software/maven/repository/org/slf4j/slf4j-api/1.7.5/slf4j-api-1.7.5.jar:/Users/seven/software/maven/repository/org/slf4j/slf4j-log4j12/1.7.5/slf4j-log4j12-1.7.5.jar:/Users/seven/software/maven/repository/org/apache/avro/avro/1.7.6-cdh5.7.0/avro-1.7.6-cdh5.7.0.jar:/Users/seven/software/maven/repository/com/google/code/gson/gson/2.2.4/gson-2.2.4.jar:/Users/seven/software/maven/repository/org/apache/curator/curator-client/2.7.1/curator-client-2.7.1.jar:/Users/seven/software/maven/repository/org/apache/curator/curator-recipes/2.7.1/curator-recipes-2.7.1.jar:/Users/seven/software/maven/repository/com/google/code/findbugs/jsr305/3.0.0/jsr305-3.0.0.jar:/Users/seven/software/maven/repository/org/apache/htrace/htrace-core4/4.0.1-incubating/htrace-core4-4.0.1-incubating.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-compress/1.4.1/commons-compress-1.4.1.jar:/Users/seven/software/maven/repository/org/tukaani/xz/1.0/xz-1.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-hdfs/2.6.0-cdh5.7.0/hadoop-hdfs-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/io/netty/netty/3.6.2.Final/netty-3.6.2.Final.jar:/Users/seven/software/maven/repository/xerces/xercesImpl/2.9.1/xercesImpl-2.9.1.jar:/Users/seven/software/maven/repository/xml-apis/xml-apis/1.3.04/xml-apis-1.3.04.jar:/Users/seven/software/maven/repository/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-app/2.6.0-cdh5.7.0/hadoop-mapreduce-client-app-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-common/2.6.0-cdh5.7.0/hadoop-mapreduce-client-common-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-yarn-client/2.6.0-cdh5.7.0/hadoop-yarn-client-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-yarn-server-common/2.6.0-cdh5.7.0/hadoop-yarn-server-common-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-shuffle/2.6.0-cdh5.7.0/hadoop-mapreduce-client-shuffle-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-yarn-api/2.6.0-cdh5.7.0/hadoop-yarn-api-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-core/2.6.0-cdh5.7.0/hadoop-mapreduce-client-core-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-yarn-common/2.6.0-cdh5.7.0/hadoop-yarn-common-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/javax/xml/bind/jaxb-api/2.2.2/jaxb-api-2.2.2.jar:/Users/seven/software/maven/repository/javax/xml/stream/stax-api/1.0-2/stax-api-1.0-2.jar:/Users/seven/software/maven/repository/javax/activation/activation/1.1/activation-1.1.jar:/Users/seven/software/maven/repository/com/sun/jersey/jersey-client/1.9/jersey-client-1.9.jar:/Users/seven/software/maven/repository/org/codehaus/jackson/jackson-xc/1.8.8/jackson-xc-1.8.8.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.6.0-cdh5.7.0/hadoop-mapreduce-client-jobclient-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-aws/2.6.0-cdh5.7.0/hadoop-aws-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/com/amazonaws/aws-java-sdk-s3/1.10.6/aws-java-sdk-s3-1.10.6.jar:/Users/seven/software/maven/repository/com/amazonaws/aws-java-sdk-kms/1.10.6/aws-java-sdk-kms-1.10.6.jar:/Users/seven/software/maven/repository/com/amazonaws/aws-java-sdk-core/1.10.6/aws-java-sdk-core-1.10.6.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-annotations/2.6.0-cdh5.7.0/hadoop-annotations-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-client/1.2.0-cdh5.7.0/hbase-client-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-annotations/1.2.0-cdh5.7.0/hbase-annotations-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-common/1.2.0-cdh5.7.0/hbase-common-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-protocol/1.2.0-cdh5.7.0/hbase-protocol-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/commons-codec/commons-codec/1.9/commons-codec-1.9.jar:/Users/seven/software/maven/repository/commons-io/commons-io/2.4/commons-io-2.4.jar:/Users/seven/software/maven/repository/commons-lang/commons-lang/2.6/commons-lang-2.6.jar:/Users/seven/software/maven/repository/commons-logging/commons-logging/1.2/commons-logging-1.2.jar:/Users/seven/software/maven/repository/com/google/guava/guava/12.0.1/guava-12.0.1.jar:/Users/seven/software/maven/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/Users/seven/software/maven/repository/io/netty/netty-all/4.0.23.Final/netty-all-4.0.23.Final.jar:/Users/seven/software/maven/repository/org/apache/zookeeper/zookeeper/3.4.5-cdh5.7.0/zookeeper-3.4.5-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/htrace/htrace-core/3.2.0-incubating/htrace-core-3.2.0-incubating.jar:/Users/seven/software/maven/repository/org/codehaus/jackson/jackson-mapper-asl/1.8.8/jackson-mapper-asl-1.8.8.jar:/Users/seven/software/maven/repository/org/jruby/jcodings/jcodings/1.0.8/jcodings-1.0.8.jar:/Users/seven/software/maven/repository/org/jruby/joni/joni/2.1.2/joni-2.1.2.jar:/Users/seven/software/maven/repository/com/yammer/metrics/metrics-core/2.2.0/metrics-core-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-auth/2.6.0-cdh5.7.0/hadoop-auth-2.6.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/httpcomponents/httpclient/4.2.5/httpclient-4.2.5.jar:/Users/seven/software/maven/repository/org/apache/httpcomponents/httpcore/4.2.4/httpcore-4.2.4.jar:/Users/seven/software/maven/repository/org/apache/directory/server/apacheds-kerberos-codec/2.0.0-M15/apacheds-kerberos-codec-2.0.0-M15.jar:/Users/seven/software/maven/repository/org/apache/directory/server/apacheds-i18n/2.0.0-M15/apacheds-i18n-2.0.0-M15.jar:/Users/seven/software/maven/repository/org/apache/directory/api/api-asn1-api/1.0.0-M20/api-asn1-api-1.0.0-M20.jar:/Users/seven/software/maven/repository/org/apache/directory/api/api-util/1.0.0-M20/api-util-1.0.0-M20.jar:/Users/seven/software/maven/repository/org/apache/curator/curator-framework/2.7.1/curator-framework-2.7.1.jar:/Users/seven/software/maven/repository/com/github/stephenc/findbugs/findbugs-annotations/1.3.9-1/findbugs-annotations-1.3.9-1.jar:/Users/seven/software/maven/repository/junit/junit/4.12/junit-4.12.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-server/1.2.0-cdh5.7.0/hbase-server-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-procedure/1.2.0-cdh5.7.0/hbase-procedure-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-common/1.2.0-cdh5.7.0/hbase-common-1.2.0-cdh5.7.0-tests.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-prefix-tree/1.2.0-cdh5.7.0/hbase-prefix-tree-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar:/Users/seven/software/maven/repository/commons-collections/commons-collections/3.2.2/commons-collections-3.2.2.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-hadoop-compat/1.2.0-cdh5.7.0/hbase-hadoop-compat-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/org/apache/hbase/hbase-hadoop2-compat/1.2.0-cdh5.7.0/hbase-hadoop2-compat-1.2.0-cdh5.7.0.jar:/Users/seven/software/maven/repository/com/sun/jersey/jersey-core/1.9/jersey-core-1.9.jar:/Users/seven/software/maven/repository/com/sun/jersey/jersey-server/1.9/jersey-server-1.9.jar:/Users/seven/software/maven/repository/asm/asm/3.1/asm-3.1.jar:/Users/seven/software/maven/repository/commons-cli/commons-cli/1.2/commons-cli-1.2.jar:/Users/seven/software/maven/repository/com/github/stephenc/high-scale-lib/high-scale-lib/1.1.1/high-scale-lib-1.1.1.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-math/2.1/commons-math-2.1.jar:/Users/seven/software/maven/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jetty/6.1.26.cloudera.4/jetty-6.1.26.cloudera.4.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jetty-util/6.1.26.cloudera.4/jetty-util-6.1.26.cloudera.4.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jetty-sslengine/6.1.26.cloudera.4/jetty-sslengine-6.1.26.cloudera.4.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jsp-2.1/6.1.14/jsp-2.1-6.1.14.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/jsp-api-2.1/6.1.14/jsp-api-2.1-6.1.14.jar:/Users/seven/software/maven/repository/org/mortbay/jetty/servlet-api-2.5/6.1.14/servlet-api-2.5-6.1.14.jar:/Users/seven/software/maven/repository/org/codehaus/jackson/jackson-core-asl/1.8.8/jackson-core-asl-1.8.8.jar:/Users/seven/software/maven/repository/org/codehaus/jackson/jackson-jaxrs/1.8.8/jackson-jaxrs-1.8.8.jar:/Users/seven/software/maven/repository/tomcat/jasper-compiler/5.5.23/jasper-compiler-5.5.23.jar:/Users/seven/software/maven/repository/tomcat/jasper-runtime/5.5.23/jasper-runtime-5.5.23.jar:/Users/seven/software/maven/repository/commons-el/commons-el/1.0/commons-el-1.0.jar:/Users/seven/software/maven/repository/org/jamon/jamon-runtime/2.4.1/jamon-runtime-2.4.1.jar:/Users/seven/software/maven/repository/com/lmax/disruptor/3.3.0/disruptor-3.3.0.jar:/Users/seven/software/maven/repository/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar:/Users/seven/software/maven/repository/org/owasp/esapi/esapi/2.1.0/esapi-2.1.0.jar:/Users/seven/software/maven/repository/commons-beanutils/commons-beanutils-core/1.7.0/commons-beanutils-core-1.7.0.jar:/Users/seven/software/maven/repository/commons-fileupload/commons-fileupload/1.2/commons-fileupload-1.2.jar:/Users/seven/software/maven/repository/xom/xom/1.2.5/xom-1.2.5.jar:/Users/seven/software/maven/repository/xalan/xalan/2.7.0/xalan-2.7.0.jar:/Users/seven/software/maven/repository/org/beanshell/bsh-core/2.0b4/bsh-core-2.0b4.jar:/Users/seven/software/maven/repository/org/owasp/antisamy/antisamy/1.4.3/antisamy-1.4.3.jar:/Users/seven/software/maven/repository/org/apache/xmlgraphics/batik-css/1.7/batik-css-1.7.jar:/Users/seven/software/maven/repository/org/apache/xmlgraphics/batik-ext/1.7/batik-ext-1.7.jar:/Users/seven/software/maven/repository/org/apache/xmlgraphics/batik-util/1.7/batik-util-1.7.jar:/Users/seven/software/maven/repository/xml-apis/xml-apis-ext/1.3.04/xml-apis-ext-1.3.04.jar:/Users/seven/software/maven/repository/net/sourceforge/nekohtml/nekohtml/1.9.12/nekohtml-1.9.12.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-core/2.6.0-mr1-cdh5.7.0/hadoop-core-2.6.0-mr1-cdh5.7.0.jar:/Users/seven/software/maven/repository/javax/servlet/jsp/jsp-api/2.1/jsp-api-2.1.jar:/Users/seven/software/maven/repository/net/java/dev/jets3t/jets3t/0.6.1/jets3t-0.6.1.jar:/Users/seven/software/maven/repository/javax/servlet/servlet-api/2.5/servlet-api-2.5.jar:/Users/seven/software/maven/repository/hsqldb/hsqldb/1.8.0.10/hsqldb-1.8.0.10.jar:/Users/seven/software/maven/repository/org/eclipse/jdt/core/3.1.1/core-3.1.1.jar:/Users/seven/software/maven/repository/org/apache/hadoop/hadoop-hdfs/2.6.0-cdh5.7.0/hadoop-hdfs-2.6.0-cdh5.7.0-tests.jar:/Users/seven/software/maven/repository/commons-daemon/commons-daemon/1.0.13/commons-daemon-1.0.13.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-lang3/3.5/commons-lang3-3.5.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-pool2/2.4.2/commons-pool2-2.4.2.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-mllib_2.11/2.2.0/spark-mllib_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-core_2.11/2.2.0/spark-core_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/avro/avro-mapred/1.7.7/avro-mapred-1.7.7-hadoop2.jar:/Users/seven/software/maven/repository/org/apache/avro/avro-ipc/1.7.7/avro-ipc-1.7.7-tests.jar:/Users/seven/software/maven/repository/com/twitter/chill_2.11/0.8.0/chill_2.11-0.8.0.jar:/Users/seven/software/maven/repository/com/esotericsoftware/kryo-shaded/3.0.3/kryo-shaded-3.0.3.jar:/Users/seven/software/maven/repository/com/esotericsoftware/minlog/1.3.0/minlog-1.3.0.jar:/Users/seven/software/maven/repository/org/objenesis/objenesis/2.1/objenesis-2.1.jar:/Users/seven/software/maven/repository/com/twitter/chill-java/0.8.0/chill-java-0.8.0.jar:/Users/seven/software/maven/repository/org/apache/xbean/xbean-asm5-shaded/4.4/xbean-asm5-shaded-4.4.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-launcher_2.11/2.2.0/spark-launcher_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-network-common_2.11/2.2.0/spark-network-common_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-network-shuffle_2.11/2.2.0/spark-network-shuffle_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-unsafe_2.11/2.2.0/spark-unsafe_2.11-2.2.0.jar:/Users/seven/software/maven/repository/javax/servlet/javax.servlet-api/3.1.0/javax.servlet-api-3.1.0.jar:/Users/seven/software/maven/repository/org/slf4j/jul-to-slf4j/1.7.16/jul-to-slf4j-1.7.16.jar:/Users/seven/software/maven/repository/org/slf4j/jcl-over-slf4j/1.7.16/jcl-over-slf4j-1.7.16.jar:/Users/seven/software/maven/repository/com/ning/compress-lzf/1.0.3/compress-lzf-1.0.3.jar:/Users/seven/software/maven/repository/org/xerial/snappy/snappy-java/1.1.2.6/snappy-java-1.1.2.6.jar:/Users/seven/software/maven/repository/org/roaringbitmap/RoaringBitmap/0.5.11/RoaringBitmap-0.5.11.jar:/Users/seven/software/maven/repository/org/json4s/json4s-jackson_2.11/3.2.11/json4s-jackson_2.11-3.2.11.jar:/Users/seven/software/maven/repository/org/json4s/json4s-core_2.11/3.2.11/json4s-core_2.11-3.2.11.jar:/Users/seven/software/maven/repository/org/json4s/json4s-ast_2.11/3.2.11/json4s-ast_2.11-3.2.11.jar:/Users/seven/software/maven/repository/org/scala-lang/scalap/2.11.0/scalap-2.11.0.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/core/jersey-client/2.22.2/jersey-client-2.22.2.jar:/Users/seven/software/maven/repository/javax/ws/rs/javax.ws.rs-api/2.0.1/javax.ws.rs-api-2.0.1.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/hk2-api/2.4.0-b34/hk2-api-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/hk2-utils/2.4.0-b34/hk2-utils-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/external/aopalliance-repackaged/2.4.0-b34/aopalliance-repackaged-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/external/javax.inject/2.4.0-b34/javax.inject-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/hk2-locator/2.4.0-b34/hk2-locator-2.4.0-b34.jar:/Users/seven/software/maven/repository/org/javassist/javassist/3.18.1-GA/javassist-3.18.1-GA.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/core/jersey-common/2.22.2/jersey-common-2.22.2.jar:/Users/seven/software/maven/repository/javax/annotation/javax.annotation-api/1.2/javax.annotation-api-1.2.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/bundles/repackaged/jersey-guava/2.22.2/jersey-guava-2.22.2.jar:/Users/seven/software/maven/repository/org/glassfish/hk2/osgi-resource-locator/1.0.1/osgi-resource-locator-1.0.1.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/core/jersey-server/2.22.2/jersey-server-2.22.2.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/media/jersey-media-jaxb/2.22.2/jersey-media-jaxb-2.22.2.jar:/Users/seven/software/maven/repository/javax/validation/validation-api/1.1.0.Final/validation-api-1.1.0.Final.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/containers/jersey-container-servlet/2.22.2/jersey-container-servlet-2.22.2.jar:/Users/seven/software/maven/repository/org/glassfish/jersey/containers/jersey-container-servlet-core/2.22.2/jersey-container-servlet-core-2.22.2.jar:/Users/seven/software/maven/repository/com/clearspring/analytics/stream/2.7.0/stream-2.7.0.jar:/Users/seven/software/maven/repository/io/dropwizard/metrics/metrics-core/3.1.2/metrics-core-3.1.2.jar:/Users/seven/software/maven/repository/io/dropwizard/metrics/metrics-jvm/3.1.2/metrics-jvm-3.1.2.jar:/Users/seven/software/maven/repository/io/dropwizard/metrics/metrics-json/3.1.2/metrics-json-3.1.2.jar:/Users/seven/software/maven/repository/io/dropwizard/metrics/metrics-graphite/3.1.2/metrics-graphite-3.1.2.jar:/Users/seven/software/maven/repository/org/apache/ivy/ivy/2.4.0/ivy-2.4.0.jar:/Users/seven/software/maven/repository/oro/oro/2.0.8/oro-2.0.8.jar:/Users/seven/software/maven/repository/net/razorvine/pyrolite/4.13/pyrolite-4.13.jar:/Users/seven/software/maven/repository/net/sf/py4j/py4j/0.10.4/py4j-0.10.4.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-crypto/1.0.0/commons-crypto-1.0.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-streaming_2.11/2.2.0/spark-streaming_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-sql_2.11/2.2.0/spark-sql_2.11-2.2.0.jar:/Users/seven/software/maven/repository/com/univocity/univocity-parsers/2.2.1/univocity-parsers-2.2.1.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-sketch_2.11/2.2.0/spark-sketch_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-catalyst_2.11/2.2.0/spark-catalyst_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/codehaus/janino/janino/3.0.0/janino-3.0.0.jar:/Users/seven/software/maven/repository/org/codehaus/janino/commons-compiler/3.0.0/commons-compiler-3.0.0.jar:/Users/seven/software/maven/repository/org/antlr/antlr4-runtime/4.5.3/antlr4-runtime-4.5.3.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-column/1.8.2/parquet-column-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-common/1.8.2/parquet-common-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-encoding/1.8.2/parquet-encoding-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-hadoop/1.8.2/parquet-hadoop-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-format/2.3.1/parquet-format-2.3.1.jar:/Users/seven/software/maven/repository/org/apache/parquet/parquet-jackson/1.8.2/parquet-jackson-1.8.2.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-graphx_2.11/2.2.0/spark-graphx_2.11-2.2.0.jar:/Users/seven/software/maven/repository/com/github/fommil/netlib/core/1.1.2/core-1.1.2.jar:/Users/seven/software/maven/repository/net/sourceforge/f2j/arpack_combined_all/0.1/arpack_combined_all-0.1.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-mllib-local_2.11/2.2.0/spark-mllib-local_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/scalanlp/breeze_2.11/0.13.1/breeze_2.11-0.13.1.jar:/Users/seven/software/maven/repository/org/scalanlp/breeze-macros_2.11/0.13.1/breeze-macros_2.11-0.13.1.jar:/Users/seven/software/maven/repository/net/sf/opencsv/opencsv/2.3/opencsv-2.3.jar:/Users/seven/software/maven/repository/com/github/rwl/jtransforms/2.4.0/jtransforms-2.4.0.jar:/Users/seven/software/maven/repository/org/spire-math/spire_2.11/0.13.0/spire_2.11-0.13.0.jar:/Users/seven/software/maven/repository/org/spire-math/spire-macros_2.11/0.13.0/spire-macros_2.11-0.13.0.jar:/Users/seven/software/maven/repository/org/typelevel/machinist_2.11/0.6.1/machinist_2.11-0.6.1.jar:/Users/seven/software/maven/repository/com/chuusai/shapeless_2.11/2.3.2/shapeless_2.11-2.3.2.jar:/Users/seven/software/maven/repository/org/typelevel/macro-compat_2.11/1.1.1/macro-compat_2.11-1.1.1.jar:/Users/seven/software/maven/repository/org/apache/commons/commons-math3/3.4.1/commons-math3-3.4.1.jar:/Users/seven/software/maven/repository/org/jpmml/pmml-model/1.2.15/pmml-model-1.2.15.jar:/Users/seven/software/maven/repository/org/jpmml/pmml-schema/1.2.15/pmml-schema-1.2.15.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-tags_2.11/2.2.0/spark-tags_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-streaming-kafka-0-8_2.11/2.2.0/spark-streaming-kafka-0-8_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/kafka/kafka_2.11/0.8.2.1/kafka_2.11-0.8.2.1.jar:/Users/seven/software/maven/repository/com/101tec/zkclient/0.3/zkclient-0.3.jar:/Users/seven/software/maven/repository/org/apache/kafka/kafka-clients/0.8.2.1/kafka-clients-0.8.2.1.jar:/Users/seven/software/maven/repository/mysql/mysql-connector-java/5.1.31/mysql-connector-java-5.1.31.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/module/jackson-module-scala_2.11/2.6.5/jackson-module-scala_2.11-2.6.5.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/core/jackson-core/2.6.5/jackson-core-2.6.5.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/core/jackson-annotations/2.6.5/jackson-annotations-2.6.5.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/core/jackson-databind/2.6.5/jackson-databind-2.6.5.jar:/Users/seven/software/maven/repository/com/fasterxml/jackson/module/jackson-module-paranamer/2.6.5/jackson-module-paranamer-2.6.5.jar:/Users/seven/software/maven/repository/com/thoughtworks/paranamer/paranamer/2.6/paranamer-2.6.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-streaming-flume_2.11/2.2.0/spark-streaming-flume_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/flume/flume-ng-core/1.6.0/flume-ng-core-1.6.0.jar:/Users/seven/software/maven/repository/org/apache/flume/flume-ng-configuration/1.6.0/flume-ng-configuration-1.6.0.jar:/Users/seven/software/maven/repository/org/apache/avro/avro-ipc/1.7.4/avro-ipc-1.7.4.jar:/Users/seven/software/maven/repository/org/apache/velocity/velocity/1.7/velocity-1.7.jar:/Users/seven/software/maven/repository/joda-time/joda-time/2.1/joda-time-2.1.jar:/Users/seven/software/maven/repository/org/apache/mina/mina-core/2.0.4/mina-core-2.0.4.jar:/Users/seven/software/maven/repository/org/apache/flume/flume-ng-sdk/1.6.0/flume-ng-sdk-1.6.0.jar:/Users/seven/software/maven/repository/org/apache/spark/spark-streaming-flume-sink_2.11/2.2.0/spark-streaming-flume-sink_2.11-2.2.0.jar:/Users/seven/software/maven/repository/org/apache/flume/flume-ng-clients/flume-ng-log4jappender/1.6.0/flume-ng-log4jappender-1.6.0.jar:/Users/seven/software/maven/repository/net/jpountz/lz4/lz4/1.3.0/lz4-1.3.0.jar:/Applications/IntelliJ IDEA.app/Contents/lib/idea_rt.jar
[INFO][2018-05-24 21:19:06,056][org.apache.zookeeper.ZooKeeper]Client environment:java.library.path=/Users/seven/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:.
[INFO][2018-05-24 21:19:06,056][org.apache.zookeeper.ZooKeeper]Client environment:java.io.tmpdir=/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/
[INFO][2018-05-24 21:19:06,057][org.apache.zookeeper.ZooKeeper]Client environment:java.compiler=<NA>
[INFO][2018-05-24 21:19:06,057][org.apache.zookeeper.ZooKeeper]Client environment:os.name=Mac OS X
[INFO][2018-05-24 21:19:06,057][org.apache.zookeeper.ZooKeeper]Client environment:os.arch=x86_64
[INFO][2018-05-24 21:19:06,057][org.apache.zookeeper.ZooKeeper]Client environment:os.version=10.13.4
[INFO][2018-05-24 21:19:06,057][org.apache.zookeeper.ZooKeeper]Client environment:user.name=seven
[INFO][2018-05-24 21:19:06,057][org.apache.zookeeper.ZooKeeper]Client environment:user.home=/Users/seven
[INFO][2018-05-24 21:19:06,057][org.apache.zookeeper.ZooKeeper]Client environment:user.dir=/Users/seven/project/scala/dataMining
[INFO][2018-05-24 21:19:06,058][org.apache.zookeeper.ZooKeeper]Initiating client connection, connectString=vm-xaj-bigdata-da-d01:2181,vm-xaj-bigdata-da-d02:2181,vm-xaj-bigdata-da-d03:2181 sessionTimeout=60000 watcher=hconnection-0x4841d9cd0x0, quorum=vm-xaj-bigdata-da-d01:2181,vm-xaj-bigdata-da-d02:2181,vm-xaj-bigdata-da-d03:2181, baseZNode=/hbase
[INFO][2018-05-24 21:19:08,532][org.apache.spark.SparkContext]Running Spark version 2.2.0
[INFO][2018-05-24 21:19:09,304][org.apache.spark.SparkContext]Submitted application: SimulationKafkaSendOutData$
[INFO][2018-05-24 21:19:09,325][org.apache.spark.SecurityManager]Changing view acls to: seven
[INFO][2018-05-24 21:19:09,326][org.apache.spark.SecurityManager]Changing modify acls to: seven
[INFO][2018-05-24 21:19:09,327][org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO][2018-05-24 21:19:09,328][org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO][2018-05-24 21:19:09,329][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(seven); groups with view permissions: Set(); users  with modify permissions: Set(seven); groups with modify permissions: Set()
[INFO][2018-05-24 21:19:09,720][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 51165.
[INFO][2018-05-24 21:19:09,746][org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO][2018-05-24 21:19:09,763][org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO][2018-05-24 21:19:09,766][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2018-05-24 21:19:09,766][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
[INFO][2018-05-24 21:19:09,777][org.apache.spark.storage.DiskBlockManager]Created local directory at /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/blockmgr-d11bb7ff-9868-4ec8-945f-54850ad1c4d3
[INFO][2018-05-24 21:19:09,798][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 912.3 MB
[INFO][2018-05-24 21:19:09,900][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO][2018-05-24 21:19:09,995][org.spark_project.jetty.util.log]Logging initialized @2794ms
[INFO][2018-05-24 21:19:10,054][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527167950000 ms
[INFO][2018-05-24 21:19:10,057][org.spark_project.jetty.server.Server]jetty-9.3.z-SNAPSHOT
[INFO][2018-05-24 21:19:10,070][org.spark_project.jetty.server.Server]Started @2870ms
[WARN][2018-05-24 21:19:10,084][org.apache.spark.util.Utils]Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[INFO][2018-05-24 21:19:10,088][org.spark_project.jetty.server.AbstractConnector]Started ServerConnector@1643d68f{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}
[INFO][2018-05-24 21:19:10,089][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4041.
[INFO][2018-05-24 21:19:10,109][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2a39fe6a{/jobs,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:19:10,110][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@649725e3{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:19:10,110][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4c168660{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:19:10,111][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4eed46ee{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:19:10,111][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4fad94a7{/stages,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:19:10,112][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6326d182{/stages/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:19:10,113][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@716a7124{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:19:10,114][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5cc126dc{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:19:10,115][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@72bd06ca{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:19:10,115][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5dbe30be{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:19:10,116][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@55795845{/storage,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:19:10,116][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@119f1f2a{/storage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:19:10,117][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5b970f7{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:19:10,118][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@165b8a71{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:19:10,118][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2f058b8a{/environment,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:19:10,119][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3f2ef586{/environment/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:19:10,120][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@76c7beb3{/executors,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:19:10,120][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2cf92cc7{/executors/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:19:10,121][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7b139eab{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:19:10,122][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@611df6e3{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:19:10,134][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6273c5a4{/static,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:19:10,135][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3e2943ab{/,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:19:10,136][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4a9f80d3{/api,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:19:10,137][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@191ae03f{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:19:10,138][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@53ab0286{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:19:10,140][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://192.168.0.102:4041
[INFO][2018-05-24 21:19:10,232][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
[INFO][2018-05-24 21:19:10,257][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 51168.
[INFO][2018-05-24 21:19:10,259][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 192.168.0.102:51168
[INFO][2018-05-24 21:19:10,261][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2018-05-24 21:19:10,263][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 192.168.0.102, 51168, None)
[INFO][2018-05-24 21:19:10,266][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 192.168.0.102:51168 with 912.3 MB RAM, BlockManagerId(driver, 192.168.0.102, 51168, None)
[INFO][2018-05-24 21:19:10,269][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 192.168.0.102, 51168, None)
[INFO][2018-05-24 21:19:10,269][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 192.168.0.102, 51168, None)
[INFO][2018-05-24 21:19:10,451][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6c2f1700{/metrics/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:19:10,959][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0 stored as values in memory (estimated size 228.1 KB, free 912.1 MB)
[INFO][2018-05-24 21:19:11,026][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0_piece0 stored as bytes in memory (estimated size 22.1 KB, free 912.1 MB)
[INFO][2018-05-24 21:19:11,029][org.apache.spark.storage.BlockManagerInfo]Added broadcast_0_piece0 in memory on 192.168.0.102:51168 (size: 22.1 KB, free: 912.3 MB)
[INFO][2018-05-24 21:19:11,033][org.apache.spark.SparkContext]Created broadcast 0 from textFile at SimulationKafkaSendOutData.scala:25
[INFO][2018-05-24 21:19:16,186][org.apache.zookeeper.ClientCnxn]Opening socket connection to server vm-xaj-bigdata-da-d03/10.213.4.27:2181. Will not attempt to authenticate using SASL (unknown error)
[INFO][2018-05-24 21:19:16,205][org.apache.zookeeper.ClientCnxn]Socket connection established, initiating session, client: /10.0.8.39:51169, server: vm-xaj-bigdata-da-d03/10.213.4.27:2181
[INFO][2018-05-24 21:19:16,225][org.apache.zookeeper.ClientCnxn]Session establishment complete on server vm-xaj-bigdata-da-d03/10.213.4.27:2181, sessionid = 0x362b50d75ec5f8f, negotiated timeout = 60000
[WARN][2018-05-24 21:19:16,630][org.apache.hadoop.util.NativeCodeLoader]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO][2018-05-24 21:19:16,799][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 1
[WARN][2018-05-24 21:19:16,804][org.apache.hadoop.util.NativeCodeLoader]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO][2018-05-24 21:19:16,935][org.apache.spark.SparkContext]Starting job: collect at SimulationKafkaSendOutData.scala:25
[INFO][2018-05-24 21:19:16,946][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:19:16,948][org.apache.spark.scheduler.DAGScheduler]Got job 0 (collect at SimulationKafkaSendOutData.scala:25) with 2 output partitions
[INFO][2018-05-24 21:19:16,948][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 0 (collect at SimulationKafkaSendOutData.scala:25)
[INFO][2018-05-24 21:19:16,949][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:19:16,950][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:19:16,958][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 0 (hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/* MapPartitionsRDD[1] at textFile at SimulationKafkaSendOutData.scala:25), which has no missing parents
[INFO][2018-05-24 21:19:16,966][org.apache.spark.executor.Executor]Finished task 0.0 in stage 0.0 (TID 0). 794 bytes result sent to driver
[INFO][2018-05-24 21:19:16,977][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 0.0 (TID 0) in 11333 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:19:16,979][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:19:16,981][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1 stored as values in memory (estimated size 3.3 KB, free 912.1 MB)
[INFO][2018-05-24 21:19:16,984][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1_piece0 stored as bytes in memory (estimated size 1969.0 B, free 912.1 MB)
[INFO][2018-05-24 21:19:16,985][org.apache.spark.storage.BlockManagerInfo]Added broadcast_1_piece0 in memory on 192.168.0.102:51168 (size: 1969.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:19:16,986][org.apache.spark.SparkContext]Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:19:16,987][org.apache.spark.scheduler.DAGScheduler]ResultStage 0 (foreachPartition at ReceiveKafkaData.scala:66) finished in 11.354 s
[INFO][2018-05-24 21:19:16,995][org.apache.spark.scheduler.DAGScheduler]Job 0 finished: foreachPartition at ReceiveKafkaData.scala:66, took 11.733577 s
[INFO][2018-05-24 21:19:17,001][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527167940000 ms.0 from job set of time 1527167940000 ms
[INFO][2018-05-24 21:19:17,002][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 17.000 s for time 1527167940000 ms (execution: 11.791 s)
[INFO][2018-05-24 21:19:17,003][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527167945000 ms.0 from job set of time 1527167945000 ms
[INFO][2018-05-24 21:19:17,012][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:19:17,013][org.apache.spark.scheduler.DAGScheduler]Got job 1 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:19:17,013][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 1 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:19:17,013][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:19:17,013][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:19:17,013][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ResultStage 0 (hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/* MapPartitionsRDD[1] at textFile at SimulationKafkaSendOutData.scala:25) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2018-05-24 21:19:17,013][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 1 (MapPartitionsRDD[3] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:19:17,015][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 0.0 with 2 tasks
[INFO][2018-05-24 21:19:17,017][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 21:19:17,018][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1_piece0 stored as bytes in memory (estimated size 1868.0 B, free 912.3 MB)
[INFO][2018-05-24 21:19:17,019][org.apache.spark.storage.BlockManagerInfo]Added broadcast_1_piece0 in memory on 192.168.0.102:51153 (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:19:17,020][org.apache.spark.SparkContext]Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:19:17,021][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:19:17,021][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 1.0 with 1 tasks
[INFO][2018-05-24 21:19:17,023][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:19:17,032][org.apache.spark.executor.Executor]Running task 0.0 in stage 1.0 (TID 1)
[INFO][2018-05-24 21:19:17,073][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 4883 bytes)
[INFO][2018-05-24 21:19:17,078][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 4883 bytes)
[INFO][2018-05-24 21:19:17,087][org.apache.spark.executor.Executor]Running task 1.0 in stage 0.0 (TID 1)
[INFO][2018-05-24 21:19:17,087][org.apache.spark.executor.Executor]Running task 0.0 in stage 0.0 (TID 0)
[INFO][2018-05-24 21:19:17,154][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12204 is the same as ending offset skipping seven 0
[INFO][2018-05-24 21:19:17,154][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:19:17,177][org.apache.spark.executor.Executor]Finished task 0.0 in stage 1.0 (TID 1). 708 bytes result sent to driver
[INFO][2018-05-24 21:19:17,187][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 1.0 (TID 1) in 165 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:19:17,188][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:19:17,190][org.apache.spark.scheduler.DAGScheduler]ResultStage 1 (foreachPartition at ReceiveKafkaData.scala:66) finished in 0.167 s
[INFO][2018-05-24 21:19:17,191][org.apache.spark.scheduler.DAGScheduler]Job 1 finished: foreachPartition at ReceiveKafkaData.scala:66, took 0.178596 s
[INFO][2018-05-24 21:19:17,192][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527167945000 ms.0 from job set of time 1527167945000 ms
[INFO][2018-05-24 21:19:17,192][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 12.192 s for time 1527167945000 ms (execution: 0.190 s)
[INFO][2018-05-24 21:19:17,194][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527167950000 ms.0 from job set of time 1527167950000 ms
[INFO][2018-05-24 21:19:17,202][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:19:17,203][org.apache.spark.scheduler.DAGScheduler]Got job 2 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:19:17,203][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 2 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:19:17,203][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:19:17,203][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:19:17,203][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 2 (MapPartitionsRDD[5] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:19:17,206][org.apache.spark.storage.memory.MemoryStore]Block broadcast_2 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 21:19:17,208][org.apache.spark.storage.memory.MemoryStore]Block broadcast_2_piece0 stored as bytes in memory (estimated size 1868.0 B, free 912.3 MB)
[INFO][2018-05-24 21:19:17,209][org.apache.spark.storage.BlockManagerInfo]Added broadcast_2_piece0 in memory on 192.168.0.102:51153 (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:19:17,210][org.apache.spark.SparkContext]Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:19:17,211][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[5] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:19:17,211][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 2.0 with 1 tasks
[INFO][2018-05-24 21:19:17,212][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:19:17,213][org.apache.spark.executor.Executor]Running task 0.0 in stage 2.0 (TID 2)
[INFO][2018-05-24 21:19:17,217][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12204 is the same as ending offset skipping seven 0
[INFO][2018-05-24 21:19:17,218][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:19:17,220][org.apache.spark.executor.Executor]Finished task 0.0 in stage 2.0 (TID 2). 708 bytes result sent to driver
[INFO][2018-05-24 21:19:17,221][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 2.0 (TID 2) in 9 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:19:17,222][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:19:17,222][org.apache.spark.scheduler.DAGScheduler]ResultStage 2 (foreachPartition at ReceiveKafkaData.scala:66) finished in 0.010 s
[INFO][2018-05-24 21:19:17,223][org.apache.spark.scheduler.DAGScheduler]Job 2 finished: foreachPartition at ReceiveKafkaData.scala:66, took 0.021201 s
[INFO][2018-05-24 21:19:17,225][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527167950000 ms.0 from job set of time 1527167950000 ms
[INFO][2018-05-24 21:19:17,225][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 7.224 s for time 1527167950000 ms (execution: 0.030 s)
[INFO][2018-05-24 21:19:17,230][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/part-00000:0+10717435
[INFO][2018-05-24 21:19:17,230][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/part-00000:10717435+10717436
[INFO][2018-05-24 21:19:20,580][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527167955000 ms
[INFO][2018-05-24 21:19:20,583][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527167955000 ms.0 from job set of time 1527167955000 ms
[INFO][2018-05-24 21:19:20,596][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 21:19:20,597][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:19:20,600][org.apache.spark.scheduler.DAGScheduler]Got job 3 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:19:20,600][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 3 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:19:20,600][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:19:20,600][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:19:20,601][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 
[INFO][2018-05-24 21:19:20,605][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 1 from persistence list
[INFO][2018-05-24 21:19:20,606][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 3 (MapPartitionsRDD[7] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:19:20,609][org.apache.spark.storage.memory.MemoryStore]Block broadcast_3 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 21:19:20,611][org.apache.spark.storage.memory.MemoryStore]Block broadcast_3_piece0 stored as bytes in memory (estimated size 1868.0 B, free 912.3 MB)
[INFO][2018-05-24 21:19:20,615][org.apache.spark.storage.BlockManagerInfo]Added broadcast_3_piece0 in memory on 192.168.0.102:51153 (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:19:20,616][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 0 from persistence list
[INFO][2018-05-24 21:19:20,618][org.apache.spark.SparkContext]Created broadcast 3 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:19:20,619][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[7] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:19:20,619][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 3.0 with 1 tasks
[INFO][2018-05-24 21:19:20,619][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 21:19:20,620][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 
[INFO][2018-05-24 21:19:20,620][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 3 from persistence list
[INFO][2018-05-24 21:19:20,621][org.apache.spark.storage.BlockManager]Removing RDD 1
[INFO][2018-05-24 21:19:20,622][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 3.0 (TID 3, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:19:20,624][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 2 from persistence list
[INFO][2018-05-24 21:19:20,624][org.apache.spark.executor.Executor]Running task 0.0 in stage 3.0 (TID 3)
[INFO][2018-05-24 21:19:20,626][org.apache.spark.storage.BlockManager]Removing RDD 0
[INFO][2018-05-24 21:19:20,626][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 21:19:20,627][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527167940000 ms
[INFO][2018-05-24 21:19:20,628][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12204 is the same as ending offset skipping seven 0
[INFO][2018-05-24 21:19:20,629][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:19:20,630][org.apache.spark.executor.Executor]Finished task 0.0 in stage 3.0 (TID 3). 708 bytes result sent to driver
[INFO][2018-05-24 21:19:20,631][org.apache.spark.storage.BlockManager]Removing RDD 3
[INFO][2018-05-24 21:19:20,632][org.apache.spark.storage.BlockManager]Removing RDD 2
[INFO][2018-05-24 21:19:20,634][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 3.0 (TID 3) in 14 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:19:20,634][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 3.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:19:20,635][org.apache.spark.scheduler.DAGScheduler]ResultStage 3 (foreachPartition at ReceiveKafkaData.scala:66) finished in 0.016 s
[INFO][2018-05-24 21:19:20,636][org.apache.spark.scheduler.DAGScheduler]Job 3 finished: foreachPartition at ReceiveKafkaData.scala:66, took 0.037998 s
[INFO][2018-05-24 21:19:20,636][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527167955000 ms.0 from job set of time 1527167955000 ms
[INFO][2018-05-24 21:19:20,636][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 5.636 s for time 1527167955000 ms (execution: 0.053 s)
[INFO][2018-05-24 21:19:20,704][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527167960000 ms
[INFO][2018-05-24 21:19:20,705][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 5 from persistence list
[INFO][2018-05-24 21:19:20,707][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527167960000 ms.0 from job set of time 1527167960000 ms
[INFO][2018-05-24 21:19:20,708][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 4 from persistence list
[INFO][2018-05-24 21:19:20,708][org.apache.spark.storage.BlockManager]Removing RDD 5
[INFO][2018-05-24 21:19:20,709][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 21:19:20,710][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527167945000 ms
[INFO][2018-05-24 21:19:20,710][org.apache.spark.storage.BlockManager]Removing RDD 4
[INFO][2018-05-24 21:19:20,715][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:19:20,717][org.apache.spark.scheduler.DAGScheduler]Got job 4 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:19:20,717][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 4 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:19:20,717][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:19:20,718][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:19:20,718][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 4 (MapPartitionsRDD[9] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:19:20,721][org.apache.spark.storage.memory.MemoryStore]Block broadcast_4 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 21:19:20,723][org.apache.spark.storage.memory.MemoryStore]Block broadcast_4_piece0 stored as bytes in memory (estimated size 1868.0 B, free 912.3 MB)
[INFO][2018-05-24 21:19:20,724][org.apache.spark.storage.BlockManagerInfo]Added broadcast_4_piece0 in memory on 192.168.0.102:51153 (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:19:20,724][org.apache.spark.SparkContext]Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:19:20,725][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[9] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:19:20,725][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 4.0 with 1 tasks
[INFO][2018-05-24 21:19:20,726][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 4.0 (TID 4, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:19:20,727][org.apache.spark.executor.Executor]Running task 0.0 in stage 4.0 (TID 4)
[INFO][2018-05-24 21:19:20,731][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12204 is the same as ending offset skipping seven 0
[INFO][2018-05-24 21:19:20,732][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:19:20,733][org.apache.spark.executor.Executor]Finished task 0.0 in stage 4.0 (TID 4). 708 bytes result sent to driver
[INFO][2018-05-24 21:19:20,735][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 4.0 (TID 4) in 8 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:19:20,735][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 4.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:19:20,737][org.apache.spark.scheduler.DAGScheduler]ResultStage 4 (foreachPartition at ReceiveKafkaData.scala:66) finished in 0.010 s
[INFO][2018-05-24 21:19:20,740][org.apache.spark.scheduler.DAGScheduler]Job 4 finished: foreachPartition at ReceiveKafkaData.scala:66, took 0.023860 s
[INFO][2018-05-24 21:19:20,740][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527167960000 ms.0 from job set of time 1527167960000 ms
[INFO][2018-05-24 21:19:20,741][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.740 s for time 1527167960000 ms (execution: 0.033 s)
[INFO][2018-05-24 21:19:20,741][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 7 from persistence list
[INFO][2018-05-24 21:19:20,742][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 6 from persistence list
[INFO][2018-05-24 21:19:20,742][org.apache.spark.storage.BlockManager]Removing RDD 7
[INFO][2018-05-24 21:19:20,745][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 21:19:20,745][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527167950000 ms
[INFO][2018-05-24 21:19:20,745][org.apache.spark.storage.BlockManager]Removing RDD 6
[INFO][2018-05-24 21:19:20,841][org.apache.spark.SparkContext]Invoking stop() from shutdown hook
[INFO][2018-05-24 21:19:20,854][org.spark_project.jetty.server.AbstractConnector]Stopped Spark@1643d68f{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}
[INFO][2018-05-24 21:19:20,856][org.apache.spark.ui.SparkUI]Stopped Spark web UI at http://192.168.0.102:4041
[INFO][2018-05-24 21:19:20,876][org.apache.spark.scheduler.DAGScheduler]ResultStage 0 (collect at SimulationKafkaSendOutData.scala:25) failed in 3.835 s due to Stage cancelled because SparkContext was shut down
[INFO][2018-05-24 21:19:20,873][org.apache.spark.scheduler.DAGScheduler]Job 0 failed: collect at SimulationKafkaSendOutData.scala:25, took 3.936438 s
[ERROR][2018-05-24 21:19:20,884][org.apache.spark.scheduler.LiveListenerBus]SparkListenerBus has already stopped! Dropping event SparkListenerStageCompleted(org.apache.spark.scheduler.StageInfo@3939ee88)
[ERROR][2018-05-24 21:19:20,896][org.apache.spark.scheduler.LiveListenerBus]SparkListenerBus has already stopped! Dropping event SparkListenerJobEnd(0,1527167960891,JobFailed(org.apache.spark.SparkException: Job 0 cancelled because SparkContext was shut down))
[INFO][2018-05-24 21:19:20,906][org.apache.spark.MapOutputTrackerMasterEndpoint]MapOutputTrackerMasterEndpoint stopped!
[INFO][2018-05-24 21:19:20,937][org.apache.spark.storage.memory.MemoryStore]MemoryStore cleared
[INFO][2018-05-24 21:19:20,941][org.apache.spark.storage.BlockManager]BlockManager stopped
[INFO][2018-05-24 21:19:20,957][org.apache.spark.storage.BlockManagerMaster]BlockManagerMaster stopped
[INFO][2018-05-24 21:19:20,960][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint]OutputCommitCoordinator stopped!
[INFO][2018-05-24 21:19:20,964][org.apache.spark.SparkContext]Successfully stopped SparkContext
[INFO][2018-05-24 21:19:20,965][org.apache.spark.util.ShutdownHookManager]Shutdown hook called
[INFO][2018-05-24 21:19:20,966][org.apache.spark.util.ShutdownHookManager]Deleting directory /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/spark-7f76c4bf-49f3-4825-8441-80f7f14a489c
[ERROR][2018-05-24 21:19:21,038][org.apache.spark.TaskContextImpl]Error in TaskCompletionListener
java.lang.IllegalStateException: Block broadcast_0 not found
	at org.apache.spark.storage.BlockInfoManager$$anonfun$2.apply(BlockInfoManager.scala:293)
	at org.apache.spark.storage.BlockInfoManager$$anonfun$2.apply(BlockInfoManager.scala:293)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.storage.BlockInfoManager.unlock(BlockInfoManager.scala:292)
	at org.apache.spark.storage.BlockManager.releaseLock(BlockManager.scala:720)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$releaseLock$1.apply(TorrentBroadcast.scala:250)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$releaseLock$1.apply(TorrentBroadcast.scala:250)
	at org.apache.spark.TaskContext$$anon$1.onTaskCompletion(TaskContext.scala:128)
	at org.apache.spark.TaskContextImpl$$anonfun$markTaskCompleted$1.apply(TaskContextImpl.scala:117)
	at org.apache.spark.TaskContextImpl$$anonfun$markTaskCompleted$1.apply(TaskContextImpl.scala:117)
	at org.apache.spark.TaskContextImpl$$anonfun$invokeListeners$1.apply(TaskContextImpl.scala:130)
	at org.apache.spark.TaskContextImpl$$anonfun$invokeListeners$1.apply(TaskContextImpl.scala:128)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:128)
	at org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:116)
	at org.apache.spark.scheduler.Task.run(Task.scala:118)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[ERROR][2018-05-24 21:19:21,043][org.apache.spark.TaskContextImpl]Error in TaskCompletionListener
java.lang.IllegalStateException: Block broadcast_1 not found
	at org.apache.spark.storage.BlockInfoManager$$anonfun$2.apply(BlockInfoManager.scala:293)
	at org.apache.spark.storage.BlockInfoManager$$anonfun$2.apply(BlockInfoManager.scala:293)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.storage.BlockInfoManager.unlock(BlockInfoManager.scala:292)
	at org.apache.spark.storage.BlockManager.releaseLock(BlockManager.scala:720)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$releaseLock$1.apply(TorrentBroadcast.scala:250)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$releaseLock$1.apply(TorrentBroadcast.scala:250)
	at org.apache.spark.TaskContext$$anon$1.onTaskCompletion(TaskContext.scala:128)
	at org.apache.spark.TaskContextImpl$$anonfun$markTaskCompleted$1.apply(TaskContextImpl.scala:117)
	at org.apache.spark.TaskContextImpl$$anonfun$markTaskCompleted$1.apply(TaskContextImpl.scala:117)
	at org.apache.spark.TaskContextImpl$$anonfun$invokeListeners$1.apply(TaskContextImpl.scala:130)
	at org.apache.spark.TaskContextImpl$$anonfun$invokeListeners$1.apply(TaskContextImpl.scala:128)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:128)
	at org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:116)
	at org.apache.spark.scheduler.Task.run(Task.scala:118)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[ERROR][2018-05-24 21:19:21,045][org.apache.spark.util.Utils]Uncaught exception in thread Executor task launch worker for task 0
java.lang.NullPointerException
	at org.apache.spark.scheduler.Task$$anonfun$run$1.apply$mcV$sp(Task.scala:129)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1317)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[ERROR][2018-05-24 21:19:21,071][org.apache.spark.TaskContextImpl]Error in TaskCompletionListener
java.lang.IllegalStateException: Block broadcast_0 not found
	at org.apache.spark.storage.BlockInfoManager$$anonfun$2.apply(BlockInfoManager.scala:293)
	at org.apache.spark.storage.BlockInfoManager$$anonfun$2.apply(BlockInfoManager.scala:293)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.storage.BlockInfoManager.unlock(BlockInfoManager.scala:292)
	at org.apache.spark.storage.BlockManager.releaseLock(BlockManager.scala:720)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$releaseLock$1.apply(TorrentBroadcast.scala:250)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$releaseLock$1.apply(TorrentBroadcast.scala:250)
	at org.apache.spark.TaskContext$$anon$1.onTaskCompletion(TaskContext.scala:128)
	at org.apache.spark.TaskContextImpl$$anonfun$markTaskCompleted$1.apply(TaskContextImpl.scala:117)
	at org.apache.spark.TaskContextImpl$$anonfun$markTaskCompleted$1.apply(TaskContextImpl.scala:117)
	at org.apache.spark.TaskContextImpl$$anonfun$invokeListeners$1.apply(TaskContextImpl.scala:130)
	at org.apache.spark.TaskContextImpl$$anonfun$invokeListeners$1.apply(TaskContextImpl.scala:128)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:128)
	at org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:116)
	at org.apache.spark.scheduler.Task.run(Task.scala:118)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[ERROR][2018-05-24 21:19:21,072][org.apache.spark.TaskContextImpl]Error in TaskCompletionListener
java.lang.IllegalStateException: Block broadcast_1 not found
	at org.apache.spark.storage.BlockInfoManager$$anonfun$2.apply(BlockInfoManager.scala:293)
	at org.apache.spark.storage.BlockInfoManager$$anonfun$2.apply(BlockInfoManager.scala:293)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.storage.BlockInfoManager.unlock(BlockInfoManager.scala:292)
	at org.apache.spark.storage.BlockManager.releaseLock(BlockManager.scala:720)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$releaseLock$1.apply(TorrentBroadcast.scala:250)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$releaseLock$1.apply(TorrentBroadcast.scala:250)
	at org.apache.spark.TaskContext$$anon$1.onTaskCompletion(TaskContext.scala:128)
	at org.apache.spark.TaskContextImpl$$anonfun$markTaskCompleted$1.apply(TaskContextImpl.scala:117)
	at org.apache.spark.TaskContextImpl$$anonfun$markTaskCompleted$1.apply(TaskContextImpl.scala:117)
	at org.apache.spark.TaskContextImpl$$anonfun$invokeListeners$1.apply(TaskContextImpl.scala:130)
	at org.apache.spark.TaskContextImpl$$anonfun$invokeListeners$1.apply(TaskContextImpl.scala:128)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:128)
	at org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:116)
	at org.apache.spark.scheduler.Task.run(Task.scala:118)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[ERROR][2018-05-24 21:19:21,072][org.apache.spark.util.Utils]Uncaught exception in thread Executor task launch worker for task 1
java.lang.NullPointerException
	at org.apache.spark.scheduler.Task$$anonfun$run$1.apply$mcV$sp(Task.scala:129)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1317)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO][2018-05-24 21:19:33,306][org.apache.spark.SparkContext]Running Spark version 2.2.0
[INFO][2018-05-24 21:19:34,034][org.apache.spark.SparkContext]Submitted application: SimulationKafkaSendOutData$
[INFO][2018-05-24 21:19:34,052][org.apache.spark.SecurityManager]Changing view acls to: seven
[INFO][2018-05-24 21:19:34,053][org.apache.spark.SecurityManager]Changing modify acls to: seven
[INFO][2018-05-24 21:19:34,053][org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO][2018-05-24 21:19:34,054][org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO][2018-05-24 21:19:34,055][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(seven); groups with view permissions: Set(); users  with modify permissions: Set(seven); groups with modify permissions: Set()
[INFO][2018-05-24 21:19:34,358][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 51187.
[INFO][2018-05-24 21:19:34,381][org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO][2018-05-24 21:19:34,397][org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO][2018-05-24 21:19:34,399][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2018-05-24 21:19:34,399][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
[INFO][2018-05-24 21:19:34,408][org.apache.spark.storage.DiskBlockManager]Created local directory at /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/blockmgr-0ba97365-b2ec-4620-a57f-dbe73d75b4ca
[INFO][2018-05-24 21:19:34,430][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 912.3 MB
[INFO][2018-05-24 21:19:34,520][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO][2018-05-24 21:19:34,643][org.spark_project.jetty.util.log]Logging initialized @2573ms
[INFO][2018-05-24 21:19:34,705][org.spark_project.jetty.server.Server]jetty-9.3.z-SNAPSHOT
[INFO][2018-05-24 21:19:34,716][org.spark_project.jetty.server.Server]Started @2647ms
[WARN][2018-05-24 21:19:34,734][org.apache.spark.util.Utils]Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[INFO][2018-05-24 21:19:34,739][org.spark_project.jetty.server.AbstractConnector]Started ServerConnector@1643d68f{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}
[INFO][2018-05-24 21:19:34,739][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4041.
[INFO][2018-05-24 21:19:34,761][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2a39fe6a{/jobs,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:19:34,762][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@649725e3{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:19:34,762][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4c168660{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:19:34,764][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4eed46ee{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:19:34,765][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4fad94a7{/stages,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:19:34,765][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6326d182{/stages/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:19:34,766][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@716a7124{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:19:34,767][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5cc126dc{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:19:34,768][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@72bd06ca{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:19:34,768][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5dbe30be{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:19:34,768][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@55795845{/storage,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:19:34,769][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@119f1f2a{/storage/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:19:34,769][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5b970f7{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:19:34,770][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@165b8a71{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:19:34,771][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2f058b8a{/environment,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:19:34,771][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3f2ef586{/environment/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:19:34,772][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@76c7beb3{/executors,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:19:34,773][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2cf92cc7{/executors/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:19:34,774][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7b139eab{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:19:34,774][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@611df6e3{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:19:34,781][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6273c5a4{/static,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:19:34,782][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3e2943ab{/,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:19:34,784][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4a9f80d3{/api,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:19:34,784][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@191ae03f{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:19:34,785][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@53ab0286{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:19:34,792][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://192.168.0.102:4041
[INFO][2018-05-24 21:19:34,902][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
[INFO][2018-05-24 21:19:34,947][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 51188.
[INFO][2018-05-24 21:19:34,947][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 192.168.0.102:51188
[INFO][2018-05-24 21:19:34,949][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2018-05-24 21:19:34,950][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 192.168.0.102, 51188, None)
[INFO][2018-05-24 21:19:34,954][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 192.168.0.102:51188 with 912.3 MB RAM, BlockManagerId(driver, 192.168.0.102, 51188, None)
[INFO][2018-05-24 21:19:34,968][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 192.168.0.102, 51188, None)
[INFO][2018-05-24 21:19:34,970][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 192.168.0.102, 51188, None)
[INFO][2018-05-24 21:19:35,083][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527167965000 ms
[INFO][2018-05-24 21:19:35,083][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527167965000 ms.0 from job set of time 1527167965000 ms
[INFO][2018-05-24 21:19:35,091][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:19:35,092][org.apache.spark.scheduler.DAGScheduler]Got job 5 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:19:35,092][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 5 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:19:35,092][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:19:35,092][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:19:35,092][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 5 (MapPartitionsRDD[11] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:19:35,095][org.apache.spark.storage.memory.MemoryStore]Block broadcast_5 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 21:19:35,096][org.apache.spark.storage.memory.MemoryStore]Block broadcast_5_piece0 stored as bytes in memory (estimated size 1868.0 B, free 912.3 MB)
[INFO][2018-05-24 21:19:35,097][org.apache.spark.storage.BlockManagerInfo]Added broadcast_5_piece0 in memory on 192.168.0.102:51153 (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:19:35,098][org.apache.spark.SparkContext]Created broadcast 5 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:19:35,098][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[11] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:19:35,098][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 5.0 with 1 tasks
[INFO][2018-05-24 21:19:35,099][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 5.0 (TID 5, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:19:35,100][org.apache.spark.executor.Executor]Running task 0.0 in stage 5.0 (TID 5)
[INFO][2018-05-24 21:19:35,102][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12204 is the same as ending offset skipping seven 0
[INFO][2018-05-24 21:19:35,103][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:19:35,104][org.apache.spark.executor.Executor]Finished task 0.0 in stage 5.0 (TID 5). 708 bytes result sent to driver
[INFO][2018-05-24 21:19:35,104][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 5.0 (TID 5) in 5 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:19:35,105][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 5.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:19:35,105][org.apache.spark.scheduler.DAGScheduler]ResultStage 5 (foreachPartition at ReceiveKafkaData.scala:66) finished in 0.006 s
[INFO][2018-05-24 21:19:35,105][org.apache.spark.scheduler.DAGScheduler]Job 5 finished: foreachPartition at ReceiveKafkaData.scala:66, took 0.014538 s
[INFO][2018-05-24 21:19:35,106][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527167965000 ms.0 from job set of time 1527167965000 ms
[INFO][2018-05-24 21:19:35,106][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 10.106 s for time 1527167965000 ms (execution: 0.023 s)
[INFO][2018-05-24 21:19:35,133][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527167970000 ms
[INFO][2018-05-24 21:19:35,134][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527167970000 ms.0 from job set of time 1527167970000 ms
[INFO][2018-05-24 21:19:35,141][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:19:35,141][org.apache.spark.scheduler.DAGScheduler]Got job 6 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:19:35,141][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 6 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:19:35,142][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:19:35,142][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:19:35,142][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 6 (MapPartitionsRDD[13] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:19:35,144][org.apache.spark.storage.memory.MemoryStore]Block broadcast_6 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 21:19:35,147][org.apache.spark.storage.memory.MemoryStore]Block broadcast_6_piece0 stored as bytes in memory (estimated size 1868.0 B, free 912.3 MB)
[INFO][2018-05-24 21:19:35,149][org.apache.spark.storage.BlockManagerInfo]Added broadcast_6_piece0 in memory on 192.168.0.102:51153 (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:19:35,149][org.apache.spark.SparkContext]Created broadcast 6 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:19:35,150][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[13] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:19:35,150][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 6.0 with 1 tasks
[INFO][2018-05-24 21:19:35,151][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 6.0 (TID 6, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:19:35,151][org.apache.spark.executor.Executor]Running task 0.0 in stage 6.0 (TID 6)
[INFO][2018-05-24 21:19:35,154][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12204 is the same as ending offset skipping seven 0
[INFO][2018-05-24 21:19:35,154][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:19:35,155][org.apache.spark.executor.Executor]Finished task 0.0 in stage 6.0 (TID 6). 708 bytes result sent to driver
[INFO][2018-05-24 21:19:35,156][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 6.0 (TID 6) in 5 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:19:35,156][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 6.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:19:35,157][org.apache.spark.scheduler.DAGScheduler]ResultStage 6 (foreachPartition at ReceiveKafkaData.scala:66) finished in 0.006 s
[INFO][2018-05-24 21:19:35,157][org.apache.spark.scheduler.DAGScheduler]Job 6 finished: foreachPartition at ReceiveKafkaData.scala:66, took 0.016298 s
[INFO][2018-05-24 21:19:35,158][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527167970000 ms.0 from job set of time 1527167970000 ms
[INFO][2018-05-24 21:19:35,158][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 5.158 s for time 1527167970000 ms (execution: 0.024 s)
[INFO][2018-05-24 21:19:35,283][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6c2f1700{/metrics/json,null,AVAILABLE,@Spark}
[INFO][2018-05-24 21:19:35,895][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0 stored as values in memory (estimated size 228.1 KB, free 912.1 MB)
[INFO][2018-05-24 21:19:36,014][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0_piece0 stored as bytes in memory (estimated size 22.1 KB, free 912.1 MB)
[INFO][2018-05-24 21:19:36,017][org.apache.spark.storage.BlockManagerInfo]Added broadcast_0_piece0 in memory on 192.168.0.102:51188 (size: 22.1 KB, free: 912.3 MB)
[INFO][2018-05-24 21:19:36,023][org.apache.spark.SparkContext]Created broadcast 0 from textFile at SimulationKafkaSendOutData.scala:25
[INFO][2018-05-24 21:19:40,214][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527167975000 ms
[INFO][2018-05-24 21:19:40,218][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527167975000 ms.0 from job set of time 1527167975000 ms
[INFO][2018-05-24 21:19:40,221][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 9 from persistence list
[INFO][2018-05-24 21:19:40,231][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:19:40,232][org.apache.spark.storage.BlockManager]Removing RDD 9
[INFO][2018-05-24 21:19:40,235][org.apache.spark.scheduler.DAGScheduler]Got job 7 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:19:40,236][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 7 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:19:40,236][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:19:40,236][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:19:40,237][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 7 (MapPartitionsRDD[15] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:19:40,239][org.apache.spark.storage.memory.MemoryStore]Block broadcast_7 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 21:19:40,241][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 8 from persistence list
[INFO][2018-05-24 21:19:40,241][org.apache.spark.storage.memory.MemoryStore]Block broadcast_7_piece0 stored as bytes in memory (estimated size 1868.0 B, free 912.3 MB)
[INFO][2018-05-24 21:19:40,242][org.apache.spark.storage.BlockManager]Removing RDD 8
[INFO][2018-05-24 21:19:40,243][org.apache.spark.storage.BlockManagerInfo]Added broadcast_7_piece0 in memory on 192.168.0.102:51153 (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:19:40,245][org.apache.spark.SparkContext]Created broadcast 7 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:19:40,245][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[15] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:19:40,245][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 7.0 with 1 tasks
[INFO][2018-05-24 21:19:40,247][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 7.0 (TID 7, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:19:40,251][org.apache.spark.executor.Executor]Running task 0.0 in stage 7.0 (TID 7)
[INFO][2018-05-24 21:19:40,254][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12204 is the same as ending offset skipping seven 0
[INFO][2018-05-24 21:19:40,254][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:19:40,257][org.apache.spark.executor.Executor]Finished task 0.0 in stage 7.0 (TID 7). 708 bytes result sent to driver
[INFO][2018-05-24 21:19:40,258][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 7.0 (TID 7) in 11 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:19:40,258][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 7.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:19:40,258][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 21:19:40,258][org.apache.spark.scheduler.DAGScheduler]ResultStage 7 (foreachPartition at ReceiveKafkaData.scala:66) finished in 0.012 s
[INFO][2018-05-24 21:19:40,259][org.apache.spark.scheduler.DAGScheduler]Job 7 finished: foreachPartition at ReceiveKafkaData.scala:66, took 0.027624 s
[INFO][2018-05-24 21:19:40,260][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527167975000 ms.0 from job set of time 1527167975000 ms
[INFO][2018-05-24 21:19:40,260][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 5.260 s for time 1527167975000 ms (execution: 0.043 s)
[INFO][2018-05-24 21:19:40,259][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527167955000 ms
[INFO][2018-05-24 21:19:40,260][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 11 from persistence list
[INFO][2018-05-24 21:19:40,260][org.apache.spark.storage.BlockManager]Removing RDD 11
[INFO][2018-05-24 21:19:40,261][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 10 from persistence list
[INFO][2018-05-24 21:19:40,261][org.apache.spark.storage.BlockManager]Removing RDD 10
[INFO][2018-05-24 21:19:40,261][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 21:19:40,261][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527167960000 ms
[WARN][2018-05-24 21:19:41,622][org.apache.hadoop.util.NativeCodeLoader]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO][2018-05-24 21:19:41,767][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 1
[INFO][2018-05-24 21:19:41,863][org.apache.spark.SparkContext]Starting job: collect at SimulationKafkaSendOutData.scala:25
[INFO][2018-05-24 21:19:41,873][org.apache.spark.scheduler.DAGScheduler]Got job 0 (collect at SimulationKafkaSendOutData.scala:25) with 2 output partitions
[INFO][2018-05-24 21:19:41,874][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 0 (collect at SimulationKafkaSendOutData.scala:25)
[INFO][2018-05-24 21:19:41,874][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:19:41,876][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:19:41,883][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 0 (hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/* MapPartitionsRDD[1] at textFile at SimulationKafkaSendOutData.scala:25), which has no missing parents
[INFO][2018-05-24 21:19:41,898][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1 stored as values in memory (estimated size 3.3 KB, free 912.1 MB)
[INFO][2018-05-24 21:19:41,900][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1_piece0 stored as bytes in memory (estimated size 1969.0 B, free 912.1 MB)
[INFO][2018-05-24 21:19:41,900][org.apache.spark.storage.BlockManagerInfo]Added broadcast_1_piece0 in memory on 192.168.0.102:51188 (size: 1969.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:19:41,901][org.apache.spark.SparkContext]Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:19:41,916][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ResultStage 0 (hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/* MapPartitionsRDD[1] at textFile at SimulationKafkaSendOutData.scala:25) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2018-05-24 21:19:41,916][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 0.0 with 2 tasks
[INFO][2018-05-24 21:19:41,970][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 4883 bytes)
[INFO][2018-05-24 21:19:41,973][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 4883 bytes)
[INFO][2018-05-24 21:19:41,982][org.apache.spark.executor.Executor]Running task 1.0 in stage 0.0 (TID 1)
[INFO][2018-05-24 21:19:41,982][org.apache.spark.executor.Executor]Running task 0.0 in stage 0.0 (TID 0)
[INFO][2018-05-24 21:19:42,097][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/part-00000:0+10717435
[INFO][2018-05-24 21:19:42,097][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://vm-xaj-bigdata-da-d01:8020/yst/vem/sales/order/part-00000:10717435+10717436
[INFO][2018-05-24 21:19:45,325][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527167980000 ms
[INFO][2018-05-24 21:19:45,325][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 13 from persistence list
[INFO][2018-05-24 21:19:45,325][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527167980000 ms.0 from job set of time 1527167980000 ms
[INFO][2018-05-24 21:19:45,325][org.apache.spark.storage.BlockManager]Removing RDD 13
[INFO][2018-05-24 21:19:45,326][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 12 from persistence list
[INFO][2018-05-24 21:19:45,327][org.apache.spark.storage.BlockManager]Removing RDD 12
[INFO][2018-05-24 21:19:45,327][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 21:19:45,327][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527167965000 ms
[INFO][2018-05-24 21:19:45,335][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:19:45,336][org.apache.spark.scheduler.DAGScheduler]Got job 8 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:19:45,336][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 8 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:19:45,336][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:19:45,336][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:19:45,337][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 8 (MapPartitionsRDD[17] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:19:45,341][org.apache.spark.storage.memory.MemoryStore]Block broadcast_8 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 21:19:45,344][org.apache.spark.storage.memory.MemoryStore]Block broadcast_8_piece0 stored as bytes in memory (estimated size 1868.0 B, free 912.3 MB)
[INFO][2018-05-24 21:19:45,346][org.apache.spark.storage.BlockManagerInfo]Added broadcast_8_piece0 in memory on 192.168.0.102:51153 (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:19:45,347][org.apache.spark.SparkContext]Created broadcast 8 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:19:45,348][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[17] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:19:45,349][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 8.0 with 1 tasks
[INFO][2018-05-24 21:19:45,350][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 8.0 (TID 8, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:19:45,351][org.apache.spark.executor.Executor]Running task 0.0 in stage 8.0 (TID 8)
[INFO][2018-05-24 21:19:45,356][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12204 is the same as ending offset skipping seven 0
[INFO][2018-05-24 21:19:45,356][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:19:45,357][org.apache.spark.executor.Executor]Finished task 0.0 in stage 8.0 (TID 8). 751 bytes result sent to driver
[INFO][2018-05-24 21:19:45,358][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 8.0 (TID 8) in 8 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:19:45,358][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 8.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:19:45,364][org.apache.spark.scheduler.DAGScheduler]ResultStage 8 (foreachPartition at ReceiveKafkaData.scala:66) finished in 0.015 s
[INFO][2018-05-24 21:19:45,365][org.apache.spark.scheduler.DAGScheduler]Job 8 finished: foreachPartition at ReceiveKafkaData.scala:66, took 0.029541 s
[INFO][2018-05-24 21:19:45,365][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527167980000 ms.0 from job set of time 1527167980000 ms
[INFO][2018-05-24 21:19:45,366][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 5.365 s for time 1527167980000 ms (execution: 0.040 s)
[INFO][2018-05-24 21:19:45,383][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527167985000 ms
[INFO][2018-05-24 21:19:45,383][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 15 from persistence list
[INFO][2018-05-24 21:19:45,384][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527167985000 ms.0 from job set of time 1527167985000 ms
[INFO][2018-05-24 21:19:45,384][org.apache.spark.storage.BlockManager]Removing RDD 15
[INFO][2018-05-24 21:19:45,386][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 14 from persistence list
[INFO][2018-05-24 21:19:45,386][org.apache.spark.storage.BlockManager]Removing RDD 14
[INFO][2018-05-24 21:19:45,387][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 21:19:45,388][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527167970000 ms
[INFO][2018-05-24 21:19:45,396][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:19:45,398][org.apache.spark.scheduler.DAGScheduler]Got job 9 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:19:45,398][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 9 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:19:45,398][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:19:45,398][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:19:45,399][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 9 (MapPartitionsRDD[19] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:19:45,401][org.apache.spark.storage.memory.MemoryStore]Block broadcast_9 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 21:19:45,407][org.apache.spark.storage.memory.MemoryStore]Block broadcast_9_piece0 stored as bytes in memory (estimated size 1868.0 B, free 912.3 MB)
[INFO][2018-05-24 21:19:45,408][org.apache.spark.storage.BlockManagerInfo]Added broadcast_9_piece0 in memory on 192.168.0.102:51153 (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:19:45,409][org.apache.spark.SparkContext]Created broadcast 9 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:19:45,410][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[19] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:19:45,411][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 9.0 with 1 tasks
[INFO][2018-05-24 21:19:45,412][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 9.0 (TID 9, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:19:45,413][org.apache.spark.executor.Executor]Running task 0.0 in stage 9.0 (TID 9)
[INFO][2018-05-24 21:19:45,417][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12204 is the same as ending offset skipping seven 0
[INFO][2018-05-24 21:19:45,418][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:19:45,419][org.apache.spark.executor.Executor]Finished task 0.0 in stage 9.0 (TID 9). 708 bytes result sent to driver
[INFO][2018-05-24 21:19:45,420][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 9.0 (TID 9) in 8 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:19:45,420][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 9.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:19:45,423][org.apache.spark.scheduler.DAGScheduler]ResultStage 9 (foreachPartition at ReceiveKafkaData.scala:66) finished in 0.011 s
[INFO][2018-05-24 21:19:45,424][org.apache.spark.scheduler.DAGScheduler]Job 9 finished: foreachPartition at ReceiveKafkaData.scala:66, took 0.027195 s
[INFO][2018-05-24 21:19:45,424][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527167985000 ms.0 from job set of time 1527167985000 ms
[INFO][2018-05-24 21:19:45,425][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.424 s for time 1527167985000 ms (execution: 0.041 s)
[INFO][2018-05-24 21:19:45,425][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 17 from persistence list
[INFO][2018-05-24 21:19:45,425][org.apache.spark.storage.BlockManager]Removing RDD 17
[INFO][2018-05-24 21:19:45,426][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 16 from persistence list
[INFO][2018-05-24 21:19:45,426][org.apache.spark.storage.BlockManager]Removing RDD 16
[INFO][2018-05-24 21:19:45,426][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 21:19:45,426][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527167975000 ms
[INFO][2018-05-24 21:19:50,067][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527167990000 ms
[INFO][2018-05-24 21:19:50,067][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527167990000 ms.0 from job set of time 1527167990000 ms
[INFO][2018-05-24 21:19:50,076][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:19:50,076][org.apache.spark.scheduler.DAGScheduler]Got job 10 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:19:50,077][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 10 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:19:50,077][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:19:50,077][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:19:50,077][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 10 (MapPartitionsRDD[21] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:19:50,080][org.apache.spark.storage.memory.MemoryStore]Block broadcast_10 stored as values in memory (estimated size 3.3 KB, free 912.2 MB)
[INFO][2018-05-24 21:19:50,085][org.apache.spark.storage.memory.MemoryStore]Block broadcast_10_piece0 stored as bytes in memory (estimated size 1868.0 B, free 912.2 MB)
[INFO][2018-05-24 21:19:50,086][org.apache.spark.storage.BlockManagerInfo]Added broadcast_10_piece0 in memory on 192.168.0.102:51153 (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:19:50,086][org.apache.spark.SparkContext]Created broadcast 10 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:19:50,087][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[21] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:19:50,087][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 10.0 with 1 tasks
[INFO][2018-05-24 21:19:50,088][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 10.0 (TID 10, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:19:50,089][org.apache.spark.executor.Executor]Running task 0.0 in stage 10.0 (TID 10)
[INFO][2018-05-24 21:19:50,091][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12204 is the same as ending offset skipping seven 0
[INFO][2018-05-24 21:19:50,091][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:19:50,093][org.apache.spark.executor.Executor]Finished task 0.0 in stage 10.0 (TID 10). 708 bytes result sent to driver
[INFO][2018-05-24 21:19:50,099][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 10.0 (TID 10) in 11 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:19:50,099][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 10.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:19:50,100][org.apache.spark.scheduler.DAGScheduler]ResultStage 10 (foreachPartition at ReceiveKafkaData.scala:66) finished in 0.013 s
[INFO][2018-05-24 21:19:50,101][org.apache.spark.scheduler.DAGScheduler]Job 10 finished: foreachPartition at ReceiveKafkaData.scala:66, took 0.024666 s
[INFO][2018-05-24 21:19:50,104][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527167990000 ms.0 from job set of time 1527167990000 ms
[INFO][2018-05-24 21:19:50,105][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 19 from persistence list
[INFO][2018-05-24 21:19:50,105][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.104 s for time 1527167990000 ms (execution: 0.037 s)
[INFO][2018-05-24 21:19:50,105][org.apache.spark.storage.BlockManager]Removing RDD 19
[INFO][2018-05-24 21:19:50,105][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 18 from persistence list
[INFO][2018-05-24 21:19:50,108][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 21:19:50,109][org.apache.spark.storage.BlockManager]Removing RDD 18
[INFO][2018-05-24 21:19:50,109][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527167980000 ms
[INFO][2018-05-24 21:19:51,582][org.apache.spark.storage.memory.MemoryStore]Block taskresult_1 stored as bytes in memory (estimated size 10.4 MB, free 901.7 MB)
[INFO][2018-05-24 21:19:51,583][org.apache.spark.storage.BlockManagerInfo]Added taskresult_1 in memory on 192.168.0.102:51188 (size: 10.4 MB, free: 901.9 MB)
[INFO][2018-05-24 21:19:51,583][org.apache.spark.executor.Executor]Finished task 1.0 in stage 0.0 (TID 1). 10855899 bytes result sent via BlockManager)
[INFO][2018-05-24 21:19:51,624][org.apache.spark.network.client.TransportClientFactory]Successfully created connection to /192.168.0.102:51188 after 25 ms (0 ms spent in bootstraps)
[INFO][2018-05-24 21:19:51,951][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 0.0 (TID 1) in 9976 ms on localhost (executor driver) (1/2)
[INFO][2018-05-24 21:19:51,954][org.apache.spark.storage.BlockManagerInfo]Removed taskresult_1 on 192.168.0.102:51188 in memory (size: 10.4 MB, free: 912.3 MB)
[INFO][2018-05-24 21:19:53,606][org.apache.spark.storage.memory.MemoryStore]Block taskresult_0 stored as bytes in memory (estimated size 10.4 MB, free 901.7 MB)
[INFO][2018-05-24 21:19:53,607][org.apache.spark.storage.BlockManagerInfo]Added taskresult_0 in memory on 192.168.0.102:51188 (size: 10.4 MB, free: 901.9 MB)
[INFO][2018-05-24 21:19:53,607][org.apache.spark.executor.Executor]Finished task 0.0 in stage 0.0 (TID 0). 10856349 bytes result sent via BlockManager)
[INFO][2018-05-24 21:19:53,684][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 0.0 (TID 0) in 11738 ms on localhost (executor driver) (2/2)
[INFO][2018-05-24 21:19:53,685][org.apache.spark.storage.BlockManagerInfo]Removed taskresult_0 on 192.168.0.102:51188 in memory (size: 10.4 MB, free: 912.3 MB)
[INFO][2018-05-24 21:19:53,687][org.apache.spark.scheduler.DAGScheduler]ResultStage 0 (collect at SimulationKafkaSendOutData.scala:25) finished in 11.752 s
[INFO][2018-05-24 21:19:53,687][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:19:53,691][org.apache.spark.scheduler.DAGScheduler]Job 0 finished: collect at SimulationKafkaSendOutData.scala:25, took 11.828028 s
[INFO][2018-05-24 21:19:53,799][org.spark_project.jetty.server.AbstractConnector]Stopped Spark@1643d68f{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}
[INFO][2018-05-24 21:19:53,801][org.apache.spark.ui.SparkUI]Stopped Spark web UI at http://192.168.0.102:4041
[INFO][2018-05-24 21:19:53,809][org.apache.spark.MapOutputTrackerMasterEndpoint]MapOutputTrackerMasterEndpoint stopped!
[INFO][2018-05-24 21:19:53,821][org.apache.spark.storage.memory.MemoryStore]MemoryStore cleared
[INFO][2018-05-24 21:19:53,822][org.apache.spark.storage.BlockManager]BlockManager stopped
[INFO][2018-05-24 21:19:53,822][org.apache.spark.storage.BlockManagerMaster]BlockManagerMaster stopped
[INFO][2018-05-24 21:19:53,824][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint]OutputCommitCoordinator stopped!
[INFO][2018-05-24 21:19:53,828][org.apache.spark.SparkContext]Successfully stopped SparkContext
[INFO][2018-05-24 21:19:53,842][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	compression.type = none
	metric.reporters = []
	metadata.max.age.ms = 300000
	metadata.fetch.timeout.ms = 60000
	acks = all
	batch.size = 16384
	reconnect.backoff.ms = 10
	bootstrap.servers = [vm-xaj-bigdata-da-d01:9092, vm-xaj-bigdata-da-d02:9092, vm-xaj-bigdata-da-d03:9092, vm-xaj-bigdata-da-d04:9092, vm-xaj-bigdata-da-d05:9092, vm-xaj-bigdata-da-d06:9092, vm-xaj-bigdata-da-d07:9092]
	receive.buffer.bytes = 32768
	retry.backoff.ms = 100
	buffer.memory = 33554432
	timeout.ms = 30000
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	retries = 0
	max.request.size = 1048576
	block.on.buffer.full = true
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
	metrics.sample.window.ms = 30000
	send.buffer.bytes = 131072
	max.in.flight.requests.per.connection = 5
	metrics.num.samples = 2
	linger.ms = 1
	client.id = 

[INFO][2018-05-24 21:20:00,082][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527167995000 ms
[INFO][2018-05-24 21:20:00,082][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527167995000 ms.0 from job set of time 1527167995000 ms
[INFO][2018-05-24 21:20:00,101][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:20:00,102][org.apache.spark.scheduler.DAGScheduler]Got job 11 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:20:00,102][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 11 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:20:00,102][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:20:00,102][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:20:00,102][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 11 (MapPartitionsRDD[23] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:20:00,104][org.apache.spark.storage.memory.MemoryStore]Block broadcast_11 stored as values in memory (estimated size 3.3 KB, free 912.2 MB)
[INFO][2018-05-24 21:20:00,117][org.apache.spark.storage.memory.MemoryStore]Block broadcast_11_piece0 stored as bytes in memory (estimated size 1868.0 B, free 912.2 MB)
[INFO][2018-05-24 21:20:00,118][org.apache.spark.storage.BlockManagerInfo]Added broadcast_11_piece0 in memory on 192.168.0.102:51153 (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:20:00,118][org.apache.spark.SparkContext]Created broadcast 11 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:20:00,119][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[23] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:20:00,119][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 11.0 with 1 tasks
[INFO][2018-05-24 21:20:00,120][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 11.0 (TID 11, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:20:00,120][org.apache.spark.executor.Executor]Running task 0.0 in stage 11.0 (TID 11)
[INFO][2018-05-24 21:20:00,122][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12204 is the same as ending offset skipping seven 0
[INFO][2018-05-24 21:20:00,122][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:20:00,123][org.apache.spark.executor.Executor]Finished task 0.0 in stage 11.0 (TID 11). 708 bytes result sent to driver
[INFO][2018-05-24 21:20:00,124][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 11.0 (TID 11) in 5 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:20:00,124][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 11.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:20:00,125][org.apache.spark.scheduler.DAGScheduler]ResultStage 11 (foreachPartition at ReceiveKafkaData.scala:66) finished in 0.006 s
[INFO][2018-05-24 21:20:00,126][org.apache.spark.scheduler.DAGScheduler]Job 11 finished: foreachPartition at ReceiveKafkaData.scala:66, took 0.024518 s
[INFO][2018-05-24 21:20:00,126][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527167995000 ms.0 from job set of time 1527167995000 ms
[INFO][2018-05-24 21:20:00,127][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 5.126 s for time 1527167995000 ms (execution: 0.044 s)
[INFO][2018-05-24 21:20:00,130][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_8_piece0 on 192.168.0.102:51153 in memory (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:20:00,132][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_5_piece0 on 192.168.0.102:51153 in memory (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:20:00,136][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_4_piece0 on 192.168.0.102:51153 in memory (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:20:00,140][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_10_piece0 on 192.168.0.102:51153 in memory (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:20:00,143][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_2_piece0 on 192.168.0.102:51153 in memory (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:20:00,146][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_1_piece0 on 192.168.0.102:51153 in memory (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:20:00,152][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_3_piece0 on 192.168.0.102:51153 in memory (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:20:00,157][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_6_piece0 on 192.168.0.102:51153 in memory (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:20:00,159][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_9_piece0 on 192.168.0.102:51153 in memory (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:20:00,163][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_7_piece0 on 192.168.0.102:51153 in memory (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:20:10,159][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527168000000 ms
[INFO][2018-05-24 21:20:10,161][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 21 from persistence list
[INFO][2018-05-24 21:20:10,162][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527168000000 ms.0 from job set of time 1527168000000 ms
[INFO][2018-05-24 21:20:10,163][org.apache.spark.storage.BlockManager]Removing RDD 21
[INFO][2018-05-24 21:20:10,163][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 20 from persistence list
[INFO][2018-05-24 21:20:10,166][org.apache.spark.storage.BlockManager]Removing RDD 20
[INFO][2018-05-24 21:20:10,173][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 21:20:10,173][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527167985000 ms
[INFO][2018-05-24 21:20:10,182][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:20:10,183][org.apache.spark.scheduler.DAGScheduler]Got job 12 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:20:10,184][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 12 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:20:10,184][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:20:10,184][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:20:10,184][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 12 (MapPartitionsRDD[25] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:20:10,192][org.apache.spark.storage.memory.MemoryStore]Block broadcast_12 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 21:20:10,194][org.apache.spark.storage.memory.MemoryStore]Block broadcast_12_piece0 stored as bytes in memory (estimated size 1868.0 B, free 912.3 MB)
[INFO][2018-05-24 21:20:10,195][org.apache.spark.storage.BlockManagerInfo]Added broadcast_12_piece0 in memory on 192.168.0.102:51153 (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:20:10,195][org.apache.spark.SparkContext]Created broadcast 12 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:20:10,199][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[25] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:20:10,199][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 12.0 with 1 tasks
[INFO][2018-05-24 21:20:10,200][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 12.0 (TID 12, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:20:10,201][org.apache.spark.executor.Executor]Running task 0.0 in stage 12.0 (TID 12)
[INFO][2018-05-24 21:20:10,206][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12204 is the same as ending offset skipping seven 0
[INFO][2018-05-24 21:20:10,206][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:20:10,207][org.apache.spark.executor.Executor]Finished task 0.0 in stage 12.0 (TID 12). 708 bytes result sent to driver
[INFO][2018-05-24 21:20:10,208][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 12.0 (TID 12) in 8 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:20:10,208][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 12.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:20:10,209][org.apache.spark.scheduler.DAGScheduler]ResultStage 12 (foreachPartition at ReceiveKafkaData.scala:66) finished in 0.009 s
[INFO][2018-05-24 21:20:10,209][org.apache.spark.scheduler.DAGScheduler]Job 12 finished: foreachPartition at ReceiveKafkaData.scala:66, took 0.026520 s
[INFO][2018-05-24 21:20:10,210][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527168000000 ms.0 from job set of time 1527168000000 ms
[INFO][2018-05-24 21:20:10,210][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 10.210 s for time 1527168000000 ms (execution: 0.049 s)
[INFO][2018-05-24 21:20:10,241][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527168005000 ms
[INFO][2018-05-24 21:20:10,241][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527168005000 ms.0 from job set of time 1527168005000 ms
[INFO][2018-05-24 21:20:10,247][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:20:10,248][org.apache.spark.scheduler.DAGScheduler]Got job 13 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:20:10,249][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 13 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:20:10,249][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:20:10,249][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:20:10,249][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 13 (MapPartitionsRDD[27] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:20:10,251][org.apache.spark.storage.memory.MemoryStore]Block broadcast_13 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 21:20:10,252][org.apache.spark.storage.memory.MemoryStore]Block broadcast_13_piece0 stored as bytes in memory (estimated size 1868.0 B, free 912.3 MB)
[INFO][2018-05-24 21:20:10,253][org.apache.spark.storage.BlockManagerInfo]Added broadcast_13_piece0 in memory on 192.168.0.102:51153 (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:20:10,253][org.apache.spark.SparkContext]Created broadcast 13 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:20:10,254][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[27] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:20:10,254][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 13.0 with 1 tasks
[INFO][2018-05-24 21:20:10,255][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 13.0 (TID 13, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:20:10,255][org.apache.spark.executor.Executor]Running task 0.0 in stage 13.0 (TID 13)
[INFO][2018-05-24 21:20:10,256][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12204 is the same as ending offset skipping seven 0
[INFO][2018-05-24 21:20:10,257][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:20:10,258][org.apache.spark.executor.Executor]Finished task 0.0 in stage 13.0 (TID 13). 665 bytes result sent to driver
[INFO][2018-05-24 21:20:10,258][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 13.0 (TID 13) in 3 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:20:10,258][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 13.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:20:10,259][org.apache.spark.scheduler.DAGScheduler]ResultStage 13 (foreachPartition at ReceiveKafkaData.scala:66) finished in 0.005 s
[INFO][2018-05-24 21:20:10,259][org.apache.spark.scheduler.DAGScheduler]Job 13 finished: foreachPartition at ReceiveKafkaData.scala:66, took 0.011896 s
[INFO][2018-05-24 21:20:10,260][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527168005000 ms.0 from job set of time 1527168005000 ms
[INFO][2018-05-24 21:20:10,260][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 5.260 s for time 1527168005000 ms (execution: 0.019 s)
[INFO][2018-05-24 21:20:10,296][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527168010000 ms
[INFO][2018-05-24 21:20:10,297][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 23 from persistence list
[INFO][2018-05-24 21:20:10,297][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527168010000 ms.0 from job set of time 1527168010000 ms
[INFO][2018-05-24 21:20:10,297][org.apache.spark.storage.BlockManager]Removing RDD 23
[INFO][2018-05-24 21:20:10,297][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 22 from persistence list
[INFO][2018-05-24 21:20:10,298][org.apache.spark.storage.BlockManager]Removing RDD 22
[INFO][2018-05-24 21:20:10,298][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 21:20:10,298][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527167990000 ms
[INFO][2018-05-24 21:20:10,298][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 25 from persistence list
[INFO][2018-05-24 21:20:10,299][org.apache.spark.storage.BlockManager]Removing RDD 25
[INFO][2018-05-24 21:20:10,299][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 24 from persistence list
[INFO][2018-05-24 21:20:10,300][org.apache.spark.storage.BlockManager]Removing RDD 24
[INFO][2018-05-24 21:20:10,300][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 21:20:10,300][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527167995000 ms
[INFO][2018-05-24 21:20:10,302][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:20:10,303][org.apache.spark.scheduler.DAGScheduler]Got job 14 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:20:10,303][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 14 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:20:10,303][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:20:10,303][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:20:10,303][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 14 (MapPartitionsRDD[29] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:20:10,305][org.apache.spark.storage.memory.MemoryStore]Block broadcast_14 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 21:20:10,307][org.apache.spark.storage.memory.MemoryStore]Block broadcast_14_piece0 stored as bytes in memory (estimated size 1868.0 B, free 912.3 MB)
[INFO][2018-05-24 21:20:10,307][org.apache.spark.storage.BlockManagerInfo]Added broadcast_14_piece0 in memory on 192.168.0.102:51153 (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:20:10,308][org.apache.spark.SparkContext]Created broadcast 14 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:20:10,309][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[29] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:20:10,309][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 14.0 with 1 tasks
[INFO][2018-05-24 21:20:10,309][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 14.0 (TID 14, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:20:10,310][org.apache.spark.executor.Executor]Running task 0.0 in stage 14.0 (TID 14)
[INFO][2018-05-24 21:20:10,311][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12204 is the same as ending offset skipping seven 0
[INFO][2018-05-24 21:20:10,311][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:20:10,312][org.apache.spark.executor.Executor]Finished task 0.0 in stage 14.0 (TID 14). 708 bytes result sent to driver
[INFO][2018-05-24 21:20:10,313][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 14.0 (TID 14) in 4 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:20:10,313][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 14.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:20:10,314][org.apache.spark.scheduler.DAGScheduler]ResultStage 14 (foreachPartition at ReceiveKafkaData.scala:66) finished in 0.004 s
[INFO][2018-05-24 21:20:10,314][org.apache.spark.scheduler.DAGScheduler]Job 14 finished: foreachPartition at ReceiveKafkaData.scala:66, took 0.011465 s
[INFO][2018-05-24 21:20:10,314][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527168010000 ms.0 from job set of time 1527168010000 ms
[INFO][2018-05-24 21:20:10,314][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 27 from persistence list
[INFO][2018-05-24 21:20:10,314][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.314 s for time 1527168010000 ms (execution: 0.017 s)
[INFO][2018-05-24 21:20:10,315][org.apache.spark.storage.BlockManager]Removing RDD 27
[INFO][2018-05-24 21:20:10,315][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 26 from persistence list
[INFO][2018-05-24 21:20:10,315][org.apache.spark.storage.BlockManager]Removing RDD 26
[INFO][2018-05-24 21:20:10,315][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 21:20:10,315][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527168000000 ms
[INFO][2018-05-24 21:20:15,055][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527168015000 ms
[INFO][2018-05-24 21:20:15,056][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527168015000 ms.0 from job set of time 1527168015000 ms
[INFO][2018-05-24 21:20:15,063][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:20:15,064][org.apache.spark.scheduler.DAGScheduler]Got job 15 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:20:15,064][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 15 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:20:15,064][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:20:15,065][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:20:15,065][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 15 (MapPartitionsRDD[31] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:20:15,067][org.apache.spark.storage.memory.MemoryStore]Block broadcast_15 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 21:20:15,068][org.apache.spark.storage.memory.MemoryStore]Block broadcast_15_piece0 stored as bytes in memory (estimated size 1868.0 B, free 912.3 MB)
[INFO][2018-05-24 21:20:15,069][org.apache.spark.storage.BlockManagerInfo]Added broadcast_15_piece0 in memory on 192.168.0.102:51153 (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:20:15,069][org.apache.spark.SparkContext]Created broadcast 15 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:20:15,070][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 15 (MapPartitionsRDD[31] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:20:15,070][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 15.0 with 1 tasks
[INFO][2018-05-24 21:20:15,071][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 15.0 (TID 15, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:20:15,072][org.apache.spark.executor.Executor]Running task 0.0 in stage 15.0 (TID 15)
[INFO][2018-05-24 21:20:15,074][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12204 is the same as ending offset skipping seven 0
[INFO][2018-05-24 21:20:15,075][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:20:15,076][org.apache.spark.executor.Executor]Finished task 0.0 in stage 15.0 (TID 15). 708 bytes result sent to driver
[INFO][2018-05-24 21:20:15,077][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 15.0 (TID 15) in 6 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:20:15,077][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 15.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:20:15,077][org.apache.spark.scheduler.DAGScheduler]ResultStage 15 (foreachPartition at ReceiveKafkaData.scala:66) finished in 0.006 s
[INFO][2018-05-24 21:20:15,078][org.apache.spark.scheduler.DAGScheduler]Job 15 finished: foreachPartition at ReceiveKafkaData.scala:66, took 0.014816 s
[INFO][2018-05-24 21:20:15,078][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527168015000 ms.0 from job set of time 1527168015000 ms
[INFO][2018-05-24 21:20:15,079][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 29 from persistence list
[INFO][2018-05-24 21:20:15,079][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.078 s for time 1527168015000 ms (execution: 0.023 s)
[INFO][2018-05-24 21:20:15,079][org.apache.spark.storage.BlockManager]Removing RDD 29
[INFO][2018-05-24 21:20:15,079][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 28 from persistence list
[INFO][2018-05-24 21:20:15,080][org.apache.spark.storage.BlockManager]Removing RDD 28
[INFO][2018-05-24 21:20:15,080][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 21:20:15,080][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527168005000 ms
[INFO][2018-05-24 21:20:20,062][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527168020000 ms
[INFO][2018-05-24 21:20:20,062][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527168020000 ms.0 from job set of time 1527168020000 ms
[INFO][2018-05-24 21:20:20,068][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:20:20,068][org.apache.spark.scheduler.DAGScheduler]Got job 16 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:20:20,068][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 16 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:20:20,068][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:20:20,069][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:20:20,069][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 16 (MapPartitionsRDD[33] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:20:20,071][org.apache.spark.storage.memory.MemoryStore]Block broadcast_16 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 21:20:20,072][org.apache.spark.storage.memory.MemoryStore]Block broadcast_16_piece0 stored as bytes in memory (estimated size 1868.0 B, free 912.3 MB)
[INFO][2018-05-24 21:20:20,073][org.apache.spark.storage.BlockManagerInfo]Added broadcast_16_piece0 in memory on 192.168.0.102:51153 (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:20:20,073][org.apache.spark.SparkContext]Created broadcast 16 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:20:20,074][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 16 (MapPartitionsRDD[33] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:20:20,074][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 16.0 with 1 tasks
[INFO][2018-05-24 21:20:20,074][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 16.0 (TID 16, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:20:20,075][org.apache.spark.executor.Executor]Running task 0.0 in stage 16.0 (TID 16)
[INFO][2018-05-24 21:20:20,076][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12204 is the same as ending offset skipping seven 0
[INFO][2018-05-24 21:20:20,077][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:20:20,077][org.apache.spark.executor.Executor]Finished task 0.0 in stage 16.0 (TID 16). 665 bytes result sent to driver
[INFO][2018-05-24 21:20:20,078][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 16.0 (TID 16) in 4 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:20:20,078][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 16.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:20:20,078][org.apache.spark.scheduler.DAGScheduler]ResultStage 16 (foreachPartition at ReceiveKafkaData.scala:66) finished in 0.004 s
[INFO][2018-05-24 21:20:20,079][org.apache.spark.scheduler.DAGScheduler]Job 16 finished: foreachPartition at ReceiveKafkaData.scala:66, took 0.010684 s
[INFO][2018-05-24 21:20:20,079][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527168020000 ms.0 from job set of time 1527168020000 ms
[INFO][2018-05-24 21:20:20,079][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 31 from persistence list
[INFO][2018-05-24 21:20:20,079][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.079 s for time 1527168020000 ms (execution: 0.017 s)
[INFO][2018-05-24 21:20:20,080][org.apache.spark.storage.BlockManager]Removing RDD 31
[INFO][2018-05-24 21:20:20,080][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 30 from persistence list
[INFO][2018-05-24 21:20:20,080][org.apache.spark.storage.BlockManager]Removing RDD 30
[INFO][2018-05-24 21:20:20,080][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 21:20:20,081][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527168010000 ms
[INFO][2018-05-24 21:20:30,070][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527168025000 ms
[INFO][2018-05-24 21:20:30,070][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527168025000 ms.0 from job set of time 1527168025000 ms
[INFO][2018-05-24 21:20:30,075][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:20:30,076][org.apache.spark.scheduler.DAGScheduler]Got job 17 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:20:30,076][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 17 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:20:30,076][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:20:30,076][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:20:30,076][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 17 (MapPartitionsRDD[35] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:20:30,077][org.apache.spark.storage.memory.MemoryStore]Block broadcast_17 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 21:20:30,079][org.apache.spark.storage.memory.MemoryStore]Block broadcast_17_piece0 stored as bytes in memory (estimated size 1868.0 B, free 912.3 MB)
[INFO][2018-05-24 21:20:30,079][org.apache.spark.storage.BlockManagerInfo]Added broadcast_17_piece0 in memory on 192.168.0.102:51153 (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:20:30,079][org.apache.spark.SparkContext]Created broadcast 17 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:20:30,080][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[35] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:20:30,080][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 17.0 with 1 tasks
[INFO][2018-05-24 21:20:30,080][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 17.0 (TID 17, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:20:30,081][org.apache.spark.executor.Executor]Running task 0.0 in stage 17.0 (TID 17)
[INFO][2018-05-24 21:20:30,085][org.apache.spark.streaming.kafka.KafkaRDD]Computing topic seven, partition 0 offsets 12204 -> 12216
[INFO][2018-05-24 21:20:30,085][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 21:20:30,085][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 21:20:30,086][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 21:20:30,536][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:20:30,537][org.apache.spark.executor.Executor]Finished task 0.0 in stage 17.0 (TID 17). 665 bytes result sent to driver
[INFO][2018-05-24 21:20:30,537][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 17.0 (TID 17) in 457 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:20:30,538][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 17.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:20:30,538][org.apache.spark.scheduler.DAGScheduler]ResultStage 17 (foreachPartition at ReceiveKafkaData.scala:66) finished in 0.458 s
[INFO][2018-05-24 21:20:30,538][org.apache.spark.scheduler.DAGScheduler]Job 17 finished: foreachPartition at ReceiveKafkaData.scala:66, took 0.463037 s
[INFO][2018-05-24 21:20:30,539][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527168025000 ms.0 from job set of time 1527168025000 ms
[INFO][2018-05-24 21:20:30,539][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 5.539 s for time 1527168025000 ms (execution: 0.469 s)
[INFO][2018-05-24 21:20:35,133][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527168030000 ms
[INFO][2018-05-24 21:20:35,134][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 33 from persistence list
[INFO][2018-05-24 21:20:35,134][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527168030000 ms.0 from job set of time 1527168030000 ms
[INFO][2018-05-24 21:20:35,134][org.apache.spark.storage.BlockManager]Removing RDD 33
[INFO][2018-05-24 21:20:35,134][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 32 from persistence list
[INFO][2018-05-24 21:20:35,136][org.apache.spark.storage.BlockManager]Removing RDD 32
[INFO][2018-05-24 21:20:35,136][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 21:20:35,136][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527168015000 ms
[INFO][2018-05-24 21:20:35,140][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:20:35,141][org.apache.spark.scheduler.DAGScheduler]Got job 18 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:20:35,141][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 18 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:20:35,141][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:20:35,141][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:20:35,141][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 18 (MapPartitionsRDD[37] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:20:35,143][org.apache.spark.storage.memory.MemoryStore]Block broadcast_18 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 21:20:35,144][org.apache.spark.storage.memory.MemoryStore]Block broadcast_18_piece0 stored as bytes in memory (estimated size 1868.0 B, free 912.3 MB)
[INFO][2018-05-24 21:20:35,144][org.apache.spark.storage.BlockManagerInfo]Added broadcast_18_piece0 in memory on 192.168.0.102:51153 (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:20:35,144][org.apache.spark.SparkContext]Created broadcast 18 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:20:35,145][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 18 (MapPartitionsRDD[37] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:20:35,145][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 18.0 with 1 tasks
[INFO][2018-05-24 21:20:35,146][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 18.0 (TID 18, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:20:35,146][org.apache.spark.executor.Executor]Running task 0.0 in stage 18.0 (TID 18)
[INFO][2018-05-24 21:20:35,148][org.apache.spark.streaming.kafka.KafkaRDD]Computing topic seven, partition 0 offsets 12216 -> 12226
[INFO][2018-05-24 21:20:35,148][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 21:20:35,148][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 21:20:35,148][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 21:20:35,215][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:20:35,216][org.apache.spark.executor.Executor]Finished task 0.0 in stage 18.0 (TID 18). 708 bytes result sent to driver
[INFO][2018-05-24 21:20:35,217][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 18.0 (TID 18) in 71 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:20:35,217][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 18.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:20:35,218][org.apache.spark.scheduler.DAGScheduler]ResultStage 18 (foreachPartition at ReceiveKafkaData.scala:66) finished in 0.071 s
[INFO][2018-05-24 21:20:35,218][org.apache.spark.scheduler.DAGScheduler]Job 18 finished: foreachPartition at ReceiveKafkaData.scala:66, took 0.077650 s
[INFO][2018-05-24 21:20:35,218][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527168030000 ms.0 from job set of time 1527168030000 ms
[INFO][2018-05-24 21:20:35,219][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 5.218 s for time 1527168030000 ms (execution: 0.084 s)
[INFO][2018-05-24 21:20:45,210][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527168035000 ms
[INFO][2018-05-24 21:20:45,211][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 35 from persistence list
[INFO][2018-05-24 21:20:45,211][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527168035000 ms.0 from job set of time 1527168035000 ms
[INFO][2018-05-24 21:20:45,211][org.apache.spark.storage.BlockManager]Removing RDD 35
[INFO][2018-05-24 21:20:45,211][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 34 from persistence list
[INFO][2018-05-24 21:20:45,212][org.apache.spark.storage.BlockManager]Removing RDD 34
[INFO][2018-05-24 21:20:45,212][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 21:20:45,213][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527168020000 ms
[INFO][2018-05-24 21:20:45,217][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:20:45,218][org.apache.spark.scheduler.DAGScheduler]Got job 19 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:20:45,218][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 19 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:20:45,218][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:20:45,218][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:20:45,218][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 19 (MapPartitionsRDD[39] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:20:45,220][org.apache.spark.storage.memory.MemoryStore]Block broadcast_19 stored as values in memory (estimated size 3.3 KB, free 912.3 MB)
[INFO][2018-05-24 21:20:45,221][org.apache.spark.storage.memory.MemoryStore]Block broadcast_19_piece0 stored as bytes in memory (estimated size 1868.0 B, free 912.3 MB)
[INFO][2018-05-24 21:20:45,222][org.apache.spark.storage.BlockManagerInfo]Added broadcast_19_piece0 in memory on 192.168.0.102:51153 (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:20:45,223][org.apache.spark.SparkContext]Created broadcast 19 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:20:45,223][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 19 (MapPartitionsRDD[39] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:20:45,223][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 19.0 with 1 tasks
[INFO][2018-05-24 21:20:45,224][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 19.0 (TID 19, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:20:45,225][org.apache.spark.executor.Executor]Running task 0.0 in stage 19.0 (TID 19)
[INFO][2018-05-24 21:20:45,226][org.apache.spark.streaming.kafka.KafkaRDD]Computing topic seven, partition 0 offsets 12226 -> 12246
[INFO][2018-05-24 21:20:45,226][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 21:20:45,226][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 21:20:45,226][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 21:20:45,305][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:20:45,306][org.apache.spark.executor.Executor]Finished task 0.0 in stage 19.0 (TID 19). 708 bytes result sent to driver
[INFO][2018-05-24 21:20:45,307][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 19.0 (TID 19) in 83 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:20:45,307][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 19.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:20:45,308][org.apache.spark.scheduler.DAGScheduler]ResultStage 19 (foreachPartition at ReceiveKafkaData.scala:66) finished in 0.083 s
[INFO][2018-05-24 21:20:45,308][org.apache.spark.scheduler.DAGScheduler]Job 19 finished: foreachPartition at ReceiveKafkaData.scala:66, took 0.090911 s
[INFO][2018-05-24 21:20:45,308][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527168035000 ms.0 from job set of time 1527168035000 ms
[INFO][2018-05-24 21:20:45,309][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 10.308 s for time 1527168035000 ms (execution: 0.097 s)
[INFO][2018-05-24 21:20:50,271][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527168040000 ms
[INFO][2018-05-24 21:20:50,271][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527168040000 ms.0 from job set of time 1527168040000 ms
[INFO][2018-05-24 21:20:50,277][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:20:50,277][org.apache.spark.scheduler.DAGScheduler]Got job 20 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:20:50,277][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 20 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:20:50,278][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:20:50,278][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:20:50,278][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 20 (MapPartitionsRDD[41] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:20:50,279][org.apache.spark.storage.memory.MemoryStore]Block broadcast_20 stored as values in memory (estimated size 3.3 KB, free 912.2 MB)
[INFO][2018-05-24 21:20:50,281][org.apache.spark.storage.memory.MemoryStore]Block broadcast_20_piece0 stored as bytes in memory (estimated size 1868.0 B, free 912.2 MB)
[INFO][2018-05-24 21:20:50,281][org.apache.spark.storage.BlockManagerInfo]Added broadcast_20_piece0 in memory on 192.168.0.102:51153 (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:20:50,282][org.apache.spark.SparkContext]Created broadcast 20 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:20:50,282][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 20 (MapPartitionsRDD[41] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:20:50,282][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 20.0 with 1 tasks
[INFO][2018-05-24 21:20:50,283][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 20.0 (TID 20, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:20:50,283][org.apache.spark.executor.Executor]Running task 0.0 in stage 20.0 (TID 20)
[INFO][2018-05-24 21:20:50,284][org.apache.spark.streaming.kafka.KafkaRDD]Computing topic seven, partition 0 offsets 12246 -> 12256
[INFO][2018-05-24 21:20:50,284][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 21:20:50,284][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 21:20:50,284][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 21:20:50,354][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:20:50,355][org.apache.spark.executor.Executor]Finished task 0.0 in stage 20.0 (TID 20). 665 bytes result sent to driver
[INFO][2018-05-24 21:20:50,355][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 20.0 (TID 20) in 72 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:20:50,355][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 20.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:20:50,356][org.apache.spark.scheduler.DAGScheduler]ResultStage 20 (foreachPartition at ReceiveKafkaData.scala:66) finished in 0.074 s
[INFO][2018-05-24 21:20:50,357][org.apache.spark.scheduler.DAGScheduler]Job 20 finished: foreachPartition at ReceiveKafkaData.scala:66, took 0.079885 s
[INFO][2018-05-24 21:20:50,358][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527168040000 ms.0 from job set of time 1527168040000 ms
[INFO][2018-05-24 21:20:50,358][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 10.358 s for time 1527168040000 ms (execution: 0.087 s)
[INFO][2018-05-24 21:20:55,332][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527168045000 ms
[INFO][2018-05-24 21:20:55,332][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 37 from persistence list
[INFO][2018-05-24 21:20:55,332][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527168045000 ms.0 from job set of time 1527168045000 ms
[INFO][2018-05-24 21:20:55,332][org.apache.spark.storage.BlockManager]Removing RDD 37
[INFO][2018-05-24 21:20:55,333][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 36 from persistence list
[INFO][2018-05-24 21:20:55,333][org.apache.spark.storage.BlockManager]Removing RDD 36
[INFO][2018-05-24 21:20:55,333][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 21:20:55,334][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527168025000 ms
[INFO][2018-05-24 21:20:55,337][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:20:55,338][org.apache.spark.scheduler.DAGScheduler]Got job 21 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:20:55,338][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 21 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:20:55,338][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:20:55,338][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:20:55,338][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 21 (MapPartitionsRDD[43] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:20:55,340][org.apache.spark.storage.memory.MemoryStore]Block broadcast_21 stored as values in memory (estimated size 3.3 KB, free 912.2 MB)
[INFO][2018-05-24 21:20:55,345][org.apache.spark.storage.memory.MemoryStore]Block broadcast_21_piece0 stored as bytes in memory (estimated size 1868.0 B, free 912.2 MB)
[INFO][2018-05-24 21:20:55,346][org.apache.spark.storage.BlockManagerInfo]Added broadcast_21_piece0 in memory on 192.168.0.102:51153 (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:20:55,346][org.apache.spark.SparkContext]Created broadcast 21 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:20:55,346][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 21 (MapPartitionsRDD[43] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:20:55,347][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 21.0 with 1 tasks
[INFO][2018-05-24 21:20:55,347][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 21.0 (TID 21, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:20:55,348][org.apache.spark.executor.Executor]Running task 0.0 in stage 21.0 (TID 21)
[INFO][2018-05-24 21:20:55,349][org.apache.spark.streaming.kafka.KafkaRDD]Computing topic seven, partition 0 offsets 12256 -> 12266
[INFO][2018-05-24 21:20:55,349][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 21:20:55,350][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 21:20:55,350][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 21:20:55,384][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527168050000 ms
[INFO][2018-05-24 21:20:55,385][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 39 from persistence list
[INFO][2018-05-24 21:20:55,385][org.apache.spark.storage.BlockManager]Removing RDD 39
[INFO][2018-05-24 21:20:55,385][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 38 from persistence list
[INFO][2018-05-24 21:20:55,385][org.apache.spark.storage.BlockManager]Removing RDD 38
[INFO][2018-05-24 21:20:55,385][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 21:20:55,385][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527168030000 ms
[INFO][2018-05-24 21:20:55,415][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:20:55,416][org.apache.spark.executor.Executor]Finished task 0.0 in stage 21.0 (TID 21). 751 bytes result sent to driver
[INFO][2018-05-24 21:20:55,416][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 21.0 (TID 21) in 69 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:20:55,417][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 21.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:20:55,417][org.apache.spark.scheduler.DAGScheduler]ResultStage 21 (foreachPartition at ReceiveKafkaData.scala:66) finished in 0.070 s
[INFO][2018-05-24 21:20:55,418][org.apache.spark.scheduler.DAGScheduler]Job 21 finished: foreachPartition at ReceiveKafkaData.scala:66, took 0.080053 s
[INFO][2018-05-24 21:20:55,418][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527168045000 ms.0 from job set of time 1527168045000 ms
[INFO][2018-05-24 21:20:55,419][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 10.418 s for time 1527168045000 ms (execution: 0.086 s)
[INFO][2018-05-24 21:20:55,419][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527168050000 ms.0 from job set of time 1527168050000 ms
[INFO][2018-05-24 21:20:55,426][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:20:55,427][org.apache.spark.scheduler.DAGScheduler]Got job 22 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:20:55,427][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 22 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:20:55,427][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:20:55,427][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:20:55,428][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 22 (MapPartitionsRDD[45] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:20:55,430][org.apache.spark.storage.memory.MemoryStore]Block broadcast_22 stored as values in memory (estimated size 3.3 KB, free 912.2 MB)
[INFO][2018-05-24 21:20:55,433][org.apache.spark.storage.memory.MemoryStore]Block broadcast_22_piece0 stored as bytes in memory (estimated size 1868.0 B, free 912.2 MB)
[INFO][2018-05-24 21:20:55,433][org.apache.spark.storage.BlockManagerInfo]Added broadcast_22_piece0 in memory on 192.168.0.102:51153 (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:20:55,434][org.apache.spark.SparkContext]Created broadcast 22 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:20:55,434][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 22 (MapPartitionsRDD[45] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:20:55,434][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 22.0 with 1 tasks
[INFO][2018-05-24 21:20:55,435][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 22.0 (TID 22, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:20:55,436][org.apache.spark.executor.Executor]Running task 0.0 in stage 22.0 (TID 22)
[INFO][2018-05-24 21:20:55,436][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527168055000 ms
[INFO][2018-05-24 21:20:55,436][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 41 from persistence list
[INFO][2018-05-24 21:20:55,437][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12266 is the same as ending offset skipping seven 0
[INFO][2018-05-24 21:20:55,438][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:20:55,438][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 40 from persistence list
[INFO][2018-05-24 21:20:55,438][org.apache.spark.executor.Executor]Finished task 0.0 in stage 22.0 (TID 22). 708 bytes result sent to driver
[INFO][2018-05-24 21:20:55,438][org.apache.spark.storage.BlockManager]Removing RDD 41
[INFO][2018-05-24 21:20:55,439][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 21:20:55,439][org.apache.spark.storage.BlockManager]Removing RDD 40
[INFO][2018-05-24 21:20:55,439][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527168035000 ms
[INFO][2018-05-24 21:20:55,440][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 22.0 (TID 22) in 5 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:20:55,440][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 22.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:20:55,440][org.apache.spark.scheduler.DAGScheduler]ResultStage 22 (foreachPartition at ReceiveKafkaData.scala:66) finished in 0.005 s
[INFO][2018-05-24 21:20:55,441][org.apache.spark.scheduler.DAGScheduler]Job 22 finished: foreachPartition at ReceiveKafkaData.scala:66, took 0.014450 s
[INFO][2018-05-24 21:20:55,441][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527168050000 ms.0 from job set of time 1527168050000 ms
[INFO][2018-05-24 21:20:55,441][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 5.441 s for time 1527168050000 ms (execution: 0.022 s)
[INFO][2018-05-24 21:20:55,441][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 43 from persistence list
[INFO][2018-05-24 21:20:55,441][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527168055000 ms.0 from job set of time 1527168055000 ms
[INFO][2018-05-24 21:20:55,442][org.apache.spark.storage.BlockManager]Removing RDD 43
[INFO][2018-05-24 21:20:55,442][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 42 from persistence list
[INFO][2018-05-24 21:20:55,442][org.apache.spark.storage.BlockManager]Removing RDD 42
[INFO][2018-05-24 21:20:55,443][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 21:20:55,443][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527168040000 ms
[INFO][2018-05-24 21:20:55,446][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:20:55,446][org.apache.spark.scheduler.DAGScheduler]Got job 23 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:20:55,446][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 23 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:20:55,446][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:20:55,446][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:20:55,447][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 23 (MapPartitionsRDD[47] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:20:55,448][org.apache.spark.storage.memory.MemoryStore]Block broadcast_23 stored as values in memory (estimated size 3.3 KB, free 912.2 MB)
[INFO][2018-05-24 21:20:55,451][org.apache.spark.storage.memory.MemoryStore]Block broadcast_23_piece0 stored as bytes in memory (estimated size 1867.0 B, free 912.2 MB)
[INFO][2018-05-24 21:20:55,452][org.apache.spark.storage.BlockManagerInfo]Added broadcast_23_piece0 in memory on 192.168.0.102:51153 (size: 1867.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:20:55,452][org.apache.spark.SparkContext]Created broadcast 23 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:20:55,453][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 23 (MapPartitionsRDD[47] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:20:55,453][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 23.0 with 1 tasks
[INFO][2018-05-24 21:20:55,453][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 23.0 (TID 23, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:20:55,454][org.apache.spark.executor.Executor]Running task 0.0 in stage 23.0 (TID 23)
[INFO][2018-05-24 21:20:55,456][org.apache.spark.streaming.kafka.KafkaRDD]Computing topic seven, partition 0 offsets 12266 -> 12267
[INFO][2018-05-24 21:20:55,456][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 21:20:55,456][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 21:20:55,456][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 21:20:55,522][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:20:55,522][org.apache.spark.executor.Executor]Finished task 0.0 in stage 23.0 (TID 23). 708 bytes result sent to driver
[INFO][2018-05-24 21:20:55,523][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 23.0 (TID 23) in 70 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:20:55,523][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 23.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:20:55,524][org.apache.spark.scheduler.DAGScheduler]ResultStage 23 (foreachPartition at ReceiveKafkaData.scala:66) finished in 0.070 s
[INFO][2018-05-24 21:20:55,524][org.apache.spark.scheduler.DAGScheduler]Job 23 finished: foreachPartition at ReceiveKafkaData.scala:66, took 0.078283 s
[INFO][2018-05-24 21:20:55,525][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527168055000 ms.0 from job set of time 1527168055000 ms
[INFO][2018-05-24 21:20:55,525][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.525 s for time 1527168055000 ms (execution: 0.084 s)
[INFO][2018-05-24 21:20:55,525][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 45 from persistence list
[INFO][2018-05-24 21:20:55,526][org.apache.spark.storage.BlockManager]Removing RDD 45
[INFO][2018-05-24 21:20:55,526][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 44 from persistence list
[INFO][2018-05-24 21:20:55,526][org.apache.spark.storage.BlockManager]Removing RDD 44
[INFO][2018-05-24 21:20:55,527][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 21:20:55,527][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527168045000 ms
[INFO][2018-05-24 21:21:00,059][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527168060000 ms
[INFO][2018-05-24 21:21:00,059][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527168060000 ms.0 from job set of time 1527168060000 ms
[INFO][2018-05-24 21:21:00,064][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:21:00,065][org.apache.spark.scheduler.DAGScheduler]Got job 24 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:21:00,065][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 24 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:21:00,065][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:21:00,065][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:21:00,065][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 24 (MapPartitionsRDD[49] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:21:00,066][org.apache.spark.storage.memory.MemoryStore]Block broadcast_24 stored as values in memory (estimated size 3.3 KB, free 912.2 MB)
[INFO][2018-05-24 21:21:00,070][org.apache.spark.storage.memory.MemoryStore]Block broadcast_24_piece0 stored as bytes in memory (estimated size 1868.0 B, free 912.2 MB)
[INFO][2018-05-24 21:21:00,070][org.apache.spark.storage.BlockManagerInfo]Added broadcast_24_piece0 in memory on 192.168.0.102:51153 (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:21:00,071][org.apache.spark.SparkContext]Created broadcast 24 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:21:00,071][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 24 (MapPartitionsRDD[49] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:21:00,071][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 24.0 with 1 tasks
[INFO][2018-05-24 21:21:00,072][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 24.0 (TID 24, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:21:00,072][org.apache.spark.executor.Executor]Running task 0.0 in stage 24.0 (TID 24)
[INFO][2018-05-24 21:21:00,073][org.apache.spark.streaming.kafka.KafkaRDD]Computing topic seven, partition 0 offsets 12267 -> 12276
[INFO][2018-05-24 21:21:00,073][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 21:21:00,073][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 21:21:00,073][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 21:21:00,138][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:21:00,139][org.apache.spark.executor.Executor]Finished task 0.0 in stage 24.0 (TID 24). 665 bytes result sent to driver
[INFO][2018-05-24 21:21:00,140][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 24.0 (TID 24) in 68 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:21:00,140][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 24.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:21:00,140][org.apache.spark.scheduler.DAGScheduler]ResultStage 24 (foreachPartition at ReceiveKafkaData.scala:66) finished in 0.068 s
[INFO][2018-05-24 21:21:00,141][org.apache.spark.scheduler.DAGScheduler]Job 24 finished: foreachPartition at ReceiveKafkaData.scala:66, took 0.076437 s
[INFO][2018-05-24 21:21:00,141][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527168060000 ms.0 from job set of time 1527168060000 ms
[INFO][2018-05-24 21:21:00,141][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.141 s for time 1527168060000 ms (execution: 0.082 s)
[INFO][2018-05-24 21:21:00,141][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 47 from persistence list
[INFO][2018-05-24 21:21:00,142][org.apache.spark.storage.BlockManager]Removing RDD 47
[INFO][2018-05-24 21:21:00,142][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 46 from persistence list
[INFO][2018-05-24 21:21:00,142][org.apache.spark.storage.BlockManager]Removing RDD 46
[INFO][2018-05-24 21:21:00,143][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 21:21:00,143][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527168050000 ms
[INFO][2018-05-24 21:21:05,047][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527168065000 ms
[INFO][2018-05-24 21:21:05,047][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527168065000 ms.0 from job set of time 1527168065000 ms
[INFO][2018-05-24 21:21:05,053][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:21:05,053][org.apache.spark.scheduler.DAGScheduler]Got job 25 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:21:05,054][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 25 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:21:05,054][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:21:05,054][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:21:05,054][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 25 (MapPartitionsRDD[51] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:21:05,056][org.apache.spark.storage.memory.MemoryStore]Block broadcast_25 stored as values in memory (estimated size 3.3 KB, free 912.2 MB)
[INFO][2018-05-24 21:21:05,059][org.apache.spark.storage.memory.MemoryStore]Block broadcast_25_piece0 stored as bytes in memory (estimated size 1868.0 B, free 912.2 MB)
[INFO][2018-05-24 21:21:05,060][org.apache.spark.storage.BlockManagerInfo]Added broadcast_25_piece0 in memory on 192.168.0.102:51153 (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:21:05,060][org.apache.spark.SparkContext]Created broadcast 25 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:21:05,061][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 25 (MapPartitionsRDD[51] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:21:05,061][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 25.0 with 1 tasks
[INFO][2018-05-24 21:21:05,061][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 25.0 (TID 25, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:21:05,062][org.apache.spark.executor.Executor]Running task 0.0 in stage 25.0 (TID 25)
[INFO][2018-05-24 21:21:05,063][org.apache.spark.streaming.kafka.KafkaRDD]Computing topic seven, partition 0 offsets 12276 -> 12286
[INFO][2018-05-24 21:21:05,063][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 21:21:05,063][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 21:21:05,064][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 21:21:05,130][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:21:05,131][org.apache.spark.executor.Executor]Finished task 0.0 in stage 25.0 (TID 25). 665 bytes result sent to driver
[INFO][2018-05-24 21:21:05,132][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 25.0 (TID 25) in 71 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:21:05,132][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 25.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:21:05,133][org.apache.spark.scheduler.DAGScheduler]ResultStage 25 (foreachPartition at ReceiveKafkaData.scala:66) finished in 0.071 s
[INFO][2018-05-24 21:21:05,133][org.apache.spark.scheduler.DAGScheduler]Job 25 finished: foreachPartition at ReceiveKafkaData.scala:66, took 0.080034 s
[INFO][2018-05-24 21:21:05,134][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527168065000 ms.0 from job set of time 1527168065000 ms
[INFO][2018-05-24 21:21:05,134][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 49 from persistence list
[INFO][2018-05-24 21:21:05,134][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.134 s for time 1527168065000 ms (execution: 0.087 s)
[INFO][2018-05-24 21:21:05,134][org.apache.spark.storage.BlockManager]Removing RDD 49
[INFO][2018-05-24 21:21:05,135][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 48 from persistence list
[INFO][2018-05-24 21:21:05,135][org.apache.spark.storage.BlockManager]Removing RDD 48
[INFO][2018-05-24 21:21:05,135][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 21:21:05,135][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527168055000 ms
[INFO][2018-05-24 21:21:14,552][org.apache.spark.util.ShutdownHookManager]Shutdown hook called
[INFO][2018-05-24 21:21:14,554][org.apache.spark.util.ShutdownHookManager]Deleting directory /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/spark-f4b6eff1-3412-40c7-94e7-407d32344d93
[INFO][2018-05-24 21:21:15,064][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527168070000 ms
[INFO][2018-05-24 21:21:15,064][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527168070000 ms.0 from job set of time 1527168070000 ms
[INFO][2018-05-24 21:21:15,070][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:21:15,071][org.apache.spark.scheduler.DAGScheduler]Got job 26 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:21:15,071][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 26 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:21:15,071][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:21:15,071][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:21:15,071][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 26 (MapPartitionsRDD[53] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:21:15,072][org.apache.spark.storage.memory.MemoryStore]Block broadcast_26 stored as values in memory (estimated size 3.3 KB, free 912.2 MB)
[INFO][2018-05-24 21:21:15,076][org.apache.spark.storage.memory.MemoryStore]Block broadcast_26_piece0 stored as bytes in memory (estimated size 1868.0 B, free 912.2 MB)
[INFO][2018-05-24 21:21:15,076][org.apache.spark.storage.BlockManagerInfo]Added broadcast_26_piece0 in memory on 192.168.0.102:51153 (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:21:15,077][org.apache.spark.SparkContext]Created broadcast 26 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:21:15,077][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 26 (MapPartitionsRDD[53] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:21:15,078][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 26.0 with 1 tasks
[INFO][2018-05-24 21:21:15,078][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 26.0 (TID 26, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:21:15,079][org.apache.spark.executor.Executor]Running task 0.0 in stage 26.0 (TID 26)
[INFO][2018-05-24 21:21:15,081][org.apache.spark.streaming.kafka.KafkaRDD]Computing topic seven, partition 0 offsets 12286 -> 12304
[INFO][2018-05-24 21:21:15,081][kafka.utils.VerifiableProperties]Verifying properties
[INFO][2018-05-24 21:21:15,081][kafka.utils.VerifiableProperties]Property group.id is overridden to 
[INFO][2018-05-24 21:21:15,081][kafka.utils.VerifiableProperties]Property zookeeper.connect is overridden to 
[INFO][2018-05-24 21:21:15,120][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527168075000 ms
[INFO][2018-05-24 21:21:15,167][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:21:15,168][org.apache.spark.executor.Executor]Finished task 0.0 in stage 26.0 (TID 26). 708 bytes result sent to driver
[INFO][2018-05-24 21:21:15,169][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 26.0 (TID 26) in 91 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:21:15,169][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 26.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:21:15,170][org.apache.spark.scheduler.DAGScheduler]ResultStage 26 (foreachPartition at ReceiveKafkaData.scala:66) finished in 0.092 s
[INFO][2018-05-24 21:21:15,171][org.apache.spark.scheduler.DAGScheduler]Job 26 finished: foreachPartition at ReceiveKafkaData.scala:66, took 0.100446 s
[INFO][2018-05-24 21:21:15,171][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527168070000 ms.0 from job set of time 1527168070000 ms
[INFO][2018-05-24 21:21:15,173][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 5.171 s for time 1527168070000 ms (execution: 0.107 s)
[INFO][2018-05-24 21:21:15,173][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527168075000 ms.0 from job set of time 1527168075000 ms
[INFO][2018-05-24 21:21:15,173][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 51 from persistence list
[INFO][2018-05-24 21:21:15,174][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 50 from persistence list
[INFO][2018-05-24 21:21:15,175][org.apache.spark.storage.BlockManager]Removing RDD 51
[INFO][2018-05-24 21:21:15,177][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:21:15,178][org.apache.spark.storage.BlockManager]Removing RDD 50
[INFO][2018-05-24 21:21:15,178][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 21:21:15,178][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527168060000 ms
[INFO][2018-05-24 21:21:15,178][org.apache.spark.scheduler.DAGScheduler]Got job 27 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:21:15,178][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 27 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:21:15,178][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:21:15,178][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:21:15,179][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 27 (MapPartitionsRDD[55] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:21:15,180][org.apache.spark.storage.memory.MemoryStore]Block broadcast_27 stored as values in memory (estimated size 3.3 KB, free 912.2 MB)
[INFO][2018-05-24 21:21:15,184][org.apache.spark.storage.memory.MemoryStore]Block broadcast_27_piece0 stored as bytes in memory (estimated size 1868.0 B, free 912.2 MB)
[INFO][2018-05-24 21:21:15,185][org.apache.spark.storage.BlockManagerInfo]Added broadcast_27_piece0 in memory on 192.168.0.102:51153 (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:21:15,185][org.apache.spark.SparkContext]Created broadcast 27 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:21:15,186][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 27 (MapPartitionsRDD[55] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:21:15,186][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 27.0 with 1 tasks
[INFO][2018-05-24 21:21:15,187][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 27.0 (TID 27, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:21:15,188][org.apache.spark.executor.Executor]Running task 0.0 in stage 27.0 (TID 27)
[INFO][2018-05-24 21:21:15,190][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12304 is the same as ending offset skipping seven 0
[INFO][2018-05-24 21:21:15,190][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:21:15,190][org.apache.spark.executor.Executor]Finished task 0.0 in stage 27.0 (TID 27). 665 bytes result sent to driver
[INFO][2018-05-24 21:21:15,191][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 27.0 (TID 27) in 4 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:21:15,191][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 27.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:21:15,192][org.apache.spark.scheduler.DAGScheduler]ResultStage 27 (foreachPartition at ReceiveKafkaData.scala:66) finished in 0.006 s
[INFO][2018-05-24 21:21:15,193][org.apache.spark.scheduler.DAGScheduler]Job 27 finished: foreachPartition at ReceiveKafkaData.scala:66, took 0.015933 s
[INFO][2018-05-24 21:21:15,193][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527168075000 ms.0 from job set of time 1527168075000 ms
[INFO][2018-05-24 21:21:15,194][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 53 from persistence list
[INFO][2018-05-24 21:21:15,195][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.193 s for time 1527168075000 ms (execution: 0.020 s)
[INFO][2018-05-24 21:21:15,195][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 52 from persistence list
[INFO][2018-05-24 21:21:15,195][org.apache.spark.storage.BlockManager]Removing RDD 53
[INFO][2018-05-24 21:21:15,197][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 21:21:15,197][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527168065000 ms
[INFO][2018-05-24 21:21:15,197][org.apache.spark.storage.BlockManager]Removing RDD 52
[INFO][2018-05-24 21:21:25,072][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527168080000 ms
[INFO][2018-05-24 21:21:25,073][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527168080000 ms.0 from job set of time 1527168080000 ms
[INFO][2018-05-24 21:21:25,079][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:21:25,080][org.apache.spark.scheduler.DAGScheduler]Got job 28 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:21:25,080][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 28 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:21:25,080][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:21:25,080][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:21:25,080][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 28 (MapPartitionsRDD[57] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:21:25,082][org.apache.spark.storage.memory.MemoryStore]Block broadcast_28 stored as values in memory (estimated size 3.3 KB, free 912.2 MB)
[INFO][2018-05-24 21:21:25,085][org.apache.spark.storage.memory.MemoryStore]Block broadcast_28_piece0 stored as bytes in memory (estimated size 1868.0 B, free 912.2 MB)
[INFO][2018-05-24 21:21:25,086][org.apache.spark.storage.BlockManagerInfo]Added broadcast_28_piece0 in memory on 192.168.0.102:51153 (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:21:25,086][org.apache.spark.SparkContext]Created broadcast 28 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:21:25,086][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 28 (MapPartitionsRDD[57] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:21:25,086][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 28.0 with 1 tasks
[INFO][2018-05-24 21:21:25,087][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 28.0 (TID 28, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:21:25,087][org.apache.spark.executor.Executor]Running task 0.0 in stage 28.0 (TID 28)
[INFO][2018-05-24 21:21:25,088][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12304 is the same as ending offset skipping seven 0
[INFO][2018-05-24 21:21:25,088][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:21:25,089][org.apache.spark.executor.Executor]Finished task 0.0 in stage 28.0 (TID 28). 665 bytes result sent to driver
[INFO][2018-05-24 21:21:25,089][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 28.0 (TID 28) in 2 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:21:25,089][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 28.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:21:25,090][org.apache.spark.scheduler.DAGScheduler]ResultStage 28 (foreachPartition at ReceiveKafkaData.scala:66) finished in 0.003 s
[INFO][2018-05-24 21:21:25,090][org.apache.spark.scheduler.DAGScheduler]Job 28 finished: foreachPartition at ReceiveKafkaData.scala:66, took 0.011053 s
[INFO][2018-05-24 21:21:25,091][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527168080000 ms.0 from job set of time 1527168080000 ms
[INFO][2018-05-24 21:21:25,091][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 5.091 s for time 1527168080000 ms (execution: 0.018 s)
[INFO][2018-05-24 21:21:30,127][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527168085000 ms
[INFO][2018-05-24 21:21:30,127][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527168085000 ms.0 from job set of time 1527168085000 ms
[INFO][2018-05-24 21:21:30,128][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 55 from persistence list
[INFO][2018-05-24 21:21:30,128][org.apache.spark.storage.BlockManager]Removing RDD 55
[INFO][2018-05-24 21:21:30,128][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 54 from persistence list
[INFO][2018-05-24 21:21:30,129][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 21:21:30,130][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527168070000 ms
[INFO][2018-05-24 21:21:30,130][org.apache.spark.storage.BlockManager]Removing RDD 54
[INFO][2018-05-24 21:21:30,135][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:21:30,136][org.apache.spark.scheduler.DAGScheduler]Got job 29 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:21:30,136][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 29 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:21:30,136][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:21:30,136][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:21:30,136][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 29 (MapPartitionsRDD[59] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:21:30,137][org.apache.spark.storage.memory.MemoryStore]Block broadcast_29 stored as values in memory (estimated size 3.3 KB, free 912.2 MB)
[INFO][2018-05-24 21:21:30,140][org.apache.spark.storage.memory.MemoryStore]Block broadcast_29_piece0 stored as bytes in memory (estimated size 1868.0 B, free 912.2 MB)
[INFO][2018-05-24 21:21:30,140][org.apache.spark.storage.BlockManagerInfo]Added broadcast_29_piece0 in memory on 192.168.0.102:51153 (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:21:30,141][org.apache.spark.SparkContext]Created broadcast 29 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:21:30,141][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 29 (MapPartitionsRDD[59] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:21:30,141][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 29.0 with 1 tasks
[INFO][2018-05-24 21:21:30,142][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 29.0 (TID 29, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:21:30,142][org.apache.spark.executor.Executor]Running task 0.0 in stage 29.0 (TID 29)
[INFO][2018-05-24 21:21:30,143][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12304 is the same as ending offset skipping seven 0
[INFO][2018-05-24 21:21:30,144][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:21:30,144][org.apache.spark.executor.Executor]Finished task 0.0 in stage 29.0 (TID 29). 665 bytes result sent to driver
[INFO][2018-05-24 21:21:30,146][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 29.0 (TID 29) in 4 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:21:30,146][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 29.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:21:30,146][org.apache.spark.scheduler.DAGScheduler]ResultStage 29 (foreachPartition at ReceiveKafkaData.scala:66) finished in 0.005 s
[INFO][2018-05-24 21:21:30,146][org.apache.spark.scheduler.DAGScheduler]Job 29 finished: foreachPartition at ReceiveKafkaData.scala:66, took 0.011248 s
[INFO][2018-05-24 21:21:30,147][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527168085000 ms.0 from job set of time 1527168085000 ms
[INFO][2018-05-24 21:21:30,147][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 5.147 s for time 1527168085000 ms (execution: 0.020 s)
[INFO][2018-05-24 21:21:35,191][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527168090000 ms
[INFO][2018-05-24 21:21:35,191][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 57 from persistence list
[INFO][2018-05-24 21:21:35,191][org.apache.spark.storage.BlockManager]Removing RDD 57
[INFO][2018-05-24 21:21:35,191][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 56 from persistence list
[INFO][2018-05-24 21:21:35,192][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527168090000 ms.0 from job set of time 1527168090000 ms
[INFO][2018-05-24 21:21:35,192][org.apache.spark.storage.BlockManager]Removing RDD 56
[INFO][2018-05-24 21:21:35,192][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 21:21:35,192][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527168075000 ms
[INFO][2018-05-24 21:21:35,198][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:21:35,198][org.apache.spark.scheduler.DAGScheduler]Got job 30 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:21:35,198][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 30 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:21:35,198][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:21:35,199][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:21:35,199][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 30 (MapPartitionsRDD[61] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:21:35,200][org.apache.spark.storage.memory.MemoryStore]Block broadcast_30 stored as values in memory (estimated size 3.3 KB, free 912.2 MB)
[INFO][2018-05-24 21:21:35,203][org.apache.spark.storage.memory.MemoryStore]Block broadcast_30_piece0 stored as bytes in memory (estimated size 1868.0 B, free 912.2 MB)
[INFO][2018-05-24 21:21:35,204][org.apache.spark.storage.BlockManagerInfo]Added broadcast_30_piece0 in memory on 192.168.0.102:51153 (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:21:35,204][org.apache.spark.SparkContext]Created broadcast 30 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:21:35,205][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 30 (MapPartitionsRDD[61] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:21:35,205][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 30.0 with 1 tasks
[INFO][2018-05-24 21:21:35,205][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 30.0 (TID 30, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:21:35,206][org.apache.spark.executor.Executor]Running task 0.0 in stage 30.0 (TID 30)
[INFO][2018-05-24 21:21:35,207][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12304 is the same as ending offset skipping seven 0
[INFO][2018-05-24 21:21:35,208][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:21:35,208][org.apache.spark.executor.Executor]Finished task 0.0 in stage 30.0 (TID 30). 665 bytes result sent to driver
[INFO][2018-05-24 21:21:35,209][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 30.0 (TID 30) in 4 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:21:35,209][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 30.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:21:35,209][org.apache.spark.scheduler.DAGScheduler]ResultStage 30 (foreachPartition at ReceiveKafkaData.scala:66) finished in 0.004 s
[INFO][2018-05-24 21:21:35,209][org.apache.spark.scheduler.DAGScheduler]Job 30 finished: foreachPartition at ReceiveKafkaData.scala:66, took 0.011461 s
[INFO][2018-05-24 21:21:35,210][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527168090000 ms.0 from job set of time 1527168090000 ms
[INFO][2018-05-24 21:21:35,210][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 5.210 s for time 1527168090000 ms (execution: 0.019 s)
[INFO][2018-05-24 21:21:40,263][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527168095000 ms
[INFO][2018-05-24 21:21:40,264][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 59 from persistence list
[INFO][2018-05-24 21:21:40,265][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527168095000 ms.0 from job set of time 1527168095000 ms
[INFO][2018-05-24 21:21:40,269][org.apache.spark.storage.BlockManager]Removing RDD 59
[INFO][2018-05-24 21:21:40,270][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 58 from persistence list
[INFO][2018-05-24 21:21:40,271][org.apache.spark.storage.BlockManager]Removing RDD 58
[INFO][2018-05-24 21:21:40,271][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 21:21:40,271][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527168080000 ms
[INFO][2018-05-24 21:21:40,273][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:21:40,274][org.apache.spark.scheduler.DAGScheduler]Got job 31 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:21:40,274][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 31 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:21:40,274][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:21:40,274][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:21:40,274][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 31 (MapPartitionsRDD[63] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:21:40,276][org.apache.spark.storage.memory.MemoryStore]Block broadcast_31 stored as values in memory (estimated size 3.3 KB, free 912.2 MB)
[INFO][2018-05-24 21:21:40,279][org.apache.spark.storage.memory.MemoryStore]Block broadcast_31_piece0 stored as bytes in memory (estimated size 1868.0 B, free 912.2 MB)
[INFO][2018-05-24 21:21:40,279][org.apache.spark.storage.BlockManagerInfo]Added broadcast_31_piece0 in memory on 192.168.0.102:51153 (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:21:40,280][org.apache.spark.SparkContext]Created broadcast 31 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:21:40,280][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 31 (MapPartitionsRDD[63] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:21:40,280][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 31.0 with 1 tasks
[INFO][2018-05-24 21:21:40,281][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 31.0 (TID 31, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:21:40,281][org.apache.spark.executor.Executor]Running task 0.0 in stage 31.0 (TID 31)
[INFO][2018-05-24 21:21:40,282][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12304 is the same as ending offset skipping seven 0
[INFO][2018-05-24 21:21:40,282][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:21:40,283][org.apache.spark.executor.Executor]Finished task 0.0 in stage 31.0 (TID 31). 665 bytes result sent to driver
[INFO][2018-05-24 21:21:40,283][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 31.0 (TID 31) in 3 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:21:40,283][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 31.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:21:40,283][org.apache.spark.scheduler.DAGScheduler]ResultStage 31 (foreachPartition at ReceiveKafkaData.scala:66) finished in 0.003 s
[INFO][2018-05-24 21:21:40,284][org.apache.spark.scheduler.DAGScheduler]Job 31 finished: foreachPartition at ReceiveKafkaData.scala:66, took 0.010110 s
[INFO][2018-05-24 21:21:40,284][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527168095000 ms.0 from job set of time 1527168095000 ms
[INFO][2018-05-24 21:21:40,284][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 5.284 s for time 1527168095000 ms (execution: 0.019 s)
[INFO][2018-05-24 21:21:40,323][org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1527168100000 ms
[INFO][2018-05-24 21:21:40,324][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 61 from persistence list
[INFO][2018-05-24 21:21:40,326][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 60 from persistence list
[INFO][2018-05-24 21:21:40,327][org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1527168100000 ms.0 from job set of time 1527168100000 ms
[INFO][2018-05-24 21:21:40,328][org.apache.spark.storage.BlockManager]Removing RDD 61
[INFO][2018-05-24 21:21:40,329][org.apache.spark.storage.BlockManager]Removing RDD 60
[INFO][2018-05-24 21:21:40,330][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 21:21:40,330][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527168085000 ms
[INFO][2018-05-24 21:21:40,333][org.apache.spark.SparkContext]Starting job: foreachPartition at ReceiveKafkaData.scala:66
[INFO][2018-05-24 21:21:40,333][org.apache.spark.scheduler.DAGScheduler]Got job 32 (foreachPartition at ReceiveKafkaData.scala:66) with 1 output partitions
[INFO][2018-05-24 21:21:40,333][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 32 (foreachPartition at ReceiveKafkaData.scala:66)
[INFO][2018-05-24 21:21:40,333][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2018-05-24 21:21:40,333][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2018-05-24 21:21:40,334][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 32 (MapPartitionsRDD[65] at map at ReceiveKafkaData.scala:64), which has no missing parents
[INFO][2018-05-24 21:21:40,335][org.apache.spark.storage.memory.MemoryStore]Block broadcast_32 stored as values in memory (estimated size 3.3 KB, free 912.2 MB)
[INFO][2018-05-24 21:21:40,339][org.apache.spark.storage.memory.MemoryStore]Block broadcast_32_piece0 stored as bytes in memory (estimated size 1868.0 B, free 912.2 MB)
[INFO][2018-05-24 21:21:40,339][org.apache.spark.storage.BlockManagerInfo]Added broadcast_32_piece0 in memory on 192.168.0.102:51153 (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:21:40,340][org.apache.spark.SparkContext]Created broadcast 32 from broadcast at DAGScheduler.scala:1006
[INFO][2018-05-24 21:21:40,340][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 32 (MapPartitionsRDD[65] at map at ReceiveKafkaData.scala:64) (first 15 tasks are for partitions Vector(0))
[INFO][2018-05-24 21:21:40,340][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 32.0 with 1 tasks
[INFO][2018-05-24 21:21:40,341][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 32.0 (TID 32, localhost, executor driver, partition 0, ANY, 4747 bytes)
[INFO][2018-05-24 21:21:40,341][org.apache.spark.executor.Executor]Running task 0.0 in stage 32.0 (TID 32)
[INFO][2018-05-24 21:21:40,345][org.apache.spark.streaming.kafka.KafkaRDD]Beginning offset 12304 is the same as ending offset skipping seven 0
[INFO][2018-05-24 21:21:40,345][com.seven.spark.streaming.ReceiveKafkaData$]put hbase is success . . .
[INFO][2018-05-24 21:21:40,346][org.apache.spark.executor.Executor]Finished task 0.0 in stage 32.0 (TID 32). 708 bytes result sent to driver
[INFO][2018-05-24 21:21:40,347][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 32.0 (TID 32) in 6 ms on localhost (executor driver) (1/1)
[INFO][2018-05-24 21:21:40,347][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 32.0, whose tasks have all completed, from pool 
[INFO][2018-05-24 21:21:40,347][org.apache.spark.scheduler.DAGScheduler]ResultStage 32 (foreachPartition at ReceiveKafkaData.scala:66) finished in 0.007 s
[INFO][2018-05-24 21:21:40,348][org.apache.spark.scheduler.DAGScheduler]Job 32 finished: foreachPartition at ReceiveKafkaData.scala:66, took 0.014493 s
[INFO][2018-05-24 21:21:40,348][org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1527168100000 ms.0 from job set of time 1527168100000 ms
[INFO][2018-05-24 21:21:40,348][org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.348 s for time 1527168100000 ms (execution: 0.022 s)
[INFO][2018-05-24 21:21:40,348][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 63 from persistence list
[INFO][2018-05-24 21:21:40,348][org.apache.spark.storage.BlockManager]Removing RDD 63
[INFO][2018-05-24 21:21:40,349][org.apache.spark.streaming.kafka.KafkaRDD]Removing RDD 62 from persistence list
[INFO][2018-05-24 21:21:40,349][org.apache.spark.storage.BlockManager]Removing RDD 62
[INFO][2018-05-24 21:21:40,349][org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO][2018-05-24 21:21:40,349][org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1527168090000 ms
[INFO][2018-05-24 21:21:42,864][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_29_piece0 on 192.168.0.102:51153 in memory (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:21:42,867][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_24_piece0 on 192.168.0.102:51153 in memory (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:21:42,868][org.apache.spark.streaming.StreamingContext]Invoking stop(stopGracefully=false) from shutdown hook
[INFO][2018-05-24 21:21:42,869][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_13_piece0 on 192.168.0.102:51153 in memory (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:21:42,870][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_26_piece0 on 192.168.0.102:51153 in memory (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:21:42,871][org.apache.spark.streaming.scheduler.ReceiverTracker]ReceiverTracker stopped
[INFO][2018-05-24 21:21:42,872][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_25_piece0 on 192.168.0.102:51153 in memory (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:21:42,872][org.apache.spark.streaming.scheduler.JobGenerator]Stopping JobGenerator immediately
[INFO][2018-05-24 21:21:42,873][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_14_piece0 on 192.168.0.102:51153 in memory (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:21:42,873][org.apache.spark.streaming.util.RecurringTimer]Stopped timer for JobGenerator after time 1527168100000
[INFO][2018-05-24 21:21:42,874][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_20_piece0 on 192.168.0.102:51153 in memory (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:21:42,875][org.apache.spark.streaming.scheduler.JobGenerator]Stopped JobGenerator
[INFO][2018-05-24 21:21:42,875][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_22_piece0 on 192.168.0.102:51153 in memory (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:21:42,877][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_19_piece0 on 192.168.0.102:51153 in memory (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:21:42,877][org.apache.spark.streaming.scheduler.JobScheduler]Stopped JobScheduler
[INFO][2018-05-24 21:21:42,878][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_12_piece0 on 192.168.0.102:51153 in memory (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:21:42,880][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_18_piece0 on 192.168.0.102:51153 in memory (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:21:42,881][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_27_piece0 on 192.168.0.102:51153 in memory (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:21:42,882][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_23_piece0 on 192.168.0.102:51153 in memory (size: 1867.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:21:42,883][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_21_piece0 on 192.168.0.102:51153 in memory (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:21:42,885][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_15_piece0 on 192.168.0.102:51153 in memory (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:21:42,886][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_11_piece0 on 192.168.0.102:51153 in memory (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:21:42,886][org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@57a48985{/streaming,null,UNAVAILABLE,@Spark}
[INFO][2018-05-24 21:21:42,887][org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@3f672204{/streaming/batch,null,UNAVAILABLE,@Spark}
[INFO][2018-05-24 21:21:42,887][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_30_piece0 on 192.168.0.102:51153 in memory (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:21:42,889][org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@2d000e80{/static/streaming,null,UNAVAILABLE,@Spark}
[INFO][2018-05-24 21:21:42,889][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_17_piece0 on 192.168.0.102:51153 in memory (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:21:42,890][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_32_piece0 on 192.168.0.102:51153 in memory (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:21:42,890][org.apache.spark.streaming.StreamingContext]StreamingContext stopped successfully
[INFO][2018-05-24 21:21:42,890][org.apache.spark.SparkContext]Invoking stop() from shutdown hook
[INFO][2018-05-24 21:21:42,891][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_28_piece0 on 192.168.0.102:51153 in memory (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:21:42,892][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_31_piece0 on 192.168.0.102:51153 in memory (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:21:42,893][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_16_piece0 on 192.168.0.102:51153 in memory (size: 1868.0 B, free: 912.3 MB)
[INFO][2018-05-24 21:21:42,898][org.spark_project.jetty.server.AbstractConnector]Stopped Spark@25177d29{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2018-05-24 21:21:42,901][org.apache.spark.ui.SparkUI]Stopped Spark web UI at http://192.168.0.102:4040
[INFO][2018-05-24 21:21:42,909][org.apache.spark.MapOutputTrackerMasterEndpoint]MapOutputTrackerMasterEndpoint stopped!
[INFO][2018-05-24 21:21:42,929][org.apache.spark.storage.memory.MemoryStore]MemoryStore cleared
[INFO][2018-05-24 21:21:42,929][org.apache.spark.storage.BlockManager]BlockManager stopped
[INFO][2018-05-24 21:21:42,930][org.apache.spark.storage.BlockManagerMaster]BlockManagerMaster stopped
[INFO][2018-05-24 21:21:42,932][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint]OutputCommitCoordinator stopped!
[INFO][2018-05-24 21:21:42,934][org.apache.spark.SparkContext]Successfully stopped SparkContext
[INFO][2018-05-24 21:21:42,934][org.apache.spark.util.ShutdownHookManager]Shutdown hook called
[INFO][2018-05-24 21:21:42,935][org.apache.spark.util.ShutdownHookManager]Deleting directory /private/var/folders/b9/cjmz7ztj5157cb6qyk2txmfw0000gn/T/spark-afc1956a-fafa-47f4-9dfb-e8e2c33a0d65
